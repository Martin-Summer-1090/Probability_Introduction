<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>lecture5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="05-lecture5_files/libs/clipboard/clipboard.min.js"></script>
<script src="05-lecture5_files/libs/quarto-html/quarto.js"></script>
<script src="05-lecture5_files/libs/quarto-html/popper.min.js"></script>
<script src="05-lecture5_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="05-lecture5_files/libs/quarto-html/anchor.min.js"></script>
<link href="05-lecture5_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="05-lecture5_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="05-lecture5_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="05-lecture5_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="05-lecture5_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="continuous-random-variables-and-monte-carlo-simulation" class="level1">
<h1>Continuous random variables and Monte Carlo Simulation</h1>
<p>In this lecture we will introduce the most important probability distribution, the <strong>normal distribution</strong>. While we have discussed discrete random variables so far, where the number of possible outcomes for <span class="math inline">\(X\)</span> is finite (or countably infinite), to discuss the normal distribution we need to deal with the case that the number of outcomes for <span class="math inline">\(X\)</span> is uncountable infinite, or a continuum. This leads us to the concept of a continuous random variable.</p>
<section id="continuous-random-variables-and-probability" class="level2">
<h2 class="anchored" data-anchor-id="continuous-random-variables-and-probability">Continuous Random Variables and Probability</h2>
<p>Here, we discuss the most important concepts for practical work with continuous random variables. For a mathematically rigorous treatment, advanced techniques such as measure theory are required. However, we will not delve into those here (see, for example, <span class="citation" data-cites="Billingsley1995">@Billingsley1995</span>). Instead, we focus on applied and practical aspects.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Continuous Random Variable
</div>
</div>
<div class="callout-body-container callout-body">
<p>A <strong>continuous random variable</strong> <span class="math inline">\(X\)</span> can take on a continuum of possible values within a given range.</p>
</div>
</div>
<p>Random variables that can take on a continuum of values, rather than discrete values like the fair coin we discussed earlier, play a crucial role in practical applications. For instance, consider asset prices or returns. A stock’s price, in principle, could take any value in <span class="math inline">\([0, \infty)\)</span>. Or could it?</p>
<p>This is a modeling assumption that can be debated in terms of realism. After all, stock prices are quoted in currency, which has a smallest unit (e.g., cents or pennies). In Lecture 1, we discussed the assumption of unbounded stock prices in the context of sample spaces. Similarly, for other practical cases—such as task completion times, lengths, or weights—a continuum of outcomes often provides a natural model.</p>
<p>You might argue that these examples are not <em>truly</em> continuous. For instance, time is measured in hours, minutes, or seconds. However, we can refine our measurements to a much finer scale, with the limit imposed only by our measuring instruments. Time itself <em>is</em> continuous—it does not jump.</p>
<p>In contrast, stock prices do have a smallest monetary unit (e.g., cents in the Eurozone). Yet, for practical modeling, treating prices as continuous simplifies computations and analysis.</p>
<p>Even without delving into the mathematical machinery of measure theory, it is crucial to grasp the implications of continuous random variables. Let’s explore an example of a continuous random variable that can take any value in the interval <span class="math inline">\([0,1]\)</span>. In R, we can generate such numbers easily using the <code>runif()</code> function. Consider the following example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">runif</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055
 [8] 0.8924190 0.5514350 0.4566147</code></pre>
</div>
</div>
<p>Now, consider the probability of this random variable taking on a specific value, say <span class="math inline">\(0.4566147\)</span>, one of the values in our list. To investigate, we simulate one million uniformly distributed random numbers in <span class="math inline">\([0,1]\)</span> and calculate the relative frequency of <span class="math inline">\(0.4566147\)</span> occurring:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>uniform_rv <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">10</span><span class="sc">^</span><span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(uniform_rv <span class="sc">==</span> <span class="fl">0.4566147</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
</div>
<p>The result is zero—literally zero. Even with one million draws, the random number generator in R produced unique values each time. The probability of hitting any specific value is zero.</p>
<p>This occurs because the interval <span class="math inline">\([0,1]\)</span> contains an infinite number of points. For any number in this interval, there are infinitely many numbers both larger and smaller. Assigning a positive probability to any single point would result in probabilities summing to a value greater than 1, violating the laws of probability.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>For every continuous random variable <span class="math inline">\(X\)</span>, we have <span class="math inline">\(P(X = x) = 0\)</span> for all <span class="math inline">\(x\)</span>.</p>
</div>
</div>
<p>This is a fundamental shift from discrete random variables: for continuous random variables, we cannot assign positive probabilities to individual points. Instead, probabilities are associated with <strong>intervals of real numbers</strong>.</p>
<p>For example, consider the probability that a uniformly distributed random variable <span class="math inline">\(X \sim U[0,1]\)</span> takes a value between <span class="math inline">\(0\)</span> and <span class="math inline">\(1/4\)</span>. Using the simulated numbers, we calculate:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="dv">0</span> <span class="sc">&lt;=</span> uniform_rv <span class="sc">&amp;</span> uniform_rv <span class="sc">&lt;=</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.250841</code></pre>
</div>
</div>
<p>The result is 25%, as expected. Using R’s cumulative distribution function (CDF), we confirm:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">punif</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.25</code></pre>
</div>
</div>
<p>For continuous random variables, probabilities are represented as <strong>areas under a curve</strong>. This is the major distinction from discrete random variables, where probabilities are assigned to individual points.</p>
<p>Consider this density function example:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/uniform_dist.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption>With continuous random variables, probabilities are areas under the density function</figcaption>
</figure>
</div>
</div>
</div>
<p>Mathematically, these areas are calculated using integrals. The <strong>probability density function</strong> (PDF) describes the distribution as follows:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Probability Density Function of a Continuous Random Variable
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a continuous random variable <span class="math inline">\(X\)</span> with density function <span class="math inline">\(f(x)\)</span>:</p>
<ol type="1">
<li><span class="math inline">\(f(x) \geq 0\)</span>, for all <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(\int_{-\infty}^{\infty} f(x) \, dx = 1\)</span>.</li>
<li>For any <span class="math inline">\(a &lt; b\)</span>, the probability that <span class="math inline">\(X\)</span> falls within <span class="math inline">\((a, b)\)</span> is given by the integral:<br>
<span class="math inline">\(P(a &lt; X &lt; b) = \int_a^b f(x) \, dx\)</span>.</li>
</ol>
</div>
</div>
<p>The <strong>cumulative distribution function</strong> (CDF) provides another essential tool for continuous random variables:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cumulative Distribution Function of a Continuous Random Variable
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>cumulative distribution function</strong> (CDF) shows the probability that <span class="math inline">\(X\)</span> takes a value less than or equal to <span class="math inline">\(x\)</span>:<br>
<span class="math inline">\(F(x) = P(X \leq x)\)</span>.<br>
For any <span class="math inline">\(a &lt; b\)</span>:<br>
<span class="math inline">\(P(a &lt; X &lt; b) = F(b) - F(a) = \int_a^b f(x) \, dx\)</span>.</p>
</div>
</div>
<p>This transition from discrete points to areas under a curve marks a crucial conceptual shift in working with continuous random variables.</p>
</section>
<section id="normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="normal-distribution">Normal Distribution</h2>
<p>The <strong>normal distribution</strong> is perhaps the most iconic and fundamental probability distribution in all of probability theory. Its bell-shaped curve has become synonymous with ideas of natural variability and randomness. From the heights of people to measurement errors, and from stock returns to the central limit theorem, the normal distribution underpins countless phenomena in the natural and social sciences.</p>
<p>What makes the normal distribution truly remarkable is its simplicity and universality. With just two parameters—the mean (<span class="math inline">\(\mu\)</span>) and variance (<span class="math inline">\(\sigma^2\)</span>)—it captures the essence of variability in a way that is mathematically elegant and empirically ubiquitous. This distribution lies at the heart of probability theory, serving as the cornerstone for much stochastic modeling. For the application of probability to the modelling of data, the field of statistics, the normal distribution is foundational.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Normal Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>normal distribution</strong> is a continuous probability distribution that is centered around the mean, bell-shaped, symmetric, and completely determined by two parameters: the mean <span class="math inline">\(\mu\)</span> and the variance <span class="math inline">\(\sigma^2\)</span>. The notation is <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>. Its probability density function is given by: <span class="math display">\[
f(x, \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)
\]</span></p>
</div>
</div>
<p>Often referred to as the <strong>Gaussian distribution</strong>, after the German mathematician Karl Friedrich Gauss (1777–1855), it is also known as the <strong>Gauss-Laplace distribution</strong>, honoring Pierre-Simon Laplace (1749–1827). The shape of its probability density function has earned it the nickname <strong>bell curve</strong>, an image deeply ingrained in the language of science, education, and beyond.</p>
<p>Let’s use R to make this image tangible.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the sequence of x values</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the probability density function of the standard normal distribution</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>, </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"The Bell Curve: Standard Normal Distribution"</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Value"</span>, <span class="at">ylab =</span> <span class="st">"Density"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="05-lecture5_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The code above demonstrates how to create a simple visualization of the bell curve for the standard normal distribution using base R. Here’s a breakdown of the key steps:</p>
<p>We first define a range of values: The <code>seq()</code> function generates a sequence of values for the x-axis. In this case, it creates 1,000 equally spaced values between -4 and 4, which is sufficient to capture the central shape of the bell curve.</p>
<p>In a second step we compute the density.The <code>dnorm()</code> function computes the probability density of the standard normal distribution at each value in the sequence. If the mean and standard deviation are not explicitly specified in <code>dnorm()</code>, the function assumes the standard normal distribution by default, with a mean of 0 and a standard deviation of 1This function is part of a family of functions for working with random variables in R: - <code>dnorm(x)</code> computes the density at <code>x</code>. - <code>pnorm(x)</code> computes the cumulative distribution function at <code>x</code>. - <code>qnorm(p)</code> gives the quantile for a given probability <code>p</code>. - <code>rnorm(n)</code> generates <code>n</code> random samples from the normal distribution.</p>
<p>We need not discuss the basic syntax of the plotting function once more at this stage, because by now we have often done so.</p>
<section id="standardization-and-the-standard-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="standardization-and-the-standard-normal-distribution">Standardization and the Standard Normal Distribution</h3>
<p>One of the most powerful properties of the normal distribution is the ability to standardize it. By transforming any normally distributed random variable into a standard form, we unlock the ability to compare and analyze data across scales and contexts.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Standard Normal Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>standard normal distribution</strong> is a normal distribution with a mean <span class="math inline">\(\mu = 0\)</span> and variance <span class="math inline">\(\sigma^2 = 1\)</span>. Any normally distributed random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> can be rewritten as a standard normal random variable <span class="math inline">\(Z\)</span> using the transformation: <span class="math display">\[
Z = \frac{X - \mu}{\sigma}
\]</span> By definition, <span class="math inline">\(Z \sim N(0,1)\)</span>.</p>
</div>
</div>
<p>Here’s a draft for discussing the probability mass within 1, 2, and 3 standard deviations of the mean for the normal distribution, along with its significance:</p>
</section>
<section id="the-68-95-99.7-rule-probability-mass-in-the-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="the-68-95-99.7-rule-probability-mass-in-the-normal-distribution">The 68-95-99.7 Rule: Probability Mass in the Normal Distribution</h3>
<p>One of the most useful properties of the normal distribution is that it has a predictable concentration of probability mass around the mean. This is often summarized by the <strong>68-95-99.7 rule</strong>, which states:</p>
<ul>
<li>Approximately <strong>68%</strong> of the data falls within <strong>1 standard deviation</strong> of the mean.</li>
<li>Approximately <strong>95%</strong> of the data falls within <strong>2 standard deviations</strong> of the mean.</li>
<li>Approximately <strong>99.7%</strong> of the data falls within <strong>3 standard deviations</strong> of the mean.</li>
</ul>
<p>This property is not only fundamental to understanding the normal distribution but also provides a quick and intuitive way to interpret variability in data, regardless of the units of measurement.</p>
<p>Why Is This Important?</p>
<ol type="1">
<li><p><strong>Universal Applicability:</strong><br>
The percentages remain the same no matter the scale or units of the data. For instance, whether we measure test scores, heights, or stock returns, this property holds for all normally distributed data.</p></li>
<li><p><strong>Quick Validation of Normality:</strong> The 68-95-99.7 rule provides a straightforward diagnostic tool for assessing whether a dataset is approximately normal. If the proportions of data falling within 1, 2, and 3 standard deviations deviate significantly from these benchmarks, it is a strong indicator that the data may not be normally distributed.</p></li>
<li><p><strong>Practical Insight:</strong><br>
It allows us to quickly assess how “unusual” a value is. For example:</p>
<ul>
<li>A value more than 2 standard deviations from the mean is relatively rare (occurring in only 5% of cases).</li>
<li>A value more than 3 standard deviations from the mean is exceptionally rare (occurring in only 0.3% of cases).</li>
</ul></li>
<li><p><strong>Decision-Making:</strong><br>
This rule aids in many practical applications, such as quality control, where v alues falling outside of 3 standard deviations might indicate defects or anomalies. Similarly, in finance, it helps in risk assessment by estimating the likelihood of extreme losses or gains.</p></li>
</ol>
<p>To illustrate this property, we can plot the standard normal distribution and shade the areas corresponding to 1, 2, and 3 standard deviations from the mean.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the x-axis range and density</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"The 68-95-99.7 Rule"</span>,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Standard Deviations from the Mean"</span>, <span class="at">ylab =</span> <span class="st">"Density"</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Add shaded areas with distinct colors</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Shade 3 SD region first (light pink)</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>), <span class="dv">3</span>), </span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">dnorm</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)), <span class="dv">0</span>),</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="st">"#FBB4AE"</span>, <span class="at">border =</span> <span class="cn">NA</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Shade 2 SD region (light green)</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="at">length.out =</span> <span class="dv">100</span>), <span class="dv">2</span>), </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">dnorm</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="at">length.out =</span> <span class="dv">100</span>)), <span class="dv">0</span>),</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="st">"#CCEBC5"</span>, <span class="at">border =</span> <span class="cn">NA</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Shade 1 SD region (light blue)</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">100</span>), <span class="dv">1</span>), </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">dnorm</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">100</span>)), <span class="dv">0</span>),</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="st">"#B3CDE3"</span>, <span class="at">border =</span> <span class="cn">NA</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Redraw the outline of the curve on top for clarity</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, y, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Add legend positioned middle-left at y = 0.2</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="fl">0.2</span>, </span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"68% (1 SD)"</span>, <span class="st">"95% (2 SDs, includes 1 SD)"</span>, <span class="st">"99.7% (3 SDs, includes 1 &amp; 2 SDs)"</span>),</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill =</span> <span class="fu">c</span>(<span class="st">"#B3CDE3"</span>, <span class="st">"#CCEBC5"</span>, <span class="st">"#FBB4AE"</span>), </span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>       <span class="at">border =</span> <span class="cn">NA</span>, <span class="at">box.lty =</span> <span class="dv">0</span>, <span class="at">bg =</span> <span class="st">"white"</span>, <span class="at">x.intersp =</span> <span class="fl">0.5</span>, <span class="at">y.intersp =</span> <span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="05-lecture5_files/figure-html/visualize-probability-mass-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>To confirm these proportions, we compute the probabilities using R’s cumulative distribution function (<code>pnorm</code>):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Probabilities for 1, 2, and 3 standard deviations</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(<span class="dv">1</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">1</span>)  <span class="co"># ~68%</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(<span class="dv">2</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">2</span>)  <span class="co"># ~95%</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(<span class="dv">3</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">3</span>)  <span class="co"># ~99.7%</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Probability within 1 SD: "</span>, p1, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Probability within 1 SD:  0.6826895 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Probability within 2 SDs: "</span>, p2, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Probability within 2 SDs:  0.9544997 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Probability within 3 SDs: "</span>, p3, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Probability within 3 SDs:  0.9973002 </code></pre>
</div>
</div>
<p>These results reinforce the importance of the 68-95-99.7 rule as a tool for interpreting data variability and making informed decisions.</p>
</section>
</section>
<section id="lognormal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="lognormal-distribution">Lognormal Distribution</h2>
<p>While the normal distribution models many natural and financial phenomena, it is not always suitable for modeling certain quantities—such as stock prices—that are constrained to be positive. This is where the <strong>lognormal distribution</strong> becomes essential. It serves as a natural model for variables that are strictly positive and exhibit multiplicative growth, such as asset prices in financial markets.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Lognormal Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>A <strong>lognormal random variable</strong> <span class="math inline">\(Y\)</span> is one whose natural logarithm is normally distributed. If <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then <span class="math inline">\(Y = \exp(X)\)</span> follows a lognormal distribution. The probability density function of <span class="math inline">\(Y\)</span> is given by: <span class="math display">\[
f(y, \mu, \sigma) = \frac{1}{y \sigma \sqrt{2 \pi}} \exp\left(-\frac{(\ln y - \mu)^2}{2 \sigma^2}\right), \quad y &gt; 0.
\]</span></p>
</div>
</div>
<section id="why-the-lognormal-distribution-for-stock-prices" class="level3">
<h3 class="anchored" data-anchor-id="why-the-lognormal-distribution-for-stock-prices">Why the Lognormal Distribution for Stock Prices?</h3>
<p>Stock prices, by their nature, cannot fall below zero and often grow in a multiplicative manner over time. If the logarithm of a stock price follows a normal distribution, then the stock price itself is lognormally distributed. This aligns with the widely used geometric Brownian motion model This aligns with the widely used geometric Brownian motion model for stock price dynamics.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>In finance, returns are a natural way to measure changes in stock prices over time. Remember that for a discrete time interval, the return is defined as: <span class="math display">\[
R = \frac{S_t - S_0}{S_0},
\]</span> where <span class="math inline">\(S_0\)</span> is the initial price and <span class="math inline">\(S_t\)</span> is the price at time <span class="math inline">\(t\)</span>. However, this formulation has limitations for very short time intervals or when returns are compounded over time.</p>
<p>Instead, <strong>logarithmic returns</strong> (or continuously compounded returns) are defined as: <span class="math display">\[
r = \ln\left(\frac{S_t}{S_0}\right).
\]</span> This definition arises naturally because it allows for: 1. <strong>Additivity in Continuous Time:</strong><br>
Over small time intervals, the log of cumulative returns adds up, making it easy to model and sum returns over time. For instance, if a stock moves from <span class="math inline">\(S_0\)</span> to <span class="math inline">\(S_t\)</span> and then to <span class="math inline">\(S_T\)</span>, the total log return is: <span class="math display">\[
   r = \ln\left(\frac{S_t}{S_0}\right) + \ln\left(\frac{S_T}{S_t}\right) = \ln\left(\frac{S_T}{S_0}\right).
   \]</span> This property simplifies modeling in continuous time frameworks.</p>
<ol start="2" type="1">
<li><p><strong>Consistency with Compounding:</strong><br>
Financial returns often compound multiplicatively (e.g., reinvested dividends or reinvested profits). Logarithmic returns handle compounding naturally and ensure that the total return across intervals corresponds to the product of growth factors.</p></li>
<li><p><strong>Symmetry in Statistical Analysis:</strong><br>
While absolute returns can grow unboundedly in a positive direction, logarithmic returns are symmetric around the mean, simplifying statistical analysis and aligning better with the assumptions of models like geometric Brownian motion.</p></li>
</ol>
<p>When stock prices follow geometric Brownian motion, their logarithmic returns <span class="math inline">\(r\)</span> are normally distributed: <span class="math display">\[
r \sim N(\mu, \sigma^2),
\]</span> where <span class="math inline">\(\mu\)</span> is the mean log return and <span class="math inline">\(\sigma^2\)</span> is the variance. As a result, the stock price itself, given by <span class="math inline">\(S_t = S_0 \exp(r)\)</span>, follows a lognormal distribution. The lognormal distribution is particularly suitable for stock prices because:</p>
<ul>
<li><strong>Positive Skewness:</strong> It allows for rare but extreme positive returns, reflecting real-world market behavior.</li>
<li><strong>Non-Negativity:</strong> Stock prices cannot fall below zero.</li>
<li><strong>Compounding Effects:</strong> It captures the multiplicative nature of price movements over time.</li>
</ul>
</section>
<section id="comparing-real-stock-data-with-the-lognormal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="comparing-real-stock-data-with-the-lognormal-distribution">Comparing Real Stock Data with the Lognormal Distribution</h3>
<p>To connect the theoretical discussion with real-world data, we’ll analyze historical stock prices from the S&amp;P 500 index. Using data from Yahoo Finance, we compute daily log returns, overlay the empirical distribution with a fitted lognormal distribution, and discuss the fit’s implications.</p>
<p>Let’s use our R-tools fro overlaying empirical data with the theoretical model of the random variable.</p>
<p>We first load tidyquant:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyquant)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then we fit the model to actual stock market data for the SP500.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="05-lecture5_files/figure-html/lognormal-fit-sp500-tidyquant-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The R-code behind this visualization is somewhat involved and I will not go through it here because I want to focus on a more important point of potential confusion, which needs to be explained carefully. Those of you who want to look at the code just unfold the code chunk.</p>
<p>So what are we doing here? We started by assuming that <strong>stock prices follow a lognormal distribution</strong><br>
- Prices are <strong>strictly positive</strong>, which makes the <strong>lognormal</strong> distribution a natural choice. - If <span class="math inline">\(S_t\)</span> is a stock price and follows geometric Brownian motion:</p>
<p><span class="math display">\[
S_t = S_0 \exp(X_t)
\]</span> where <span class="math inline">\(X_t\)</span> is normally distributed.</p>
<p>By our assumption that stock prices are modeled by a lognormal distribution, <strong>log-returns follow a normal distribution</strong></p>
<ul>
<li>Returns are often modeled as additive over time.</li>
<li><strong>Log-returns</strong> are defined as: <span class="math display">\[
r_t = \ln\left(\frac{S_t}{S_{t-1}}\right)
\]</span></li>
<li>If stock prices follow a <strong>lognormal</strong> distribution, then <strong>log-returns</strong> must follow a <strong>normal</strong> distribution.</li>
</ul>
<p>We work with <strong>log-returns</strong> because they simplify calculations, but prices are what we observe. It would be a mistake to fitting a lognormal distribution to log-returns (instead of prices). The correct modeling framework depends on whether we are working with <strong>prices or returns</strong>.</p>
<p>Let me summarize these remarks in a side by side comparison table.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 44%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Distribution</th>
<th>Why?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Stock Prices <span class="math inline">\(S_t\)</span></td>
<td>Lognormal</td>
<td>Prices can’t be negative, and returns compound multiplicatively.</td>
</tr>
<tr class="even">
<td>Log-Returns <span class="math inline">\(r_t\)</span></td>
<td>Normal</td>
<td>Returns add over time and often appear symmetric.</td>
</tr>
</tbody>
</table>
<p>Now let us go back to the discussion of what we see in the graph:</p>
<ol type="1">
<li><p><strong>Fat Tails:</strong><br>
The empirical distribution may display so called <strong>fat tails</strong>, meaning extreme returns are more frequent in real data than predicted by the lognormal model. These events are crucial for risk assessment and portfolio stress testing.</p>
<p>To see more clearly whether we have fat tails in our daily log return data, let us visually zoom in to the region of more extreme negative returns.</p></li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="05-lecture5_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>To put this graph into perspective remember the <strong>68-95-99.7 rule</strong>. It tells us that in a normal distribution:</p>
<ul>
<li>About <strong>68%</strong> of observations fall within <strong>1 standard deviation</strong> of the mean.</li>
<li>About <strong>95%</strong> fall within <strong>2 standard deviations</strong>.</li>
<li>About <strong>99.7%</strong> fall within <strong>3 standard deviations</strong>.</li>
</ul>
<p>If <strong>daily log-returns</strong> followed a normal distribution, we would expect <strong>only 0.15% of observations</strong> to be more extreme than <strong>-3σ</strong>. However, in our empirical data, <strong>0.99% of returns</strong> fall below this threshold—<strong>more than six times the expected frequency</strong>.</p>
<p>This gives us a <strong>critical insight for risk management</strong>:</p>
<ul>
<li><p><strong>Normal models underestimate extreme downside risk</strong>—leading to potential miscalculations in risk measures like <strong>Value-at-Risk (VaR)</strong>, a concept we will discuss later in more detail.</p></li>
<li><p>Market downturns often exhibit far worse losses than a normal distribution would predict.</p></li>
<li><p>Alternative distributions, such as the <strong>t-distribution</strong> or <strong>generalized extreme value (GEV) models</strong>, may be better suited to capturing these extreme tail risks.</p></li>
</ul>
<ol start="2" type="1">
<li><strong>Asymmetry (Skewness):</strong><br>
Real stock returns often show <strong>negative skewness</strong>, where extreme negative returns (e.g., during crashes) are more pronounced than extreme positive returns.</li>
</ol>
<p>This is another critical property of financial returns which is <strong>asymmetry</strong>, or <strong>skewness</strong>.</p>
<p>What Does Skewness Mean? A normal distribution is <strong>symmetric</strong>, meaning that extreme positive and negative values are equally likely. However, real-world stock returns often show negative skewness, meaning that large negative returns occur more frequently than large positive returns. This asymmetry is particularly visible during market crashes, when prices tend to decline much faster than they rise during bull markets.</p>
<p>In our <strong>fat-tail visualization</strong>, we focused on the <strong>left tail</strong> of the distribution (extreme losses). If stock returns were truly <strong>symmetric</strong>, we would expect to see a similar excess probability mass on the <strong>right tail</strong> (large gains). However: The left tail extends much further and is more pronounced than the right. Large losses tend to be larger in magnitude than large gains.</p>
<p>This is why risk management focuses more on downside risk —investors care more about avoiding catastrophic losses than capturing rare, extreme gains.</p>
<p>To quantify this asymmetry, we can compute the <strong>skewness statistic</strong> of our dataset. A normal distribution has a skewness of 0, while: Negative skewness (&lt; 0) indicates that the left tail is heavier than the right. Positive skewness (&gt; 0) indicates the opposite. In our data the skewness is</p>
<p>-1.05. For reference: A normal distribution has a skewness of 0 (perfect symmetry). A skewness of -1.05 indicates a strongly asymmetric distribution with heavier left tails.</p>
</section>
<section id="applications-and-limitations-a-balanced-perspective" class="level3">
<h3 class="anchored" data-anchor-id="applications-and-limitations-a-balanced-perspective">Applications and Limitations: A Balanced Perspective</h3>
<p>Looking at our empirical data, we see that the normal approximation to log-returns fits quite well in the center of the distribution. You can check for yourself that as you go to considering weekly or monthly returns instead of daily ones this fit in the center becomes actually quite good. The lognormal model remains a widely used and valuable framework for understanding stock price dynamics.</p>
<p>However, while the center of the distribution aligns well with theory, the tails remain problematic. As we saw in the fat-tail visualization, extreme negative returns occur far more often than a normal model would suggest. This is a critical issue in risk management, where tail events—such as financial crises or sudden market drops—can have disproportionate consequences.</p>
<p>That said, in many other applications, where the focus is on general trends, valuation models, or portfolio optimization, a good fit in the center may be sufficient. The profession does not work with a “wrong” model—rather, different models are used depending on the question being asked. For example:</p>
<ul>
<li>In option pricing (Black-Scholes), the lognormal assumption is a reasonable starting point, though corrections (e.g., stochastic volatility models) are often needed.</li>
<li>In long-term investment strategies, where extreme short-term fluctuations average out, the normal/lognormal framework remains quite effective.</li>
</ul>
<p>Thus, while tail risks must be explicitly accounted for in risk management, the lognormal assumption remains a useful and practical tool in many areas of finance.</p>
</section>
</section>
<section id="inverse-normal-and-quantiles-in-risk-management" class="level2">
<h2 class="anchored" data-anchor-id="inverse-normal-and-quantiles-in-risk-management">Inverse Normal and Quantiles in Risk Management</h2>
<p>In risk management, a fundamental question is:</p>
<p>What is the worst-case loss I should expect, given a certain probability threshold?</p>
<p>This is different from what we studied earlier. Previously, we were given a threshold and asked for the probability of falling below it. Now, we flip the question:</p>
<ul>
<li>We are given a probability (e.g., 1%)</li>
<li>We want to find the threshold such that losses exceed it only with that probability.</li>
</ul>
<p>This is known as the inverse problem in probability, and it plays a central role in Value at Risk (VaR) calculations.</p>
<p>Suppose you manage a portfolio with uncertain (random) returns. A key risk management question is:</p>
<p>How large can losses be over a given time horizon, with a probability of only 1% (or another predefined risk threshold)?</p>
<p>For example, a bank may want to ensure that the probability of losing more than a certain percentage of its capital remains below 1%. In this case, the 1% quantile of portfolio returns (often called the 1% Value at Risk, or VaR) is the key statistic.</p>
<section id="the-inverse-normal-function-in-r" class="level3">
<h3 class="anchored" data-anchor-id="the-inverse-normal-function-in-r">The Inverse Normal Function in R</h3>
<p>The quantile function (or inverse cumulative distribution function) helps solve this problem. For a normally distributed random variable <span class="math inline">\(X\)</span>, we want to find the threshold <span class="math inline">\(x\)</span> such that: <span class="math inline">\(P(X≤x)=p\)</span> where <span class="math inline">\(p\)</span> is a given probability.</p>
<p>In R, we compute this using the <code>qnorm()</code> function. Let’s demonstrate how it works using our data from before:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.01</span>, <span class="at">mean =</span> <span class="fu">mean</span>(log_returns), <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fu">var</span>(log_returns)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.02709901</code></pre>
</div>
</div>
<p>This function finds the 1% quantile of a normal distribution with a given mean and a given standard deviation derived from the logarithmic returns of the SP500. It answers the question:</p>
<p>What is the worst-case daily return we should expect, such that losses exceed this level only 1% of the time?</p>
<p>This is often referred to as the inverse normal problem, since it inverts the cumulative distribution function (CDF). Definition: Quantile</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
p-quantile
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <span class="math inline">\(p\)</span>-th quantile (or percentile) of a probability distribution is the value <span class="math inline">\(x\)</span> such that: <span class="math display">\[
P(X≤x)=p
\]</span> If <span class="math inline">\(X\)</span> is normally distributed, i.e., <span class="math inline">\(X\sim N(\mu,\sigma^2)\)</span>, then: <span class="math display">\[
x=F^{−1}(p),
\]</span></p>
<p>where <span class="math inline">\(F^{−1}\)</span> is the inverse CDF (quantile function) of the normal distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
</div>
</div>
<p>The <strong>median</strong> is the 50% quantile (<span class="math inline">\(P(X≤x)=0.5\)</span>), dividing the distribution in half. The <strong>1%-quantile</strong> (or 99% left-tail quantile) gives us a worst-case threshold, which is critical for risk management models, like value at risk. You can consider any other percentile you might be interested in in this way.</p>
<p>The inverse CDF is essential in many areas of finance, including risk management, stress testing, and capital adequacy planning.</p>
<p>Now that we understand quantiles and the inverse normal, we can directly apply this concept to Value at Risk (VaR).</p>
</section>
</section>
<section id="value-at-risk" class="level2">
<h2 class="anchored" data-anchor-id="value-at-risk">Value at risk</h2>
<p>Designing portfolios in a way that enable an acceptable trade off between risk of loss and the potential for profit is a key consideration in portfolio management. Quantitative risk measures are one way to achieve goals like this.</p>
<p>Quantitative measures of risk a broad topic that we can not fully cover here. But one particular popular risk measure <strong>value at risk</strong> is directly based on the concept of quantiles of a normally distributed random variable and thus is the perfect application case for appreciating the significance of quantiles as an analytical tool in finance.</p>
<p>Let us imagine a financial position modeled by a continuous random variable <span class="math inline">\(X\)</span> denoting the change in value of a position at a given future time <span class="math inline">\(T\)</span>. In general the variable may take on either positive or negative values depending on its realization. We refer to the random variable <span class="math inline">\(X\)</span> for convenience as <strong>position</strong>. From the risk perspective, we may focus on the associated loss, which is <span class="math inline">\(-X\)</span>.</p>
<p>The concept of <strong>value at risk</strong> (abbreviated VaR) is motivated by the concern about loss. We start by specifying a <strong>loss tolerance</strong> <span class="math inline">\(h\)</span> between 0 and 1 and a companion <strong>confidence level</strong> equal to <span class="math inline">\(1-h\)</span>. For example, we could choose a loss tolerance <span class="math inline">\(h=0.05\)</span> and a corresponding confidence level of <span class="math inline">\(1-h = 0.95\)</span></p>
<p>For a particular position <span class="math inline">\(X\)</span> and a given loss tolerance <span class="math inline">\(h\)</span>, VaR is then the smallest number <span class="math inline">\(V\)</span> such that the probability of a loss greater than <span class="math inline">\(V\)</span> is no more than <span class="math inline">\(h\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition: Value at risk
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a given position <span class="math inline">\(X\)</span> and a given loss toleracne <span class="math inline">\(h\)</span>, VaR is the smallest number <span class="math inline">\(V\)</span> such that the probability of a loss greater tan <span class="math inline">\(V\)</span> is no more than <span class="math inline">\(h\)</span>:</p>
<p><span class="math display">\[
VaR_h(X) = \min_{h} \{ V: P\left[ - X &gt; V \right] \leq h \}
\]</span> Equivalently, VaR is the smallest number <span class="math inline">\(V\)</span> such that the probability of the loss beeing no more than <span class="math inline">\(V\)</span> is gretaer than <span class="math inline">\(1-h\)</span> or:</p>
<p><span class="math display">\[
VaR_h(X) = \min_{h} \{ V: P\left[ - X \leq V \right] &gt; 1 - h \}
\]</span></p>
</div>
</div>
<p>Here is a visualization:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="05-lecture5_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>95% Value at Risk (VaR) Visualization</figcaption>
</figure>
</div>
</div>
</div>
<p>The graph illustrates the 95% Value at Risk (VaR) concept using a normal distribution of daily portfolio log returns. The blue curve represents the probability density function of log-returns. The red-shaded area on the left highlights the 5% tail probability, indicating extreme negative returns that occur with only a 5% likelihood. The green-shaded area represents the complementary 95% probability mass, where returns are expected to fall under normal conditions.</p>
<p>Two vertical dashed lines mark key reference points:</p>
<ul>
<li>The black dashed line represents the mean return (expected value).</li>
<li>The red dashed line represents the VaR threshold, the level of loss that is only exceeded 5% of the time.</li>
</ul>
<p>This visualization helps quantify downside risk: A risk manager using VaR at 95% confidence would focus on the red-shaded region to assess the worst-case loss threshold. However, as we have seen with empirical stock return data, real-world distributions often exhibit fat tails, meaning extreme losses occur more frequently than the normal model predicts. This suggests that while VaR is a useful benchmark, adjustments may be needed for more accurate risk assessments.</p>
<p>The value at risk as defined here and in the literture comes with an implicit definition of a given time horizon <span class="math inline">\(T\)</span> at which <span class="math inline">\(X\)</span> is realized. If the position is liquid this horizon may be one or a few days. Often there are also regulatry requirements setting the rules how this horizon can or must be chosen.</p>
<p>Now you can see how the concepts of the inverse normal can be directly brough to bear in the case of normally distributed log returns of stock prices.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition: VaR for the normal distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose <span class="math inline">\(X\)</span> follows a normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. Then</p>
<p><span class="math display">\[
VaR_h(X) = - \sigma \, F^{-1}_N(h) - \mu
\]</span> where <span class="math inline">\(F_N\)</span> is the cumulative probability distribution function of the standardized normal variable (with mean 0 and standard deviation 1).</p>
</div>
</div>
<p>Here’s a structured <strong>draft</strong> covering the <strong>three Value at Risk (VaR) examples</strong> along with <strong>R code</strong> to illustrate each case.</p>
<p>Let’s consider three examples:</p>
<p><strong>Example 1: Highly Liquid Portfolio with Small Mean Return</strong></p>
<p>A <strong>highly liquid portfolio</strong> consists of assets that can be easily bought or sold with minimal impact on price. Examples include:</p>
<ul>
<li>Short-term U.S. Treasury bills</li>
<li>Large-cap ETFs (e.g., SPY, QQQ)</li>
<li>Highly traded currency pairs (EUR/USD, USD/JPY)</li>
</ul>
<p>For such portfolios: Expected returns are very small over short time horizons. VaR is then mainly driven by portfolio variance (volatility) rather than the mean return. In this case the VaR can be approximated as: <span class="math display">\[
  VaR_{95\%} \approx 1.65 \times \sigma
\]</span> since the mean return is negligible over short periods and <span class="math inline">\(-F^{-1}_N(0.05) = 1.65\)</span>. Check using the quantile function of the normal distribution.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.05</span>)<span class="sc">*</span>(<span class="sc">-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.644854</code></pre>
</div>
</div>
<p>Here is a numerical R-example for typical values of such a portfolio</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define parameters</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>sigma_liquid <span class="ot">&lt;-</span> <span class="fl">0.015</span>  <span class="co"># 1.5% daily volatility (assumption)</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>mu_liquid <span class="ot">&lt;-</span> <span class="dv">0</span>         <span class="co"># Negligible mean return</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span>          <span class="co"># 95% confidence level</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute 1-day VaR</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>VaR_liquid <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(alpha, <span class="at">mean =</span> mu_liquid, <span class="at">sd =</span> sigma_liquid)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Output result</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="fu">sprintf</span>(<span class="st">"1-day 95%% VaR for a highly liquid portfolio: %.4f (or %.2f%%)"</span>, VaR_liquid, VaR_liquid <span class="sc">*</span> <span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "1-day 95% VaR for a highly liquid portfolio: -0.0247 (or -2.47%)"</code></pre>
</div>
</div>
<p>For a <strong>highly liquid</strong> asset with daily volatility of <strong>1.5%</strong>, the <strong>1-day 95% VaR</strong> is approximately <strong>-2.47%</strong>, meaning that on 5% of days, the portfolio could lose at least 2.47%** under normal conditions.</p>
<p><strong>Example 2: A pension fund</strong>:</p>
<p>Let’s consider next the example of a <strong>10-Day VaR for a Pension Fund</strong>. A pension fund typically invests in a <strong>diversified mix of stocks, bonds, and alternative assets</strong>. Suppose a fund manager wants to compute <strong>10-day VaR</strong> for a <strong>$500 million endowment</strong>.</p>
<p>To scale VaR from <strong>1-day to N-days</strong>, we assume returns follow a <strong>normal distribution</strong> and use the square-root rule:</p>
<p><span class="math display">\[
VaR_{N-\text{day}} = VaR_{1-\text{day}} \times \sqrt{N}
\]</span> Here is a numerical R-example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define parameters</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>sigma_fund <span class="ot">&lt;-</span> <span class="fl">0.02</span>    <span class="co"># 2% daily volatility</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>mu_fund <span class="ot">&lt;-</span> <span class="fl">0.0002</span>     <span class="co"># 0.02% daily return (assumed)</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">10</span>               <span class="co"># 10-day horizon</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>portfolio_value <span class="ot">&lt;-</span> <span class="dv">500</span> <span class="co"># $500 million</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute 1-day VaR</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>VaR_fund_1d <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(alpha, <span class="at">mean =</span> mu_fund, <span class="at">sd =</span> sigma_fund)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute 10-day VaR using square-root scaling</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>VaR_fund_10d <span class="ot">&lt;-</span> VaR_fund_1d <span class="sc">*</span> <span class="fu">sqrt</span>(N) <span class="sc">*</span> portfolio_value</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Output result</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="fu">sprintf</span>(<span class="st">"10-day 95%% VaR for a $500M pension fund: $%.2f million"</span>, VaR_fund_10d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "10-day 95% VaR for a $500M pension fund: $-51.70 million"</code></pre>
</div>
</div>
<p>For a pension fund with 2% daily volatility, a $ 500 million portfolio, and a 10-day horizon, the 10-day 95% VaR is around $ X million. This means the fund can expect to lose at least this amount over a 10-day period with 5% probability.</p>
<p><strong>Example 3: Portfolio diversification</strong>: Finally, let’s look at the example of a diversified portfolio. Now, suppose the pension fund invests 50% in equities and 50% in bonds, with:</p>
<ul>
<li>Stock volatility = 2.5%</li>
<li>Bond volatility = 1.0%</li>
<li>Negative correlation (-0.3) between stocks and bonds</li>
</ul>
<p>Under normal conditions, VaR satisfies subadditivity:</p>
<p><span class="math display">\[
VaR(A + B) \leq VaR(A) + VaR(B)
\]</span></p>
<p>which means <strong>diversification reduces overall risk</strong>.</p>
<p>Here is a numerical R-example_</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define portfolio components</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>sigma_stocks <span class="ot">&lt;-</span> <span class="fl">0.025</span>  <span class="co"># 2.5% daily volatility</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>sigma_bonds <span class="ot">&lt;-</span> <span class="fl">0.01</span>    <span class="co"># 1.0% daily volatility</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>w_stocks <span class="ot">&lt;-</span> <span class="fl">0.5</span>        <span class="co"># 50% allocation to stocks</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>w_bonds <span class="ot">&lt;-</span> <span class="fl">0.5</span>         <span class="co"># 50% allocation to bonds</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>correlation <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.3</span>    <span class="co"># Negative correlation</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute portfolio volatility</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>portfolio_volatility <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>  (w_stocks <span class="sc">*</span> sigma_stocks)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>  (w_bonds <span class="sc">*</span> sigma_bonds)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>  <span class="dv">2</span> <span class="sc">*</span> w_stocks <span class="sc">*</span> w_bonds <span class="sc">*</span> sigma_stocks <span class="sc">*</span> sigma_bonds <span class="sc">*</span> correlation</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute portfolio VaR</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>VaR_portfolio <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(alpha, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> portfolio_volatility) <span class="sc">*</span> portfolio_value</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute individual VaRs</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>VaR_stocks <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(alpha, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma_stocks) <span class="sc">*</span> (w_stocks <span class="sc">*</span> portfolio_value)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>VaR_bonds <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(alpha, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma_bonds) <span class="sc">*</span> (w_bonds <span class="sc">*</span> portfolio_value)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Output results</span></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a><span class="fu">sprintf</span>(<span class="st">"VaR without diversification: $%.2f million"</span>, VaR_stocks <span class="sc">+</span> VaR_bonds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "VaR without diversification: $-14.39 million"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sprintf</span>(<span class="st">"VaR with diversification: $%.2f million"</span>, VaR_portfolio)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "VaR with diversification: $-9.86 million"</code></pre>
</div>
</div>
<p>Without diversification, the combined VaR of individual assets would be higher than the VaR of the diversified portfolio. This illustrates the subadditivity property of VaR, which states that risk should not increase when assets are combined. However, this property holds only when log-returns follow a normal distribution. If the normality assumption does not hold—such as in cases with fat tails, skewness, or extreme market events—VaR may no longer be subadditive, and diversification benefits could be overestimated. If you are interested in details of risk management the go to referecne is still <span class="citation" data-cites="McNeilEmbrechtsFrey2015">@McNeilEmbrechtsFrey2015</span>.</p>
</section>
<section id="empirical-value-at-risk-var-and-its-limitations" class="level2">
<h2 class="anchored" data-anchor-id="empirical-value-at-risk-var-and-its-limitations">Empirical Value at Risk (VaR) and Its Limitations</h2>
<p>So far, we have used parametric VaR based on the normal distribution. However, we can also estimate VaR empirically, directly from historical data, without assuming a particular distribution.</p>
<p>To compute the empirical 95% VaR, we:</p>
<ol type="1">
<li>Sort the historical log returns in ascending order.</li>
<li>Find the return at the 5th percentile of the empirical distribution.</li>
</ol>
<p>For a <strong>10-day VaR</strong>, we use <strong>weekly returns</strong> rather than daily data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert daily log returns to 5-day log returns by summing </span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="co">#over non-overlapping 5-day periods</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>log_returns_10d <span class="ot">&lt;-</span> </span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">colSums</span>(<span class="fu">matrix</span>(log_returns, <span class="at">nrow =</span> <span class="dv">5</span>, </span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">byrow =</span> <span class="cn">TRUE</span>), <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in matrix(log_returns, nrow = 5, byrow = TRUE): data length [1509] is
not a sub-multiple or multiple of the number of rows [5]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort 10-day log returns in ascending order</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>sorted_returns <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">na.omit</span>(log_returns_10d))</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute empirical cumulative distribution function (ECDF)</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(sorted_returns)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>ecdf_values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, n) <span class="sc">/</span> n  <span class="co"># Explicitly named for clarity</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Identify empirical quantiles for 95% and 99% VaR</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>VaR_95_empirical <span class="ot">&lt;-</span> sorted_returns[<span class="fu">min</span>(<span class="fu">which</span>(ecdf_values <span class="sc">&gt;=</span> <span class="fl">0.05</span>))]</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>VaR_99_empirical <span class="ot">&lt;-</span> sorted_returns[<span class="fu">min</span>(<span class="fu">which</span>(ecdf_values <span class="sc">&gt;=</span> <span class="fl">0.01</span>))]</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Output results</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="fu">sprintf</span>(<span class="st">"Empirical 95%% VaR: %.4f (10-day horizon)"</span>, VaR_95_empirical)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Empirical 95% VaR: -0.0386 (10-day horizon)"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sprintf</span>(<span class="st">"Empirical 99%% VaR: %.4f (10-day horizon)"</span>, VaR_99_empirical)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Empirical 99% VaR: -0.0866 (10-day horizon)"</code></pre>
</div>
</div>
<p>The 95% empirical VaR** suggests that, based purely on historical data, the worst 5% of observed weeks had returns of at least -4% or lower. The 99% empirical VaR tells us that in the worst 1% of historical weeks, losses exceeded -0.09.</p>
<p>Let’s look at a visualization:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="05-lecture5_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Empirical CDF of 5-Day Log Returns with 95% and 99% VaR Thresholds</figcaption>
</figure>
</div>
</div>
</div>
<p>Empirical VaRs are straightforward but they also have <strong>important limitations</strong>, particularly with small samples:</p>
<ul>
<li>If we only have a few years of weekly returns, the 5th percentile may be based on very few observations.</li>
<li>VaR then depends heavily on the worst few weeks, making it unreliable.</li>
<li>The 99% quantile requires even fewer observations, making it very sensitive to individual extreme weeks.</li>
<li>More advanced techniques (such as Extreme Value Theory (EVT)) can help estimate tail risks beyond observed data.</li>
<li>If we haven’t observed an extreme event, empirical VaR ignores it.</li>
<li>This happened in the case of <strong>Long-Term Capital Management (LTCM)</strong>.</li>
</ul>
<p><strong>Example: The LTCM Case: A Real-World Lesson</strong> In the late 1990s, the hedge fund <strong>Long-Term Capital Management (LTCM)</strong> collapsed due to ignoring large, low-probability tail risks. The fund’s risk models were based on historical market behavior, assuming that extreme losses were too unlikely to be of concern.</p>
<p>However, during the 1998 Russian financial crisis, markets experienced far greater volatility than LTCM had anticipated. The fund suffered catastrophic losses, requiring a $3.6 billion bailout coordinated by the Federal Reserve to prevent wider market contagion.</p>
<p>For those of you who are interested in <strong>advanced risk modeling</strong>, a deeper discussion can be found in <span class="citation" data-cites="McNeilEmbrechtsFrey2015">@McNeilEmbrechtsFrey2015</span>.</p>
</section>
<section id="data-and-statistics" class="level2">
<h2 class="anchored" data-anchor-id="data-and-statistics">Data and statistics</h2>
<p>Over the course we have now so many times estimated moments for log-returns and then plugged theses estimates into our software provided functions. It seems necessary at this stage to clarify a few things about the statistics of return data.</p>
<p>Unlike in probability, where we start from the assumption of a random model, typically one or many random variables and think about the consequences for the outcomes, like the properties shape and moments of the distribution and so on, in statistics we take the reverse perspective. We observe data and then try to find out what could be the random variables that might have generated these data, if there is a random process in the background of our observations.</p>
<p>I would therefore like to discuss some key issues in the empirical analysis of return data. The key data source for estimation is historical returns data, which are today available on the internet at daily frequency. This approach is reasonably reliable fro some parameters such as variances and covariances. It is, however, decidedly <strong>unreliable</strong> for other parameters such as expected return. The reason why I want to discuss this problem is that the root cause is a fundamental limitation of the estimation process not the quality of the data or measurement.</p>
<section id="peridod-length-effects" class="level3">
<h3 class="anchored" data-anchor-id="peridod-length-effects">Peridod-Length Effects</h3>
<p>Suppose that the annual return of a stock is <span class="math inline">\(1+r_y\)</span>. It can be thought of as the result of 12 monthly returns and can be written as a product <span class="math display">\[
1+r_y = (1+r_1)(1+r_2)(1+r_3)\dots (1+r_{12})
\]</span> Note that in this equation tge monthly returns are not measured per annum. They are the actual returns over a month. If the returns are small, we can expand the product and keep only the first order terms as a good approximation: <span class="math display">\[
1+r_y \approx 1 + r_1 + r_2 + r_3 + \dots + r_{12}
\]</span> In this approximation the compounding effects are ignored, which is for the purpose of a rough estimates of orders of magnitude of parameters good enough.</p>
<p>Now let’s think about these returns from the perspective of probability theory and imagine that there is an underlying random variable model generating them. Let these random variables be mutually uncorrelated and each monthly return <span class="math inline">\(r_i\)</span> has the same expected value <span class="math inline">\(\bar{r}\)</span> and the same variance <span class="math inline">\(\sigma^2\)</span>. using our approximation we find that <span class="math display">\[
\bar{r_y} = 12\, \bar{r}
\]</span> Likewise <span class="math display">\[
\sigma_y^2 = \mathbb{E}\left[ \sum_{i=1}^{12}(r_i-\bar{r}) \right] = \mathbb{E}\left[ \sum_{i=1}^{12}(r_i - \bar{r})^2 \right] = 12 \sigma^2
\]</span> where the pull of the exponent into the squared brackets is a consequence of the assumption that the returns are uncorrelated. Now turn these equations around and taking the suqare root of the variance, we obtain an expression for the monthly values in terms of annual values <span class="math display">\[\begin{eqnarray*}
\bar{r} &amp;=&amp; \frac{1}{12} \bar{r}_y \\
\bar{\sigma} &amp;=&amp; \frac{1}{\sqrt{12}} \sigma_y
\end{eqnarray*}\]</span> This can be generalized to any length of period. If the period is <span class="math inline">\(p\)</span> part of a year (expressed as a fraction of a year) then the expected return and the standard error of the 1-period rate of return can be found by generalizing from monthly periods <span class="math inline">\(p = 1/12\)</span>. This gives us</p>
<p><span class="math display">\[\begin{eqnarray*}
\bar{r_p}&amp;=&amp; p \, \bar{r}_y \\
\bar{\sigma_p}&amp;=&amp; \sqrt{p} \, \sigma_y
\end{eqnarray*}\]</span></p>
<p>Because the expected return decreases linearly with the period, the standard deviation is proportional to the square root of the length of the period. Therefore the ratio if the two <strong>increases dramatically</strong> as the length is reduced. In the limit, as the length goes to zero, this ratio diverges. Thus rates of return for small perios have high standard deviations compared to their expected values.</p>
<p>Let#s put this into perspective. The mean annual return for stocks ranges from around <span class="math inline">\(6%\)</span> to <span class="math inline">\(30%\)</span> with a typical value at around <span class="math inline">\(12%\)</span>. These mean values change over time so any particual value is meaningful roughly for about 2 or three years. The standard deviation of yearly stock returns ranges from 10% to 60% with typically 15%.</p>
<p>Let#s translate these numbers into monthly values, thus <span class="math inline">\(p=1/12\)</span>. With <span class="math inline">\(\bar{r}_y = 12 %\)</span> and and <span class="math inline">\(\sigma_y = 15%\)</span> this leds to $r_{1/12} = 1% and $_{1/12} = 4.33%. So while for the yearly figure the ratio is 1.25 it is 4.3 for the monthly.</p>
<p>If we assume the returns are generated through independent daily returns and assume 25o trading days then <span class="math inline">\(\r_{1/250} = 0.048\)</span> % and <span class="math inline">\(\sigma_{1/250} = 0.95\)</span> %. The ratio is now 19.8.</p>
<p>Now we can show how this amplification effect makes the estimation of expected mean rates nearly impossible. Let’s select a basic period length <span class="math inline">\(p\)</span> and try to estimate the mean for this period. We assume that the returns of each period are independent random variables with mean <span class="math inline">\(\bar{r}\)</span> and standard error <span class="math inline">\(\sigma\)</span>. We also assume that individual returns are mtually uncorrelated.</p>
<p>Suppose we have <span class="math inline">\(n\)</span> samples of period returns The best estimate for the mean is <span class="math display">\[
\hat{\bar{r}} = \frac{1}{n} \sum_{i=1}^n r_i
\]</span></p>
<p>The estimate is itself a random variables. If we used different samples, we got different v alues for this estimate. However the expected value of the estimate is the true value <span class="math display">\[
\mathbb{E}(\hat{\bar{r}}) = \hat{\mathbb{E}} \left( \frac{1}{n} \sum_{i = 1}^n r_i \right) = \bar{r}
\]</span> We compute the standard deviation of the estimate to asess the accuracy of the estimator for the mean returns. <span class="math display">\[
\sigma_{\hat{\bar{r}}}^2 = \mathbb{E} \left[ (\hat{\bar{r}} - \bar{r})^2 \right] = \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n (r_i - \bar{r}) \right]^2 = \frac{1}{n} \sigma^2
\]</span> Hence <span class="math display">\[
\sigma_{\hat{\bar{r}}} = \frac{\sigma}{\sqrt{n}}
\]</span></p>
<p>This is a standard formula for the error in the estimate of a mean value.</p>
<p>If the period is 1 month, the monthly values used earlier ar <span class="math inline">\(\bar{r} = 1\)</span> % and <span class="math inline">\(\sigma = 4.33\)</span> %. If we use 12 month of data we get <span class="math display">\[
\sigma_{\hat{\bar{r}}} = \frac{4.33}{\sqrt{12}}
\]</span> which is 1,25 %. The standard error of the mean return estimate is then larger than the mean return itself. If we use 4 years of data we can cut this standard deviation by a factor 2, which still must count as a very poor estimate. For an estimate to be considered good we need to be able to cut down the standard deviation to about 1/10th of the mean. This would require about <span class="math inline">\(n = 43.3^2 = 1875\)</span> or about 156 years of data.</p>
</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Geometric Brownian Motion (GBM) is a stochastic process widely used to model stock prices. It follows the equation: <span class="math display">\[
dS_t=μ\,S_t \,dt+σ\,S_t\,dW_t
\]</span> where <span class="math inline">\(\mu\)</span> is the drift (expected return), <span class="math inline">\(\sigma\)</span> is the volatility, and <span class="math inline">\(W_t\)</span> is a Wiener process (also called a Brownian motion). A Wiener process is a continuous-time stochastic process with independent, normally distributed increments and is fundamental in modeling randomness in finance. GBM ensures that stock prices remain strictly positive. For a more extensive discussion see for example <span class="citation" data-cites="Luenberger2009">@Luenberger2009</span><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>