# Continuous random variables and Monte Carlo Simulation

In this lecture we will introduce the most important probability distribution, the **normal
distribution**. While we have discussed discrete random variables so far, where the 
number of possible outcomes for $X$ is finite (or countably infinite), to 
discuss the normal distribution we
need to deal with the case that the number of outcomes for $X$ is uncountable infinite, or
a continuum. This leads us to the concept of a continuous random variable.

## Continuous Random Variables and Probability

Here, we discuss the most important concepts for practical work with continuous random variables. For a mathematically rigorous treatment, advanced techniques such as measure theory are required. However, we will not delve into those here (see, for example, @Billingsley1995). Instead, we focus on applied and practical aspects.

::: {.callout-tip}
## Definition: Continuous Random Variable

A **continuous random variable** $X$ can take on a continuum of possible values within a given range.
:::

Random variables that can take on a continuum of values, rather than discrete 
values like the fair coin we discussed earlier, play a crucial role in practical 
applications. For instance, consider asset prices or returns. A stock's 
price, in principle, could take any value in $[0, \infty)$. Or could it?  

This is a modeling assumption that can be debated in terms of realism. After 
all, stock prices are quoted in currency, which has a smallest unit (e.g., 
cents or pennies). In Lecture 1, we discussed the assumption of unbounded 
stock prices in the context of sample spaces. Similarly, for other practical 
cases—such as task completion times, lengths, or weights—a continuum of 
outcomes often provides a natural model. 

You might argue that these examples are not *truly* continuous. For 
instance, time is measured in hours, minutes, or seconds. However, we 
can refine our measurements to a much finer scale, with the limit 
imposed only by our measuring instruments. Time itself *is* continuous—it 
does not jump.  

In contrast, stock prices do have a smallest monetary unit (e.g., cents 
in the Eurozone). Yet, for practical modeling, treating prices 
as continuous simplifies computations and analysis.

Even without delving into the mathematical machinery of measure theory, it 
is crucial to grasp the implications of continuous random variables. Let’s 
explore an example of a continuous random variable that can take any value 
in the interval $[0,1]$. In R, we can generate such numbers easily using 
the `runif()` function. Consider the following example:

```{r}
set.seed(123)
runif(10, 0, 1)
```

Now, consider the probability of this random variable taking on a specific 
value, say $0.4566147$, one of the values in our list. To investigate, we 
simulate one million uniformly distributed random numbers in $[0,1]$ and 
calculate the relative frequency of $0.4566147$ occurring:

```{r}
uniform_rv <- runif(10^6, 0, 1)
mean(uniform_rv == 0.4566147)
```

The result is zero—literally zero. Even with one million draws, the random 
number generator in R produced unique values each time. The probability of 
hitting any specific value is zero.

This occurs because the interval $[0,1]$ contains an infinite number of points. 
For any number in this interval, there are infinitely many numbers both larger 
and smaller. Assigning a positive probability to any single point would result 
in probabilities summing to a value greater than 1, violating the laws of 
probability. 

::: {.callout-important}
For every continuous random variable $X$, we 
have $P(X = x) = 0$ for all $x$.
:::

This is a fundamental shift from discrete random variables: for 
continuous random variables, we cannot assign positive probabilities 
to individual points. Instead, probabilities are associated with 
**intervals of real numbers**.

For example, consider the probability that a uniformly distributed 
random variable $X \sim U[0,1]$ takes a value between $0$ and $1/4$. Using 
the simulated numbers, we calculate:

```{r}
mean(0 <= uniform_rv & uniform_rv <= 1/4)
```

The result is 25%, as expected. Using R's cumulative distribution 
function (CDF), we confirm:

```{r}
punif(1/4)
```

For continuous random variables, probabilities are represented 
as **areas under a curve**. This is the major distinction from discrete 
random variables, where probabilities are assigned to individual points. 

Consider this density function example:

```{r, out.width='90%', fig.align='center', fig.cap='With continuous random variables, probabilities are areas under the density function', echo = F}
knitr::include_graphics('figures/uniform_dist.png')
```

Mathematically, these areas are calculated using integrals. The **probability density function** (PDF) describes the distribution as follows:

::: {.callout-tip}
## Probability Density Function of a Continuous Random Variable

For a continuous random variable $X$ with density function $f(x)$:

1. $f(x) \geq 0$, for all $x$.
2. $\int_{-\infty}^{\infty} f(x) \, dx = 1$.
3. For any $a < b$, the probability that $X$ falls within $(a, b)$ is given by the integral:  
   $P(a < X < b) = \int_a^b f(x) \, dx$.
:::

The **cumulative distribution function** (CDF) provides another essential tool for continuous random variables:

::: {.callout-tip}
## Cumulative Distribution Function of a Continuous Random Variable

The **cumulative distribution function** (CDF) shows the probability that $X$ takes a value less than or equal to $x$:  
$F(x) = P(X \leq x)$.  
For any $a < b$:  
$P(a < X < b) = F(b) - F(a) = \int_a^b f(x) \, dx$.
:::

This transition from discrete points to areas under a curve marks a 
crucial conceptual shift in working with continuous random variables.

## Normal Distribution

The **normal distribution** is perhaps the most iconic and fundamental 
probability distribution in all of probability theory. Its bell-shaped curve 
has become synonymous with ideas of natural variability and randomness. 
From the heights of people to measurement errors, and from stock returns to 
the central limit theorem, the normal distribution underpins countless 
phenomena in the natural and social sciences.

What makes the normal distribution truly remarkable is its simplicity and 
universality. With just two parameters—the mean ($\mu$) and 
variance ($\sigma^2$)—it captures the essence of variability in a way 
that is mathematically elegant and empirically ubiquitous. This 
distribution lies at the heart of probability theory, serving as 
the cornerstone for much stochastic modeling. For the 
application of probability to the modelling of data, the field
of statistics, the normal distribution is foundational.

::: {.callout-tip}
## Definition: Normal Distribution

The **normal distribution** is a continuous probability distribution 
that is centered around the mean, bell-shaped, symmetric, and completely 
determined by two parameters: the mean $\mu$ and the variance $\sigma^2$. 
The notation is $X \sim N(\mu, \sigma^2)$. Its probability density function 
is given by:
$$
f(x, \mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)
$$
:::

Often referred to as the **Gaussian distribution**, after the German 
mathematician Karl Friedrich Gauss (1777–1855), it is also known 
as the **Gauss-Laplace distribution**, honoring Pierre-Simon Laplace (1749–1827). 
The shape of its probability density function has earned 
it the nickname **bell curve**, an image deeply ingrained in the 
language of science, education, and beyond.

Let's use R to make this image tangible.

```{r}
# Set up the sequence of x values
x <- seq(-4, 4, length.out = 1000)

# Compute the probability density function of the standard normal distribution
y <- dnorm(x)

# Create the plot
plot(x, y, type = "l", lwd = 2, col = "blue", 
     main = "The Bell Curve: Standard Normal Distribution",
     xlab = "Value", ylab = "Density")
grid()
```

The code above demonstrates how to create a simple visualization of the bell curve for the standard normal distribution using base R. Here's a breakdown of the key steps:

We first define a range of values:  The `seq()` function generates 
a sequence of values for the x-axis. In this case, it creates 1,000 
equally spaced values between -4 and 4, which is sufficient to capture 
the central shape of the bell curve.

In a second step we compute the density.The `dnorm()` function computes the 
probability density of the standard normal distribution at each value in 
the sequence. If the mean and standard deviation are not explicitly specified 
in `dnorm()`, the function assumes the standard normal distribution by 
default, with a mean of 0 and a standard deviation of 1This function is 
part of a family of functions for working 
with random variables in R:
   - `dnorm(x)` computes the density at `x`.
   - `pnorm(x)` computes the cumulative distribution function at `x`.
   - `qnorm(p)` gives the quantile for a given probability `p`.
   - `rnorm(n)` generates `n` random samples from the normal distribution.

We need not discuss the basic syntax of the plotting function once more at
this stage, because by now we have often done so.

### Standardization and the Standard Normal Distribution

One of the most powerful properties of the normal distribution is the 
ability to standardize it. By transforming any normally distributed 
random variable into a standard form, we unlock the ability to 
compare and analyze data across scales and contexts.

::: {.callout-tip}
## Definition: Standard Normal Distribution

The **standard normal distribution** is a normal distribution with 
a mean $\mu = 0$ and variance $\sigma^2 = 1$. Any normally distributed 
random variable $X$ with mean $\mu$ and variance $\sigma^2$ can be 
rewritten as a standard normal random variable $Z$ using the transformation:
$$
Z = \frac{X - \mu}{\sigma}
$$
By definition, $Z \sim N(0,1)$.
:::

Here's a draft for discussing the probability mass within 1, 2, and 3 standard deviations of the mean for the normal distribution, along with its significance:

### The 68-95-99.7 Rule: Probability Mass in the Normal Distribution

One of the most useful properties of the normal distribution is that it has a 
predictable concentration of probability mass around the mean. This is 
often summarized by the **68-95-99.7 rule**, which states:

- Approximately **68%** of the data falls within **1 standard deviation** of the mean.
- Approximately **95%** of the data falls within **2 standard deviations** of the mean.
- Approximately **99.7%** of the data falls within **3 standard deviations** of the mean.

This property is not only fundamental to understanding the normal distribution 
but also provides a quick and intuitive way to interpret variability in data, 
regardless of the units of measurement.

Why Is This Important?

1. **Universal Applicability:**  
   The percentages remain the same no matter the scale or units of the data. 
   For instance, whether we measure test scores, heights, or stock returns, 
   this property holds for all normally distributed data.
   
2. **Quick Validation of Normality:**
   The 68-95-99.7 rule provides a straightforward diagnostic tool for assessing 
   whether a dataset is approximately normal. If the proportions of data 
   falling within 1, 2, and 3 standard deviations deviate significantly from 
   these benchmarks, it is a strong indicator that the data may not be 
   normally distributed.

3. **Practical Insight:**  
   It allows us to quickly assess how "unusual" a value is. For example:
   - A value more than 2 standard deviations from the mean is relatively rare (occurring in only 5% of cases).
   - A value more than 3 standard deviations from the mean is exceptionally rare (occurring in only 0.3% of cases).

4. **Decision-Making:**  
   This rule aids in many practical applications, such as quality control, where v
   alues falling outside of 3 standard deviations might indicate defects or 
   anomalies. Similarly, in finance, it helps in risk assessment by estimating 
   the likelihood of extreme losses or gains.

To illustrate this property, we can plot the standard normal distribution 
and shade the areas corresponding to 1, 2, and 3 standard deviations 
from the mean.

```{r visualize-probability-mass}

# Define the x-axis range and density
x <- seq(-4, 4, length.out = 1000)
y <- dnorm(x)

# Create the plot
plot(x, y, type = "l", lwd = 2, col = "blue",
     main = "The 68-95-99.7 Rule",
     xlab = "Standard Deviations from the Mean", ylab = "Density")

# Add shaded areas with distinct colors
# Shade 3 SD region first (light pink)
polygon(c(-3, seq(-3, 3, length.out = 100), 3), 
        c(0, dnorm(seq(-3, 3, length.out = 100)), 0),
        col = "#FBB4AE", border = NA)

# Shade 2 SD region (light green)
polygon(c(-2, seq(-2, 2, length.out = 100), 2), 
        c(0, dnorm(seq(-2, 2, length.out = 100)), 0),
        col = "#CCEBC5", border = NA)

# Shade 1 SD region (light blue)
polygon(c(-1, seq(-1, 1, length.out = 100), 1), 
        c(0, dnorm(seq(-1, 1, length.out = 100)), 0),
        col = "#B3CDE3", border = NA)

# Redraw the outline of the curve on top for clarity
lines(x, y, lwd = 2, col = "blue")

# Add legend positioned middle-left at y = 0.2
legend(-4, 0.2, 
       legend = c("68% (1 SD)", "95% (2 SDs, includes 1 SD)", "99.7% (3 SDs, includes 1 & 2 SDs)"),
       fill = c("#B3CDE3", "#CCEBC5", "#FBB4AE"), 
       border = NA, box.lty = 0, bg = "white", x.intersp = 0.5, y.intersp = 1.5)

```

To confirm these proportions, we compute the probabilities using R's cumulative 
distribution function (`pnorm`):

```{r confirm-probability-mass}
# Probabilities for 1, 2, and 3 standard deviations
p1 <- pnorm(1) - pnorm(-1)  # ~68%
p2 <- pnorm(2) - pnorm(-2)  # ~95%
p3 <- pnorm(3) - pnorm(-3)  # ~99.7%

cat("Probability within 1 SD: ", p1, "\n")
cat("Probability within 2 SDs: ", p2, "\n")
cat("Probability within 3 SDs: ", p3, "\n")
```

These results reinforce the importance of the 68-95-99.7 rule as a tool for 
interpreting data variability and making informed decisions.

## Lognormal Distribution

While the normal distribution models many natural and 
financial phenomena, it is not always suitable for modeling certain 
quantities—such as stock prices—that are constrained to be positive. 
This is where the **lognormal distribution** becomes essential. 
It serves as a natural model for variables that are strictly positive and 
exhibit multiplicative growth, such as asset prices in financial markets.

::: {.callout-tip}
## Definition: Lognormal Distribution

A **lognormal random variable** $Y$ is one whose natural logarithm is 
normally distributed. If $X \sim N(\mu, \sigma^2)$, then $Y = \exp(X)$ follows 
a lognormal distribution. The probability density function of $Y$ is given by:
$$
f(y, \mu, \sigma) = \frac{1}{y \sigma \sqrt{2 \pi}} \exp\left(-\frac{(\ln y - \mu)^2}{2 \sigma^2}\right), \quad y > 0.
$$
:::

### Why the Lognormal Distribution for Stock Prices?

Stock prices, by their nature, cannot fall below zero and often grow in 
a multiplicative manner over time. If the logarithm of a stock price follows 
a normal distribution, then the stock price itself is lognormally distributed. 
This aligns with the widely used geometric Brownian motion model 
This aligns with the widely used geometric Brownian motion model 
for stock price dynamics.^[Geometric Brownian Motion (GBM) is 
a stochastic process widely used to model stock prices. It 
follows the equation:
$$
dS_t=μ\,S_t \,dt+σ\,S_t\,dW_t
$$
where $\mu$ is the drift (expected return), $\sigma$ is the volatility, 
and $W_t$ is a Wiener process (also called a Brownian motion). A Wiener 
process is a continuous-time stochastic process with independent, normally 
distributed increments and is fundamental in modeling randomness in finance. 
GBM ensures that stock prices remain strictly positive. For a more extensive
discussion see for example @Luenberger2009]

In finance, returns are a natural way to measure changes in stock prices 
over time. Remember that for a discrete time interval, the return is defined as:
$$
R = \frac{S_t - S_0}{S_0},
$$
where $S_0$ is the initial price and $S_t$ is the price at time $t$. However, 
this formulation has limitations for very short time intervals or when returns 
are compounded over time.

Instead, **logarithmic returns** (or continuously compounded returns) are 
defined as:
$$
r = \ln\left(\frac{S_t}{S_0}\right).
$$
This definition arises naturally because it allows for:
1. **Additivity in Continuous Time:**  
   Over small time intervals, the log of cumulative returns adds up, making 
   it easy to model and sum returns over time. For instance, if a 
   stock moves from $S_0$ to $S_t$ and then to $S_T$, the total log return is:
   $$
   r = \ln\left(\frac{S_t}{S_0}\right) + \ln\left(\frac{S_T}{S_t}\right) = \ln\left(\frac{S_T}{S_0}\right).
   $$
   This property simplifies modeling in continuous time frameworks.

2. **Consistency with Compounding:**  
   Financial returns often compound multiplicatively (e.g., reinvested 
   dividends or reinvested profits). Logarithmic returns handle compounding 
   naturally and ensure that the total return across intervals corresponds 
   to the product of growth factors.

3. **Symmetry in Statistical Analysis:**  
   While absolute returns can grow unboundedly in a positive direction, 
   logarithmic returns are symmetric around the mean, simplifying statistical 
   analysis and aligning better with the assumptions of models like 
   geometric Brownian motion.

When stock prices follow geometric Brownian motion, their logarithmic 
returns $r$ are normally distributed:
$$
r \sim N(\mu, \sigma^2),
$$
where $\mu$ is the mean log return and $\sigma^2$ is the variance. As a 
result, the stock price itself, given by $S_t = S_0 \exp(r)$, follows a 
lognormal distribution. The lognormal distribution is particularly suitable 
for stock prices because:

- **Positive Skewness:** It allows for rare but extreme positive returns, reflecting real-world market behavior.
- **Non-Negativity:** Stock prices cannot fall below zero.
- **Compounding Effects:** It captures the multiplicative nature of price movements over time.

### Comparing Real Stock Data with the Lognormal Distribution

To connect the theoretical discussion with real-world data, we’ll 
analyze historical stock prices from the S&P 500 index. Using data 
from Yahoo Finance, we compute daily log returns, overlay the 
empirical distribution with a fitted lognormal distribution, and 
discuss the fit's implications.

Let's use our R-tools fro overlaying empirical data with the theoretical
model of the random variable.

We first load tidyquant:

```{r}
#| message: false

library(tidyquant)
```

Then we fit the model to actual stock market data
for the SP500.

```{r lognormal-fit-sp500-tidyquant}
#| code-fold: true
#| echo: false

# Retrieve S&P 500 data (adjustable end date)
end_date <- "2020-12-31"  # Replace this with any desired date
sp500_data <- tq_get("^GSPC", from = "2015-01-01", to = end_date, get = "stock.prices")

# Extract adjusted closing prices
sp500_prices <- sp500_data$adjusted

# Compute daily log returns using base R
log_returns <- diff(log(sp500_prices))  # Log differences
log_returns <- log_returns[!is.na(log_returns)]  # Remove NA values

# Fit a normal distribution (NOT lognormal) to the log-returns
fit_mu <- mean(log_returns)
fit_sigma <- sd(log_returns)

# Generate fitted normal density
x <- seq(min(log_returns), max(log_returns), length.out = 1000)
fitted_density <- dnorm(x, mean = fit_mu, sd = fit_sigma)

# Normalize histogram
# Compute histogram data without plotting
hist_data <- hist(log_returns, breaks = 50, plot = FALSE)
# Normalize counts
hist_data$density <- hist_data$counts / sum(hist_data$counts) / 
  diff(hist_data$breaks[1:2])  

# Plot histogram using density scale
plot(hist_data$mids, hist_data$density, type = "h", lwd = 6, col = "lightblue", 
     main = "Empirical vs Fitted Normal Distribution (S&P 500 Log-Returns)", 
     xlab = "Daily Log Returns", ylab = "Density", 
     ylim = c(0, max(hist_data$density, fitted_density)))

# Overlay fitted normal distribution (not lognormal!)
lines(x, fitted_density, col = "red", lwd = 2)

# Add a simple legend
legend("topright", 
       legend = c("Empirical Density", "Fitted Normal"), 
       fill = c("lightblue", NA), 
       lty = c(NA, 1), 
       col = c("lightblue", "red"))

```

The R-code behind this visualization is somewhat involved and I will not go through
it here because I want to focus on a more important point of 
potential confusion, which needs to be explained carefully. Those of you who want 
to look at the code just unfold the code chunk.

So what are we doing here? We started by assuming that
**stock prices follow a lognormal distribution**  
   - Prices are **strictly positive**, which makes the **lognormal** distribution a natural choice.
   - If $S_t$ is a stock price and follows geometric Brownian motion:
     
$$
S_t = S_0 \exp(X_t)
$$
     where $X_t$ is normally distributed.

By our assumption that stock prices are modeled by a lognormal distribution,
**log-returns follow a normal distribution**

   - Returns are often modeled as additive over time.
   - **Log-returns** are defined as:
$$
     r_t = \ln\left(\frac{S_t}{S_{t-1}}\right)
$$
   - If stock prices follow a **lognormal** distribution, then **log-returns** 
     must follow a **normal** distribution.

We work with **log-returns** because they simplify calculations, but prices are 
what we observe. It would be a mistake to fitting a lognormal distribution 
to log-returns (instead of prices). 
The correct modeling framework depends on whether we are working 
with **prices or returns**.

Let me summarize these remarks in a side by side comparison table.

  | Variable | Distribution | Why? |
  |----------|-------------|------|
  | Stock Prices $S_t$ | Lognormal | Prices can't be negative, and returns compound multiplicatively. |
  | Log-Returns $r_t$ | Normal | Returns add over time and often appear symmetric. |


Now let us go back to the discussion of what we see in the graph:

1. **Fat Tails:**  
   The empirical distribution may display so called **fat tails**, meaning 
   extreme returns are more frequent in real data than predicted by the 
   lognormal model. These events are crucial for risk assessment and portfolio 
   stress testing.
   
   To see more clearly whether we have fat tails in our daily log return data, let us visually
   zoom in to the region of more extreme negative returns.
   
```{r}
#| code-fold: true
#| echo: false

# Define extreme left tail threshold (beyond 2 to 3 standard deviations)
left_tail_threshold <- fit_mu - 2.5 * fit_sigma  # Approx. 2.5 sigma left tail

# Define new x range for extreme tail visualization
x_tail <- seq(min(log_returns), left_tail_threshold, length.out = 1000)
fitted_tail_density <- dnorm(x_tail, mean = fit_mu, sd = fit_sigma)

# Normalize histogram
hist_data <- hist(log_returns, breaks = 50, plot = FALSE)
hist_data$density <- hist_data$counts / sum(hist_data$counts) / 
  diff(hist_data$breaks[1:2])  

# Filter histogram data for left tail (beyond 2 sigma)
left_tail_indices <- hist_data$mids <= left_tail_threshold
hist_tail_mids <- hist_data$mids[left_tail_indices]
hist_tail_density <- hist_data$density[left_tail_indices]

# Plot zoomed-in histogram for extreme negative returns
plot(hist_tail_mids, hist_tail_density, type = "h", lwd = 6, col = "lightblue", 
     main = "Fat Tails: Extreme Negative Returns (S&P 500)", 
     xlab = "Daily Log Returns", ylab = "Density",
     xlim = c(min(log_returns), left_tail_threshold), 
     ylim = c(0, max(hist_tail_density, fitted_tail_density)))

# Overlay fitted normal distribution (which underestimates extreme losses)
lines(x_tail, fitted_tail_density, col = "red", lwd = 2)

# Highlight excess empirical risk (where the empirical density is larger)
excess_indices <- which(hist_tail_density > approx(x_tail, fitted_tail_density, hist_tail_mids, rule=2)$y)
if (length(excess_indices) > 0) {
  polygon(c(hist_tail_mids[excess_indices], rev(hist_tail_mids[excess_indices])),
          c(hist_tail_density[excess_indices], rep(0, length(excess_indices))),
          col = rgb(1, 0, 0, alpha = 0.3), border = NA)
}

# Add a legend clarifying that excess tail risk is empirical
legend("topright", 
       legend = c("Empirical Density", "Fitted Normal", "Excess Tail Risk"),
       fill = c("lightblue", NA, rgb(1, 0, 0, alpha = 0.3)), 
       lty = c(NA, 1, NA), 
       col = c("lightblue", "red", rgb(1, 0, 0, alpha = 0.3)))

```

To put this graph into perspective remember the 
**68-95-99.7 rule**. It tells us that in a normal distribution:

- About **68%** of observations fall within **1 standard deviation** of the mean.
- About **95%** fall within **2 standard deviations**.
- About **99.7%** fall within **3 standard deviations**.

If **daily log-returns** followed a normal distribution, we would 
expect **only 0.15% of observations** to be more extreme than **-3σ**. 
However, in our empirical data, **0.99% of returns** fall below this 
threshold—**more than six times the expected frequency**. 

This gives us a **critical insight for risk management**:

- **Normal models underestimate extreme downside risk**—leading to 
potential miscalculations in risk measures like **Value-at-Risk (VaR)**, a 
concept we will discuss later in more detail.

- Market downturns often exhibit far worse losses than a normal distribution 
  would predict.
  
- Alternative distributions, such as the **t-distribution** or 
  **generalized extreme value (GEV) models**, may be better suited 
  to capturing these extreme tail risks.

2. **Asymmetry (Skewness):**  
   Real stock returns often show **negative skewness**, where extreme 
   negative returns (e.g., during crashes) are more pronounced than 
   extreme positive returns.
   
This is another critical property of financial returns which 
is **asymmetry**, or **skewness**.

What Does Skewness Mean? A normal distribution is **symmetric**, meaning 
that extreme positive and negative values are equally likely.
However, real-world stock returns often show negative skewness, meaning that 
large negative returns occur more frequently than large positive returns.
This asymmetry is particularly visible during market crashes, when prices 
tend to decline much faster than they rise during bull markets.

In our **fat-tail visualization**, we focused on the **left tail** of the 
distribution (extreme losses). If stock returns were truly **symmetric**, we 
would expect to see a similar excess probability mass on the **right tail** 
(large gains). However: The left tail extends much further and is more 
pronounced than the right. Large losses tend to be larger in magnitude 
than large gains.

This is why risk management focuses more on downside risk —investors 
care more about avoiding catastrophic losses than capturing rare, extreme gains.

To quantify this asymmetry, we can compute the **skewness statistic** of our 
dataset. A normal distribution has a skewness of 0, while:
Negative skewness (< 0) indicates that the left tail is heavier than the right.
Positive skewness (> 0) indicates the opposite. In our data
the skewness is
```{r}
#| code-fold: true
#| echo: false
#| messages: false

# Load required package without attachment messages
suppressPackageStartupMessages(library(moments))  # Provides skewness function

# Compute skewness of log returns
log_returns_skewness <- skewness(log_returns)
```
`r round(log_returns_skewness, 2)`. For reference:
A normal distribution has a skewness of 0 (perfect symmetry).
A skewness of `r round(log_returns_skewness, 2)` indicates a strongly 
asymmetric distribution with heavier left tails.


### Applications and Limitations: A Balanced Perspective

Looking at our empirical data, we see that the normal approximation to 
log-returns fits quite well in the center of the distribution. You can check
for yourself that as you go to considering weekly or monthly returns instead 
of daily ones this fit in the center becomes actually quite good. 
The lognormal model remains a widely used and valuable framework for 
understanding stock price dynamics.

However, while the center of the distribution aligns well with theory, the 
tails remain problematic. As we saw in the fat-tail visualization, extreme 
negative returns occur far more often than a normal model would suggest. 
This is a critical issue in risk management, where tail events—such as financial crises or sudden market drops—can have disproportionate consequences.

That said, in many other applications, where the focus is on general 
trends, valuation models, or portfolio optimization, a good fit in the 
center may be sufficient. The profession does not work with 
a "wrong" model—rather, different models are used depending 
on the question being asked. For example:

- In option pricing (Black-Scholes), the lognormal assumption is a reasonable starting point, though corrections (e.g., stochastic volatility models) are often needed.
- In long-term investment strategies, where extreme short-term fluctuations average out, the normal/lognormal framework remains quite effective.

Thus, while tail risks must be explicitly accounted for in risk management, 
the lognormal assumption remains a useful and practical tool in many 
areas of finance.

## Inverse Normal and Quantiles in Risk Management

In risk management, a fundamental question is:

What is the worst-case loss I should expect, given a certain probability threshold?

This is different from what we studied earlier. Previously, we were 
given a threshold and asked for the probability of falling below it. 
Now, we flip the question:

- We are given a probability (e.g., 1%)
- We want to find the threshold such that losses exceed it only with that probability.

This is known as the inverse problem in probability, and it plays a 
central role in Value at Risk (VaR) calculations.

Suppose you manage a portfolio with uncertain (random) returns. 
A key risk management question is:

How large can losses be over a given time horizon, with a 
probability of only 1% (or another predefined risk threshold)?

For example, a bank may want to ensure that the probability of losing 
more than a certain percentage of its capital remains below 1%. In 
this case, the 1% quantile of portfolio 
returns (often called the 1% Value at Risk, or VaR) is 
the key statistic.


### The Inverse Normal Function in R

The quantile function (or inverse cumulative distribution function) helps 
solve this problem. For a normally distributed random variable $X$, we want to 
find the threshold $x$ such that:
$P(X≤x)=p$
where $p$ is a given probability.

In R, we compute this using the `qnorm()` function. Let's demonstrate
how it works using our data from before:

```{r}
qnorm(0.01, mean = mean(log_returns), sd = sqrt(var(log_returns)))
```

This function finds the 1% quantile of a normal distribution with a given mean
and a given standard deviation derived from the logarithmic returns
of the SP500. It answers the question:

What is the worst-case daily return we should expect, such that losses 
exceed this level only 1% of the time?

This is often referred to as the inverse normal problem, since 
it inverts the cumulative distribution function (CDF).
Definition: Quantile

::: {.callout-tip}

## p-quantile

The $p$-th quantile (or percentile) of a probability distribution is the 
value $x$ such that:
$$
P(X≤x)=p
$$
If $X$ is normally distributed, i.e., $X\sim N(\mu,\sigma^2)$, then:
$$
x=F^{−1}(p),
$$

where $F^{−1}$ is the inverse CDF (quantile function) of the normal distribution 
with parameters $\mu$ and $\sigma$. 

:::

The **median** is the 50% quantile ($P(X≤x)=0.5$), dividing the distribution in half.
The **1%-quantile** (or 99% left-tail quantile) gives us a worst-case threshold, 
which is critical for risk management models, like value at risk. You can
consider any other percentile you might be interested in in this way.

The inverse CDF is essential in many areas of finance, including risk 
management, stress testing, and capital adequacy planning.

Now that we understand quantiles and the inverse normal, we can 
directly apply this concept to Value at Risk (VaR).

## Value at risk

Designing portfolios in a way that enable an acceptable trade off between risk of
loss and the potential for profit is a key consideration in portfolio management.
Quantitative risk measures are one way to achieve goals like this. 

Quantitative measures of risk a broad topic that we can not fully cover here. But
one particular popular risk measure **value at risk** is directly based on the
concept of quantiles of a normally distributed random variable and thus is 
the perfect application case for appreciating the significance of quantiles as
an analytical tool in finance.

Let us imagine a financial position modeled by a continuous 
random variable $X$ denoting the 
change in value of a position at a given future time $T$. In general the variable
may take on either positive or negative values depending on its realization. We
refer to the random variable $X$ for convenience as **position**. From the risk
perspective, we may focus on the associated loss, which is $-X$.

The concept of **value at risk** (abbreviated VaR) is motivated by the
concern about loss. We start by specifying a **loss tolerance** $h$ between
0 and 1 and a companion **confidence level** equal to $1-h$. For example,
we could choose a loss tolerance $h=0.05$ and a corresponding confidence
level of $1-h = 0.95$

For a particular position $X$ and a given loss tolerance $h$, VaR is then the
smallest number $V$ such that the probability of a loss greater than $V$ is no
more than $h$.

::: {.callout-tip}
## Definition: Value at risk

For a given position $X$ and a given loss toleracne $h$, VaR is the smallest
number $V$ such that the probability of a loss greater tan $V$ is no more than $h$:

$$
VaR_h(X) = \min_{h} \{ V: P\left[ - X > V \right] \leq h \}
$$
Equivalently, VaR is the smallest number $V$ such that the probability of the loss beeing no more than $V$ is gretaer than $1-h$ or:

$$
VaR_h(X) = \min_{h} \{ V: P\left[ - X \leq V \right] > 1 - h \}
$$
:::

Here is a visualization:

```{r}
#| code-fold: true
#| echo: false
#| fig-align: "center"
#| fig-cap: "95% Value at Risk (VaR) Visualization"

#| code-fold: true
#| echo: false
#| fig-align: "center"
#| fig-cap: "95% Value at Risk (VaR) Visualization"

# Load necessary package
library(ggplot2)

# Define parameters for normal distribution
mu <- 0      # Mean return (adjust if needed)
sigma <- 1   # Standard deviation (adjust if needed)
alpha <- 0.05  # Confidence level (5% left-tail)

# Compute 95% VaR threshold
VaR_95 <- qnorm(alpha, mean = mu, sd = sigma)

# Generate x values for normal distribution
x_vals <- seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 1000)
y_vals <- dnorm(x_vals, mean = mu, sd = sigma)

# Create data frame for plotting
data <- data.frame(x = x_vals, y = y_vals)

# Create ggplot visualization
ggplot(data, aes(x = x, y = y)) +
  geom_line(color = "blue", linewidth = 1.2) +  # Normal distribution curve
  geom_area(data = subset(data, x <= VaR_95), aes(y = y), fill = "red", alpha = 0.4) +  # Left tail
  geom_area(data = subset(data, x > VaR_95), aes(y = y), fill = "green", alpha = 0.3) + # Right tail
  geom_vline(xintercept = mu, linetype = "dashed", color = "black", linewidth = 1) + # Mean line
  geom_vline(xintercept = VaR_95, linetype = "dashed", color = "red", linewidth = 1.2) + # VaR line
  annotate("text", x = VaR_95, y = 0.05, label = sprintf("VaR 95%%: %.2f", VaR_95), color = "red", hjust = 1.2, size = 5) +
  annotate("text", x = mu, y = 0.05, label = "Mean", color = "black", hjust = -0.2, size = 5) +
  labs(title = "95% Value at Risk (VaR) Visualization", x = "Portfolio Returns", y = "Density") +
  theme_minimal(base_size = 14)

```

The graph illustrates the 95% Value at Risk (VaR) concept using a 
normal distribution of daily portfolio log returns. The blue curve represents 
the probability density function of log-returns. The red-shaded area on 
the left highlights the 5% tail probability, indicating extreme negative 
returns that occur with only a 5% likelihood. The green-shaded area 
represents the complementary 95% probability mass, where returns are expected to fall under normal conditions.

Two vertical dashed lines mark key reference points:

- The black dashed line represents the mean return (expected value).
- The red dashed line represents the VaR threshold, the level of 
  loss that is only exceeded 5% of the time.

This visualization helps quantify downside risk: A risk manager using VaR 
at 95% confidence would focus on the red-shaded region to assess the 
worst-case loss threshold. However, as we have seen with empirical 
stock return data, real-world distributions often exhibit fat 
tails, meaning extreme losses occur more frequently than the 
normal model predicts. This suggests that while VaR is a useful 
benchmark, adjustments may be needed for more accurate risk assessments.

The value at risk as defined here and in the literture comes with an
implicit definition of a given time horizon $T$ at which $X$ is realized.
If the position is liquid this horizon may be one or a few days. Often there
are also regulatry requirements setting the rules how this horizon can or must
be chosen.

Now you can see how the concepts of the inverse normal can be directly
brough to bear in the case of normally distributed log returns of
stock prices.

::: {.callout-tip}
## Proposition: VaR for the normal distribution

Suppose $X$ follows a normal distribution with mean $\mu$ and standard deviation
$\sigma$. Then

$$
VaR_h(X) = - \sigma \, F^{-1}_N(h) - \mu
$$
where $F_N$ is the cumulative probability distribution function of the 
standardized normal variable (with mean 0 and standard deviation 1).
:::

Here’s a structured **draft** covering the **three Value at Risk (VaR) examples** along with **R code** to illustrate each case.

Let's consider three examples:

**Example 1: Highly Liquid Portfolio with Small Mean Return**

A **highly liquid portfolio** consists of assets that can be easily 
bought or sold with minimal impact on price. Examples include:

- Short-term U.S. Treasury bills
- Large-cap ETFs (e.g., SPY, QQQ)
- Highly traded currency pairs (EUR/USD, USD/JPY)

For such portfolios:
Expected returns are very small over short time horizons. VaR is then 
mainly driven by portfolio variance (volatility) rather than the mean return.
In this case the VaR can be approximated as:
$$
  VaR_{95\%} \approx 1.65 \times \sigma
$$
since the mean return is negligible over short periods and $-F^{-1}_N(0.05) = 1.65$.
Check using the quantile function of the normal distribution.
```{r}
qnorm(0.05)*(-1)
```

Here is a numerical R-example for typical values of such a
portfolio

```{r}

# Define parameters
sigma_liquid <- 0.015  # 1.5% daily volatility (assumption)
mu_liquid <- 0         # Negligible mean return
alpha <- 0.05          # 95% confidence level

# Compute 1-day VaR
VaR_liquid <- qnorm(alpha, mean = mu_liquid, sd = sigma_liquid)

# Output result
sprintf("1-day 95%% VaR for a highly liquid portfolio: %.4f (or %.2f%%)", VaR_liquid, VaR_liquid * 100)
```

For a **highly liquid** asset with daily volatility of **1.5%**, the **1-day 95% VaR**
is approximately **-2.47%**, meaning that on 5% of days, the portfolio could lose 
at least 2.47%** under normal conditions.


**Example 2: A pension fund**: 

Let's consider next the example of a 
**10-Day VaR for a Pension Fund**.
A pension fund typically invests in 
a **diversified mix of stocks, bonds, and alternative assets**. 
Suppose a fund manager wants to compute **10-day VaR** for 
a **$500 million endowment**.

To scale VaR from **1-day to N-days**, we assume 
returns follow a **normal distribution** and use the square-root rule:

$$
VaR_{N-\text{day}} = VaR_{1-\text{day}} \times \sqrt{N}
$$
Here is a numerical R-example:

```{r}

# Define parameters
sigma_fund <- 0.02    # 2% daily volatility
mu_fund <- 0.0002     # 0.02% daily return (assumed)
N <- 10               # 10-day horizon
portfolio_value <- 500 # $500 million

# Compute 1-day VaR
VaR_fund_1d <- qnorm(alpha, mean = mu_fund, sd = sigma_fund)

# Compute 10-day VaR using square-root scaling
VaR_fund_10d <- VaR_fund_1d * sqrt(N) * portfolio_value

# Output result
sprintf("10-day 95%% VaR for a $500M pension fund: $%.2f million", VaR_fund_10d)
```

For a pension fund with 2% daily volatility, a \$ 500 million portfolio, 
and a 10-day horizon, the 10-day 95% VaR is around \$ X million. 
This means the fund can expect to lose at least this amount 
over a 10-day period with 5% probability.

**Example 3: Portfolio diversification**: Finally, let's look at the example 
of a diversified portfolio.
Now, suppose the pension fund invests 50% in equities and 50% in bonds, with:

- Stock volatility = 2.5%
- Bond volatility = 1.0%
- Negative correlation (-0.3) between stocks and bonds

Under normal conditions, VaR satisfies subadditivity:

$$
VaR(A + B) \leq VaR(A) + VaR(B)
$$

which means **diversification reduces overall risk**.

Here is a numerical R-example_

```{r}

# Define portfolio components
sigma_stocks <- 0.025  # 2.5% daily volatility
sigma_bonds <- 0.01    # 1.0% daily volatility
w_stocks <- 0.5        # 50% allocation to stocks
w_bonds <- 0.5         # 50% allocation to bonds
correlation <- -0.3    # Negative correlation

# Compute portfolio volatility
portfolio_volatility <- sqrt(
  (w_stocks * sigma_stocks)^2 +
  (w_bonds * sigma_bonds)^2 +
  2 * w_stocks * w_bonds * sigma_stocks * sigma_bonds * correlation
)

# Compute portfolio VaR
VaR_portfolio <- qnorm(alpha, mean = 0, sd = portfolio_volatility) * portfolio_value

# Compute individual VaRs
VaR_stocks <- qnorm(alpha, mean = 0, sd = sigma_stocks) * (w_stocks * portfolio_value)
VaR_bonds <- qnorm(alpha, mean = 0, sd = sigma_bonds) * (w_bonds * portfolio_value)

# Output results
sprintf("VaR without diversification: $%.2f million", VaR_stocks + VaR_bonds)
sprintf("VaR with diversification: $%.2f million", VaR_portfolio)
```

Without diversification, the combined VaR of individual assets would be 
higher than the VaR of the diversified portfolio. This illustrates the 
subadditivity property of VaR, which states that risk should not 
increase when assets are combined. However, this property holds 
only when log-returns follow a normal distribution. If the 
normality assumption does not hold—such as in cases with 
fat tails, skewness, or extreme market events—VaR may no 
longer be subadditive, and diversification benefits could 
be overestimated. If you are interested in details of
risk management the go to referecne is still
@McNeilEmbrechtsFrey2015.

## Empirical Value at Risk (VaR) and Its Limitations

So far, we have used parametric VaR based on the normal distribution. 
However, we can also estimate VaR empirically, directly 
from historical data, without assuming a particular distribution.

To compute the empirical 95% VaR, we:

1. Sort the historical log returns in ascending order.
2. Find the return at the 5th percentile of the empirical distribution.

For a **10-day VaR**, we use **weekly returns** rather than daily data.

```{r}

# Convert daily log returns to 5-day log returns by summing 
#over non-overlapping 5-day periods

log_returns_10d <- 
  colSums(matrix(log_returns, nrow = 5, 
                 byrow = TRUE), na.rm = TRUE)

# Sort 10-day log returns in ascending order
sorted_returns <- sort(na.omit(log_returns_10d))

# Compute empirical cumulative distribution function (ECDF)
n <- length(sorted_returns)
ecdf_values <- seq(1, n) / n  # Explicitly named for clarity

# Identify empirical quantiles for 95% and 99% VaR
VaR_95_empirical <- sorted_returns[min(which(ecdf_values >= 0.05))]
VaR_99_empirical <- sorted_returns[min(which(ecdf_values >= 0.01))]

# Output results
sprintf("Empirical 95%% VaR: %.4f (10-day horizon)", VaR_95_empirical)
sprintf("Empirical 99%% VaR: %.4f (10-day horizon)", VaR_99_empirical)

```


The 95% empirical VaR** suggests that, based purely on historical data, 
the worst 5% of observed weeks had returns of at least `r round(VaR_95_empirical,2)*100`% or lower. 
The 99% empirical VaR tells us that in the worst 1% of historical weeks, 
losses exceeded `r round(VaR_99_empirical,2)`.

Let's look at a visualization:

```{r}
#| code-fold: true
#| echo: false
#| fig-align: "center"
#| fig-cap: "Empirical CDF of 5-Day Log Returns with 95% and 99% VaR Thresholds"

# Load necessary package
library(ggplot2)

# Ensure log_returns is a multiple of 5 (drop extra observations)
n_trimmed <- floor(length(log_returns) / 5) * 5
log_returns_trimmed <- log_returns[1:n_trimmed]

# Convert daily log returns to 5-day log returns
log_returns_5d <- colSums(matrix(log_returns_trimmed, nrow = 5, byrow = TRUE), na.rm = TRUE)

# Sort 5-day log returns in ascending order
sorted_returns <- sort(na.omit(log_returns_5d))

# Compute empirical cumulative distribution function (ECDF)
n <- length(sorted_returns)
ecdf_values <- seq(1, n) / n  

# Create a data frame for visualization
ecdf_data <- data.frame(
  sorted_returns = sorted_returns,
  ecdf_values = ecdf_values
)

# Identify empirical quantiles for 95% and 99% VaR
VaR_95_empirical <- sorted_returns[min(which(ecdf_values >= 0.05))]
VaR_99_empirical <- sorted_returns[min(which(ecdf_values >= 0.01))]

# Improved ECDF plot with properly shifted labels
ggplot(ecdf_data, aes(x = sorted_returns, y = ecdf_values)) +
  geom_step(color = "blue", linewidth = 1) +  # ECDF curve
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red", linewidth = 1) +  # 95% VaR line
  geom_hline(yintercept = 0.01, linetype = "dashed", color = "darkred", linewidth = 1) + # 99% VaR line
  geom_vline(xintercept = VaR_95_empirical, linetype = "dashed", color = "red", linewidth = 1) + # VaR 95%
  geom_vline(xintercept = VaR_99_empirical, linetype = "dashed", color = "darkred", linewidth = 1) + # VaR 99%
  annotate("text", x = VaR_95_empirical - 0.01, y = 0.05, label = sprintf("VaR 95%%: %.4f", VaR_95_empirical), 
           color = "red", hjust = 1, size = 5) +  # Proper left shift
  annotate("text", x = VaR_99_empirical - 0.01, y = 0.01, label = sprintf("VaR 99%%: %.4f", VaR_99_empirical), 
           color = "darkred", hjust = 1, size = 5) + # Proper left shift
  labs(title = "Empirical CDF of 5-Day Log Returns with VaR Thresholds",
       x = "5-Day Log Returns",
       y = "Cumulative Probability") +
  theme_minimal(base_size = 14)


```

Empirical VaRs are straightforward but they also have
**important limitations**, 
particularly with small samples:

- If we only have a few years of weekly returns, 
the 5th percentile may be based on very few observations.
- VaR then depends heavily on the worst few weeks, making it unreliable.
- The 99% quantile requires even fewer observations, making it 
very sensitive to individual extreme weeks.
- More advanced techniques (such as Extreme Value Theory (EVT)) 
can help estimate tail risks beyond observed data.
- If we haven't observed an extreme event, empirical VaR ignores it.
- This happened in the case of **Long-Term Capital Management (LTCM)**.

**Example: The LTCM Case: A Real-World Lesson**
In the late 1990s, the hedge fund **Long-Term Capital Management (LTCM)** 
collapsed due to ignoring large, low-probability tail risks. 
The fund's risk models were based on historical market behavior, 
assuming that extreme losses were too unlikely to be of concern. 

However, during the 1998 Russian financial crisis, markets 
experienced far greater volatility than LTCM had anticipated. 
The fund suffered catastrophic losses, requiring a \$3.6 billion bailout 
coordinated by the Federal Reserve to prevent wider market contagion.

For those of you who are interested 
in **advanced risk modeling**, a deeper 
discussion can be found in @McNeilEmbrechtsFrey2015.

## Data and statistics

Over the course we have now so many times estimated moments
for log-returns and then plugged theses estimates into our
software provided functions. It seems necessary at this stage
to clarify a few things about the statistics of return data.

Unlike in probability, where we start from the assumption of a
random model, typically one or many random variables and think
about the consequences for the outcomes, like the properties
shape and moments of the distribution and so on, in statistics we
take the reverse perspective. We observe data and then try to find out 
what could be the random variables that might have generated these
data, if there is a random process in the background of our observations.

I would therefore like to discuss some key issues in the empirical analysis
of return data. The key data source for estimation is historical returns
data, which are today available on the internet at daily frequency. This
approach is reasonably reliable fro some parameters such as variances and
covariances. It is, however, decidedly **unreliable** for other
parameters such as expected return. The reason why I want to discuss 
this problem is that the root cause is a fundamental limitation of the
estimation process not the quality of the data or measurement.

### Peridod-Length Effects

Suppose that the annual return of a stock is $1+r_y$. It can be
thought of as the result of 12 monthly returns and can be written
as a product
$$
1+r_y = (1+r_1)(1+r_2)(1+r_3)\dots (1+r_{12})
$$
Note that in this equation tge monthly returns are not measured per annum. They 
are the actual returns over a month. If the returns are small, we can expand the
product and keep only the first order terms as a good approximation:
$$
1+r_y \approx 1 + r_1 + r_2 + r_3 + \dots + r_{12}
$$
In this approximation the compounding effects are ignored, which is for the
purpose of a rough estimates of orders of magnitude of
parameters good enough.

Now let's think about these returns from the perspective of probability theory and
imagine that there is an underlying random variable model generating them. Let these
random variables be mutually uncorrelated  and each monthly return $r_i$ has the
same expected value $\bar{r}$ and the same variance $\sigma^2$. using our approximation
we find that
$$
\bar{r_y} = 12\, \bar{r}
$$
Likewise
$$
\sigma_y^2 = \mathbb{E}\left[ \sum_{i=1}^{12}(r_i-\bar{r}) \right] = \mathbb{E}\left[ \sum_{i=1}^{12}(r_i - \bar{r})^2 \right] = 12 \sigma^2
$$
where the pull of the exponent into the squared brackets is a consequence of the
assumption that the returns are uncorrelated. Now turn these equations
around and taking the suqare root of the variance, we obtain an expression
for the monthly values in terms of annual values
\begin{eqnarray*}
\bar{r} &=& \frac{1}{12} \bar{r}_y \\
\bar{\sigma} &=& \frac{1}{\sqrt{12}} \sigma_y
\end{eqnarray*}
This can be generalized to any  length of period. If the period
is $p$ part of a year (expressed as a fraction of a year) then the expected
return and the standard error of the 1-period rate of return can be found by
generalizing from monthly periods $p = 1/12$. This gives us

\begin{eqnarray*}
\bar{r_p}&=& p \, \bar{r}_y \\
\bar{\sigma_p}&=& \sqrt{p} \, \sigma_y
\end{eqnarray*}

Because the expected return decreases linearly
with the period, the standard deviation is 
proportional to the square root of the length
of the period. Therefore the ratio if the two
**increases dramatically** as the length is
reduced. In the limit, as the
length goes to zero, this ratio diverges. Thus
rates of return for small perios have high standard deviations
compared to their expected values.

Let#s put this into perspective. The mean annual return for stocks
ranges from around $6%$ to $30%$ with a typical value at around $12%$. These
mean values change over time so any particual value is meaningful roughly for 
about 2 or three years. The standard deviation of yearly stock returns
ranges from 10% to 60% with typically 15%.

Let#s translate these numbers into monthly values, 
thus $p=1/12$. With $\bar{r}_y = 12 %$ and 
and $\sigma_y = 15%$ this leds to $r_{1/12} = 1% and $\sigma_{1/12} = 4.33%.
So while for the yearly figure the ratio is 1.25 it is 4.3 for the monthly.  

If we assume the returns are generated through independent daily returns and
assume 25o trading days then $\r_{1/250} = 0.048$ % and $\sigma_{1/250} = 0.95$ %.
The ratio is now 19.8.

Now we can show how this amplification effect makes the estimation
of expected mean rates nearly impossible. Let's select a basic 
period length $p$ and try to estimate the mean for this period. We assume that 
the returns of each period are independent random variables with 
mean $\bar{r}$ and standard error $\sigma$. We also assume that individual returns
are mtually uncorrelated.

Suppose we have $n$ samples of period returns The best estimate for the mean is
$$
\hat{\bar{r}} = \frac{1}{n} \sum_{i=1}^n r_i
$$

The estimate is itself a random variables. If we used different samples, we got
different v alues for this estimate. However the expected value of the estimate is 
the true value
$$
\mathbb{E}(\hat{\bar{r}}) = \hat{\mathbb{E}} \left( \frac{1}{n} \sum_{i = 1}^n r_i \right) = \bar{r}
$$
We compute the standard deviation of the estimate to asess the accuracy of
the estimator for the mean returns.
$$
\sigma_{\hat{\bar{r}}}^2 = \mathbb{E} \left[ (\hat{\bar{r}} - \bar{r})^2 \right] = \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n (r_i - \bar{r}) \right]^2 = \frac{1}{n} \sigma^2
$$
Hence
$$
\sigma_{\hat{\bar{r}}} = \frac{\sigma}{\sqrt{n}}
$$

This is a standard formula for the error in the estimate of a mean value.

If the period is 1 month, the monthly values used earlier ar
$\bar{r} = 1$ % and $\sigma = 4.33$ %. If we use 12 month of data we get
$$
\sigma_{\hat{\bar{r}}} = \frac{4.33}{\sqrt{12}}
$$
which is 1,25 %. The standard error of the mean return estimate is then larger
than the mean return itself. If we use 4 years of data we can cut this standard
deviation by a factor 2, which still must count as a very poor estimate. For an 
estimate
to be considered good we need to be able to cut down the standard deviation 
to about 1/10th of the mean. 
This would require about $n = 43.3^2 = 1875$ or about 156 years of data.

This is a well known problem in empirical finance which has even a name. It is
called the **historical blur** problem for the measurement of $\bar{r}$. It is
basically impossible to measure $\bar{r}$ to within workable accuracy using
historical data. The problem can not be improved much by changing the
period length. If longer periods are used, each sample is more reliable but 
fewer independent samples are obtained in any year. If the period is smaller, 
more samples are available but each is worse in terms of the ratio of standard 
deviation to mean value.

## Introdcution to Monte Carlo Simulation in Finance

Monte Carlo simulation is a computational technique that uses 
repeated random sampling to estimate uncertain outcomes. It is 
particularly useful when dealing with **probabilistic models** 
and **complex systems** where analytical solutions are difficult 
or impossible to derive. It is also an excellent application context
where we can discuss how to optimize speed and efficiency in R
code.

The term Monte Carlo Simulation originates from the 
Manhattan Project during World War II, where scientists, 
including Stanislaw Ulam and John von Neumann, used random 
sampling techniques to model complex physical processes, such 
as neutron diffusion in nuclear reactions. The name "Monte Carlo" 
was inspired by the famous Monte Carlo Casino in Monaco, reflecting 
the method's reliance on randomness and probability—just like 
games of chance in a casino.

Monte Carlo methods are used extensively in computational Finance.
Financial markets are inherently uncertain, and Monte Carlo methods allow 
us to model this uncertainty by simulating a large number of 
possible outcomes. Some key applications in finance include:

- **Risk estimation** (e.g., computing Value at Risk).  
- **Pricing derivatives** (e.g., options pricing using risk-neutral Monte Carlo methods).  
- **Portfolio optimization** (e.g., estimating expected returns and volatility distributions).  

In this lecture, we focus on **estimating Value at Risk (VaR)** which we have
discussed from a conceptual viewpoint in this lecture and where we have focussed
on cases of normally distributed log returns of stocks. We also briefly touched
on the approach of historical simulation, where past return data are used
to estimated the risk

In contrast to these methods Monte Carlo Simulation simulates future returns using
random draws based on estimated statistical properties like mean and
standard deviation. This is an apporach where the computer can shine.

### Simulating Portfolio Returns for VaR Calculation Step-by-Step Implementation in R

We will now implement a **basic Monte Carlo simulation** in R to estimate VaR. 

The approach consists of:

1. **Estimating the portfolio’s mean return and standard deviation** based on historical data.  
2. **Generating thousands of random return scenarios** assuming a normal distribution.  
3. **Calculating the simulated portfolio losses** and extracting the VaR from the loss distribution.  
 

#### Step 1: Simulating Portfolio Returns


We begin by assuming that **log-returns** of a portfolio 
follow a normal distribution, which is a common assumption in 
risk modeling. 

Given **historical return data**, we estimate 
the **mean** and **standard deviation**, then simulate 
thousands of potential returns. I do not do an explicit
estimation in the follwoing code chunk because we have done
so explicitly at many parts in this lecture. This could be
done with historical data (keeping the blur problem in mind) and then
- since the normal distribution is fully characterized by mean and standard 
deviation - we can use the normality assumption to pin down 
a simulation distribution. For the sake of demonstration I do
10000 simulations here fro somewhat typical returns parameter
values. You can play with this. This can be in principle
scaled up a lot. Note also that we set a random seed for 
reproduceability here. For the visualization I take the
`ggplot2` package, which I think, produced the visualy most
appealing results in R.

```{r}
# Load necessary packages
library(ggplot2)  # For visualization


# Set seed for reproducibility
set.seed(123)

# Simulated historical daily log-returns (e.g., from a stock index)
historical_returns <- rnorm(250, mean = 0.0005, sd = 0.01)  # 250 trading days

# Estimate parameters (mean and standard deviation)
mu <- mean(historical_returns)
sigma <- sd(historical_returns)

# Simulate 10,000 future return scenarios using Monte Carlo
n_sim <- 10000
simulated_returns <- rnorm(n_sim, mean = mu, sd = sigma)

# Quick visualization of simulated returns
ggplot(data.frame(returns = simulated_returns), aes(x = returns)) +
  geom_histogram(bins = 50, fill = "blue", alpha = 0.5) +
  labs(title = "Histogram of Simulated Portfolio Returns",
       x = "Simulated Return", y = "Frequency")
```

#### Step 2: Estimating Value at Risk (VaR)

Once we have simulated thousands of possible portfolio returns, we 
extract **Value at Risk (VaR)** from the distribution. 
Since **VaR is the quantile of the loss distribution**, we 
take the **left-tail quantile** (e.g., 5% or 1% quantile).

```{r}
# Compute portfolio losses (negative returns)
simulated_losses <- -simulated_returns  # Losses are negative returns

# Compute VaR at 95% and 99% confidence levels
VaR_95 <- quantile(simulated_losses, probs = 0.95)
VaR_99 <- quantile(simulated_losses, probs = 0.99)

# Print results
cat("Monte Carlo Estimated VaR:\n")
cat("95% VaR:", round(VaR_95, 4), "\n")
cat("99% VaR:", round(VaR_99, 4), "\n")
```

The raw VaR numbers reported in our Monte Carlo simulation—
e.g., **95% VaR = 0.0151**—represent the potential **daily loss** as 
a fraction of the portfolio value. However, to make this 
result more intuitive, let’s express it in **real monetary terms**.

Suppose we are managing a $10 billion USD portfolio. 
The 95% VaR of 0.0151 means:  

- **Daily Loss Interpretation**:  
  
Under normal market conditions, there is a 95% probability that the portfolio 
will lose no more than  \$ 151 million USD in a single trading 
day (1.51% of \$ 10 billion).  
Conversely, there is a 5% chance that losses will exceed this amount.  

- **Yearly Exceedance Frequency**:  

Since 5% of trading days (approximately 12–13 days per year in 
a 250-day trading year) fall in the worst-case loss category,  
we expect to lose more than $151 million USD on about 12–13 days per year.  

You can do this kind of translation for the 99% VaR for yourself. You can
even support your intuition by leveraging the power of R to write a function
yourself to make such interpretations dynamic for arbitrary 
portfolio sizes and let R do the interpretation for you. Let us
call this function `interpret_VaR`:

```{r}
# Function to interpret Monte Carlo VaR results
interpret_var <- function(VaR_95, VaR_99, portfolio_value, trading_days = 250) {
  # Compute monetary losses
  VaR_95_dollars <- VaR_95 * portfolio_value
  VaR_99_dollars <- VaR_99 * portfolio_value
  
  # Compute expected exceedance frequency
  # ~5% exceedance
  days_exceeding_95 <- trading_days * (1 - 0.95)
  # ~1% exceedance
  days_exceeding_99 <- trading_days * (1 - 0.99)  
  
  # Print interpretation
  cat("Monte Carlo Value at Risk (VaR) Interpretation:\n")
  cat("-------------------------------------------------\n")
  cat("Portfolio Value: $", format(portfolio_value, big.mark = ","), "\n\n")
  
  cat("95% VaR Interpretation:\n")
  cat("- Expected daily loss will not exceed", 
      round(VaR_95 * 100, 2), "% in 95% of cases.\n")
  cat("- This corresponds to a daily loss limit of approximately $", 
      format(round(VaR_95_dollars, 2), big.mark = ","), ".\n")
  cat("- However, about", 
      round(days_exceeding_95, 1), "days per year, losses may exceed this level.\n\n")
  
  cat("99% VaR Interpretation:\n")
  cat("- Expected daily loss will not exceed", 
      round(VaR_99 * 100, 2), "% in 99% of cases.\n")
  cat("- This corresponds to a daily loss limit of approximately $", 
      format(round(VaR_99_dollars, 2), big.mark = ","), ".\n")
  cat("- However, about", 
      round(days_exceeding_99, 1), "days per year, losses may exceed this level.\n")
}

# Example usage: Assume a portfolio of $10 billion
portfolio_value <- 10e9  # $10 billion

# Call the function with Monte Carlo estimated VaR
interpret_var(0.0151 , 0.0214 , portfolio_value)

```

#### Step 3: Optimizing the Monte Carlo Simulation

A naive Monte Carlo approach can be slow for large-scale simulations. 
To **optimize performance**, we introduce:

- **Vectorization**: Using matrix operations instead of loops.
- **Parallel computing**: Running simulations in parallel using the `future.apply` package.
- **Efficient data handling**: Using `data.table` instead of standard data frames.


These are excellent points! To **expand and refine** this section, I’ll structure it as follows:

---

### **Step 3: Optimizing the Monte Carlo Simulation**
Monte Carlo simulations can become computationally expensive when scaled up, making **performance optimization** an important consideration. 

We introduce three key techniques:  
1. **Vectorization** – Using efficient matrix operations instead of loops.  
2. **Parallel Computing (`future.apply`)** – Distributing tasks across multiple CPU cores.  
3. **Efficient Data Handling (`data.table`)** – Reducing memory overhead and improving speed.  


R has built in functionality for parallel computing. Here we will
discuss parallel computing with `future.apply`.

So what us `future.apply` and how does t help?
The `future.apply` package allows us to run 
**apply-type functions in parallel** across multiple CPU 
cores, improving computational speed when processing large datasets.
Normally, R executes code **sequentially** (one operation at a time).  
What `future.apply` does is that it enables **parallel execution**, meaning 
tasks run simultaneously across multiple CPU cores, reducing computation time.  

Parallelization **is essential** when:

- You need to **simulate millions of scenarios** (e.g., high-frequency Monte Carlo simulations).  
- Your computations involve **nested loops** (e.g., pricing derivatives across many strike prices).  
- You work with **large financial datasets** that slow down single-threaded processing.

However, parallelization is not always beneficial for small-scale 
problems due to **overhead costs** of setting up parallel workers.


Let us compare sequential vs. parallel Monte Carlo Simulation to demonstrate
these points.
We first run a **single-core Monte Carlo simulation** and then 
compare it to a **parallelized version**.

```{r}
# Load necessary package
library(future.apply)

# Set up parallel processing using all available CPU cores
plan(multisession)

# Define Monte Carlo function for Value at Risk (VaR)
monte_carlo_var <- function(n_sim, mu, sigma, confidence = 0.95) {
  simulated_losses <- -rnorm(n_sim, mean = mu, sd = sigma)
  return(quantile(simulated_losses, probs = confidence))
}

# Define number of simulations
n_sim <- 100000  # Large-scale simulation

# Measure time for sequential computation (single core)
start_time_seq <- Sys.time()
VaR_95_seq <- sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95))
end_time_seq <- Sys.time()
time_seq <- as.numeric(difftime(end_time_seq, start_time_seq, units = "secs"))  # Convert to numeric seconds

# Measure time for parallel computation (multiple cores)
start_time_par <- Sys.time()
VaR_95_par <- future_sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95), future.seed = TRUE)
end_time_par <- Sys.time()
time_par <- as.numeric(difftime(end_time_par, start_time_par, units = "secs"))  # Convert to numeric seconds

# Print timing results
cat("Execution Time Comparison:\n")
cat("Sequential Execution Time: ", round(time_seq, 2), " seconds\n")
cat("Parallel Execution Time: ", round(time_par, 2), " seconds\n")
cat("Speedup Factor: ", round(time_seq / time_par, 2), "x faster with parallel computing\n")

```

Let’s break down the key steps in 
our parallel Monte Carlo simulation for VaR 
and explain why each part matters.  

```{r}
#| eval: false

library(future.apply)
plan(multisession)
```

`future.apply` is an enhanced version of `apply()` functions** that supports parallel execution.
`plan(multisession)` tells R to use multiple CPU cores instead of executing code sequentially.
Normally, R runs tasks one at a time (single-threaded).
With `multisession`, R splits tasks across CPU cores, reducing 
execution time for large simulations.

```{r}
#| eval: false
monte_carlo_var <- function(n_sim, mu, sigma, confidence = 0.95) {
  simulated_losses <- -rnorm(n_sim, mean = mu, sd = sigma)
  return(quantile(simulated_losses, probs = confidence))
}
```

We generate `n_sim` random portfolio losses from a normal distribution 
(negating returns to represent losses).  
We extract the VaR quantile from the simulated loss distribution. 
This function encapsulates our Monte Carlo 
simulation into a modular, reusable block of code.
Later, we parallelize multiple independent simulations 
using this function.

```{r}
#| eval: false

start_time_seq <- Sys.time()
VaR_95_seq <- sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95))
end_time_seq <- Sys.time()
time_seq <- as.numeric(difftime(end_time_seq, start_time_seq, units = "secs"))
```
 
We measure execution time for the sequential (single-core) simulation.  
`sapply(1:10, function(x) ...)` runs 10 independent Monte Carlo simulations.  
This serves as our baseline for performance comparison.
We need to measure the cost of 
computing independent Monte Carlo simulations one by one.

```{r}
#| eval: false
start_time_par <- Sys.time()
VaR_95_par <- future_sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95), future.seed = TRUE)
end_time_par <- Sys.time()
time_par <- as.numeric(difftime(end_time_par, start_time_par, units = "secs"))
```

We now run 10 Monte Carlo simulations in parallel, distributing them across CPU cores.
`future.seed = TRUE` ensures statistically valid random number generation 
across parallel workers.
Unlike sequential execution, multiple CPU cores now share the workload.
This should reduce execution time, but the actual gain 
depends on the computational task and system configuration.

The last step just compares performance

At first glance, the results of our output seems counterintuitive—why is 
parallel execution **slower** than sequential execution here?  
It is key to understand that parallelization does not always improve speed.
Small tasks don’t benefit from parallel execution.

  - The **overhead cost** of setting up parallel workers (process spawning, memory sharing, inter-process communication) can **exceed the time saved** for small problems.  
- **Parallelization shines for large-scale computations**.  

Let’s increase the number of simulations to **10 million (`1e7`) per iteration** 
to demonstrate when parallel computing becomes a necessity.

```{r}
# Define larger-scale simulation
n_sim <- 1e7  # 10 million simulations per iteration

# Run the comparison again
start_time_seq <- Sys.time()
VaR_95_seq <- sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95))
end_time_seq <- Sys.time()
time_seq <- as.numeric(difftime(end_time_seq, start_time_seq, units = "secs"))

start_time_par <- Sys.time()
VaR_95_par <- future_sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95), future.seed = TRUE)
end_time_par <- Sys.time()
time_par <- as.numeric(difftime(end_time_par, start_time_par, units = "secs"))

cat("Execution Time Comparison (Larger Simulations):\n")
cat("Sequential Execution Time: ", round(time_seq, 2), " seconds\n")
cat("Parallel Execution Time: ", round(time_par, 2), " seconds\n")
cat("Speedup Factor: ", round(time_seq / time_par, 2), "x faster with parallel computing\n")
```

Now we see a doubling of speed achieved by our prallelisation. You can play
around yourself to get a feeling for the performance enhancements you may
gain for computationally large problems. Maybe you can go back to the
very first probelm we dealt with in this coures financial-transaction-identifyer
collision probability ofr large transaction volume and large output space $M$.

Let's visualize the execution time comparison of our example:

```{r}
# Load necessary package
library(ggplot2)

# Simulated execution times (adjust these with actual results if needed)
execution_times <- data.frame(
  Simulation_Size = factor(c("100,000 Sims", "10,000,000 Sims"), 
                           levels = c("100,000 Sims", "10,000,000 Sims")),
  Execution_Time = c(0.07 , 1.22, 6.17, 2.85),  # Replace with actual results
  Method = rep(c("Sequential", "Parallel"), each = 2)
)

# Create bar plot
ggplot(execution_times, aes(x = Simulation_Size, y = Execution_Time, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
  labs(title = "Execution Time Comparison: Sequential vs. Parallel Monte Carlo Simulation",
       x = "Simulation Size",
       y = "Execution Time (seconds)") +
  scale_fill_manual(values = c("blue", "red")) +
  theme_minimal()

```

Here are the key Takeaways from the visualization:

For small-scale simulations (100,000 draws), parallelization adds 
overhead and is actually slower.
For large-scale simulations (10 million draws), parallel computing 
significantly reduces execution time.
The break-even point depends on system resources, but the lesson 
is clear: parallel computing is most useful for large problems.

If we need even more computational power, tools such 
as GPU-based parallelization can help:

- `gpuR`: Allows running matrix operations on GPUs in R.
- `torch (for R)`: Enables deep learning-like parallel computations on GPUs.
- `cuda.ml`: Uses NVIDIA’s CUDA for ultra-fast numerical processing.

These methods are beyond the scope of our discussion but become crucial 
for high-frequency trading, derivative pricing, and deep 
learning applications in finance.

#### Efficient Data Handling with `data.table`

`data.table`is an R package that increases the efficiency of data handling
and becomes useful if you are dealing with big datasets. Monte Carlo Simulation
is a typical context where such issues can arise quickly. In this case
the use of `data.table`brings enhanced memory efficiency: Standard
`data.frame` operations create copies of data, increasing memory usage. 
`data.table` avoids this by modifying objects in place.  
`data.table` is optimized for speed. It can process 
millions of rows significantly faster than `data.frame`.  

Let's discuss an example where we compare data frames and the
`data.table`package.
Let's compare performance for **storing and manipulating simulated losses**.

```{r}
# Load the data.table package
library(data.table)

# Generate a large dataset of simulated losses
n_sim <- 1e6  # 1 million simulations
simulated_losses <- -rnorm(n_sim, mean = mu, sd = sigma)

# Store as a standard data frame
df_losses <- data.frame(losses = simulated_losses)

# Store as a data.table
dt_losses <- data.table(losses = simulated_losses)

# Measure time to compute quantiles using data.frame
start_df <- Sys.time()
VaR_95_df <- quantile(df_losses$losses, probs = 0.95)
end_df <- Sys.time()
time_df <- as.numeric(difftime(end_df, start_df, units = "secs"))  # Convert to numeric

# Measure time to compute quantiles using data.table
start_dt <- Sys.time()
VaR_95_dt <- dt_losses[, quantile(losses, probs = 0.95)]
end_dt <- Sys.time()
time_dt <- as.numeric(difftime(end_dt, start_dt, units = "secs"))  # Convert to numeric

# Print timing results
cat("Execution Time Comparison for VaR Calculation:\n")
cat("Data Frame Execution Time: ", round(time_df, 4), " seconds\n")
cat("Data Table Execution Time: ", round(time_dt, 4), " seconds\n")
cat("Speedup Factor: ", round(time_df / time_dt, 2), "x faster with data.table\n")

```

### Reporting and Documentation of R Code

 Monte Carlo simulations often generate large volumes of data, and proper 
 documentation and reporting are essential for: 
 
- **Reproducibility**: Ensuring others (or future you) can 
    understand and replicate your work.  
- **Clarity**: Providing a clear summary of simulation results, 
    assumptions, and methodology.  
- **Communication**: Making results accessible to different 
    stakeholders (e.g., researchers, risk managers).  

---

#### Writing Readable and Reproducible R Code  

Here are three best practice tips:

1. **Use clear function names and comments**  
   - Avoid cryptic variable names.  
   - Add comments to explain key steps.
   
2. **Include metadata in scripts**  
   - Define **input parameters** at the beginning.  
   - Set a **random seed** for reproducibility.  
   
3. **Use docstrings with `roxygen2`** (for functions in packages).

The `roxygen2` package is a widely used tool for automatically 
generating documentation for R functions, especially in package 
development. Instead of manually writing separate documentation 
files, roxygen2 allows you to write structured 
docstrings directly above function definitions using 
specially formatted comments (#').

Why use `roxygen2`? Here are three reasons:

1. Consistency – Ensures documentation is always in sync with the function code.
2. Efficiency – Eliminates the need for manually maintaining help files.
3. Integration – Works seamlessly with RStudio, devtools, and pkgdown for package development.

`roxygen` works by extracting special comments (#') and converting them 
into R help files.
You can specify parameters, return values, usage examples, and references.
The devtools::document() function compiles the documentation.

For example, this is how a well documented Monte Carlo
function could look like.

```{r}
#' Monte Carlo Simulation for Value at Risk (VaR)
#'
#' This function estimates Value at Risk using Monte Carlo simulations.
#'
#' @param n_sim Number of simulations
#' @param mu Mean return of asset
#' @param sigma Standard deviation of returns
#' @param confidence Confidence level for VaR (default = 0.95)
#' @return Estimated VaR
#' @examples
#' monte_carlo_var(10000, 0.0005, 0.01, 0.95)
monte_carlo_var <- function(n_sim, mu, sigma, confidence = 0.95) {
  simulated_losses <- -rnorm(n_sim, mean = mu, sd = sigma)
  return(quantile(simulated_losses, probs = confidence))
}
```

- This uses **`roxygen2`** syntax for automatic documentation generation.  
- Calling `devtools::document()` in a package project 
generates documentation from comments.  


Once we run a Monte Carlo simulation, we need to **summarize and present** 
the results effectively.  


A structured summary table is often clearer than raw printouts.

```{r}
# Example results from Monte Carlo VaR estimation
Confidence_Level <- c("95%", "99%")
VaR_Percentage <- c(0.0151, 0.0214)  # Example VaR values
Portfolio_Value <- 10e9  # Assume $10 billion portfolio

# Compute VaR amount in monetary terms
VaR_Amount <- VaR_Percentage * Portfolio_Value

# Create a data frame
results <- data.frame(
  Confidence_Level = Confidence_Level,
  VaR_Percentage = VaR_Percentage,
  Portfolio_Value = Portfolio_Value,
  VaR_Amount = VaR_Amount
)

# Print formatted table
print(results)
```

Instead of manually copying outputs, **Quarto/RMarkdown** can 
generate reports dynamically.

Here’s the **Base R** version of the **Quarto (qmd) report snippet**, avoiding `ggplot2` and `dplyr`:

This is how a rewritten example report snippet in Quarto (`.qmd`) would
- for instance - look like:

```{r}
#| eval: false

---
title: "Monte Carlo Simulation Report"
author: "Your Name"
date: "`r Sys.Date()`"
format: html
---
  
# Load necessary base R functions
  
# Print results as an HTML table using knitr::kable
library(knitr)
kable(results, format = "html", caption = "Monte Carlo Estimated VaR")

# Define execution times (example data)
Simulation_Size <- c("100,000 Sims", "10,000,000 Sims")
Execution_Time_Sequential <- c(0.07, 25.3)  # Hypothetical values
Execution_Time_Parallel <- c(1.22, 5.8)

# Create bar plot using base R
barplot(
  height = rbind(Execution_Time_Sequential, Execution_Time_Parallel),
  beside = TRUE,
  col = c("blue", "red"),
  names.arg = Simulation_Size,
  legend.text = c("Sequential", "Parallel"),
  main = "Execution Time Comparison",
  xlab = "Simulation Size",
  ylab = "Execution Time (seconds)"
)

# Save summary table as CSV
write.csv(results, "monte_carlo_var_results.csv", row.names = FALSE)

# Save entire workspace (useful for large simulations)
save.image("monte_carlo_workspace.RData")
```

In a report you would of course interweave text and code, but the
example chunk gives you an idea. Quarto is very versatily and
visually appealing. For instance the lecture notes for this code
as well as the slides and all other materials are entirely written
in Quarto.



