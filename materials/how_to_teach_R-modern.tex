\documentclass[11pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[a4paper,margin=2.7cm]{geometry}
\usepackage{hyperref}
\usepackage{enumitem}

\title{Base-R--First, Workflow--First: A Minimal-Tooling Pedagogy for Teaching Probability with R}
\author{Martin Summer (notes drafted with ChatGPT)}
\date{\today}

\begin{document}
	\maketitle
	
	\paragraph{Context.}
	These notes respond to a pedagogical question motivated by my January bootcamp course in probability with R for a mixed cohort in quantitative and computational finance:
	\[
	\href{https://martin-summer-1090.github.io/Probability_Introduction/}{\texttt{martin-summer-1090.github.io/Probability\_Introduction/}}.
	\]
	The key issue is how to update the course to reflect current best practice in teaching R and RStudio, while avoiding a rigid ``tidyverse-first'' approach and instead emphasizing principles, computational thinking, and minimal tooling.
	
	\section*{Position in one sentence}
	A \emph{workflow-first} course design (Quarto, reproducibility, projects, simulation-as-thinking) is the genuinely modern part; a \emph{tidyverse-first} syntax is optional. In fact, a \emph{base-R-first} approach can be more future-proof today, especially in the age of LLM-assisted coding.
	
	\section*{Why base-R-first makes even more sense now (especially with LLMs)}
	If ``syntax is cheap,'' then the scarce skills students must develop are not primarily about remembering verbs or piping rules, but about reasoning:
	\begin{itemize}[leftmargin=*,itemsep=0.25em]
		\item \textbf{Computational problem formulation:} mapping a question into inputs $\rightarrow$ transformations $\rightarrow$ outputs.
		\item \textbf{Simulation literacy:} seeds, replication, Monte Carlo error, convergence intuition.
		\item \textbf{Verification habits:} sanity checks, invariants, dimensions, edge cases.
		\item \textbf{Code comprehension:} reading and interrogating code (including code generated by an LLM).
		\item \textbf{Reproducible practice:} organizing work so results can be rerun, shared, and audited.
	\end{itemize}
	Base R supports these goals cleanly because it keeps attention on objects, data structures, and semantics rather than on a parallel ``verb universe.''
	
	\section*{A modern but minimal ``tooling'' stack (not a Hadleyverse commitment)}
	Minimalism need not mean a 2005 workflow. A lean, contemporary setup can be:
	\begin{enumerate}[leftmargin=*,itemsep=0.25em]
		\item \textbf{Quarto project} as the course spine (notes, exercises, solutions, labs).
		\item \textbf{Base R} for the computational core: \texttt{read.csv}, indexing \texttt{[ ]}, \texttt{subset}, \texttt{aggregate}/\texttt{tapply}, \texttt{apply}, \texttt{replicate}, the RNG family (\texttt{rbinom}, \texttt{runif}, \texttt{rnorm}, \dots).
		\item \textbf{\texttt{renv}} to freeze package versions (even with few packages, it prevents January chaos).
		\item \textbf{Optional Git/GitHub} for distributing templates and collecting assignments (even if students do not love Git, seeing it once is valuable).
	\end{enumerate}
	Crucially, none of this requires a tidyverse-first curriculum.
	
	\section*{A base-R-first structure for a probability bootcamp course}
	For a mixed audience in a bootcamp phase, a coherent base-R curriculum can be organized around three pillars.
	
	\subsection*{Pillar A: Simulation as the primary microscope}
	Lean hard into simulation early:
	\begin{itemize}[leftmargin=*,itemsep=0.25em]
		\item \texttt{set.seed()}, \texttt{sample()}, \texttt{rbinom()}, \texttt{runif()}, \texttt{rnorm()} as foundational tools.
		\item \texttt{replicate()} and vectorization; explicit loops when they improve clarity.
		\item Monte Carlo error as a first-class concept: repeated simulations to quantify noise and stability.
	\end{itemize}
	This aligns with the core probabilistic ideas (LLN/CLT) and builds computational confidence quickly.
	
	\subsection*{Pillar B: ``Thinking in objects'' and data structures}
	Students should become fluent in what R \emph{is}:
	\begin{itemize}[leftmargin=*,itemsep=0.25em]
		\item atomic vectors vs.\ lists vs.\ matrices vs.\ data frames;
		\item indexing as a superpower: \texttt{x[i]}, \texttt{x[x > 0]}, \texttt{x[order(x)]}, \texttt{which()}.
		\item writing small functions early: \texttt{f <- function(...) \{ ... \}}.
	\end{itemize}
	This supports both probability content (random variables as objects) and later professional work.
	
	\subsection*{Pillar C: Verification habits (the under-taught skill)}
	The ``LLM era'' upgrade is to teach students how to trust results appropriately:
	\begin{itemize}[leftmargin=*,itemsep=0.25em]
		\item invariants (probabilities sum to one; expectations in bounds; monotonicity);
		\item dimension checks (\texttt{stopifnot()}, \texttt{length()}, \texttt{dim()});
		\item compare simulation to known results (LLN/CLT as unit tests for reasoning).
	\end{itemize}
	Students will generate code; the course teaches them to \emph{audit} it.
	
	\section*{Where the tidyverse can still appear (without becoming a religion)}
	A pragmatic compromise is:
	\begin{itemize}[leftmargin=*,itemsep=0.25em]
		\item \textbf{Base track} as the core course (required).
		\item \textbf{Tidy track} as an appendix or optional translation layer: ``here is the same idea in a pipeline style.''
	\end{itemize}
	This frames tidyverse as a convenience layer and avoids turning it into ``what R is.''
	
	\section*{One cutting-edge, minimal addition: ``LLM-generated code audit'' exercises}
	A high-payoff exercise format is to give students a short code snippet (possibly produced by an LLM) and ask:
	\begin{enumerate}[leftmargin=*,itemsep=0.25em]
		\item What is it trying to compute?
		\item What assumptions does it make (distributional, independence, parameterization, scaling)?
		\item What checks would you add (invariants, dimensions, edge cases)?
		\item Can you construct a counterexample where it breaks or misleads?
	\end{enumerate}
	This trains transferable competence: reading code, validating results, and reasoning under uncertainty.
	
	\section*{Closing remark}
	In short: keep the modern \emph{workflow} (Quarto, reproducibility, project structure), keep the \emph{thinking} (simulation and verification), and keep the \emph{tooling minimal}. Base R is not a retreat from modernity---it can be the cleanest way to teach durable principles.
	
\end{document}
