---
title: "Jelena_Simic_Project_1"
format: html
editor: visual
---

## Student: Jelena Simic

### Project: Designing transaction identifiers for digital payment systems

Calculating the SHA-256 hash function

```{r}
# install.packages("digest")
# install.packages("purrr")
library(digest)
library(purrr)
```

Firstly we define functions

**get_transaction_id** which gives as result n random transaction_ids from M range of numbers. Those transaction identificators are not yet hashed!

```{r}
get_transaction_id <- function(M, n) {
  sample(1:M, n) # tacka 1 - uzimam random n brojeva izmedju 1 i M 
}
```

**hash_transaction_id** which hashes our transactions given the id and on the type of hashing algorithm

```{r}
hash_transaction_id <- function(id, hashing_alg="sha256") {
  digest(id, algo=hashing_alg)
}
```

**generate_result** that takes our 2 previous functions combines it, checks for duplicates after hashing and then, after doing that 1000 times checks for average number of times that hashed function failed.

-   Problem (?) - Should I check how many times was the hashed string been duplicated? For example if id2, id3, id4 are all hashed into x234, any(duplicated(hashed)) will just say hey it was duplicated but not how many times. Is this info important?

```{r}
generate_result <- function(M, n, hashing_alg = "sha256", trials = 1000) {
  res <- replicate(trials, {
    transactions <- get_transaction_id(M, n)
    hashed <- sapply(transactions, 
                     function(id) {hash_transaction_id(id, hashing_alg)})
    any(duplicated(hashed))
  })
  mean(res)
}
# generate_result(10^6, 1000)
```

This is our simulation window. We have some values for M -\> M_s and values for n -\> n_s. We want to play around and see if for any number of transactions taken from how many big of a pool range can we get a duplicate somewhere?

It's extremely slow, especially when n \>= 10\^6. I could have probably optimized it a bit.

Maybe solution idea, but could be even slower (didn't check), to have a map of hashed values that are being added as soon as it hashes. (Map: key: hashed value of transaction id, value: counter how many times it occured already). If after adding it to the map value is \> 1, we can break our further computation and return true because duplication occurred.

```{r}
M_s <- c(10^6, 10^9)
n_s <- c(10^3, 10^6, 10^9)
pairs <- subset(expand.grid(M=M_s, n=n_s), M > n)
simulation <- function(M_s, n_s) {
  means <- pmap_dbl(pairs, function(M, n) generate_result(M, n), .progress=TRUE)
  # means <- apply(pairs, 1, function(pair) {
  #   M <- pair[1]
  #   n <- pair[2]
  #   print(M)
  #   print(n)
  #   generate_result(M, n)
  # })
  means
}
results <- simulation(M_s, n_s)
```

![](images/paste-0A3F24FC.png)

Lower numbers like n = 1000, can be calculated, and usual result is 0 duplications occurred.

```{r}

generate_result(M = 10^6, n = 10^3, hashing_alg = "sha256")
```

```{r}

generate_result(M = 10^6, n = 10^4, hashing_alg = "sha256")
```
