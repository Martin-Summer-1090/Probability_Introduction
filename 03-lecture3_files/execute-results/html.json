{
  "hash": "aae19b95caadc42cbf72b75f86cc1e9e",
  "result": {
    "engine": "knitr",
    "markdown": "# Conditional Probability\n\nIn this lecture we are going to discuss the concept of\n**conditional probability**. The notion of conditional\nprobability is a basic tool of probability theory which has\nparticular relevance in Finance. The ideas of conditional\nprobability are very simple but often obscured by a clumsy\nterminology. \n\nTo intuitively appreciate the significance of conditional probability\nfirst, let us start with a \nsimple but striking scenario from the world of finance. Imagine you are an \ninvestor trying to evaluate the safety of a bond portfolio. On paper, the \nbonds are rated highly by reputable agencies, and the portfolio looks \ndiversified—a dream investment. However, during a global recession, a \nfew defaults occur, and to your surprise, the entire portfolio begins \nto unravel, causing significant losses. How could this happen?\n\nThis seemingly safe portfolio turned risky because it underestimated the \nconnections between events—a core idea tied to conditional probability. \nConditional probability helps us analyze how the risk of one event changes \ngiven another has occurred. It enables us to model dependencies, a \ncritical factor in real-world scenarios, particularly during financial crises.\n\nUnderstanding these relationships isn't just an intellectual exercise—it’s \ncrucial for preventing catastrophic losses. Whether you're pricing complex \nfinancial instruments, assessing credit risk, or making investment \ndecisions, mastering conditional probability allows you to account \nfor interconnected risks and avoid misleading conclusions based on \noversimplified assumptions.\n\n## Why neglecting conditional probability may be expensive: A case study.\n\nWith this motivation in mind let us turn to a historical example that \ndemonstrates the importance of understanding conditional probability: the \nfinancial crisis of 2007-2008. This crisis revealed how wrong assumptions \nabout independence and neglect of dependence in events and conditional \nprobabilities can lead to systemic failures in \nstructured \nfinance.^[ See @Tooze2018: For an engaging and comprehensive \nexploration of the great financial crisis of 2007 - 2008 and its \ncauses and aftermath. It is highly recommended for deeper study.]\n\nTo understand the real world aspects of this example it is necessarry\nto understand some basic ideas of structured \nfinance and the engineering of specific risk profiles from a portfolio of \nrisky securities in the first place. I will explain the finance\ncontext with a simple and stylized example and then discuss how understanding\nconditional probability may help us to make better financial \ndecisions.\n\n### Bonds and Credit Risk\n\nA **bond** is a financial instrument where \nthe issuer agrees to pay the holder a specific amount, the **face value** or \n**par value**, at maturity. Bonds are widely used as fixed-income securities \nbut carry the risk of default if the issuer faces financial difficulties.\n\nTo quantify this risk, bonds are rated by agencies such as Moody’s and \nStandard & Poor’s. Investment-grade bonds are considered low-risk, while \nspeculative or \"junk\" bonds are riskier and more likely to default. Here is a summary of their rating schemes and what the ratings mean in words:\n\n| Rating Category   | Moody's | Standard & Poor's |\n| ----------------- | ------- | ----------------- |\n| High grade        | Aaa     | AAA               |\n|                   | Aa      | AA                |\n| Medium grade      | A       | A                 |\n|                   | Baa     | BBB               |\n| Speculative grade | Ba      | BB                |\n|                   | B       | B                 |\n| Default danger    | Caa     | CCC               |\n|                   | Ca      | CC                |\n|                   | C       | C                 |\n|                   |         | D                 |\n\n### Pooling and Tranching: The Innovation\n\nStructured finance emerged in the early 2000s as a way to manage risk \nthrough **pooling** and **tranching**. By pooling risky assets and dividing \ncash flows \ninto \"tranches\" with distinct risk profiles, financial engineers created new \nbonds, including investment-grade securities, from portfolios of bonds which\nindividually would be rated as speculative grade or junk bonds. \nA major product of this innovation was the **mortgage-backed security (MBS)**. \nMany other products were then invented using similar financial engineering\nideas.\n\nLet us develop an intuitive understanding of structured finance and its\nrelation to probability through a simplified example, which I learned\nfrom Karl Schmedder's course.^[See https://www.coursera.org/learn/introductiontoprobability]\n\n### A Simple Event Tree for One Bond\n\nConsider a single bond you can own today that pays €1 at maturity at some point\nin the future. Time is often abbreviated as $t$ and the points in time\nare symbolized by letting $t$ take different values like $t=0$ for today and\n$t=1$ for a future point in time, say a year from today.\n\nThis bond has a 10% chance of default, meaning there is a 90% chance it \nwill not default. With a default probability of 10%, this bond would likely \nreceive a speculative grade rating, such as 'B' or 'B-' in the rating \ntables presented earlier. This poor rating reflects the significant \nrisk of non-payment associated with such a bond, which could deter \nrisk-averse investors and highlight its 'junk' bond status. \nThe payoff is structured as follows:\n\n- If the bond does not default ($N$), the payoff is €1.\n- If the bond defaults ($D$), the payoff is €0.\n\nThis situation can be graphically represented as a simple probability tree of\n@fig-single_bond_event_tree\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Event tree for one bond](figures/single_bond_event_tree.png){#fig-single_bond_event_tree fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n\n\n\n\n\nThe graph above visualizes the outcomes of a single bond. Each node represents a possible state of the bond at different times:\n\n- $t=0$ is the starting point.\n- $t=1$ No Default $N$ occurs with a probability of $P(N) = 0.9$.\n- $t=1$ Default $D$ occurs with a probability of $P(D) = 0.1$.\n\nYou could see this in analogy to the toss of a coin with the difference\nthat both sides of the coin show with different probability. With this\nanalogy - using the concepts of the last two lectures - you can understand \nthe bond in probabilistic terms\nas a random experiment with a sample space consisting of \ntwo basic outcomes, $N$ and $D$ with given\nprobabilities $P(N)$ and $P(D)$.\n\n### Combining Two Bonds with Independence Assumption\n\nWe now combine two such bonds into a portfolio. The \nassumption of **independence** implies that the defaults \nof these bonds occur independently of each other. This\nmeans that the default of one bond has no influence\non the probability of the other bond defaulting. Under \nthe assumption of independence you would treat the\ndefault probability of one bond as unconnected with\nthe default probability of the other.\n\nWhile this assumption simplifies calculations, it was \nhistorically used by financial engineers to justify creating \ntranches from risky portfolios. The reasoning was that \ndiversification reduces the likelihood of joint \ndefaults, making some tranches appear safer.\n\nAt the time, financial engineers relied on historical \ndata and market conditions to argue for this independence. \nDefaults were often uncorrelated under normal economic \nconditions, and diversification was seen as a proven \nstrategy for mitigating risk. For example, if bond \ndefaults were driven by isolated \nevents (such as company-specific issues), \nthe assumption of independence seemed reasonable. \nMoreover, the packaging of diverse assets from different \nindustries into portfolios strengthened the appearance of \nsafety, as individual economic shocks were less \nlikely to affect the entire portfolio simultaneously.\n\nHowever, this reasoning neglected systemic risks. During economic \ndownturns or financial crises, defaults often become highly correlated due \nto shared macroeconomic pressures, such as declining housing \nmarkets or credit tightening. For instance, in the lead-up to \nthe 2008 financial crisis, rising mortgage defaults were driven \nby broader economic factors that impacted many bonds simultaneously.\nWith this in mind it would be not plausible to assume that bonds can\nbe stacked together in a portfolio without the default risks of one\nbeing not pushed up by the default risk of others.\n\nEven without the formal use of probability theory, financial \nengineers could have questioned whether diversification \ntruly guaranteed independence in the context of systemic risks.\n\nThe idea that junk plus junk could be transformed into \ninvestment-grade bonds through pooling should have raised \nskepticism. Careful critical thinking—considering broader \neconomic dependencies—would have revealed that this transformation was too \ngood to be true. By ignoring these dependencies, financial \nengineers failed to see how small cracks in the system could \ncascade into systemic failures.^[ A famous voice at the time \nwarning about the flawed reasoning\nwas Raghuram Rajan, former chief economist of the International Monetary Fund. \nHe warned that rather than reducing risk through diversification, CDOs \nand other derivatives spread risk and uncertainty about the value of the \nunderlying assets more widely. ]\n\nBut let us not jump ahead and try to see how the tree for two bonds looks like\nwhen we **assume** independence in @fig-two_bond_event_tree.\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Event tree for two bonds with independent default risk](figures/two_bonds_event_tree.png){#fig-two_bond_event_tree fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n\n\n\n\n- The outcome of combining two bonds can be represented as a double event tree, \nshowing all possible combinations of defaults and no defaults at $t=1$. Let\nme unpack this more complex tree for you.\n\n- Since we have a portfolio of two bonds, instead of one as before, we have\nnow two event trees combined. Since we have to take into account all\nof the possible combinations of basic outcomes this means that we have\nto attach a new bond event tree to each outcome of the initial tree. \n\n- Note the time index. In the example the realizations of basic outcomes\nfor the portfolio happen simultaneously. So the walk from the origin to\nthe end of the tree along a path is taking place in one time step and only\nthe outcomes at $t=1$ are observed.\n\n- At the edges we write the probability of the outcomes. For example $P(N)$ is\nthe probability of the first bond (represented by the upper tree) is not\ndefaulting whereas $P(D)$ denotes the probability of the bond not defaulting.\nThe assumption of independence is hidden in this tree by modelling\nthe probabilities of $N$ and $D$ for the second bond in *exactly* the\nsame way no matter whether the first bond defaults or not. It is modelled\nin anaology to the toss of two fair coins. The probability of the second\ncoin showing Heads is $1/2$ no matter whether the first coin shows Heads or\nTails.\n\n- At the end of the tree we have written the outcome of each path in the notation\n\\begin{equation*}\nB_{\\text{state of bond 1 at} \\, t = 1 \\, \\text{state of bond 2 at} \\, t=1} = \\binom{\\text{payoff of bond 1 at } \\, t= 1}{\\text{payoff of bond 2 at } \\, t = 1}\n\\end{equation*}\nSo, for example,  $B_{NN} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ means that bond 1 does not default and\nbond two does not default ($B_{NN}$). Bond 1 has in this case a payoff of 1 and bond 2 also has \na payoff of 1.\n\n\n\n### Pooling and Tranching\n\nWhen you look at this portfolio **under the independence assumption** there\nseems to be room for re-engineering the risk profile of this portfolio. In all\noutcomes but in the one where both bonds default you can guarantee a payoff of\n1. There remains an adverse outcome, where both bonds default in which case\nyou can't pay out anything. But **under the assumption of independence** \nthis risk is small. The\nprobability of this event - remember our treatment of independent events in the\nfirst lecture - would be:\n$P(D) \\times P(D) = 0.1 \\times 0.1 = 0.01$.\nPretty low, actually. For example assume that the probability of default refer to\nthe probability of the bond defaulting over a year, the usual time frame \ntaken in ratings, this would be a one in a hundred years event. In $99%$ we\nwould get a sure payoff of 1. So under this restructuring the\nfirst restructured bond would qualify as an investment grade bond.\n\nSo this is the idea. We **pool** the payoffs of both securities and define\ntwo new securities by changing their payoff profile. The first one pays\nalways 1 except when both bonds default in which case this bond pays 0. The \nother one will always pay 0 except in the case where both bonds do not default.\nThis is under independence an event with probability $P(N) \\times P(N) = 0.81$.\nRember the complementarity rule? This says that the second restructured \nbond will thus have \na default probability of $19$ % instead of $10%$ it would be \nspeculative grade or close to toxic junk now.\n\nHere is picture how you can visualize this piece fo financial magic.\nThis picture can be read in exactly the same way as the previous picture. There\nis only one additional element. We have written the payoff of the original\nbonds by $B$. Underneath these original bonds we draw a black horizontal like,\nthink of it as the financial engineering lab that does the restructuring and\nbelow we get new bonds, with different payout promises, which we denote\nby $R$ (for *restructured*).\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Event tree for two bonds with independent default risk with pooling and tranching](figures/pooling_and_tranching.png){#fig-pooling-and-tranching fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n\n\n\n\nWhat is done here is that the aggregate payoffs of both bonds are collected\nin a pool and new securities - called R - in this picture are issued against the\npool. One, the upper one is now an investment grade asset paying 1 in every state\nexcept one and the other is a toxic junk bond paying always 0 except in one state.\nNote that the investment grade status could be engineered under the\nassumption that the risks are independent.\n\n### Pooling and Tranching without Independent Risks \n\nNow, let us consider a hypothetical question: \n**How would the event tree change if the independence assumption does not hold?** Dependence would alter the probabilities in a way that reflects \nthe increased likelihood of joint defaults during systemic events.\n\nSuppose we now assume that the probability of Bond 2 defaulting changes rather than staying unchanged under the condition that Bond 1 has defaulted:\n\n- If Bond 1 does **not** default, Bond 2 defaults with probability 0.1 (as before).\n- If Bond 1 **does** default, Bond 2 defaults with a higher probability \nof 0.6 due to systemic dependence.\n\nTo express this formally we need a piece of new notation. The convention\nin probability theory is that the notation is - for example - $P(N | D)$. This\nwould read as * bond 2 does not default given bond 1 has defaulted *. The event\nwe are interested in is written first separated by a $|$ from the conditioning\nevent, which is in our case the outcome that bond 1 defaults.\n\nThis dependence reflects a scenario where defaults are more \nlikely to occur together, such as during an economic downturn. \nThe resulting event tree can be visualized as follows:\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Event tree for two bonds with independent default risk with pooling and tranching with dependent default risk](figures/two_bonds_event_tree_dependent_events.png){#fig-pooling-and-tranching-dependent fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n\n\n\n\nOn the first sight this looks almost identical to the picture before. Only\nthe numbers on the edges of the second tree have changed. These changed numbers\nreflect the idea that the event that one bond has defaulted changes the\nprobability of the second bond defaulting as well. How could such a dependence\noccur?\n\nHere’s a real-world example to illustrate how dependence can occur:\nIn the context of bonds, dependence in default probabilities can \narise from shared exposure to systemic risks or interconnected \nfactors. For instance, consider two companies that \nissue bonds and operate in the same industry, such as \nthe energy sector. If oil prices plummet due to an \neconomic downturn or geopolitical instability, both \ncompanies might experience financial stress, making it \nmore likely that one default is followed by another. \n\nAnother example is during a financial crisis, such as the \n2008 global financial meltdown. A bank's default on its \nobligations can lead to cascading defaults in other \ninstitutions due to counterparty risks or a general loss \nof confidence in the financial system. In such cases, the \nprobability of a second default is no longer independent \nof the first because the events are tied to the same underlying \nmacroeconomic factors.\n\nThese examples highlight that the assumption of independence \nbetween bond defaults might hold under normal market conditions \nbut breaks down during systemic crises. Such dependencies must \nbe carefully modeled to avoid underestimating risk, as was \nthe case in structured finance products leading up to the 2008 crisis.\n\nA prudent risk manager must keep such a scenario in mind when he analyzes\na portfolio. Think about it in the context of the toy example. In the\nfirst case the default risk of the first asset created by pooling and tranching\nwas $P(D) \\times P(D) = 0.1 \\times 0.1 = 0.01$. Under a scenario with\ndependent risks this changes to $P(D) \\times P(D | D) = 0.1 \\times 0.6 = 0.06$, a risk\nlarger by a factor of 6! While in the first case the first restructured bond \nwould be rated as investment\ngrade, in the second case the same restructured bond would be rated as \nspeculative grade and the magic from\npooling and tranching suddenly disappears. Junk plus junk remains junk after \nall.\n\nFor pooling and tranching to reduce overall risk and create safe tranches:\n\n1. **Diversification:** Assets must come from independent sectors with minimal systemic risk.\n2. **Stable Macroeconomic Conditions:** Systemic risks must be low to maintain independence assumptions.\n3. **Transparent Modeling:** Dependence structures must be explicitly modeled and accounted for in risk assessments.\n\nThe neglect of these conditions led to a flawed sense of security in structured finance, which contributed to the 2008 financial crisis.\n\n## Conditional Probability\n\nConditional probability provides us with\na concept to formalize how the probability of one event \nchanges when another event is known to occur, providing a \nframework for understanding dependencies quantitatively.\n\nHere is the mathematical definition:\n\n::: {.callout-tip title=\"Definition: Conditional probability\" #conditional-probability}\nLet $B$ be an event with positive probability. For an\narbitrary event $A$ we define the **conditional probability** \nof $A$ given $B$ as\n\\begin{equation*}\nP(A\\,|\\,B) = \\frac{P(A \\cap B)}{P(B)}\\,\\,\\, \\text{provided}\\,\\,\\, P(B) \\neq 0\n\\end{equation*}\n:::\n\nNote that conditional probabilities remain undefined when the \nconditioning event $B$ has probability 0.^[As pointed out in @Feller1968 p 115, this has no consequences\nin the case of discrete sample spaces but is important in the general\ntheory.]\n\nLet us clarify a few things about this concept. As in the\nexample of the financial crisis, which we discussed before\nwe really did not much more than introducing one piece of new\nnotation to indicate that the probabilities now have changed.\n\n### An illustration using old and new R concepts\n\nThis is an excellent moment in this lecture to make use of R to \nillustrate the concept and play with it. On the way we introduce some\nnot yet covered R concepts.\n\nLet us stay with the example of defaultable bonds and use the\nfreedom and the opprotunity of simulating probabilistic examples\non the computer.\n\nLet us make use of the `sample()` function first to create a portfolio of\nbonds. The relevant data for this portfolio should be recorded in a\ndataframe.\n\nTo make the data reproducible we need to specify a random seed. \nJust like in Python, the set.seed() function in R ensures \nreproducibility of random numbers. When generating random \ndata (like our simulated bond portfolio), R uses a \npseudo-random number generator. By setting \na \"seed\" value, you tell R to start its random number generator \nfrom a specific point. This guarantees that every time you run \nthe code, you'll get the same random results, which is crucial for \ndebugging, sharing code, or \nteaching concepts. \n\nThe `set.seed()`functions requires and argument. We can for instance\nuse a sequence of numbers, which can be arbitrary like `123`or `42`or\n`2025`. What is important is that using the same seed will always\ngive you the same numbers and thus make your example\nreproducible.\n\nHere is an example for illustration. Let's go back to our\nold coin tossing example and let us toss our coin 10 times\none time using the `set.seed()`function and one time not.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tossing a fair coin 10 times, two different runs without set.seed()\n\nexample_without_1 <- sample(c(\"H\",\"T\"), size =10, replace = T)\nexample_without_2 <- sample(c(\"H\",\"T\"), size =10, replace = T)\n\ncat(\"Without set.seed():\\n \")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWithout set.seed():\n \n```\n\n\n:::\n\n```{.r .cell-code}\nprint(example_without_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"H\" \"T\" \"T\" \"H\" \"H\" \"T\" \"H\" \"H\" \"H\" \"T\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(example_without_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"T\" \"T\" \"H\" \"T\" \"H\" \"H\" \"T\" \"H\" \"T\" \"T\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# tossing a fair coin 5 times, two different rund with set.seed()\n\nset.seed(123)\nexample_with_1 <- sample(c(\"H\",\"T\"), size =10, replace = T)\nset.seed(123)\nexample_with_2 <- sample(c(\"H\",\"T\"), size =10, replace = T)\n\ncat(\"With set.seed():\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWith set.seed():\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(example_with_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"H\" \"H\" \"H\" \"T\" \"H\" \"T\" \"T\" \"T\" \"H\" \"H\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(example_with_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"H\" \"H\" \"H\" \"T\" \"H\" \"T\" \"T\" \"T\" \"H\" \"H\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nNow let us go to our bind portfolio simulation. Let us simulate our data\nfirst:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set a random seed for reproduceability\nset.seed(123)\n\n# Number of bonds\n\nN <- 5000 \n\n# Simulate portfolio data\n\nportfolio <- data.frame(\n  BondID = 1:N,\n  CreditRating = sample(c(\"AAA\", \"BBB\", \"Junk\"), N, replace = TRUE, prob = c(0.5, 0.3, 0.2)),\n  Sector = sample(c(\"Finance\", \"Energy\", \"Real Estate\"), N, replace = TRUE),\n  Defaulted = NA  # Initialize with NA for later assignment\n)\n```\n:::\n\n\n\n\n\n\n\n\nLet me unpack this a bit:\n\nFirst we create a dataframe using the `data.frame()`function of R.  \nIt is used to create a structured dataset in R, similar to a table in Excel or \na DataFrame in Python's pandas library. Here, we are simulating a dataset \ncalled portfolio, which represents a collection of financial bonds. \nEach row corresponds to a bond, and each column represents an attribute \n(or variable) of that bond.\n\nIn the first column of our dataframe we store unique bond IDs by just assigning\nthem a sequence of integers starting at 1 and counting to $N$, the total\nnumber of bonds in the portfolio. Remember the\ncolon operator `:` we have already used before to create such sequences.\n\nIn the next column of our dataframe we assign a credit rating randomly\nusing a probability of 0.5 that the rating will be \"AAA\", 0.3 that it will be\n\"BBB\" and 0.2 that it will be \"Junk\". You know already how to do this using the\nsample function. Note that we have set `replace = TRUE`: This allows sampling \nwith replacement, so the same credit rating can appear multiple times\n\nFinally we imagine that the bonds are issued by different sectors, Finance, Energy and\nReal Estate in the\neconomy which we also assign randomly with equal probability for each sector\nusing the sample function.\n\nThe final column creates a vector of unknown values, because we\nwould like to assign these values in a separate step to\nbe consistent with actual rating probabilities.\n\nThe way we do this is to assign the value `NA` to the variable\n`Defaulted`for the moment. What does this variable mean in R?\n\nThe NA character is a special symbol in R. It stands for “not available” \nand can be used as a placeholder for missing information. \nR will treat NA exactly as you should want missing information treated. \nFor example, what result would you expect if you add 1 to a piece of \nmissing information?\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 + NA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NA\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nR will return a second piece of missing information. It would not be correct to say that \n`1 + NA = 1` because there is a good chance that the missing quantity is not zero. \nYou do not have enough information to determine the result.\n\nThere are two functions which are very useful to know about, when working\nwith data that contain `NA` which will be the case in almost all practical\ncircumstances.\n\nWhile `NA` is useful for indicating missing information it can be annoying\nin practical data work. Assume you had a vector of 100 numbers and only one value,\nsay at the beginning is `NA`.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(NA, 1:100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1]  NA   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n [19]  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n [37]  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n [55]  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n [73]  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n [91]  90  91  92  93  94  95  96  97  98  99 100\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(c(NA,1:100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NA\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThis is clearly annoying and we would like to enforce a different behavior. Most\nR functions, such as `mean()` and others come with the option `na.rm` that controls\nR's behavior when data contain missing information. Here is how it works in the case\nof mean:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(c(NA,1:100), na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 50.5\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\nA second useful function when working with missing data is\n`is.na()`. This is the case because the identity operator\n`==` used in logical subsetting does not work with `NA`.\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(1,2,3,NA) == NA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] NA NA NA NA\n```\n\n\n:::\n\n```{.r .cell-code}\nis.na(c(1,2,3,NA))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE FALSE FALSE  TRUE\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nSo while the first approach will just yield `NA`, the second will \nidentify which value in our vector is missing.\n\nNow putting it all together this will give us a dataframe where\nthe data.frame() function combines these columns into a structured dataset, \nwhere:\n      - BondID is an integer sequence labeling each bond.\n      - CreditRating is a randomly assigned credit rating, weighted by the specified probabilities.\n      - Sector is a randomly assigned economic sector.\n      - Defaulted a vector with missing values yet to be determined.\n\nYou can inspect the first rows of this dataframe by\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(portfolio)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  BondID CreditRating      Sector Defaulted\n1      1          AAA Real Estate        NA\n2      2          BBB      Energy        NA\n3      3          AAA      Energy        NA\n4      4         Junk      Energy        NA\n5      5         Junk     Finance        NA\n6      6          AAA Real Estate        NA\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nNow we assign different default probabilities for the different credit ratings\nsuch that the simulated data show similar values than with actual ratings.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_probabilities <- data.frame(\n  CreditRating = c(\"AAA\", \"BBB\", \"Junk\"),\n  DefaultProb = c(0.0001, 0.002, 0.05)  # Default probabilities\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_probabilities <- data.frame(\n  CreditRating = c(\"AAA\", \"BBB\", \"Junk\"),\n  DefaultProb = c(0.0001, 0.002, 0.05)  # Default probabilities\n)\n```\n:::\n\n\n\n\n\n\n\n\nWe do this in a separate rating dataframe, organizing\ncompactly our rating information. To see what is happening here\nobserve that a `data.frame` is created to store default probabilities for \neach credit rating. `CreditRating` is a character column containing the \ncategories: `\"AAA\"`, `\"BBB\"`, and `\"Junk\"`. `DefaultProb` is a numeric \ncolumn with the corresponding default probabilities: \nAAA: $0.01\\%$, BBB: $0.2\\%$, Junk: $5\\%$ This dataframe\norganizes the default probabilities in a clear and structured way, \nmaking the probabilities easy to reference if needed later.\n\n\nNow we can use the\npower of R's subsetting rules to fill the last column\nin our portfolio dataframe such that we get realistic \nvalues. Let me show you the code and then unpack the\nelements step by step:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nportfolio$Defaulted[portfolio$CreditRating == \"AAA\"] <- \n  sample(c(TRUE, FALSE), sum(portfolio$CreditRating == \"AAA\"), replace = TRUE, \n         prob = c(0.0001, 0.9999))\n\nportfolio$Defaulted[portfolio$CreditRating == \"BBB\"] <- \n  sample(c(TRUE, FALSE), sum(portfolio$CreditRating == \"BBB\"), replace = TRUE, \n         prob = c(0.002, 0.998))\n\nportfolio$Defaulted[portfolio$CreditRating == \"Junk\"] <- \n  sample(c(TRUE, FALSE), sum(portfolio$CreditRating == \"Junk\"), replace = TRUE, \n         prob = c(0.05, 0.95))\n```\n:::\n\n\n\n\n\n\n\n\nHere's a step-by-step explanation of the code, unpacking the logic and reinforcing R subsetting rules:\nLet's look at the AAA bonds first.\n\nHere we see the code line `portfolio$CreditRating == \"AAA\"`. This line\nchecks each row in the `CreditRating` column to see if the value is `\"AAA\"`.\nIt returns a **logical vector** (e.g., `TRUE` for `\"AAA\"` rows, `FALSE` otherwise).\nHere you see two important R concepts at work. To check whether two expressions\nare equal the appropriate symbol is `==`. It returns a logical `TRUE` when the\nequality holds and `FALSE`otherwise. \n\nWe use this logical vector created by the assignment to assign a probability\nto the `Defaulted`column whenever the rating is `AAA`. So the operation\nselects the `Defaulted` column for rows where `CreditRating` is `\"AAA\"` and\ndefault status to these rows based on a random sample. \n\nNow for the random sample we use the `sample()` function we encountered\nbefore. It generates random values `TRUE`for default and `FALSE`for non-default,\nthe two possible outcomes. It samples with replacement as many times as we\nhave an `AAA`rating. This count is achieved by `sum(portfolio$CreditRating == \"AAA\")`\nusing R's coercion rules. It returns the count because each `TRUE` is forced to\n1 and contributes one count to the total whereas `FALSE`is forced to 0 and contributes nothing\nto the count. Finally `prob = c(0.0001, 0.9999)` specifies the probabilities for\nfor this class. Note that here we could also have used the complement rule by\nwriting `prob = c(0.0001, 1 - 0.0001)`\n\nNow we do the same thing for `BBB` bonds and the `Junk`bonds, which have\neach different probabilities from the `AAA`binds and from each other.\n\nHere you have in one example the  **General Rules for R Subsetting** reviewed\n\n1. **Logical Conditions**:\n   - Use conditions like `==` to create logical vectors.\n   - Example: `portfolio$CreditRating == \"AAA\"` checks for equality \n     and returns a logical vector.\n   \n2. **Row Selection**:\n   - Logical vectors are used to select rows in a data frame.\n   - Example: `portfolio$Defaulted[...]` updates only the rows where the condition is `TRUE`.\n\n3. **Column Access**:\n   - Use `$` to access specific columns in a data frame.\n   - Example: `portfolio$Defaulted`.\n\n4. **Combining Logical Subsetting and Assignment**:\n   - Subset rows using logical conditions, then assign values to those rows.\n   - Example: `portfolio$Defaulted[portfolio$CreditRating == \"AAA\"] <- sample(...)`\n\n::: {.callout-note}\n## Now you try\n\nHere are some more assignment exercises for you to try:\n\n1. **Explore Subsetting**:\n   - Print all AAA bonds using:\n     \n     `portfolio[portfolio$CreditRating == \"AAA\", ]`\n     \n   - Verify the number of rows using:\n     \n     `sum(portfolio$CreditRating == \"AAA\")`\n\n2. **Check Assigned Values**:\n   \n    Confirm that `Defaulted` is only updated for the relevant rows:\n    \n    `table(portfolio$Defaulted[portfolio$CreditRating == \"AAA\"])`\n     \n:::\n\nLet's check how our `portfolio`dataframe looks now after the assignment.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(portfolio)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  BondID CreditRating      Sector Defaulted\n1      1          AAA Real Estate     FALSE\n2      2          BBB      Energy     FALSE\n3      3          AAA      Energy     FALSE\n4      4         Junk      Energy     FALSE\n5      5         Junk     Finance     FALSE\n6      6          AAA Real Estate     FALSE\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\nNow let us go on to the illustration of the conditional probability\nconcept. If we choose a bond at random from this portfolio we should \nget a probability\nof\n\n\\vspace{0.5cm}\n$P(D) = \\frac{\\text{number of defaultet bonds}}{\\text{total number of bonds}}$\nand \n$P(N) = \\frac{\\text{number of non-defaultet bonds}}{\\text{total number of bonds}}$\n\nLet's check these numbers in our sample, using the R subsetting rules and a new\noperator, the `$` sign, which plays an important role in R data manipulation. Let me\ndo the calculation and then explain:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPD <- mean(portfolio$Defaulted == TRUE)\nPN <- 1 - PD\n\nPD\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0108\n```\n\n\n:::\n\n```{.r .cell-code}\nPN\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9892\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nLet me explain this bit of code. The name of our dataframe is `portfolio`. We\nmay refer to this as an R-object in the R language. An operator that is\nfrequently used in R to select a column from a dataframe is the colon \noperator `$`. If we tell R `portfolio$Defaulted`, R will select the\ncolumn `Defaulted`from the dataframe portfolio. The same operator is\nused to select elements in an R list. This is no coincidence, since a\ndataframe is formally a list in the R language. You can see this by\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntypeof(portfolio)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"list\"\n```\n\n\n:::\n\n```{.r .cell-code}\nclass(portfolio)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"data.frame\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nIf you look at the type of a data frame, you will see that it is a list. \nIn fact, each data frame is a list with class data.frame. \nYou can see what types of objects are grouped together by a \nlist (or data frame) with the `str()` function:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(portfolio)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t5000 obs. of  4 variables:\n $ BondID      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ CreditRating: chr  \"AAA\" \"BBB\" \"AAA\" \"Junk\" ...\n $ Sector      : chr  \"Real Estate\" \"Energy\" \"Energy\" \"Energy\" ...\n $ Defaulted   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nNow the numbers of the unconditional probabilities of a bind defaulting or\nnot defaulting (note that we applied the complement rule here to compute\nthis probability) is as it should be given the parametrisation of our\nexample.\n\nWe now want to restrict our attention to the subset of our portfolio\nconsisting of junk bonds. What is the probability that a bond\nchosen from this sub-population is in default?\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate conditional probability: P(Default | CreditRating = \"Junk\")\n\njunk_bonds <- portfolio[portfolio$CreditRating == \"Junk\", ]\n\nP_Default_given_Junk <- mean(junk_bonds$Defaulted)\n\ncat(\"P(Default | CreditRating = 'Junk'):\", P_Default_given_Junk, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(Default | CreditRating = 'Junk'): 0.05353535 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nNow you see perhaps with your own hands how conditional probability\nworks in terms of R's subsetting rules. **Among** the subset of bonds\nwhich are classified as junk, how many are also defaulted and what is\ntheir relative frequency among the junk bonds? This would be in \nset notation:\n\\begin{equation*}\n\\frac{P(\\text{set of all defaulted bonds} \\cap \\text{set of all junk bonds})}{P(\\text{set of all junk bonds})}\n\\end{equation*}\nTo provide an appropriate notation for this we have used:\n$P(\\text{defaulted} \\, | \\,\\text{junk})$\nThis is read as the probability of event $A$ (the bond is defaulted) assuming the\nevent $B$ (the bond is a junk bond).\n\nThus taking a conditional probability of various events with respect to\na particular event $B$ amounts to choosing $B$ as a new sample sapce with\nthe probabilities proportional to the original ones. The proportionality\nfactor $P(B)$ is necessary in order to make the probabilities in the\nnew sample space sum up to 1. All general theorems on probabilities are\nvalid also for conditional probabilities with respect to any\nparticular event $B$.\n\nNote that for conditional probabilities we have for two events $A$ and $B$, that\n$P(A|B) \\neq P(B|A)$.\nTo see this assume that $P(A) \\neq P(B)$ and $P(A) \\neq 0$ and\n$P(B)\\neq 0$. \n\nWe then get:\n\\begin{equation*}\nP(A|B) = \\frac{P(A\\cap B)}{P(B)} = \\frac{P(B \\cap A)}{P(B)}\n\\end{equation*}\nsince $P(A \\cap B) = P(B \\cap A)$.\nIt follows that\n\\begin{equation*}\n\\frac{P(B \\cap A)}{P(B)} \\neq \\frac{P(B \\cap A)}{P(A)} = P(B|A)\n\\end{equation*}\nsince we\nhave assumed that $P(A) \\neq P(B)$. \nTherefore $P(A|B) \\neq P(B|A)$.\n\nLet's illustrate this remark with the example of our bond portfolio.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate conditional probability: P(CreditRating = \"Junk\" | Default)\n\ndefaulted_bonds <- portfolio[portfolio$Defaulted == TRUE, ]\n\nP_Junk_given_Default <- mean(defaulted_bonds$CreditRating == \"Junk\")\n\ncat(\"P(Junk | Default):\", P_Junk_given_Default, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(Junk | Default): 0.9814815 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nNow you can see clearly that $P(\\text{Default} \\, |\\, \\text{Junk} ) =$ \n0.0535354 which\nis clearly different from $P(\\text{Junk} \\, |\\, \\text{Default} ) =$ \n0.9814815.\n\nThe formula for conditional probability which we\nwrote down in the definition of [Conditional Probability](#conditional-probability)\n is often\nused in the form of the **multiplication rule**:\n\n::: {.callout-tip title=\"Definition: Multiplication rule\" #multiplication-rule}\nGiven events $A$ and $B$, it holds that: $P(A \\cap B) = P(A \\mid B)\\times P(B)$\n:::\n\nwhich is just an equivalent way to write the formula\nfor conditional probability. The multiplication rule can be\nthought of the\nAND rule of probability theory. \n\nWith the multiplication rule we can gain a deeper insight into \nthe meaning of independence. Remember that two events $A$ and  $B$ are independent\nif\n\\begin{equation*}\nP(A \\cap B) = P(B \\cap A) = P(A) \\times P(B).\n\\end{equation*}\n\nIf we combine this rule with the\nconcept of conditional probability, we see that if two \nevents $A$ and $B$ are independent, then\n\\begin{equation*}\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A) \\times P(B)}{P(B)} = P(A)\n\\end{equation*}\nand\n\\begin{equation*}\nP(B|A) = \\frac{P(B \\cap A)}{P(A)} = \\frac{P(A) \\times P(B)}{P(A)} = P(B)\n\\end{equation*}\n\nThis formula says that if two events are independent the probability of $A$ is\nnot influenced by the event $B$ occurring and the probability of event $B$ is not\ninfluenced by the event $A$ occurring.\n\nHere’s a proposal for a draft subsection devoted to introducing advanced R concepts (Environments, Scoping Rules, Closures) while reinforcing conditional probability ideas. The examples are tied to a financial and conditional probability context, ensuring continuity with your lecture themes.\n\n---\n\n## Advanced R Concepts: Environments, Scoping Rules, and Closures\n\nIn this section, we will explore some advanced R programming concepts that \nare essential for understanding how R evaluates and stores variables, as \nwell as how you can create reusable and dynamic functions. We will \ndemonstrate these concepts through examples related to conditional \nprobability and financial modeling.\n\n### Introduction to Environments\n\nAn **environment** in R is where objects (variables, functions, etc.) are \nstored and looked up. R uses environments to determine \nwhere a variable exists and what its value is. The most \ncommon environment is the **global environment**, where user-created \nvariables and functions are stored.\n\n**Example: Setting Global and Local Variables**\n\nSuppose we are modeling interest rates in a financial portfolio. Globally, we set the baseline interest rate. Locally, we may override this rate for specific calculations.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Global interest rate\ninterest_rate <- 0.05  # 5%\n\n# Function to calculate interest payments\ncalculate_interest <- function(principal, rate = interest_rate) {\n  interest <- principal * rate  # Uses the rate passed to the function\n  return(interest)\n}\n\n# Global calculation\nglobal_interest <- calculate_interest(1000)  # Uses global interest_rate\ncat(\"Global Interest:\", global_interest, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGlobal Interest: 50 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Local override\nlocal_interest <- calculate_interest(1000, rate = 0.07)  # Overrides global interest_rate\ncat(\"Local Interest:\", local_interest, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLocal Interest: 70 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n**Global variables** are available everywhere, but **local variables** \n(like `rate`) take precedence within a function. Understanding this \nbehavior is crucial for writing clear and predictable code.\n\n\n### Scoping Rules\n\nR follows specific **scoping rules** to determine where and how \nto find variables. These rules become important when working \nwith nested functions.\n\n**Example: Variable Lookup in Nested Functions**\n\nLet’s calculate conditional probabilities using nested functions. \nWe simulate a financial scenario where we compute probabilities \nof default for different credit ratings.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define global default rates for credit ratings\n\ndefault_rates <- c(\n  AAA = 0.01,  # Global default rate for AAA bonds\n  BBB = 0.02,  # Global default rate for BBB bonds\n  Junk = 0.05  # Global default rate for Junk bonds\n)\n\n# Function to calculate conditional default probability\nconditional_default <- function(rating) {\n  \n  # Lookup table for default rates\n  local_default_rates <- c(\n    AAA = default_rates[\"AAA\"],  # Local default for AAA\n    BBB = default_rates[\"BBB\"],  # Local default for BBB\n    Junk = default_rates[\"Junk\"] # Local default for Junk\n  )\n  \n  # Return the default rate using vectorized subsetting\n  \n  return(local_default_rates[rating])\n}\n\n# Test the function\ncat(\"Default rate for Junk bonds:\", conditional_default(\"Junk\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDefault rate for Junk bonds: NA \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Default rate for BBB bonds:\", conditional_default(\"BBB\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDefault rate for BBB bonds: NA \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Default rate for AAA bonds:\", conditional_default(\"AAA\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDefault rate for AAA bonds: NA \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nHer you see how R is using **Lexical scoping**. This \nensures that R looks for variables in the closest environment first, then \nmoves outward (from local to global). Nested functions can use both local \nand global variables.\n\nThis example uses a concept you might find useful in many other\ncontexts: The **lookup table**. \nThe concept of a lookup table is a simple yet powerful way to map input \nvalues to corresponding outputs. \nIn R, we can create a lookup table using a named vector, where \neach element has a name (the input) and a value (the corresponding output). \nThis allows us to retrieve the correct value by directly referencing the name.\n\nIn the example, we used a named vector local_default_rates to store \nthe default probabilities for different credit ratings: \"AAA\", \"BBB\", and \"Junk\". \nEach credit rating serves as a key, and the corresponding default probability \nserves as the value. When we pass the rating (e.g., \"Junk\") to the function, \nR uses it to subset the vector and directly return the associated \nprobability. This approach is efficient and avoids the need for \nverbose or complex conditional statements.\n\nBy using a lookup table, we also demonstrate an important principle of \nprogramming: separation of data and logic. The mapping of ratings to \nprobabilities is encapsulated in a single data structure (local_default_rates), \nmaking the function simpler and easier to modify. For instance, if the default \nprobabilities change, you only need to update the values in the vector—no \nchanges to the function logic are required. This approach is especially useful in financial modeling, where \nmappings like these are common and can evolve over time.\n\n### Closures\n\nA **closure** is a function that remembers the \nenvironment in which it was created. Closures are \npowerful for creating dynamic, reusable functions, such as \ncalculators for different conditional probabilities.\n\n**Example: Probability Calculator Factory**\n\nThis code demonstrates a powerful concept in R: **closures**. \nA closure \nis a function that \"remembers\" the environment in \nwhich it was created, \nallowing you to dynamically generate new functions \nwith specific behaviors.\nLet’s create a function factory that generates specific probability \ncalculators based on a given event.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function factory for conditional probability calculators\n\nprobability_calculator_factory <- function(event_probability) {\n  function(conditional_probability) {\n    joint_probability <- event_probability * conditional_probability\n    return(joint_probability)\n  }\n}\n\n# Create calculators for different events\n\njunk_calculator <- probability_calculator_factory(0.05)  # Junk bonds\nbbb_calculator <- probability_calculator_factory(0.02)   # BBB bonds\n\n# Calculate joint probabilities\n\njunk_joint <- junk_calculator(0.1)  # P(Default | Junk) * P(Junk)\nbbb_joint <- bbb_calculator(0.2)    # P(Default | BBB) * P(BBB)\n\ncat(\"Joint probability for Junk bonds:\", junk_joint, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nJoint probability for Junk bonds: 0.005 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Joint probability for BBB bonds:\", bbb_joint, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nJoint probability for BBB bonds: 0.004 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n Let’s unpack the code step by step:\n\n\nThe `probability_calculator_factory` is a **function factory**. It takes \none argument, `event_probability`, and returns a new function \nthat calculates the **joint probability** for a given conditional \nprobability:\n\n- **Input**:\n  - `event_probability`: The probability of the event (e.g., the probability of a bond being \"Junk\").\n  - The returned function takes `conditional_probability` as its argument (e.g., the probability of default given the bond is \"Junk\").\n- **Output**:\n  - The joint probability, $P(A \\cap B) = P(A | B) \\times P(B)$.\n\nThis structure encapsulates the logic for joint probability into a reusable framework.\n\nThe `junk_calculator` and `bbb_calculator` are functions created by the \nfactory. Each calculator \"remembers\" the `event_probability` it was initialized with:\n- `junk_calculator`: $P(Junk) = 0.05$.\n- `bbb_calculator`:  $P(BBB) = 0.02$.\n\nThese calculators are then used to compute joint probabilities by providing \nthe corresponding conditional probabilities:\n- `junk_joint <- junk_calculator(0.1)`:\n  - $P(\\text{Default} \\cap \\text{Junk}) = P(\\text{Default | Junk}) \\times P(\\text{Junk})$.\n  - $0.1 \\times 0.05 = 0.005$ (0.5%).\n- `bbb_joint <- bbb_calculator(0.2)`:\n  - $P(\\text{Default} \\cap \\text{BBB}) = P(\\text{Default | BBB}) \\times P(\\text{BBB})$.\n  - $0.2 \\times 0.02 = 0.004$ (0.4%).\n\nThe `cat()` function displays the results:\n\nA closure allows you to \"lock in\" parameters (like `event_probability`) when the \nfunction is created, while still allowing flexibility for additional inputs.\n\n\n::: {.callout-note title=\"Now you try\"}\n1. Modify the `calculate_interest` function to add a penalty rate for overdue payments using local variables.\n2. Extend the `conditional_default` function to include an additional credit rating (e.g., \"CC\").\n3. Use the `probability_calculator_factory` to compute joint probabilities for a new event, such as \"Real Estate Sector Default.\"\n:::\n\n## Bayes' Rule: One of the Great Ideas in Probability\n\nBayes' Rule stands among the ten great ideas in probability. \nIts power lies in solving a fundamental problem: how to infer \nunderlying chances from observed frequencies. This insight filled \na critical gap left by Bernoulli’s weak law of large numbers, which \nexplained how observed frequencies converge to probabilities but \nleft unanswered the question of how to reason from those \nfrequencies back to the chances that generated them.\n\nAt its core, Bayes' Rule provides a framework for updating our beliefs \nin light of new information. But what does it mean to assign a \nprobability to a belief? This idea rests on another deep insight in \nprobability theory: judgments about uncertainty can be measured, and \nwhen those judgments are coherent, they follow the rules of probability. \n^[This idea goes back to the work of Frank Ramsey (1903–1930). He was \na British mathematician, philosopher, and \neconomist whose profound contributions spanned multiple fields \ndespite his tragically short life. In probability, he \nestablished the foundation for subjective probability theory, \nshowing that coherent judgments about uncertainty adhere to \nthe axioms of probability. Ramsey also made groundbreaking \nadvances in decision theory, logic, and economics, including \nthe famous Ramsey pricing in public economics and \nhis foundational work in mathematical logic. For more, on Ramsey's ideas of\nconnecting judegment to probability see chapter 2 in @DiaconisSkyrms2019]\n\nThe key breakthrough of Bayes' Rule is that it ties these coherent judgments—our \ninitial beliefs, or priors—to evidence, using conditional probability. This \nprocess transforms subjective judgments into a systematic method for \nreasoning under uncertainty, with profound applications across science, finance, \nand everyday decision-making.\n\n### A simple example: Revising beliefs about market valuation\n\nWe discussed the big financial crisis of 2007-2008 earlier in this lecture. \nImagine now you are an investor in the year 2011, just a bit later. The market \nis gripped by fear of another financial crisis, this time triggered by \ntensions in the Eurozone. \n\nLet's imagine how you could have asessed the market situation by looking\nat financial data. This is also an excellent opportunity to introdcue you\nto one of many great opportunities to load real world data directly into\nR using add on packages. The package we are going to use here\nis called `tidyquant`. To be able to use it you must first install it using\nthe `install.packages()`function of base R or the package installer pane in\nRStudio. Let us do that - assuming that this package has been installed. You\nmight remember how to load a package from before.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyquant)\n```\n:::\n\n\n\n\n\n\n\n\nNow tidyquant has many functions allowing you to retrieve and transform\nreal world financial data. We do not go into any detail here. This is something\nyou can do yourself using the excellent documentation of this package, the many\nexamples on the web or by interrogating your LLM.^[Check out https://cran.r-project.org/web/packages/tidyquant/index.html]. One of the\ncore workhorse functions in tidyquant is the function `tq_get()` which allows \nyou to retrieve data.\n\nOne thing an anlyst might be interested in is how the stock market as measured\nby a broad index does at the moment compared to its historical values. Let's \nsay you are looking back 11 years from 2011 into the past and see how the\nS&P500 does now compared to this history. This is how you would \nuse `tidyquant` to do this:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fetch S&P 500 data from 2000 to 2011\n\nsp500_data <- tq_get(\"SPY\", from = \"2000-01-01\", to = \"2011-12-31\")\n```\n:::\n\n\n\n\n\n\n\n\nIn order to do load data you need the string of the name of the series\nyou are interested in, which you can learn from the documentation. In our\ncase this is \"SPY\" for the SP500. You can\nalso specify a time range or just the beginning of the series. In the latter \ncase it will give you all the data from the beginning up to the current or\nmost recent trading day. We have written the data into an R object called\n`sp500_data` and now you might inspect it.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(sp500_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 8\n  symbol date        open  high   low close   volume adjusted\n  <chr>  <date>     <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 SPY    2000-01-03  148.  148.  144.  145.  8164300     92.7\n2 SPY    2000-01-04  144.  144.  140.  140.  8089800     89.1\n3 SPY    2000-01-05  140.  142.  137.  140  12177900     89.2\n4 SPY    2000-01-06  140.  142.  138.  138.  6227200     87.8\n5 SPY    2000-01-07  140.  146.  140.  146.  8066500     92.9\n6 SPY    2000-01-10  146.  147.  145.  146.  5741700     93.2\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThe otput says that the data object is a `tibble()`. Don't worry about this\ndetail at the moment and think of a `tibble()`as something equivalent to a\ndataframe.\n\nNow let us visualize the data using base R's `plot()` function. Use the help\nfacilities or your LLM to find out abut the syntax details.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Base R plot\nplot(\n  sp500_data$date, sp500_data$adjusted,\n  type = \"l\",                # Line plot\n  col = \"blue\",              # Line color\n  lwd = 2,                   # Line width\n  xlab = \"Date\",             # X-axis label\n  ylab = \"Adjusted Price\",   # Y-axis label\n  main = \"S&P 500 Price Trends (2000–2011)\" # Title\n)\n\n# Add grid lines for better readability\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n```\n\n::: {.cell-output-display}\n![](03-lecture3_files/figure-html/visualize-sp500-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\nWe see that the index is about the same level as it was\n10 years ago. The index alone does not tell you very much. \nHow about\nrelating this to some relevant market fundamentals?\n\nOne powerful feature of `tidyquant()` is that it can\nfetch data from various data sources. Here I get,\nfor the sake of this example, data on\ncorporate profits from the FRED database of\nthe Fed St. Louis^[See https://fred.stlouisfed.org/]\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get corporate profits data from FRED\ncorporate_profits <- tq_get(\"CP\", from = \"2000-01-01\", to = \"2011-12-31\", get = \"economic.data\")\n\n# Visualize corporate profits\nplot(\n  corporate_profits$date, corporate_profits$price,\n  type = \"l\",\n  col = \"blue\",\n  lwd = 2,\n  xlab = \"Year\",\n  ylab = \"Corporate Profits (Index)\",\n  main = \"Corporate Profits (2000–2011)\"\n)\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n```\n\n::: {.cell-output-display}\n![](03-lecture3_files/figure-html/retrieveing spfundamentals-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\nProfits have roughly tripled over the same period. The market\ndoes seem to be undervalued. Let's put this\ninto perspective by looking at Price-Earning\ndata or P/E ratios. \n\nNow we run into a problem which you will often encounter\nwhen working with data. It is rare that a tool covers all cases.\nP/E rations seem to be difficult to retrieve with `tidyquant`. These\ndata do however exist on the web, for example at the website\nMacrotrends.^[The P/E ratio for the SP500 can be found for instance\nhere: https://www.macrotrends.net/2577/sp-500-pe-ratio-price-to-earnings-chart]\n\nTo retrieve data from this site, I had to download the data locally\nfirst before I could get them into R. Let's look at them a bit \ncloser now using again R's visualization tools.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the path to the downloaded CSV file\nfile_path <- \"data/sp-500-pe-ratio-price-to-earnings-chart.csv\"\n\n# Read the CSV file into R\npe_data <- read.csv(file_path, stringsAsFactors = FALSE, skip = 16,\n                    col.names = c(\"Date\", \"PE_ratio\"))\n\n# Convert the date column to Date class\npe_data$Date <- as.Date(pe_data$Date, format = \"%Y-%m-%d\")\n\n# Filter data for the desired date range\npe_data_filtered <- subset(pe_data, Date >= as.Date(\"2000-01-01\") & \n                             Date <= as.Date(\"2011-12-31\"))\n\n# Calculate the average P/E ratio over the specified period\naverage_pe <- mean(pe_data_filtered$PE_ratio, na.rm = TRUE)\n\n# Plot the P/E ratio for the y-axis\nplot(\n  pe_data_filtered$Date, pe_data_filtered$PE_ratio,\n  type = \"l\",                   # Line plot\n  col = \"blue\",                 # Line color\n  lwd = 2,                      # Line width\n  xlab = \"Year\",                # X-axis label\n  ylab = \"P/E Ratio\", # Y-axis label\n  main = \"S&P 500 P/E Ratio (2000–2011)\", # Plot title\n)\n\n# Add a horizontal line for the average P/E ratio\nabline(h = average_pe, col = \"red\", lwd = 2, lty = 2)\n\n# Add a legend to the plot\nlegend(\n  \"topright\",                   # Legend position\n  legend = c(\"P/E Ratio\", \"Average P/E Ratio\"), # Labels\n  col = c(\"blue\", \"red\"),       # Line colors\n  lty = c(1, 2),                # Line types\n  lwd = c(2, 2)                 # Line widths\n)\n```\n\n::: {.cell-output-display}\n![](03-lecture3_files/figure-html/pe-sp500-macrotrends-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\nAt first glance, the market seems undervalued, with a trailing price-to-earnings (P/E) ratio of \n14, well below the historical average \nof 29.\n\nAs a cautious investor, you form the hypothesis that these low valuations \nare deceptive. You believe that corporate profits, which are at record highs \nof 1672 compared to the historical average of \n1119, will revert to the mean.\nWhen this happens, earnings will drop, pushing the P/E ratio from a \nseemingly cheap to expensive.\n\nWith this belief in mind, you decide to hold off on investing, waiting \nfor valuations to normalize. However, as the years pass—2012, 2013, 2014, \nand beyond— corporate profits remain elevated, and the market continues to \nrally. Each year, your cautious stance leaves you with underwhelming \nreturns. \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get corporate profits data from FRED\ncorporate_profits <- tq_get(\"CP\", from = \"2000-01-01\", to = \"2019-12-31\", get = \"economic.data\")\n\n# Visualize corporate profits with a vertical line at 2011\nplot(\n  corporate_profits$date, corporate_profits$price,\n  type = \"l\",\n  col = \"blue\",\n  lwd = 2,\n  xlab = \"Year\",\n  ylab = \"Corporate Profits (Index)\",\n  main = \"Corporate Profits (2000–2019)\"\n)\n\n# Add a vertical line at 2011\nabline(v = as.Date(\"2011-01-01\"), col = \"red\", lwd = 2, lty = 2)\n\n# Add grid lines for readability\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n```\n\n::: {.cell-output-display}\n![](03-lecture3_files/figure-html/corporate profits extended-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\nBy 2019, you’re forced to confront the possibility that your \nbelief about corporate profit mean-reversion might be wrong.\n\n#### The Need for Updating Probability Asessments\n\nThis scenario highlights the importance of updating beliefs in the face of \nnew evidence. Initially, your hypothesis about profit margins reverting to \nthe mean was reasonable, based on historical data. But as year after year \npassed without mean-reversion, the accumulating evidence should have \nprompted you to revise your prior beliefs.\n\nBayes' Rule offers a principled way to do this. It allows you to combine \nyour initial belief (the prior probability) with new \nevidence (e.g., sustained elevated profit margins) to \ncalculate an updated belief (the posterior probability). \nThis process ensures that your decisions adapt as reality unfolds, \nhelping you avoid the dangers of clinging to outdated assumptions.\n\n### Bayes' Rule: Intuitive Understanding with a Speck of Sand\n\nBayes' rule, one of the cornerstone ideas of probability, provides a \nsystematic method for updating probabilities based on new evidence. \nIt is formalized as:\n\\begin{equation*}\nP(B|A) = \\frac{P(A|B) P(B)}{P(A)}\n\\end{equation*}\n\nHere \n\nHere:\n\n- $A$: Represents the new evidence or data that has been observed.\n- $B$: Represents the hypothesis or prior belief about an event.\n\nThis formula arises naturally from the multiplication rule and the \nsymmetry of $(A \\cap B)$. Here's a quick derivation:\n\n1. By the multiplication rule:\n   $P(B|A)P(A) = P(A \\cap B)$\n2. Similarly:\n   $P(A|B)P(B) = P(A \\cap B)$\n3. Equating the two expressions for $P(A \\cap B)$ and dividing by $P(A)$:\n   $P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$\n\nBayes' rule is often challenging to grasp intuitively. To build \nunderstanding, let's explore a simple and vivid example \ninvolving a speck of sand.\n\n#### The Speck of Sand: An Intuitive Illustration\n\nImagine a square of area 1, representing our entire sample space. \nWithin this square, there is a circle $B$, with area equal to $P(B)$. \nYou have a tiny speck of sand on your finger, which accidentally falls \nsomewhere within the square. The location of the speck is entirely random.\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Where is the speck of sand?](figures/where_speck.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n\n\n\nThe probability that the speck lands in $B$ is simply the area of $B$, $P(B)$, \nsince the speck could have landed anywhere within the square with equal likelihood.\n\n#### Updating Beliefs with New Information\n\nNow, suppose you are told that the speck landed within another circle $A$ that \nalso lies inside the square:\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![In fact we learn that the speck is inside circle $A$](figures/speck_in_D.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n\n\n\nHow does this new information affect the probability that the speck is in $B$ ? \nMathematically, we now want to compute $P(B|A)$, the probability that the speck \nis in $B$, given that it is inside $A$.\n\n#### Overlap Between $A$ and $B$\n\nIntuitively, the updated probability $P(B|A)$ depends on the overlap of \n$B$ and $A$. Specifically, it is the fraction of $A$ that lies within $B$, \nexpressed as:\n\\begin{equation*}\nP(B|A) = \\frac{\\text{Area of } A \\cap B}{\\text{Area of } A} = \\frac{P(A \\cap B)}{P(A)}\n\\end{equation*}\n\n- If $A$ and $B$ overlap only slightly, $P(B|A)$ will be small:\n  \n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Small overlap between $A$ and $B$](figures/small_overlap.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n\n\n\n- If the overlap is large, $P(B|A)$ will be large:\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Large overlap between $A$ and $B$](figures/big_overlap.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n\n\n\n#### Bayesian Interpretation\n\nIn Bayesian reasoning:\n\n- $P(B)$ is the **prior probability**, representing our belief in the hypothesis $B$ before observing the data.\n- $P(A|B)$ is the **likelihood**, describing how consistent the observed data $A$ is with the hypothesis $B$.\n- $P(A)$ normalizes the result, ensuring all probabilities sum to 1.\n\n\nThe speck-of-sand example illustrates how Bayesian updating works:\n\n1. Start with a **prior** $P(B)$.\n2. Receive new **evidence** $P(A|B)$.\n3. Update the probability of the hypothesis given the evidence $(P(B|A)$.\n\nBayes' theorem quantifies this intuitive process of revising \nbeliefs based on data.\n\n\n## Using an LLM to Explore Conditional Probability\n\nIn each lecture I try to involve a use case showing you\nhow you could leverage the power of LLMs to deepen your\nlearning experience. Conditional probability is an\nexcellent topic to explore with the LLM. \nUnderstanding conditional probability can be challenging because \nour intuitions about likelihoods often conflict with the \nprecise rules of probability theory. Unlike geometric \nintuition which is hardwired in the human mind through the\nneeds of our visual system, we humans lack an innate\nintuitive understanding of probability. This is something\nthat has to be acquired through training and practice.\nAn LLM like ChatGPT can serve as a dialogue partner to \nhelp you explore this gap and refine your understanding.\n\nHere is a suggested use case:\nYou can use an LLM to simulate and analyze scenarios where our \nintuition might mislead us. Here's how to structure a \ndialogue with the model:\n\n1. **Pose a Real-World Scenario**:\n   - Example: \"Suppose you test positive for a rare disease with a \n   prevalence of 1%. The test is 95% accurate, meaning it correctly identifies \n   positives 95% of the time and negatives 95% of the time. What is the \n   probability that you actually have the disease?\"\n\n2. **Engage in a Dialogue**:\n   - Ask the model to explain how to approach the problem using \n     conditional probability.\n   - Explore intermediate steps such as calculating the prior \n     probability, likelihood, and posterior probability using Bayes' Rule.\n\n3. **Challenge Your Intuition**:\n   - Share your intuitive guess about the probability with the model.\n   - Let the model explain where intuition might diverge from formal \n     reasoning and why.\n\n4. **Experiment with Variations**:\n   - Modify the problem (e.g., change the disease prevalence or test accuracy) \n   and ask the model to recompute probabilities. Compare how small changes in assumptions affect outcomes.\n\n### Example Dialogue\n\nHere’s an example interaction you could try:\n\n**You**: \"I think the probability of actually having the disease should \nbe very high since the test is 95% accurate. Is this correct?\"\n\n**Model**: \"Let’s compute it formally using Bayes’ Rule:\n\\begin{equation*}\nP(D|T) = \\frac{P(T|D) \\cdot P(D)}{P(T|D) \\cdot P(D) + P(T|ND) \\cdot P(ND)}\n\\end{equation*}\nWhere:\n- $P(D) = 0.01$ (prevalence),\n- $P(ND) = 1 - P(D) = 0.99$,\n- $P(T|D) = 0.95$ (true positive rate),\n- $P(T|ND) = 0.05$ (false positive rate).\n\nSubstituting the values:\n$P(D|T) = \\frac{0.95 \\cdot 0.01}{0.95 \\cdot 0.01 + 0.05 \\cdot 0.99} \\approx 0.16$\n\nThis means the probability you actually have the disease is about 16%, much \nlower than intuition might suggest because the disease is so rare.\"\n\nWhy can exercises like this be valuable?\n\n- **Refines Understanding**: Engaging in such dialogues clarifies the \napplication of Bayes’ Rule and highlights the importance of considering base rates.\n- **Reveals Pitfalls of Intuition**: You’ll learn to appreciate why our gut \nfeelings can sometimes mislead us in probabilistic reasoning.\n- **Encourages Exploration**: By modifying scenarios and asking \"what if\" \nquestions, you deepen your grasp of conditional probabilities in diverse contexts.\n\nLeverage this approach to build a stronger connection between the \ntheory and its real-world applications.\n\n\n## Summary\n\n### Probability Concepts\nIn this lecture, we covered the following key probability concepts:\n- **Conditional Probability**: Understanding the probability of an event \noccurring given that another event has occurred, and its relevance in real-world scenarios, especially in finance.\n- **Dependence and Independence**: How events can be dependent \nor independent and the role of conditional probabilities in analyzing \ntheir relationships.\n- **Bayes' Rule**: Using conditional probabilities to update beliefs based \non new evidence.\n\nThese concepts were illustrated with practical examples, including a \ndiscussion of the financial crisis of 2007-2008 to highlight the \nrisks of neglecting dependencies in probability modeling.\n\n### R Concepts\nDuring the lecture, we used R to:\n- **Simulate Conditional Probabilities**: Generate random data and \ncompute conditional probabilities to illustrate theoretical concepts.\n- **Visualize Dependencies**: Create intuitive visualizations of event \noverlaps and relationships between probabilities.\n- **Practical Applications**: Implement real-world examples to explore \nconditional probabilities and Bayes' Rule, showcasing how to use R for \ndata analysis in finance.\n\nThis lecture bridged theoretical probability concepts with practical \ncomputational tools in R, enabling a deeper understanding of \nthe material and its applications.\n\n\n## Project: Evaluating Credit Risk Using Conditional Probabilities\n\n### Problem Description\n\nA bank is evaluating a loan application using historical data to \nestimate the likelihood of default. The borrower has a low credit \nscore, and the bank has the following data:\n\n1. **Default Rates**:\n   - Probability of default for all customers: $P(D) = 0.04$.\n   - Probability of non-default for all customers: $P(ND) = 0.96$.\n\n2. **Evidence**:\n   - Probability of a low credit score given default: $P(L|D) = 0.7$.\n   - Probability of a low credit score given non-default: $P(L|ND) = 0.1$.\n\nThe goal is to determine the posterior probability of default given the \nborrower’s low credit score, $P(D|L)$, using **Bayes' Rule**. Additionally, \nyou will verify this theoretical result by simulating customer data and \nanalyzing outcomes.\n\n---\n\n### Questions\n\n1. **Compute $P(D|L)$ Theoretically**:\n   - Use Bayes' Rule to calculate the posterior probability of default \n   given a low credit score.\n\n2. **Simulate the Scenario in R**:\n   - Simulate a dataset of 10,000 customers where each customer is \n   randomly assigned a default status based on $P(D)$.\n   - Based on the assigned default status, simulate whether each \n   customer has a low credit score using $P(L|D)$ and $P(L|ND)$.\n\n3. **Compute $P(D|L)$ from Simulated Data**:\n   - Use the simulated data to compute $P(D|L)$ and compare it to the theoretical result.\n\n4. **Visualize Results**:\n   - Create a simple bar plot comparing the simulated and theoretical \n   probabilities. If you are unfamiliar with visualization tools, check \n   out the `barplot()` function in base R.\n\n\n\n\n\n",
    "supporting": [
      "03-lecture3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}