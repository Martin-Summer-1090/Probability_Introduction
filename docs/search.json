[
  {
    "objectID": "03-lecture3.html",
    "href": "03-lecture3.html",
    "title": "3  Conditional Probability",
    "section": "",
    "text": "3.1 Why neglecting conditional probability may be expensive: A case study.\nWith this motivation in mind let us turn to a historical example that demonstrates the importance of understanding conditional probability: the financial crisis of 2007-2008. This crisis revealed how wrong assumptions about independence and neglect of dependence in events and conditional probabilities can lead to systemic failures in structured finance.1\nTo understand the real world aspects of this example it is necessarry to understand some basic ideas of structured finance and the engineering of specific risk profiles from a portfolio of risky securities in the first place. I will explain the finance context with a simple and stylized example and then discuss how understanding conditional probability may help us to make better financial decisions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#why-neglecting-conditional-probability-may-be-expensive-a-case-study.",
    "href": "03-lecture3.html#why-neglecting-conditional-probability-may-be-expensive-a-case-study.",
    "title": "3  Conditional Probability",
    "section": "",
    "text": "1  See Tooze (2018): For an engaging and comprehensive exploration of the great financial crisis of 2007 - 2008 and its causes and aftermath. It is highly recommended for deeper study.\n\n3.1.1 Bonds and Credit Risk\nA bond is a financial instrument where the issuer agrees to pay the holder a specific amount, the face value or par value, at maturity. Bonds are widely used as fixed-income securities but carry the risk of default if the issuer faces financial difficulties.\nTo quantify this risk, bonds are rated by agencies such as Moody’s and Standard & Poor’s. Investment-grade bonds are considered low-risk, while speculative or “junk” bonds are riskier and more likely to default. Here is a summary of their rating schemes and what the ratings mean in words:\n\n\n\nRating Category\nMoody’s\nStandard & Poor’s\n\n\n\n\nHigh grade\nAaa\nAAA\n\n\n\nAa\nAA\n\n\nMedium grade\nA\nA\n\n\n\nBaa\nBBB\n\n\nSpeculative grade\nBa\nBB\n\n\n\nB\nB\n\n\nDefault danger\nCaa\nCCC\n\n\n\nCa\nCC\n\n\n\nC\nC\n\n\n\n\nD\n\n\n\n\n\n3.1.2 Pooling and Tranching: The Innovation\nStructured finance emerged in the early 2000s as a way to manage risk through pooling and tranching. By pooling risky assets and dividing cash flows into “tranches” with distinct risk profiles, financial engineers created new bonds, including investment-grade securities, from portfolios of bonds which individually would be rated as speculative grade or junk bonds. A major product of this innovation was the mortgage-backed security (MBS). Many other products were then invented using similar financial engineering ideas.\nLet us develop an intuitive understanding of structured finance and its relation to probability through a simplified example, which I learned from Karl Schmedder’s course.2\n2 See https://www.coursera.org/learn/introductiontoprobability\n\n3.1.3 A Simple Event Tree for One Bond\nConsider a single bond you can own today that pays €1 at maturity at some point in the future. Time is often abbreviated as \\(t\\) and the points in time are symbolized by letting \\(t\\) take different values like \\(t=0\\) for today and \\(t=1\\) for a future point in time, say a year from today.\nThis bond has a 10% chance of default, meaning there is a 90% chance it will not default. With a default probability of 10%, this bond would likely receive a speculative grade rating, such as ‘B’ or ‘B-’ in the rating tables presented earlier. This poor rating reflects the significant risk of non-payment associated with such a bond, which could deter risk-averse investors and highlight its ‘junk’ bond status. The payoff is structured as follows:\n\nIf the bond does not default (\\(N\\)), the payoff is €1.\nIf the bond defaults (\\(D\\)), the payoff is €0.\n\nThis situation can be graphically represented as a simple probability tree of Figure 3.1\n\n\n\n\n\n\n\n\nFigure 3.1: Event tree for one bond\n\n\n\n\n\nThe graph above visualizes the outcomes of a single bond. Each node represents a possible state of the bond at different times:\n\n\\(t=0\\) is the starting point.\n\\(t=1\\) No Default \\(N\\) occurs with a probability of \\(P(N) = 0.9\\).\n\\(t=1\\) Default \\(D\\) occurs with a probability of \\(P(D) = 0.1\\).\n\nYou could see this in analogy to the toss of a coin with the difference that both sides of the coin show with different probability. With this analogy - using the concepts of the last two lectures - you can understand the bond in probabilistic terms as a random experiment with a sample space consisting of two basic outcomes, \\(N\\) and \\(D\\) with given probabilities \\(P(N)\\) and \\(P(D)\\).\n\n\n3.1.4 Combining Two Bonds with Independence Assumption\nWe now combine two such bonds into a portfolio. The assumption of independence implies that the defaults of these bonds occur independently of each other. This means that the default of one bond has no influence on the probability of the other bond defaulting. Under the assumption of independence you would treat the default probability of one bond as unconnected with the default probability of the other.\nWhile this assumption simplifies calculations, it was historically used by financial engineers to justify creating tranches from risky portfolios. The reasoning was that diversification reduces the likelihood of joint defaults, making some tranches appear safer.\nAt the time, financial engineers relied on historical data and market conditions to argue for this independence. Defaults were often uncorrelated under normal economic conditions, and diversification was seen as a proven strategy for mitigating risk. For example, if bond defaults were driven by isolated events (such as company-specific issues), the assumption of independence seemed reasonable. Moreover, the packaging of diverse assets from different industries into portfolios strengthened the appearance of safety, as individual economic shocks were less likely to affect the entire portfolio simultaneously.\nHowever, this reasoning neglected systemic risks. During economic downturns or financial crises, defaults often become highly correlated due to shared macroeconomic pressures, such as declining housing markets or credit tightening. For instance, in the lead-up to the 2008 financial crisis, rising mortgage defaults were driven by broader economic factors that impacted many bonds simultaneously. With this in mind it would be not plausible to assume that bonds can be stacked together in a portfolio without the default risks of one being not pushed up by the default risk of others.\nEven without the formal use of probability theory, financial engineers could have questioned whether diversification truly guaranteed independence in the context of systemic risks.\nThe idea that junk plus junk could be transformed into investment-grade bonds through pooling should have raised skepticism. Careful critical thinking—considering broader economic dependencies—would have revealed that this transformation was too good to be true. By ignoring these dependencies, financial engineers failed to see how small cracks in the system could cascade into systemic failures.3\n3  A famous voice at the time warning about the flawed reasoning was Raghuram Rajan, former chief economist of the International Monetary Fund. He warned that rather than reducing risk through diversification, CDOs and other derivatives spread risk and uncertainty about the value of the underlying assets more widely. But let us not jump ahead and try to see how the tree for two bonds looks like when we assume independence in Figure 3.2.\n\n\n\n\n\n\n\n\nFigure 3.2: Event tree for two bonds with independent default risk\n\n\n\n\n\n\nThe outcome of combining two bonds can be represented as a double event tree, showing all possible combinations of defaults and no defaults at \\(t=1\\). Let me unpack this more complex tree for you.\nSince we have a portfolio of two bonds, instead of one as before, we have now two event trees combined. Since we have to take into account all of the possible combinations of basic outcomes this means that we have to attach a new bond event tree to each outcome of the initial tree.\nNote the time index. In the example the realizations of basic outcomes for the portfolio happen simultaneously. So the walk from the origin to the end of the tree along a path is taking place in one time step and only the outcomes at \\(t=1\\) are observed. There is one subtlety in this representation which I would like to explicitly point out. The tree suggests a sequence - the first bond realizes its state and then the second. This is true but it does not mean at the same time that the bond holders observe this sequence. From their point of you they only see outcomes at the indicated time steps. (at the dashed lines, if you will). This information structure suggests that we should think of these state realizations of the bonds as occurring simulateously. Think of it as if yo u would toss tow coins at once.\nAt the edges we write the probability of the outcomes. For example \\(P(N)\\) is the probability of the first bond (represented by the upper tree) is not defaulting whereas \\(P(D)\\) denotes the probability of the bond not defaulting. The assumption of independence is hidden in this tree by modelling the probabilities of \\(N\\) and \\(D\\) for the second bond in exactly the same way no matter whether the first bond defaults or not. It is modelled in anaology to the toss of two fair coins. The probability of the second coin showing Heads is \\(1/2\\) no matter whether the first coin shows Heads or Tails.\nAt the end of the tree we have written the outcome of each path in the notation \\[\\begin{equation*}\nB_{\\text{state of bond 1 at} \\, t = 1 \\, \\text{state of bond 2 at} \\, t=1} = \\binom{\\text{payoff of bond 1 at } \\, t= 1}{\\text{payoff of bond 2 at } \\, t = 1}\n\\end{equation*}\\] So, for example, \\(B_{NN} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) means that bond 1 does not default and bond two does not default (\\(B_{NN}\\)). Bond 1 has in this case a payoff of 1 and bond 2 also has a payoff of 1.\n\n\n\n3.1.5 Pooling and Tranching\nWhen you look at this portfolio under the independence assumption there seems to be room for re-engineering the risk profile of this portfolio. In all outcomes but in the one where both bonds default you can guarantee a payoff of 1. There remains an adverse outcome, where both bonds default in which case you can’t pay out anything. But under the assumption of independence this risk is small. The probability of this event - remember our treatment of independent events in the first lecture - would be: \\(P(D) \\times P(D) = 0.1 \\times 0.1 = 0.01\\). Pretty low, actually. For example assume that the probability of default refer to the probability of the bond defaulting over a year, the usual time frame taken in ratings, this would be a one in a hundred years event. In \\(99%\\) we would get a sure payoff of 1. So under this restructuring the first restructured bond would qualify as an investment grade bond.\nSo this is the idea. We pool the payoffs of both securities and define two new securities by changing their payoff profile. The first one pays always 1 except when both bonds default in which case this bond pays 0. The other one will always pay 0 except in the case where both bonds do not default. This is under independence an event with probability \\(P(N) \\times P(N) = 0.81\\). Rember the complementarity rule? This says that the second restructured bond will thus have a default probability of \\(19\\) % instead of \\(10%\\) it would be speculative grade or close to toxic junk now.\nHere is picture how you can visualize this piece fo financial magic. This picture can be read in exactly the same way as the previous picture. There is only one additional element. We have written the payoff of the original bonds by \\(B\\). Underneath these original bonds we draw a black horizontal like, think of it as the financial engineering lab that does the restructuring and below we get new bonds, with different payout promises, which we denote by \\(R\\) (for restructured).\n\n\n\n\n\n\n\n\nFigure 3.3: Event tree for two bonds with independent default risk with pooling and tranching\n\n\n\n\n\nWhat is done here is that the aggregate payoffs of both bonds are collected in a pool and new securities - called R - in this picture are issued against the pool. One, the upper one is now an investment grade asset paying 1 in every state except one and the other is a toxic junk bond paying always 0 except in one state. Note that the investment grade status could be engineered under the assumption that the risks are independent.\n\n\n3.1.6 Pooling and Tranching without Independent Risks\nNow, let us consider a hypothetical question: How would the event tree change if the independence assumption does not hold? Dependence would alter the probabilities in a way that reflects the increased likelihood of joint defaults during systemic events.\nSuppose we now assume that the probability of Bond 2 defaulting changes rather than staying unchanged under the condition that Bond 1 has defaulted:\n\nIf Bond 1 does not default, Bond 2 defaults with a lower probability than before, here 0.044 instead of 0.1.\nIf Bond 1 does default, Bond 2 defaults with a higher probability of 0.6 due to systemic dependence.\n\nTo express this formally we need a piece of new notation. The convention in probability theory is that the notation is - for example - \\(P(N | D)\\). This would read as bond 2 does not default given bond 1 has defaulted. The event we are interested in is written first separated by a \\(|\\) from the conditioning event, which is in our case the outcome that bond 1 defaults.\nThis dependence reflects a scenario where defaults are more likely to occur together, such as during an economic downturn. The resulting event tree can be visualized as follows:\n\n\n\n\n\n\n\n\nFigure 3.4: Event tree for two bonds with independent default risk with pooling and tranching with dependent default risk\n\n\n\n\n\nOn the first sight this looks almost identical to the picture before. Only the numbers on the edges of the second tree have changed. These changed numbers reflect the idea that the event that one bond has defaulted changes the probability of the second bond defaulting as well. How could such a dependence occur?\nHere’s a real-world example to illustrate how dependence can occur: In the context of bonds, dependence in default probabilities can arise from shared exposure to systemic risks or interconnected factors. For instance, consider two companies that issue bonds and operate in the same industry, such as the energy sector. If oil prices plummet due to an economic downturn or geopolitical instability, both companies might experience financial stress, making it more likely that one default is followed by another.\nAnother example is during a financial crisis, such as the 2008 global financial meltdown. A bank’s default on its obligations can lead to cascading defaults in other institutions due to counterparty risks or a general loss of confidence in the financial system. In such cases, the probability of a second default is no longer independent of the first because the events are tied to the same underlying macroeconomic factors.\nThese examples highlight that the assumption of independence between bond defaults might hold under normal market conditions but breaks down during systemic crises. Such dependencies must be carefully modeled to avoid underestimating risk, as was the case in structured finance products leading up to the 2008 crisis.\nA prudent risk manager must keep such a scenario in mind when he analyzes a portfolio. Think about it in the context of the toy example. In the first case the default risk of the first asset created by pooling and tranching was \\(P(D) \\times P(D) = 0.1 \\times 0.1 = 0.01\\). Under a scenario with dependent risks this changes to \\(P(D) \\times P(D | D) = 0.1 \\times 0.6 = 0.06\\), a risk larger by a factor of 6! While in the first case the first restructured bond would be rated as investment grade, in the second case the same restructured bond would be rated as speculative grade and the magic from pooling and tranching suddenly disappears. Junk plus junk remains junk after all.\nFor pooling and tranching to reduce overall risk and create safe tranches we need:\n\nDiversification: Assets must come from independent sectors with minimal systemic risk.\nStable Macroeconomic Conditions: Systemic risks must be low to maintain independence assumptions.\nTransparent Modeling: Dependence structures must be explicitly modeled and accounted for in risk assessments.\n\nThe neglect of these conditions led to a flawed sense of security in structured finance, which contributed to the 2008 financial crisis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#conditional-probability",
    "href": "03-lecture3.html#conditional-probability",
    "title": "3  Conditional Probability",
    "section": "3.2 Conditional Probability",
    "text": "3.2 Conditional Probability\nConditional probability provides us with a concept to formalize how the probability of one event changes when another event is known to occur, providing a framework for understanding dependencies quantitatively.\nHere is the mathematical definition:\n\n\n\n\n\n\nDefinition: Conditional probability\n\n\n\nLet \\(B\\) be an event with positive probability. For an arbitrary event \\(A\\) we define the conditional probability of \\(A\\) given \\(B\\) as \\[\\begin{equation*}\nP(A\\,|\\,B) = \\frac{P(A \\cap B)}{P(B)}\\,\\,\\, \\text{provided}\\,\\,\\, P(B) \\neq 0\n\\end{equation*}\\]\n\n\nNote that conditional probabilities remain undefined when the conditioning event \\(B\\) has probability 0.4\n4 As pointed out in Feller (1968) p 115, this has no consequences in the case of discrete sample spaces but is important in the general theory.Let us clarify a few things about this concept. As in the example of the financial crisis, which we discussed before we really did not much more than introducing one piece of new notation to indicate that the probabilities now have changed.\nLet’s revisit our example.\n\n3.2.1 The Probability Tree and Conditional Probabilities\nThe tree is labeled with edge probabilities, which correspond to the marginal and conditional probabilities of events at each level:\n\nAt \\(t=0\\): \\(P(B_1 = N) = 0.9\\) and \\(P(B_1 = D) = 0.1\\).\nAt \\(t=1\\): The probabilities for \\(B_2\\) defaulting given \\(B_1\\)’s outcome are:\n\n\\(P(B_2 = N \\,|\\, B_1 = N) = 0.956\\), \\(P(B_2 = D \\,|\\, B_1 = N) = 0.044\\).\n\\(P(B_2 = N \\,|\\, B_1 = D) = 0.4\\), \\(P(B_2 = D \\,|\\, B_1 = D) = 0.6\\).\n\n\n\n\n\n3.2.2 Defining Probabilities in R\nWe start by defining the probabilities from the probability tree:\n\n# Define the probabilities\n\n# Marginal probabilities for B_1\nP_N &lt;- 0.9  # Probability that B_1 does not default\nP_D &lt;- 0.1  # Probability that B_1 defaults\n\n# Conditional probabilities for B_2 given B_1\nP_N_given_N &lt;- 0.8604/0.9  # Probability that B_2 does not default given B_1 does not default\nP_D_given_N &lt;- 0.0396/0.9  # Probability that B_2 defaults given B_1 does not default\nP_N_given_D &lt;- 0.4    # Probability that B_2 does not default given B_1 defaults\nP_D_given_D &lt;- 0.6    # Probability that B_2 defaults given B_1 defaults\n\n\n\n\n3.2.3 Computing Joint Probabilities\nThe joint probabilities, \\(P(A \\cap B)\\), are calculated using the multiplication rule: \\[\nP(A \\cap B) = P(A \\,|\\, B) \\cdot P(B).\n\\] These correspond to probabilities computed along the edges of the tree.\n\n# Calculate joint probabilities\n\nP_NN &lt;- P_N * P_N_given_N  # Both bonds do not default\nP_ND &lt;- P_N * P_D_given_N  # B_1 does not default, B_2 defaults\nP_DN &lt;- P_D * P_N_given_D  # B_1 defaults, B_2 does not default\nP_DD &lt;- P_D * P_D_given_D  # Both bonds default\n\n\n\n\n3.2.4 Constructing the Contingency Table\nWe can summarize the joint probabilities in a contingency table, which also includes marginal probabilities:\n\n# Load necessary library\nlibrary(kableExtra)\n\n# Define marginal probabilities\nP_N_total &lt;- P_N\nP_D_total &lt;- P_D\nP_N_given_total &lt;- round(P_NN + P_DN,2)\nP_D_given_total &lt;- round(P_ND + P_DD,2)\n\n# Create the contingency table\ncontingency_table &lt;- matrix(\n  c(\n    P_DD, P_DN, P_D_total,    # Row 1: Bond 1 Default\n    P_ND, P_NN, P_N_total,    # Row 2: Bond 1 No Default\n    P_D_given_total, P_N_given_total, 1  # Row 3: Column totals\n  ),\n  nrow = 3,\n  byrow = TRUE,\n  dimnames = list(\n    c(\"Bond 1: Default (D)\", \"Bond 1: No Default (N)\", \"Total\"),\n    c(\"Bond 2: Default (D)\", \"Bond 2: No Default (N)\", \"Total\")\n  )\n)\n\n# Create a styled table\nkable_output &lt;- knitr::kable(\n  contingency_table,\n  caption = \"Contingency Table of Joint and Marginal Probabilities\",\n  format = \"html\",\n  escape = FALSE,\n  col.names = c(\"\", \"Default (D)\", \"No Default (N)\", \"Total\")\n) %&gt;%\n  kableExtra::column_spec(1, bold = TRUE)\n\n# Render the table\nkable_output\n\n\nContingency Table of Joint and Marginal Probabilities\n\n\n\nDefault (D)\nNo Default (N)\nTotal\n\n\n\n\nBond 1: Default (D)\n0.0600\n0.0400\n0.1\n\n\nBond 1: No Default (N)\n0.0396\n0.8604\n0.9\n\n\nTotal\n0.1000\n0.9000\n1.0\n\n\n\n\n\n\n\nNote that conditional probabilities are in general not symmetric.\n\n# Recalculate marginal probabilities\nP_B2_D &lt;- P_N * P_D_given_N + P_D * P_D_given_D\n\n# Recompute conditional probabilities\nP_B2_given_B1 &lt;- P_DD / P_D_total  # P(B_2 = D | B_1 = D)\nP_B1_given_B2 &lt;- P_DD / P_B2_D     # P(B_1 = D | B_2 = D)\n\n# Display results\ncat(\"P(B_2 = D | B_1 = D):\", round(P_B2_given_B1, 4), \"\\n\")\n\nP(B_2 = D | B_1 = D): 0.6 \n\ncat(\"P(B_1 = D | B_2 = D):\", round(P_B1_given_B2, 4), \"\\n\")\n\nP(B_1 = D | B_2 = D): 0.6024 \n\n\nThe example above demonstrates that conditional probabilities are, in general, not symmetric. Using the constructed contingency table and the probability tree, we calculated two conditional probabilities:\n\n\\(P(B_2 = D \\,|\\, B_1 = D)\\): the probability that bond \\(B_2\\) defaults given that bond \\(B_1\\) defaults.\n\\(P(B_1 = D \\,|\\, B_2 = D)\\): the probability that bond \\(B_1\\) defaults given that bond \\(B_2\\) defaults.\n\nThese probabilities are computed as follows: \\[\nP(B_2 = D \\,|\\, B_1 = D) = \\frac{P(B_1 = D \\cap B_2 = D)}{P(B_1 = D)}\n\\] and \\[\nP(B_1 = D \\,|\\, B_2 = D) = \\frac{P(B_1 = D \\cap B_2 = D)}{P(B_2 = D)}.\n\\]\nDespite using the same joint probability, \\(P(B_1 = D \\cap B_2 = D)\\), the denominators differ: one uses \\(P(B_1 = D)\\), while the other uses \\(P(B_2 = D)\\). Because these marginal probabilities are generally not equal, the resulting conditional probabilities are also unequal. This asymmetry reflects the fundamental property of conditional probability and underscores the importance of clearly defining the conditioning event in probabilistic reasoning.\n\n\n3.2.5 Simulating a Bond Portfolio\nWe can extend this theoretical framework to simulate a bond portfolio with defaults and compute conditional probabilities empirically:\n\n# Number of bonds\nN &lt;- 5000 \n\n# Simulate joint outcomes\nsimulate_defaults &lt;- function(n, probs) {\n  outcomes &lt;- c(\"B_DD\", \"B_DN\", \"B_ND\", \"B_NN\")\n  draws &lt;- sample(outcomes, n, replace = TRUE, prob = c(probs))\n  return(draws)\n}\n\n# Generate portfolio data\nportfolio &lt;- data.frame(\n  BondID = 1:N,\n  BondType = sample(c(\"B_1\", \"B_2\"), N, replace = TRUE, prob = c(0.5, 0.5))\n)\n\n# Assign joint default outcomes\njoint_outcomes &lt;- simulate_defaults(N, c(P_DD, P_DN, P_ND, P_NN))\nportfolio$JointOutcome &lt;- joint_outcomes\nportfolio$X_Defaulted &lt;- ifelse(portfolio$JointOutcome %in% \n                                  c(\"B_DD\", \"B_DN\"), TRUE, FALSE)\nportfolio$Y_Defaulted &lt;- ifelse(portfolio$JointOutcome %in% \n                                  c(\"B_DD\", \"B_ND\"), TRUE, FALSE)\n\n# Preview the portfolio\nhead(portfolio)\n\n  BondID BondType JointOutcome X_Defaulted Y_Defaulted\n1      1      B_1         B_NN       FALSE       FALSE\n2      2      B_1         B_NN       FALSE       FALSE\n3      3      B_1         B_ND       FALSE        TRUE\n4      4      B_2         B_NN       FALSE       FALSE\n5      5      B_2         B_NN       FALSE       FALSE\n6      6      B_1         B_ND       FALSE        TRUE\n\n\n\nNow let us compute the conditional and uncodnitional default probabilities from these data:\n\n# Compute unconditional probabilities\nP_X_D &lt;- mean(portfolio$X_Defaulted)  # P(X = D)\nP_Y_D &lt;- mean(portfolio$Y_Defaulted)  # P(Y = D)\n\n# Compute conditional probabilities\nP_X_given_Y_D &lt;- mean(portfolio$X_Defaulted[portfolio$Y_Defaulted == TRUE])  \n# P(X = D | Y = D)\nP_Y_given_X_D &lt;- mean(portfolio$Y_Defaulted[portfolio$X_Defaulted == TRUE])  \n# P(Y = D | X = D)\n\n# Display results\ncat(\"Unconditional Probabilities:\\n\")\n\nUnconditional Probabilities:\n\ncat(\"P(X = D):\", round(P_X_D, 4), \"\\n\")\n\nP(X = D): 0.1004 \n\ncat(\"P(Y = D):\", round(P_Y_D, 4), \"\\n\\n\")\n\nP(Y = D): 0.1 \n\ncat(\"Conditional Probabilities:\\n\")\n\nConditional Probabilities:\n\ncat(\"P(X = D | Y = D):\", round(P_X_given_Y_D, 4), \"\\n\")\n\nP(X = D | Y = D): 0.59 \n\ncat(\"P(Y = D | X = D):\", round(P_Y_given_X_D, 4), \"\\n\")\n\nP(Y = D | X = D): 0.5876 \n\n\nThis simulation provides an intuitive approach to understanding conditional probabilities. By simulating joint outcomes for the bond portfolio, we can empirically calculate conditional probabilities as relative frequencies, making the concept more tangible.\nFor example, to compute the conditional probability \\(P(X = D \\,|\\, Y = D)\\), we first identify the subset of simulated outcomes where \\(Y = D\\). Within this subset, we calculate the relative frequency of cases where \\(X = D\\).\nThis perspective highlights that conditional probabilities are calculated not on the entire dataset, but on a relevant subset that satisfies the conditioning event. By focusing only on rows (or simulated outcomes) where the condition \\(Y = D\\) holds, we avoid unnecessary complexity and make the dependency between events clearer. This approach complements theoretical definitions by emphasizing the practical interpretation of conditional probabilities as ratios within subsets, a viewpoint that is often easier for students to grasp.\nIn contrast to the earlier theoretical examples, which rely on abstract calculations, this method directly ties conditional probability to observable outcomes in the data, providing a more intuitive and relatable learning experience. This tangible connection between the formula and simulated data reinforces the understanding of conditional probability as a measure of dependence between events.\n\n\n3.2.6 Key Insights\n\nDependence in Default Risks:\n\nConditional probabilities highlight the dependence between \\(B_1\\) and \\(B_2\\):\n\n\\(P(B_2 = D \\,|\\, B_1 = D) = 0.6\\), while \\(P(B_2 = D) = 0.1\\).\n\n\nSymmetry Does Not Hold:\n\n\\(P(B_2 = D \\,|\\, B_1 = D) \\neq P(B_1 = D \\,|\\, B_2 = D)\\).\n\n\nThese examples demonstrate the importance of modeling dependencies explicitly in finance, especially in risk assessment. Let me know if further refinements are needed!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#advanced-r-concepts-environments-scoping-rules-and-closures",
    "href": "03-lecture3.html#advanced-r-concepts-environments-scoping-rules-and-closures",
    "title": "3  Conditional Probability",
    "section": "3.3 Advanced R Concepts: Environments, Scoping Rules, and Closures",
    "text": "3.3 Advanced R Concepts: Environments, Scoping Rules, and Closures\nIn this section, we will explore some advanced R programming concepts that are essential for understanding how R evaluates and stores variables, as well as how you can create reusable and dynamic functions. We will demonstrate these concepts through examples related to conditional probability and financial modeling.\n\n3.3.1 Introduction to Environments\nAn environment in R is where objects (variables, functions, etc.) are stored and looked up. R uses environments to determine where a variable exists and what its value is. The most common environment is the global environment, where user-created variables and functions are stored.\nExample: Setting Global and Local Variables\nSuppose we are modeling interest rates in a financial portfolio. Globally, we set the baseline interest rate. Locally, we may override this rate for specific calculations.\n\n# Global interest rate\ninterest_rate &lt;- 0.05  # 5%\n\n# Function to calculate interest payments\ncalculate_interest &lt;- function(principal, rate = interest_rate) {\n  interest &lt;- principal * rate  # Uses the rate passed to the function\n  return(interest)\n}\n\n# Global calculation\nglobal_interest &lt;- calculate_interest(1000)  # Uses global interest_rate\ncat(\"Global Interest:\", global_interest, \"\\n\")\n\nGlobal Interest: 50 \n\n# Local override\nlocal_interest &lt;- calculate_interest(1000, rate = 0.07)  # Overrides global interest_rate\ncat(\"Local Interest:\", local_interest, \"\\n\")\n\nLocal Interest: 70 \n\n\nGlobal variables are available everywhere, but local variables (like rate) take precedence within a function. Understanding this behavior is crucial for writing clear and predictable code.\n\n\n3.3.2 Scoping Rules\nR follows specific scoping rules to determine where and how to find variables. These rules become important when working with nested functions.\nExample: Variable Lookup in Nested Functions\nLet’s calculate conditional probabilities using nested functions. We simulate a financial scenario where we compute probabilities of default for different credit ratings.\n\n# Define global default rates for credit ratings\n\ndefault_rates &lt;- c(\n  AAA = 0.01,  # Global default rate for AAA bonds\n  BBB = 0.02,  # Global default rate for BBB bonds\n  Junk = 0.05  # Global default rate for Junk bonds\n)\n\n# Function to calculate conditional default probability\nconditional_default &lt;- function(rating) {\n  \n  # Lookup table for default rates\n  local_default_rates &lt;- c(\n    AAA = unname(default_rates[\"AAA\"]),  # Local default for AAA\n    BBB = unname(default_rates[\"BBB\"]),  # Local default for BBB\n    Junk = unname(default_rates[\"Junk\"]) # Local default for Junk\n  )\n  \n  # Return the default rate using vectorized subsetting\n  \n  return(local_default_rates[rating])\n}\n\n# Test the function\ncat(\"Default rate for Junk bonds:\", conditional_default(\"Junk\"), \"\\n\")\n\nDefault rate for Junk bonds: 0.05 \n\ncat(\"Default rate for BBB bonds:\", conditional_default(\"BBB\"), \"\\n\")\n\nDefault rate for BBB bonds: 0.02 \n\ncat(\"Default rate for AAA bonds:\", conditional_default(\"AAA\"), \"\\n\")\n\nDefault rate for AAA bonds: 0.01 \n\n\nHere you see how R is using Lexical scoping. This ensures that R looks for variables in the closest environment first, then moves outward (from local to global). Nested functions can use both local and global variables.\nThis example uses a concept you might find useful in many other contexts: The lookup table. The concept of a lookup table is a simple yet powerful way to map input values to corresponding outputs. In R, we can create a lookup table using a named vector, where each element has a name (the input) and a value (the corresponding output). This allows us to retrieve the correct value by directly referencing the name.\nIn the example, we used a named vector local_default_rates to store the default probabilities for different credit ratings: “AAA”, “BBB”, and “Junk”. Each credit rating serves as a key, and the corresponding default probability serves as the value. When we pass the rating (e.g., “Junk”) to the function, R uses it to subset the vector and directly return the associated probability. This approach is efficient and avoids the need for verbose or complex conditional statements.\nBy using a lookup table, we also demonstrate an important principle of programming: separation of data and logic. The mapping of ratings to probabilities is encapsulated in a single data structure (local_default_rates), making the function simpler and easier to modify. For instance, if the default probabilities change, you only need to update the values in the vector—no changes to the function logic are required. This approach is especially useful in financial modeling, where mappings like these are common and can evolve over time.\n\n\n3.3.3 Closures\nA closure is a function that remembers the environment in which it was created. Closures are powerful for creating dynamic, reusable functions, such as calculators for different conditional probabilities.\nExample: Probability Calculator Factory\nThis code demonstrates a powerful concept in R: closures. A closure is a function that “remembers” the environment in which it was created, allowing you to dynamically generate new functions with specific behaviors. Let’s create a function factory that generates specific probability calculators based on a given event.\n\n# Function factory for conditional probability calculators\n\nprobability_calculator_factory &lt;- function(event_probability) {\n  function(conditional_probability) {\n    joint_probability &lt;- event_probability * conditional_probability\n    return(joint_probability)\n  }\n}\n\n# Create calculators for different events\n\njunk_calculator &lt;- probability_calculator_factory(0.05)  # Junk bonds\nbbb_calculator &lt;- probability_calculator_factory(0.02)   # BBB bonds\n\n# Calculate joint probabilities\n\njunk_joint &lt;- junk_calculator(0.1)  # P(Default | Junk) * P(Junk)\nbbb_joint &lt;- bbb_calculator(0.2)    # P(Default | BBB) * P(BBB)\n\ncat(\"Joint probability for Junk bonds:\", junk_joint, \"\\n\")\n\nJoint probability for Junk bonds: 0.005 \n\ncat(\"Joint probability for BBB bonds:\", bbb_joint, \"\\n\")\n\nJoint probability for BBB bonds: 0.004 \n\n\nLet’s unpack the code step by step:\nThe probability_calculator_factory is a function factory. It takes one argument, event_probability, and returns a new function that calculates the joint probability for a given conditional probability:\n\nInput:\n\nevent_probability: The probability of the event (e.g., the probability of a bond being “Junk”).\nThe returned function takes conditional_probability as its argument (e.g., the probability of default given the bond is “Junk”).\n\nOutput:\n\nThe joint probability, \\(P(A \\cap B) = P(A | B) \\times P(B)\\).\n\n\nThis structure encapsulates the logic for joint probability into a reusable framework.\nThe junk_calculator and bbb_calculator are functions created by the factory. Each calculator “remembers” the event_probability it was initialized with: - junk_calculator: \\(P(Junk) = 0.05\\). - bbb_calculator: \\(P(BBB) = 0.02\\).\nThese calculators are then used to compute joint probabilities by providing the corresponding conditional probabilities:\n\njunk_joint &lt;- junk_calculator(0.1):\n\n\\(P(\\text{Default} \\cap \\text{Junk}) = P(\\text{Default | Junk}) \\times P(\\text{Junk})\\).\n\\(0.1 \\times 0.05 = 0.005\\) (0.5%).\n\nbbb_joint &lt;- bbb_calculator(0.2):\n\n\\(P(\\text{Default} \\cap \\text{BBB}) = P(\\text{Default | BBB}) \\times P(\\text{BBB})\\).\n\\(0.2 \\times 0.02 = 0.004\\) (0.4%).\n\n\nThe cat() function displays the results:\nA closure allows you to “lock in” parameters (like event_probability) when the function is created, while still allowing flexibility for additional inputs.\n\n\n\n\n\n\nNow you try\n\n\n\n\nModify the calculate_interest function to add a penalty rate for overdue payments using local variables.\nExtend the conditional_default function to include an additional credit rating (e.g., “CC”).\nUse the probability_calculator_factory to compute joint probabilities for a new event, such as “Real Estate Sector Default.”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#bayes-rule-one-of-the-great-ideas-in-probability",
    "href": "03-lecture3.html#bayes-rule-one-of-the-great-ideas-in-probability",
    "title": "3  Conditional Probability",
    "section": "3.4 Bayes’ Rule: One of the Great Ideas in Probability",
    "text": "3.4 Bayes’ Rule: One of the Great Ideas in Probability\nBayes’ Rule stands among the ten great ideas in probability. Its power lies in solving a fundamental problem: how to infer underlying chances from observed frequencies. This insight filled a critical gap left by Bernoulli’s weak law of large numbers, which explained how observed frequencies converge to probabilities but left unanswered the question of how to reason from those frequencies back to the chances that generated them.\nAt its core, Bayes’ Rule provides a framework for updating our beliefs in light of new information. But what does it mean to assign a probability to a belief? This idea rests on another deep insight in probability theory: judgments about uncertainty can be measured, and when those judgments are coherent, they follow the rules of probability. 5\n5 This idea goes back to the work of Frank Ramsey (1903–1930). He was a British mathematician, philosopher, and economist whose profound contributions spanned multiple fields despite his tragically short life. In probability, he established the foundation for subjective probability theory, showing that coherent judgments about uncertainty adhere to the axioms of probability. Ramsey also made groundbreaking advances in decision theory, logic, and economics, including the famous Ramsey pricing in public economics and his foundational work in mathematical logic. For more, on Ramsey’s ideas of connecting judegment to probability see chapter 2 in Diaconis and Skyrms (2019)The key breakthrough of Bayes’ Rule is that it ties these coherent judgments—our initial beliefs, or priors—to evidence, using conditional probability. This process transforms subjective judgments into a systematic method for reasoning under uncertainty, with profound applications across science, finance, and everyday decision-making.\n\n3.4.1 A simple example: Revising beliefs about market valuation\nWe discussed the big financial crisis of 2007-2008 earlier in this lecture. Imagine now you are an investor in the year 2011, just a bit later. The market is gripped by fear of another financial crisis, this time triggered by tensions in the Eurozone.\nLet’s imagine how you could have asessed the market situation by looking at financial data. This is also an excellent opportunity to introdcue you to one of many great opportunities to load real world data directly into R using add on packages. The package we are going to use here is called tidyquant. To be able to use it you must first install it using the install.packages()function of base R or the package installer pane in RStudio. Let us do that - assuming that this package has been installed. You might remember how to load a package from before.\n\nlibrary(tidyquant)\n\nNow tidyquant has many functions allowing you to retrieve and transform real world financial data. We do not go into any detail here. This is something you can do yourself using the excellent documentation of this package, the many examples on the web or by interrogating your LLM.6. One of the core workhorse functions in tidyquant is the function tq_get() which allows you to retrieve data.\n6 Check out https://cran.r-project.org/web/packages/tidyquant/index.htmlOne thing an anlyst might be interested in is how the stock market as measured by a broad index does at the moment compared to its historical values. Let’s say you are looking back 11 years from 2011 into the past and see how the S&P500 does now compared to this history. This is how you would use tidyquant to do this:\n\n# Fetch S&P 500 data from 2000 to 2011\n\nsp500_data &lt;- tq_get(\"SPY\", from = \"2000-01-01\", to = \"2011-12-31\")\n\nIn order to do load data you need the string of the name of the series you are interested in, which you can learn from the documentation. In our case this is “SPY” for the SP500. You can also specify a time range or just the beginning of the series. In the latter case it will give you all the data from the beginning up to the current or most recent trading day. We have written the data into an R object called sp500_data and now you might inspect it.\n\nhead(sp500_data)\n\n# A tibble: 6 × 8\n  symbol date        open  high   low close   volume adjusted\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 SPY    2000-01-03  148.  148.  144.  145.  8164300     92.7\n2 SPY    2000-01-04  144.  144.  140.  140.  8089800     89.1\n3 SPY    2000-01-05  140.  142.  137.  140  12177900     89.2\n4 SPY    2000-01-06  140.  142.  138.  138.  6227200     87.8\n5 SPY    2000-01-07  140.  146.  140.  146.  8066500     92.9\n6 SPY    2000-01-10  146.  147.  145.  146.  5741700     93.2\n\n\nThe output says that the data object is a tibble. Don’t worry about this detail at the moment and think of a tibbleas something equivalent to a dataframe.\nNow let us visualize the data using base R’s plot() function. Use the help facilities or your LLM to find out abut the syntax details.\n\n# Base R plot\nplot(\n  sp500_data$date, sp500_data$adjusted,\n  type = \"l\",                # Line plot\n  col = \"blue\",              # Line color\n  lwd = 2,                   # Line width\n  xlab = \"Date\",             # X-axis label\n  ylab = \"Adjusted Price\",   # Y-axis label\n  main = \"S&P 500 Price Trends (2000–2011)\" # Title\n)\n\n# Add grid lines for better readability\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nWe see that the index is about the same level as it was 10 years ago. The index alone does not tell you very much. How about relating this to some relevant market fundamentals?\nOne powerful feature of tidyquant() is that it can fetch data from various data sources. Here I get, for the sake of this example, data on corporate profits from the FRED database of the Fed St. Louis7\n7 See https://fred.stlouisfed.org/\n# Get corporate profits data from FRED\ncorporate_profits &lt;- tq_get(\"CP\", from = \"2000-01-01\", \n                            to = \"2011-12-31\", get = \"economic.data\")\n\n# Visualize corporate profits\nplot(\n  corporate_profits$date, corporate_profits$price,\n  type = \"l\",\n  col = \"blue\",\n  lwd = 2,\n  xlab = \"Year\",\n  ylab = \"Corporate Profits (Index)\",\n  main = \"Corporate Profits (2000–2011)\"\n)\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nProfits have roughly tripled over the same period. The market does seem to be undervalued. Let’s put this into perspective by looking at Price-Earning data or P/E ratios.\nNow we run into a problem which you will often encounter when working with data. It is rare that a tool covers all cases. P/E rations seem to be difficult to retrieve with tidyquant. These data do however exist on the web, for example at the website Macrotrends.8\n8 The P/E ratio for the SP500 can be found for instance here: https://www.macrotrends.net/2577/sp-500-pe-ratio-price-to-earnings-chartTo retrieve data from this site, I had to download the data locally first before I could get them into R. Let’s look at them a bit closer now using again R’s visualization tools.\n\n# Set the path to the downloaded CSV file\nfile_path &lt;- \"data/sp-500-pe-ratio-price-to-earnings-chart.csv\"\n\n# Read the CSV file into R\npe_data &lt;- read.csv(file_path, stringsAsFactors = FALSE, skip = 16,\n                    col.names = c(\"Date\", \"PE_ratio\"))\n\n# Convert the date column to Date class\npe_data$Date &lt;- as.Date(pe_data$Date, format = \"%Y-%m-%d\")\n\n# Filter data for the desired date range\npe_data_filtered &lt;- subset(pe_data, Date &gt;= as.Date(\"2000-01-01\") & \n                             Date &lt;= as.Date(\"2011-12-31\"))\n\n# Calculate the average P/E ratio over the specified period\naverage_pe &lt;- mean(pe_data_filtered$PE_ratio, na.rm = TRUE)\n\n# Plot the P/E ratio for the y-axis\nplot(\n  pe_data_filtered$Date, pe_data_filtered$PE_ratio,\n  type = \"l\",                   # Line plot\n  col = \"blue\",                 # Line color\n  lwd = 2,                      # Line width\n  xlab = \"Year\",                # X-axis label\n  ylab = \"P/E Ratio\", # Y-axis label\n  main = \"S&P 500 P/E Ratio (2000–2011)\", # Plot title\n)\n\n# Add a horizontal line for the average P/E ratio\nabline(h = average_pe, col = \"red\", lwd = 2, lty = 2)\n\n# Add a legend to the plot\nlegend(\n  \"topright\",                   # Legend position\n  legend = c(\"P/E Ratio\", \"Average P/E Ratio\"), # Labels\n  col = c(\"blue\", \"red\"),       # Line colors\n  lty = c(1, 2),                # Line types\n  lwd = c(2, 2)                 # Line widths\n)\n\n\n\n\n\n\n\n\nAt first glance, the market seems undervalued, with a trailing price-to-earnings (P/E) ratio of 14, well below the historical average of 29.\nAs a cautious investor, you form the hypothesis that these low valuations are deceptive. You believe that corporate profits, which are at record highs of 1672 compared to the historical average of 1119, will revert to the mean. When this happens, earnings will drop, pushing the P/E ratio from a seemingly cheap to expensive.\nWith this belief in mind, you decide to hold off on investing, waiting for valuations to normalize. However, as the years pass—2012, 2013, 2014, and beyond— corporate profits remain elevated, and the market continues to rally. Each year, your cautious stance leaves you with underwhelming returns.\n\n# Get corporate profits data from FRED\ncorporate_profits &lt;- tq_get(\"CP\", from = \"2000-01-01\", \n                            to = \"2019-12-31\", get = \"economic.data\")\n\n# Visualize corporate profits with a vertical line at 2011\nplot(\n  corporate_profits$date, corporate_profits$price,\n  type = \"l\",\n  col = \"blue\",\n  lwd = 2,\n  xlab = \"Year\",\n  ylab = \"Corporate Profits (Index)\",\n  main = \"Corporate Profits (2000–2019)\"\n)\n\n# Add a vertical line at 2011\nabline(v = as.Date(\"2011-01-01\"), col = \"red\", lwd = 2, lty = 2)\n\n# Add grid lines for readability\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nBy 2019, you’re forced to confront the possibility that your belief about corporate profit mean-reversion might be wrong.\n\n3.4.1.1 The Need for Updating Probability Asessments\nThis scenario highlights the importance of updating beliefs in the face of new evidence. Initially, your hypothesis about profit margins reverting to the mean was reasonable, based on historical data. But as year after year passed without mean-reversion, the accumulating evidence should have prompted you to revise your prior beliefs.\nBayes’ Rule offers a principled way to do this. It allows you to combine your initial belief (the prior probability) with new evidence (e.g., sustained elevated profit margins) to calculate an updated belief (the posterior probability). This process ensures that your decisions adapt as reality unfolds, helping you avoid the dangers of clinging to outdated assumptions.\n\n\n\n3.4.2 Bayes’ Rule: Intuitive Understanding with a Speck of Sand\nBayes’ rule, one of the cornerstone ideas of probability, provides a systematic method for updating probabilities based on new evidence. It is formalized as: \\[\\begin{equation*}\nP(B|A) = \\frac{P(A|B) P(B)}{P(A)}\n\\end{equation*}\\]\nHere:\n\n\\(A\\): Represents the new evidence or data that has been observed.\n\\(B\\): Represents the hypothesis or prior belief about an event.\n\nThis formula arises naturally from the multiplication rule and the symmetry of \\((A \\cap B)\\). Here’s a quick derivation:\n\nBy the multiplication rule: \\(P(B|A)P(A) = P(A \\cap B)\\)\nSimilarly: \\(P(A|B)P(B) = P(A \\cap B)\\)\nEquating the two expressions for \\(P(A \\cap B)\\) and dividing by \\(P(A)\\): \\(P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\)\n\nBayes’ rule is often challenging to grasp intuitively. To build understanding, let’s explore a simple and vivid example involving a speck of sand.\n\n3.4.2.1 The Speck of Sand: An Intuitive Illustration\nImagine a square of area 1, representing our entire sample space. Within this square, there is a circle \\(B\\), with area equal to \\(P(B)\\). You have a tiny speck of sand on your finger, which accidentally falls somewhere within the square. The location of the speck is entirely random.\n\n\n\n\n\nWhere is the speck of sand?\n\n\n\n\nThe probability that the speck lands in \\(B\\) is simply the area of \\(B\\), \\(P(B)\\), since the speck could have landed anywhere within the square with equal likelihood.\n\n\n3.4.2.2 Updating Beliefs with New Information\nNow, suppose you are told that the speck landed within another circle \\(A\\) that also lies inside the square:\n\n\n\n\n\nIn fact we learn that the speck is inside circle \\(A\\)\n\n\n\n\nHow does this new information affect the probability that the speck is in \\(B\\) ? Mathematically, we now want to compute \\(P(B|A)\\), the probability that the speck is in \\(B\\), given that it is inside \\(A\\).\n\n\n3.4.2.3 Overlap Between \\(A\\) and \\(B\\)\nIntuitively, the updated probability \\(P(B|A)\\) depends on the overlap of \\(B\\) and \\(A\\). Specifically, it is the fraction of \\(A\\) that lies within \\(B\\), expressed as: \\[\\begin{equation*}\nP(B|A) = \\frac{\\text{Area of } A \\cap B}{\\text{Area of } A} = \\frac{P(A \\cap B)}{P(A)}\n\\end{equation*}\\]\n\nIf \\(A\\) and \\(B\\) overlap only slightly, \\(P(B|A)\\) will be small:\n\n\n\n\n\n\nSmall overlap between \\(A\\) and \\(B\\)\n\n\n\n\n\nIf the overlap is large, \\(P(B|A)\\) will be large:\n\n\n\n\n\n\nLarge overlap between \\(A\\) and \\(B\\)\n\n\n\n\n\n\n3.4.2.4 Bayesian Interpretation\nIn Bayesian reasoning:\n\n\\(P(B)\\) is the prior probability, representing our belief in the hypothesis \\(B\\) before observing the data.\n\\(P(A|B)\\) is the likelihood, describing how consistent the observed data \\(A\\) is with the hypothesis \\(B\\).\n\\(P(A)\\) normalizes the result, ensuring all probabilities sum to 1.\n\nThe speck-of-sand example illustrates how Bayesian updating works:\n\nStart with a prior \\(P(B)\\).\nReceive new evidence \\(P(A|B)\\).\nUpdate the probability of the hypothesis given the evidence \\(P(B|A)\\).\n\nBayes’ theorem quantifies this intuitive process of revising beliefs based on data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#using-an-llm-to-explore-conditional-probability",
    "href": "03-lecture3.html#using-an-llm-to-explore-conditional-probability",
    "title": "3  Conditional Probability",
    "section": "3.5 Using an LLM to Explore Conditional Probability",
    "text": "3.5 Using an LLM to Explore Conditional Probability\nIn each lecture I try to involve a use case showing you how you could leverage the power of LLMs to deepen your learning experience. Conditional probability is an excellent topic to explore with the LLM. Understanding conditional probability can be challenging because our intuitions about likelihoods often conflict with the precise rules of probability theory. Unlike geometric intuition which is hardwired in the human mind through the needs of our visual system, we humans lack an innate intuitive understanding of probability. This is something that has to be acquired through training and practice. An LLM like ChatGPT can serve as a dialogue partner to help you explore this gap and refine your understanding.\nHere is a suggested use case: You can use an LLM to simulate and analyze scenarios where our intuition might mislead us. Here’s how to structure a dialogue with the model:\n\nPose a Real-World Scenario:\n\nExample: “Suppose you test positive for a rare disease with a prevalence of 1%. The test is 95% accurate, meaning it correctly identifies positives 95% of the time and negatives 95% of the time. What is the probability that you actually have the disease?”\n\nEngage in a Dialogue:\n\nAsk the model to explain how to approach the problem using conditional probability.\nExplore intermediate steps such as calculating the prior probability, likelihood, and posterior probability using Bayes’ Rule.\n\nChallenge Your Intuition:\n\nShare your intuitive guess about the probability with the model.\nLet the model explain where intuition might diverge from formal reasoning and why.\n\nExperiment with Variations:\n\nModify the problem (e.g., change the disease prevalence or test accuracy) and ask the model to recompute probabilities. Compare how small changes in assumptions affect outcomes.\n\n\n\n3.5.1 Example Dialogue\nHere’s an example interaction you could try:\nYou: “I think the probability of actually having the disease should be very high since the test is 95% accurate. Is this correct?”\nModel: “Let’s compute it formally using Bayes’ Rule: \\[\\begin{equation*}\nP(D|T) = \\frac{P(T|D) \\cdot P(D)}{P(T|D) \\cdot P(D) + P(T|ND) \\cdot P(ND)}\n\\end{equation*}\\] Where: - \\(P(D) = 0.01\\) (prevalence), - \\(P(ND) = 1 - P(D) = 0.99\\), - \\(P(T|D) = 0.95\\) (true positive rate), - \\(P(T|ND) = 0.05\\) (false positive rate).\nSubstituting the values: \\(P(D|T) = \\frac{0.95 \\cdot 0.01}{0.95 \\cdot 0.01 + 0.05 \\cdot 0.99} \\approx 0.16\\)\nThis means the probability you actually have the disease is about 16%, much lower than intuition might suggest because the disease is so rare.”\nWhy can exercises like this be valuable?\n\nRefines Understanding: Engaging in such dialogues clarifies the application of Bayes’ Rule and highlights the importance of considering base rates.\nReveals Pitfalls of Intuition: You’ll learn to appreciate why our gut feelings can sometimes mislead us in probabilistic reasoning.\nEncourages Exploration: By modifying scenarios and asking “what if” questions, you deepen your grasp of conditional probabilities in diverse contexts.\n\nLeverage this approach to build a stronger connection between the theory and its real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#summary",
    "href": "03-lecture3.html#summary",
    "title": "3  Conditional Probability",
    "section": "3.6 Summary",
    "text": "3.6 Summary\n\n3.6.1 Probability Concepts\nIn this lecture, we covered the following key probability concepts: - Conditional Probability: Understanding the probability of an event occurring given that another event has occurred, and its relevance in real-world scenarios, especially in finance. - Dependence and Independence: How events can be dependent or independent and the role of conditional probabilities in analyzing their relationships. - Bayes’ Rule: Using conditional probabilities to update beliefs based on new evidence.\nThese concepts were illustrated with practical examples, including a discussion of the financial crisis of 2007-2008 to highlight the risks of neglecting dependencies in probability modeling.\n\n\n3.6.2 R Concepts\nDuring the lecture, we used R to: - Simulate Conditional Probabilities: Generate random data and compute conditional probabilities to illustrate theoretical concepts. - Visualize Dependencies: Create intuitive visualizations of event overlaps and relationships between probabilities. - Practical Applications: Implement real-world examples to explore conditional probabilities and Bayes’ Rule, showcasing how to use R for data analysis in finance.\nThis lecture bridged theoretical probability concepts with practical computational tools in R, enabling a deeper understanding of the material and its applications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#project-evaluating-credit-risk-using-conditional-probabilities",
    "href": "03-lecture3.html#project-evaluating-credit-risk-using-conditional-probabilities",
    "title": "3  Conditional Probability",
    "section": "3.7 Project: Evaluating Credit Risk Using Conditional Probabilities",
    "text": "3.7 Project: Evaluating Credit Risk Using Conditional Probabilities\n\n3.7.1 Problem Description\nA bank is evaluating a loan application using historical data to estimate the likelihood of default. The borrower has a low credit score, and the bank has the following data:\n\nDefault Rates:\n\nProbability of default for all customers: \\(P(D) = 0.04\\).\nProbability of non-default for all customers: \\(P(ND) = 0.96\\).\n\nEvidence:\n\nProbability of a low credit score given default: \\(P(L|D) = 0.7\\).\nProbability of a low credit score given non-default: \\(P(L|ND) = 0.1\\).\n\n\nThe goal is to determine the posterior probability of default given the borrower’s low credit score, \\(P(D|L)\\), using Bayes’ Rule. Additionally, you will verify this theoretical result by simulating customer data and analyzing outcomes.\n\n\n\n3.7.2 Questions\n\nCompute \\(P(D|L)\\) Theoretically:\n\nUse Bayes’ Rule to calculate the posterior probability of default given a low credit score.\n\nSimulate the Scenario in R:\n\nSimulate a dataset of 10,000 customers where each customer is randomly assigned a default status based on \\(P(D)\\).\nBased on the assigned default status, simulate whether each customer has a low credit score using \\(P(L|D)\\) and \\(P(L|ND)\\).\n\nCompute \\(P(D|L)\\) from Simulated Data:\n\nUse the simulated data to compute \\(P(D|L)\\) and compare it to the theoretical result.\n\nVisualize Results:\n\nCreate a simple bar plot comparing the simulated and theoretical probabilities. If you are unfamiliar with visualization tools, check out the barplot() function in base R.\n\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nTooze, Adam. 2018. Crashed. How a Decade of Financial Crisis Changed the World. Viking.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  }
]