[
  {
    "objectID": "02-lecture2.html",
    "href": "02-lecture2.html",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "",
    "text": "2.1 Terminology\nProbability is a mathematical theory that provides a rigorous framework for understanding uncertainty. It is particularly useful when analyzing real-world phenomena, such as random sampling in surveys or fluctuations in stock prices.\nLet us now formalize the foundational concepts of this theory:\nTo clarify this concept, we define uncertain outcomes in advance. We pin down the possible outcomes by agreeing on the outset what we want to consider as the possible outcomes. Take the simple example of considering whether the price of a stock is going to rise or fall at the next day. In a practical situations the outcome of a move in the stock price can be that it rises or falls but it could in principle also stay the same. Still when we think about the experiment of observing the stock price tomorrow in many applications in Finance we usually agree that rise and fall are the only possible outcomes of this experiment. When we want to consider the random experiment that includes the case that the price stays the same we need to agree upfront the the possible outcomes are rise, fall and unchanged.\nThis collection of all possible outcomes in probability theory is called the\nIn the example before when we look at rises and falls in the stock price we would have \\({\\cal S} = \\{rise, fall \\}\\)\nExample: For \\({\\cal S} = \\{\\text{rise},\\text{fall} \\}\\), the event \\(\\{\\text{rise}\\}\\) is a simple event, while the event \\(\\{\\text{rise, fall}\\}\\) includes all outcomes and corresponds to the entire sample space.\nIn the first lecture we learned about two approaches to measure probability. But the theory of probability actually does not depend on how we measure it precisely. In the theory of probability this measure is an abstract concept.\nThus when we talk about probability in a precise and meaningful sense we can only do so in relation to a given sample space or to a certain conceptual experiment.\nI have introduced here probabilities with relation to a discrete sample space. The sample spaces discussed in Lecture 1 include finite sets, which can be small or large, such as the extensive output space of the SHA-256 hash function.\nThere are also more complicated discrete sample spaces: Think of the random experiment of tossing a coin as often as necessary to see Heads for the first time. We can begin writing down the basic outcomes as: \\(E_1=H, E_2=TH, E_3 = TTH, E_4 = TTTH, ...\\). An event where Heads never appear, denoted \\(E_0\\), may also be considered. In this case, when the basic events can be arranged into a simple sequence. A sample space is called discrete if it contains only finitely many points, or infinitely many points which can be arranged into a simple sequence.\nNot all sample spaces are discrete. Except for the technical tools required there is no essential difference between the two cases. In our discussion of probability in this lecture we consider mostly discrete sample spaces, however we will also discuss some basic non-discrete sample spaces later in the lectures.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#terminology",
    "href": "02-lecture2.html#terminology",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "",
    "text": "Definition: Random Experiment:\n\n\n\nA process with a set of possible outcomes, where the specific outcome cannot be predicted with certainty beforehand.\n\n\n\n\n\n\n\n\n\n\nDefinition: Sample space\n\n\n\nThe collection of all possible outcomes of an experiment is called the sample space and is denoted as the set \\({\\cal S}\\)\n\n\n\n\n\n\n\n\n\nDefinition: Basic outcome, event, simple event\n\n\n\nA basic outcome is a single possible result of a random experiment. An event is a subset of the sample space, representing one or more outcomes. A simple event is an event containing exactly one basic outcome.\n\n\n\n\n\n\n\n\n\n\nDefinition: Probability\n\n\n\nFor a given (discrete) sample space \\({\\cal S}\\), Probability is a function that assigns a value to each event, representing its likelihood. The function has to fulfill three properties:\n\n\\(P({\\cal S}) = 1\\).\nFor any event \\(A \\in {\\cal S}\\), \\(0 \\leq P(A) \\leq 1\\). The probability of an event can never be negative or larger than 1.\nFor mutually exclusive events \\(A\\) and \\(B\\), \\(P(A \\cup B) \\leq P(A) + P(B)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#probability-theory-and-applications",
    "href": "02-lecture2.html#probability-theory-and-applications",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.2 Probability in theory and applications of probability",
    "text": "2.2 Probability in theory and applications of probability\nProbabilities are expressed as numbers between 0 and 1. As mentioned by Feller (1968) in his famous probability textbook, these numbers are of the same nature as distances in geometry. In the theory we assume they are given to us.\nFrom the viewpoint of probability theory, we need not assume anything about how they are measured. In this sense probabilities in the theory of probability are an abstract measure of uncertainty.\nIn practical applications, determining probabilities or applying theory often requires sophisticated statistical methods. So, while the mathematical as well as the intuitive meaning of probability are clear only as we proceed with the theory we will get a better ability to see how we can apply this concept.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#probability-and-the-language-of-sets",
    "href": "02-lecture2.html#probability-and-the-language-of-sets",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.3 Probability and the language of sets",
    "text": "2.3 Probability and the language of sets\nProbability theory relies on the language of sets to describe relationships between events. Understanding key set operations is essential for working with probabilities effectively.\nLet’s go through them and illustrate the concepts in the context of the examples we have already developed in lecture 1.\n\n\n\n\n\n\nDefinition: Set Union\n\n\n\nThe union of two events \\(A\\) and \\(B\\) represents all outcomes that belong to \\(A\\), \\(B\\) or in both. It is written \\(A \\cup B\\).\n\n\nLet us use the example of the experiment of rolling a die. The sample space \\({\\cal S}\\) is the set of all possible outcomes of rolling the dice \\({\\cal S}=\\{1,2,3,4,5,6\\}\\). Assume one event is that the outcome is 1, 2 or 3. In set notation, we would write \\(A = \\{1,2,3\\}\\). Let us also assume that the second event is that the outcome is some even number, i.e. 2, 4, 6. Again using set notation we would write \\(B = \\{2,4,6\\}\\). The event \\(A \\cup B\\) is then the set of all outcomes such that the outcome is in \\(A\\) or in \\(B\\) or in both or these sets.\nYou can implement set operations in R because R provides functions for computing set operations. Let us use the occasion to show you briefly how to use these functions in the context of this example: We define the sets \\(A\\) and \\(B\\) first using the assignment operator:\n\nA &lt;- c(1,2,3)\nB &lt;- c(2,4,6)\n\nWe compute the union by using the function union()\n\nunion(A,B)\n\n[1] 1 2 3 4 6\n\n\nwhich gives us the union of both sets.\nTo understand this operation better we can visualize the set union in this example by looking at Figure 2.1\n\n\n\n\n\n\n\n\nFigure 2.1: The meaning of set union\n\n\n\n\n\nThe sample space \\({\\cal S}\\) is the gray set containing all possible outcomes of our random experiment. Graphically the union of \\(A\\) and \\(B\\), \\(A \\cup B\\) is a subset of the sample space, the entire colored area.\n\n\n\n\n\n\nIntersection\n\n\n\nThe intersection of two events are all outcomes that are both in \\(A\\) and in \\(B\\). It is written as \\(A \\cap B\\).\n\n\nIn R we would implement this operation by using the function intersect() and apply it to our sets \\(A\\) and \\(B\\) we have defined before.\n\nintersect(A,B)\n\n[1] 2\n\n\nFigure 2.2 visualizes this operation graphically\n\n\n\n\n\n\n\n\nFigure 2.2: The meaning of set intersection\n\n\n\n\n\nThe intersection of \\(A\\) and \\(B\\), \\(A \\cap B\\) is the orange area containing the dice face with two points. Indeed two is both in \\(A\\) and in \\(B\\), which is exactly the meaning of set intersection.\n\n\n\n\n\n\nComplement\n\n\n\nThe complement of an event \\(A\\) within the sample space \\({\\cal S}\\) is the set of all outcomes that are in \\({\\cal S}\\) but not in \\(A\\). It is written as \\({\\cal S} \\setminus A\\)\n\n\nLets say we want to get the complement, or the set difference of \\(A \\cup B\\) with respect to the sample space \\({\\cal S}\\).\nThe R implementation of the set difference operation is the function setdiff(). This is how we would tell R to compute the set difference of the union of \\(A\\) and \\(B\\) and the sample space \\({\\cal S}\\) in our example of the die:\n\nS &lt;- c(1,2,3,4,5,6)\n\nsetdiff(S, union(A,B))\n\n[1] 5\n\n\nThis can again be visualized in Figure 2.3\n\n\n\n\n\n\n\n\nFigure 2.3: The meaning of complement\n\n\n\n\n\nThis complement is the dice shown in the light redish area, i.e. all the elements of \\({\\cal S}\\) which are not in \\(A \\cup B\\).\n\n\n\n\n\n\nMutually Exclusive\n\n\n\nTwo events \\(A\\) and \\(B\\) are mutually exclusive if they can not occur simultaneously. This means \\(A \\cap B = \\emptyset\\), their intersection is empty.\n\n\nAn example in our context is the set of even outcomes \\(B=\\{2,4,6\\}\\) and the set of odd outcomes, let us call it \\(C=\\{1,3,5\\}\\). If we intersect these sets\n\nB &lt;- c(2,4,6)\nC &lt;- c(1,3,5)\n\nintersect(B,C)\n\nnumeric(0)\n\n\nwe get the empty set, which is expressed by R by giving the data type, in this case numeric, because we are intersecting sets of numeric values, followed by (0). This means, there is no numeric value in the intersection of \\(B\\) and \\(C\\).\nLet us discuss set operations a bit further by thinking about the probability of the union of two events \\(A\\) and \\(B\\) within the context of our visual examples. Remember that we had \\(A = \\{1,2,3 \\}\\) and \\(B = \\{ 2,4,6\\}\\). Look at Figure 2.4\n\n\n\n\n\n\n\n\nFigure 2.4: The meaning of set union\n\n\n\n\n\nWe would like to know what is the probability of \\(P(A \\cup B)\\)? Now if we add up \\(P(A)\\) and \\(P(B)\\) we would assign a probability to the outcome \\(2\\) twice. Such double counting must be avoided and thus we have to subtract \\(P(A \\cap B)\\) the intersection, so that: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\). Now you see what the qualification mutually exclusive does in our probability rule 3. When \\(A\\) and \\(B\\) are mutually exclusive \\(A \\cap B = \\emptyset\\) and in this case \\(P(A \\cup B) = P(A) + P(B)\\). We can add up the probabilities because when events are mutually exclusive we can not double count events by adding up individual probabilities.\n\n\n\n\n\n\nNow you try\n\n\n\nIf you drop the qualification that \\(A\\) and \\(B\\) must be mutually exclusive in point 3 in the definition of probability. How would rule 3 have to be changed?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#using-an-llm-to-deepen-your-understanding-of-set-theory-in-probability",
    "href": "02-lecture2.html#using-an-llm-to-deepen-your-understanding-of-set-theory-in-probability",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.4 Using an LLM to Deepen Your Understanding of Set Theory in Probability",
    "text": "2.4 Using an LLM to Deepen Your Understanding of Set Theory in Probability\nAn LLM like ChatGPT can be a helpful tool to explore concepts and solidify your understanding. Here are some example of what you could do:\n\n2.4.1 Ask for Clarifications\nIf a definition or concept isn’t clear, ask the LLM to explain it in simpler terms or using different examples. For instance:\n\n\n\n\n\n\nPrompt\n\n\n\n“What is the difference between the union and intersection of sets in probability? Can you give examples?”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“Can you compare this to a real-life scenario, like rolling a die or flipping a coin?”\n\n\n\n\n2.4.2 Generate Additional Examples\nUse the LLM to create new examples similar to the ones in the lecture. This will give you more practice applying the concepts.\n\n\n\n\n\n\nPrompt\n\n\n\n“Give me an example of mutually exclusive events involving sports outcomes.”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Can you show a sample space and events for tossing two coins?”\n\n\n\n\n2.4.3 Simulate Visualizations and Code Interpretation\nWhile the LLM doesn’t directly produce visuals, you can ask it to describe how a diagram or R output would look. This helps connect theoretical concepts to their graphical representations. For example:\n\n\n\n\n\n\nPrompt\n\n\n\n“Describe what a Venn diagram looks like for \\(A \\cup B\\), \\(A \\cap B\\), and \\(A \\setminus B\\).”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“What does the R function union(A, B) compute? How is it related to \\(A \\cup B\\)?”\n\n\n\n\n2.4.4 Practice Applying Definitions\nUse the LLM to test your understanding by quizzing yourself.\n\n\n\n\n\n\nPrompt\n\n\n\n“Ask me questions about the definitions of sample spaces, union, intersection, and complement.”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Give me a scenario and ask which set operation applies.”\n\n\n\n\n2.4.5 Explore Real-World Applications\nUse the LLM to explore how these concepts apply in real-world contexts beyond the lecture.\n\n\n\n\n\n\nPrompt\n\n\n\n“How is the concept of set intersection used in data science or finance?”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Explain how mutually exclusive events are important in designing experiments.”\n\n\n\n\n2.4.6 Learn R Through Step-by-Step Guidance\nIf you’re new to R, ask the LLM to guide you through using functions like union(), intersect(), and setdiff() step by step.\n\n\n\n\n\n\nPrompt\n\n\n\n“Explain how to use setdiff() in R with an example involving dice rolls.”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“How does this output relate to the complement of a set?”\n\n\n\nDive Deeper into Probability Rule 3:\n\nUse the LLM to generate explanations and examples that clarify how probabilities relate to set operations.\n\n\n\n\n\n\nFollow up\n\n\n\n“Explain why \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“Can you provide a numerical example to illustrate this rule?”\n\n\n\n\n2.4.7 Simulate Discussions\nAsk the LLM to take the role of a peer or instructor to simulate a conversation about the material.\n\n\n\n\n\n\nPrompt\n\n\n\n“Pretend you are my study partner. Let’s discuss the complement of events and its significance in probability.”\n\n\nBy actively engaging with the LLM through these kinds of prompts, you can practice, explore, and deepen your understanding of the material beyond the lecture. Try it alone or with your group.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#frequency",
    "href": "02-lecture2.html#frequency",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.5 Probability and Frequency",
    "text": "2.5 Probability and Frequency\nNow it is time to explain a connection we made so far informally when we looked at visualizations and simulations, where we used connection between relative frequencies of probabilities in repetions of a random experiment.\nThe frequency interpretation of probability is a practical approach to understanding uncertainty. It defines the probability of an event \\(A\\) as:\n\\[\\begin{equation*}\nP(A) = \\frac{\\text{Number of times $A$ occurs in repeated identical trials}}{\\text{Total number of trials}}\n\\end{equation*}\\]\nThis interpretation is intuitive and often used in fields like engineering, finance, and natural sciences. However, it raises important questions about the connection between observed frequencies and theoretical probabilities.\nThe origins of this discussion reach back to the seventeenth century. The philosophers Gottfried Wilhelm Leibnitz (1646 - 1716) and Jacob Bernoulli (1655 - 1705) had great hopes for the new field of probability to find applications in fields like medicine, law, commerce and finance. This interest in exploring new fields of potential applications drove them to study frequency evidence of events. They felt that relying on intuitively equally probable cases might not be enough for these ambitious application attempts.\nJacob Bernoulli gave an answer which is among the great ideas in probability theory (see Diaconis and Skyrms (2019)), the weak law of large numbers. It establishes one of the most important connections between frequency and probability.\n\n2.5.1 The Weak Law of Large Numbers (WLLN)\nThe Weak Law of Large Numbers provides a rigorous mathematical foundation for the frequency interpretation of probability. It states:\n\n\n\n\n\n\nWeak Law of Large Numbers\n\n\n\nAs the number of independent and identically distributed (i.i.d.) trials increases, the relative frequency of an event converges to its true probability with high probability.\n\n\nNow let us pause here and restate what the weak law says: The law says that\n\nOver many trials, the observed frequencies of outcomes will get closer to their theoretical probabilities.\nThis convergence occurs with high likelihood as the number of trials increases.\n\nObserve also what the weak law does not say:\n\nFrequencies are not probabilities. Instead frequencies approximate probabilities as trials increase.\nThe weak law does not guarantee exact convergence in finite samples - it describes long run outcomes of repeating identical experiments many times.\n\nLet’s go back to the exmple of the fair coin we played with in lecture 1: There we constructed a fair coin toss where \\(P(\\text{Heads})=0.5\\). When we have just a few tosses, say 10 times, you might observe 6 Heads (60%), which is close to but not exactly 50%. As you increase the tosses, say to 100, the frequency might be 52 Heads (52%), closer to 50%. if you go to even more tosses, say 10,000 tosses, the frequency approaches 50%.\nThis demonstrates that, as the number of trials increases, the relative frequency converges to the theoretical probability.\nIt is crucial to understand that Bernoulli’s Law of Large Numbers does not assert that frequencies are probabilities. Rather, it describes how, under certain conditions, frequencies fall within specific bounds relative to probabilities. Specifically, given a theoretical probability, a desired margin of error, and a confidence level, Bernoulli’s result provides an upper bound on the number of trials required for the relative frequency of an event to approximate its true probability.\nThis distinction highlights an important limitation: the Weak Law of Large Numbers addresses the problem of inference from probabilities to frequencies, not the reverse. That is, it shows how probabilities can predict the behavior of frequencies in repeated trials, but it does not justify using observed frequencies as definitive probabilities. Instead, the theorem ensures that as the number of trials grows, the relative frequency of an event will converge to its probability, within specified bounds.\nAlthough we will frequently use frequency-based measures of probability in practical applications, it is essential to recognize the conceptual gap between the idealized mathematical theory and the empirical realities it seeks to describe. Probability theory relies on the concept of limiting relative frequencies, which exist only in an idealized framework of infinite trials. Real-world applications, however, involve finite data and inherently require interpretation, judgment, and domain expertise.\nThis distinction between theory and practice is more than a technical nuance—it addresses a fundamental challenge for any theory with practical ambitions: How does the idealized framework relate to the messy complexities of reality? In probability, as in other fields, there is no direct or naive application of theory to practice. Bridging this gap demands a sound understanding of the theory, along with careful consideration of real-world conditions and context.1\n1 For a formal statement of the weak law of large numbers, we need more concepts which we have not yet introduced, in particular the notion of a random variable. Let me give the formal statement here for those of you who are interested and know the concept of a random variable already: Let \\(X_1, X_2, \\dots, X_n\\) be \\(n\\) independently, identically distributed random variables with expected value \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\bar{X}_n\\) denote the sample mean. Then the weak law of large numbers states: \\(P(|\\bar{X}_n - \\mu | \\geq \\epsilon) \\to 0 \\quad \\text{as } n \\to \\infty\\). This means that the probability of the sample mean deviating significantly from the expected value diminishes as the number of trials increases.\n\n2.5.2 Exploring the Weak Law of Large Numbers with R\nIn the following code chunk we first define a coin as we did in lecture 1 with the understanding that 1 represents Heads and 0 represents Tails. Then we use the replicate function and the sample function to toss the coin 1000 times.\nNow in the next step we use the logical condition results == 1 to create a logical vector indicating whether each toss resulted in Heads. cumsum()then calculates the cumulative count of Heads after each toss. Dividing this cumulative count by (1:n) (the toss number) gives the cumulative relative frequency of Heads at each step.\nThen we plot these frequencies againts the number of tosses. The \\(x\\)-axis represents the number of tosses, while the \\(y\\)-axis represents the relative frequency of heads. We draw a red horizontal line where the relative frequency is identical to the theoretical probability of \\(0.5\\).\n\n# Define the coin\ncoin &lt;- c(1, 0)\n\n# Toss the coin n times\nn &lt;- 1000\nresults &lt;- replicate(n, sample(coin, size = 1))\n\n# Calculate cumulative frequency of Heads\n\nheads_freq &lt;- cumsum(results == 1) / (1:n)\n\n# Plot the convergence\nplot(1:n, heads_freq, type = \"l\", ylim = c(0.4, 0.6),\n     xlab = \"Number of Tosses\", ylab = \"Frequency of Heads\",\n     main = \"Convergence of Relative Frequency to True Probability\")\nabline(h = 0.5, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nThis plot illustrates how the relative frequency of Heads approaches \\(P(\\text{Heads})=0.5\\) as the number of tosses increases.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#independence",
    "href": "02-lecture2.html#independence",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.6 Independence",
    "text": "2.6 Independence\nThe idealized thought experiment behind the Weak Law of Large Numbers assumes the idea of independence. Let us define this concept more precisely and explain its implications. While we have already used independence implicitly in earlier examples, it is important to formalize and understand it carefully.\n\n2.6.1 Intuition Behind Independence\nTwo events are said to be independent if the occurrence of one event does not influence the probability of the other event occurring. In other words, knowing that one event has occurred provides no information about the likelihood of the other event.\nFor example, consider rolling a fair six-sided die twice. The outcome of the first roll does not affect the outcome of the second roll because the rolls are independent. The probability of any number appearing on the second roll remains \\(1/6\\), regardless of what happened on the first roll.\n\n\n2.6.2 A Worked Example\nLet us calculate the probability of rolling a 5 on the first roll and a 6 on the second roll.\nThis is the probability of the event “5 on the first roll and 6 on the second roll,” which we write as \\(P(5 \\cap 6)\\).\nSince the rolls are independent: \\(P(5 \\cap 6) = P(5) \\times P(6)\\).\n\nThe probability of rolling a 5 on a fair six-sided die is \\(P(5) = 1/6\\).\nThe probability of rolling a 6 on a fair six-sided die is \\(P(6) = 1/6\\).\n\nThus: \\(P(5 \\cap 6) = \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{1}{36}\\).\nThis calculation uses the multiplication rule for independent events, which states that if two events \\(A\\) and \\(B\\) are independent, then the probability of both occurring is the product of their individual probabilities.\n\n\n2.6.3 Definition of Independence\nWe now formalize this concept:\n\n\n\n\n\n\nIndependence\n\n\n\nTwo events \\(A\\) and \\(B\\) are independent if and only if: \\(P(A \\cap B) = P(A) \\times P(B)\\).\n\n\nThis definition formalizes the idea that the occurrence of one event does not affect the likelihood of the other. Independence is an assumption we often make in theoretical models like the Weak Law of Large Numbers and many other probabilistic frameworks.\nWhile independence allows us to use the multiplication rule \\(P(A \\cap B) = P(A) \\times P(B)\\), it is critical to remember that the reverse is not true:\n- Just because you can multiply probabilities does not necessarily mean the events are independent.\n- For independence to hold, the probabilities of the events must truly be unaffected by one another.\nIndependence is a key assumption in the Weak Law of Large Numbers, where we consider a series of independent and identically distributed (i.i.d.) trials. Without independence, the law’s guarantees about the behavior of frequencies would not hold. As we move forward, we will see independence as a foundational concept in many probabilistic models and calculations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#moreR",
    "href": "02-lecture2.html#moreR",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.7 Some more concepts from R: Reading Data, R Objects, Subsetting and Modifying Values",
    "text": "2.7 Some more concepts from R: Reading Data, R Objects, Subsetting and Modifying Values\nLet us try out and apply some of the new ideas we just have learned and use the opportunity to learn some more important R concepts in the context of an example with stock price data.\n\n2.7.1 Reading data in R\nIn financial analysis, the first step is often acquiring and loading data. This could be historical stock prices, economic indicators, or portfolio metrics. In this section, we’ll learn how to load a prepared dataset of Apple stock prices into R for analysis.\nWhy learn to load data? Loading data is the foundation for everything else we’ll do in this course. Whether you’re calculating probabilities or visualizing stock trends, understanding how to access and manipulate data is essential.\n\n2.7.1.1 Reading csv files\nFor this example, I’ve prepared a dataset recording daily stock price information for Apple in a CSV file (aapl_prices.csv). This file contains the opening, high, low, and closing prices, trading volume, and adjusted closing prices2 for each day starting in January 1990.\n2 The adjusted closing price of a stock is a modified version of the closing price that reflects the effects of any corporate actions, such as stock splits, dividends, and rights offerings. It provides a more accurate representation of the stock’s value over time, especially when analyzing historical data or calculating investment returns.To load this file into R, we use the read.csv() function:\n\naapl_prices &lt;- read.csv(\"data/aapl_prices.csv\")\n\nHere, \"data/aapl_prices.csv\" specifies the path to the file relative to the working directory.3\n3 If you work with European-style decimals , instead of ., you might need read.csv2(). For now, we’ll stick to the default U.S. format.4 A great way to explore these options is to consult the R Data Import/Export Manual or experiment with sample datasets from various sources like Kaggle or the tidyverse’s guide.R is highly versatile when it comes to handling data formats. Beyond CSV files, R can read and write Excel files, JSON, XML, databases, and more. While many formats can be accessed using built-in functions with a syntax similar to read.csv(), others may require loading additional packages. For instance, the readxl package is excellent for Excel files, and the jsonlite package is ideal for JSON data.4\nTo check your current working directory, use the getwd() function:\n\ngetwd()\n\n[1] \"/home/martinsummer/Code/R/Probability_Introduction\"\n\n\nIf the file is not in the expected location, ensure the path is correct or adjust it as needed.\n\n\n2.7.1.2 Inspecting the data\nOnce the data is loaded, it’s important to inspect it. This helps you understand its structure and content. For example, we can use the head() function to view the first 10 rows:\n\nhead(aapl_prices, n = 10)\n\n   symbol       date     open     high      low    close    volume  adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2    AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3    AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4    AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6    AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n7    AAPL 1990-01-10 0.335938 0.335938 0.319196 0.321429 199718400 0.2533336\n8    AAPL 1990-01-11 0.323661 0.323661 0.308036 0.308036 211052800 0.2427781\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.2427781\n10   AAPL 1990-01-15 0.308036 0.319196 0.305804 0.305804 161739200 0.2410190\n\n\nThis reveals a table with columns such as the opening price, highest and lowest prices, and adjusted closing price for each day. While we won’t dive into the exact meaning of these financial terms just yet, this dataset will serve as the foundation for our analysis.\n\n\n2.7.1.3 Common pitfalls and tips\n\nIf you see an error like cannot open the connection, it likely means R can’t find the file. Use getwd() to confirm your working directory and ensure the file is in the specified location.\nExplore the dataset further! Try summary(aapl_prices) to get an overview of each variable or tail(aapl_prices, n = 10) to see the most recent rows.\n\nNow that we’ve successfully loaded and inspected the dataset, we’re ready to dive into analyzing it.\n\n\n\n2.7.2 R Objects and Stock Price Movements\nIn R, most data structures are built from atomic vectors, which are the simplest type of R objects. Atomic vectors store one-dimensional data, where all elements must be of the same type. Our stock price dataset contains examples of more complex structures made up of atomic vectors. Let’s explore atomic vectors using simplified stock price data to better understand how they work.\n\n2.7.2.1 Atomic Vectors in Stock Price Analysis\nTo introduce atomic vectors, consider a basic example of daily stock price movements. Suppose the price of a stock can change in one of three ways:\n\nDecrease by 1 (-1)\nStay the same (0)\nIncrease by 1 (+1)\n\nWe can represent these possible changes in price using an atomic vector:\n\nprice_changes &lt;- c(-1, 0, 1)\n\nAtomic vectors are one-dimensional and store data of a single type. In this case, price_changes is a numeric vector representing the possible daily price changes.\nYou can verify whether an object is an atomic vector using the is.vector() function:\n\nis.vector(price_changes)\n\n[1] TRUE\n\n\nThis will return TRUE since price_changes is indeed an atomic vector.\n\n\n2.7.2.2 Properties of Atomic Vectors\nEach atomic vector has several properties. For example:\n\nLength: The number of elements in the vector can be checked using length():\n\n\nlength(price_changes)\n\n[1] 3\n\n\nThis will return 3, as there are three possible price changes.\n\nData Type: Every atomic vector must have a single data type. You can determine the type using the typeof() function:\n\n\ntypeof(price_changes)\n\n[1] \"double\"\n\n\nIn this case, the type is double because the vector contains numeric data.\n\n\n2.7.2.3 Using Vectors in Simulations\nVectors are powerful tools for simulations. For example, we can simulate a week of stock price movements using the sample() function, which randomly selects elements from the vector:\n\nweek_movements &lt;- sample(price_changes, size = 7, replace = TRUE)\nweek_movements\n\n[1]  0  0 -1  0  1 -1  0\n\n\nHere, size = 7 simulates 7 days of movements, and replace = TRUE allows values to be selected more than once.\n\n\n2.7.2.4 Connection to Probability\nThis simulation ties into the concept of probability. Each price change (-1, 0, 1) can be treated as a basic outcome in a sample space defined by \\({\\cal S} = \\{-1,0,1\\}\\). By specifying probabilities, we can simulate scenarios where some outcomes are more likely than others. For example:\n\nweek_movements_weighted &lt;- sample(price_changes, \n                                  size = 7, \n                                  replace = TRUE, \n                                  prob = c(0.3, 0.4, 0.3))\nweek_movements_weighted\n\n[1]  0 -1 -1  0  1  0  0\n\n\nHere, the probabilities 0.3, 0.4, and 0.3 represent the likelihood of a decrease, no change, or an increase in price, respectively.\n\n\n2.7.2.5 More About Atomic Vectors\nR supports six basic types of atomic vectors:\n\nDouble: Numeric data with decimal precision (e.g., stock prices).\nInteger: Whole numbers (e.g., the number of shares traded).\nCharacter: Text strings (e.g., stock symbols like \"AAPL\").\nLogical: Boolean values (TRUE or FALSE, often used in comparisons).\nComplex: Numbers with imaginary components (not used in this course).\nRaw: Binary data (not used in this course).\n\nWe will primarily work with doubles, integers, characters, and logicals.We will primarily work with doubles, integers, characters, and logicals.5\n5 Why should we care for distinguishing integers from doubles? This has to do with how a computer performs computations. Sometimes, differences in precision can lead to surprising effects. In your computer, 64 bits of memory are allocated for each double in an R program. While this allows for precise representation of numbers, not all numbers can be exactly represented with 64 bits. For example, \\(\\pi\\) has an infinite sequence of digits and must therefore be rounded by the computer. Usually, rounding errors introduced into computations go unnoticed, but sometimes they become apparent. For instance, the expression \\(\\sqrt{2^2} - 2\\) results in a small floating-point error because the square root of 2 cannot be expressed precisely. These are called floating-point errors. While integers avoid floating-point errors, they are often impractical for many applications. Luckily, floating-point arithmetic is sufficiently precise for most use cases.For example, the stock price dataset contains variables of different types: - Prices (e.g., opening and closing prices) are stored as doubles. - Ticker symbols are stored as characters. - Logical vectors can be created by applying conditions to the data. For instance, finding days when the closing price was higher than the opening price:\n\naapl_prices$up_day &lt;- aapl_prices$close &gt; aapl_prices$open\nhead(aapl_prices$up_day, n = 10)\n\n [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n\n2.7.2.6 Attributes of Atomic Vectors\nAtomic vectors can have attributes, such as:\n\nNames: Metadata to describe each element.\nDimensions: Turning a vector into a matrix or array.\n\nFor example, we can assign names to the elements of price_changes:\n\nnames(price_changes) &lt;- c(\"Decrease\", \"No Change\", \"Increase\")\nprice_changes\n\n Decrease No Change  Increase \n       -1         0         1 \n\n\nThe names don’t affect the values but provide context for analysis. Here you see that the names are entered into R as a character vector stores, strings of text, which have to be put between quotation marks \"\". Strings are the individual elements of a character vector. We could use character vectors for instance in building our virtual coin earlier in the lecture in a more human readable form by using coin &lt;- c(\"H\", \"T\") isntead of coin &lt;- c(1,0).\nNote that a string can be more than just letters. If you type, for instance the number 1 with quotation marks, like \"1\" R would interpret the value as a string not as a number. Sometimes one can get confused in R because both objects and characters appear as text in R code. Object names are without quotation marks strings always are between quotation marks.\nWe can also reshape the vector into a matrix, which introduces dimensions:\n\ndim(price_changes) &lt;- c(3, 1)\nprice_changes\n\n     [,1]\n[1,]   -1\n[2,]    0\n[3,]    1\n\n\nThis transforms the vector into a matrix with 3 rows and 1 column. With the attribute system R allows you to represent more data types. R uses, for example a special class to represent dates and times. The data variable in our stock data is - for example - represents as a type of this kind.\nTo illustrate this we take the R-function Sys.time(). This function returns the current time on your computer. It looks like a character string when you display it but it is actually a double with class POSIXct, POSIXt (it has two classes):\n\nnow &lt;- Sys.time()\nnow\n\n[1] \"2025-01-15 01:46:41 CET\"\n\ntypeof(now)\n\n[1] \"double\"\n\nclass(now)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nPOSIXct is a widely used framework for representing dates and times. But we will skip the details here.\n\n\n\n2.7.3 Extending R Objects: Factors, Data Frames, and Lists\nIn the previous section, we explored atomic vectors, the foundation of many R objects. However, real-world data often require more sophisticated structures. Let’s extend our understanding by examining factors, data frames, and lists, using examples from stock price analysis.\n\n2.7.3.1 Factors in Stock Price Analysis\nFactors are used to store categorical data, which can have a fixed set of possible values (called levels). In stock market data, factors are useful for categorizing data, such as trading days or stock types.\nFor example, suppose we want to represent the days of the working week in our dataset:\n\ndays &lt;- factor(c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\ndays\n\n[1] Monday    Tuesday   Wednesday Thursday  Friday   \nLevels: Friday Monday Thursday Tuesday Wednesday\n\n\nThis factor has unique levels (\"Monday\", \"Tuesday\", etc.) that categorize the data. Factors can also be ordered, which is useful when the order matters:\n\nordered_days &lt;- factor(days, levels = c(\"Monday\", \"Tuesday\", \n                                        \"Wednesday\", \"Thursday\", \n                                        \"Friday\"), ordered = TRUE)\n\nordered_days\n\n[1] Monday    Tuesday   Wednesday Thursday  Friday   \nLevels: Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; Friday\n\n\nFactors are internally stored as integers but behave like characters when displayed. You can inspect their attributes to see how they’re stored:\n\nattributes(ordered_days)\n\n$levels\n[1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n\n$class\n[1] \"ordered\" \"factor\" \n\n\nFactors are powerful not only for categorizing data but also for performing analyses based on these categories. Let’s analyze how often Apple’s stock price decreased on Mondays in the given dataset.\nSuppose we take our original data about the Apple stock price and remove the ùp_day`column, we had creatde before:\n\n# Preview of the dataset\naapl_prices$up_day &lt;- NULL\nhead(aapl_prices)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6   AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n\n\nTo identify days when the price decreased, we can compute the daily price change and create a factor to categorize the changes (\"up\", \"down\", \"unchanged\"). Then, we can use factors to tabulate occurrences for each day of the week.\nWe start by adding week days and price change factors:\n\n# Convert date to Date type and extract weekdays\naapl_prices$date &lt;- as.Date(aapl_prices$date)\naapl_prices$weekday &lt;- factor(weekdays(aapl_prices$date), \n                             levels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\n# Compute daily price change and categorize as up, down, or unchanged\n\n# Create a lookup table as a named character vector\nlookup_table &lt;- c(\n  \"up\" = \"up\",\n  \"down\" = \"down\",\n  \"unchanged\" = \"unchanged\"\n)\n\n# Calculate price differences\nprice_diff &lt;- aapl_prices$close - aapl_prices$open\n\n# Create a character vector based on conditions\nprice_change &lt;- rep(NA, length(price_diff))\nprice_change[price_diff &gt; 0] &lt;- lookup_table[\"up\"]\nprice_change[price_diff &lt; 0] &lt;- lookup_table[\"down\"]\nprice_change[price_diff == 0] &lt;- lookup_table[\"unchanged\"]\n\n# Convert the result to a factor\naapl_prices$price_change &lt;- factor(price_change, levels = names(lookup_table))\n\n# View the updated data\nhead(aapl_prices)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6   AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n    weekday price_change\n1   Tuesday           up\n2 Wednesday         down\n3  Thursday         down\n4    Friday    unchanged\n5    Monday           up\n6   Tuesday         down\n\n\nThe factor structure allows us quick tabulations, like this:\n\n# Tabulate price changes by weekday\ntabulated_data &lt;- table(aapl_prices$weekday, aapl_prices$price_change)\ntabulated_data\n\n           \n             up down unchanged\n  Monday    895  726        39\n  Tuesday   887  876        45\n  Wednesday 917  855        34\n  Thursday  846  891        39\n  Friday    835  889        41\n\n\nThen we can use our table to see how many down moves we count on Mondays in our data set.\n\n# Extract the number of down moves on Mondays\ndown_on_mondays &lt;- tabulated_data[\"Monday\", \"down\"]\ndown_on_mondays\n\n[1] 726\n\n\nThis analysis reveals how many times Apple’s stock price decreased on Mondays in the dataset. Using factors, we can categorize data and extract meaningful insights, making them an invaluable tool for financial analysis and beyond. They are a powerful tool in exploratory data analysis involving categorical data.\n\n\n2.7.3.2 Data Frames: Organizing Stock Price Data\nA data frame is a two-dimensional structure that organizes data into rows and columns, where each column can store a different type of data. The stock price dataset we loaded earlier is a perfect example of a data frame.\nLet’s revisit the aapl_prices dataset:\n\nhead(aapl_prices)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6   AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n    weekday price_change\n1   Tuesday           up\n2 Wednesday         down\n3  Thursday         down\n4    Friday    unchanged\n5    Monday           up\n6   Tuesday         down\n\ntypeof(aapl_prices)\n\n[1] \"list\"\n\nclass(aapl_prices)\n\n[1] \"data.frame\"\n\n\nEach column in a data frame is an atomic vector, meaning it contains values of a single type:\n\nDoubles: Prices (e.g., opening, closing).\nCharacters: Ticker symbols (\"AAPL\").\nLogicals: New variables created based on conditions.\n\nFor example, we can create a logical column to indicate whether the stock closed higher than it opened:\n\naapl_prices$up_day &lt;- aapl_prices$close &gt; aapl_prices$open\nhead(aapl_prices[c(\"date\", \"close\", \"up_day\")], n = 5)\n\n        date    close up_day\n1 1990-01-02 0.332589   TRUE\n2 1990-01-03 0.334821  FALSE\n3 1990-01-04 0.335938  FALSE\n4 1990-01-05 0.337054  FALSE\n5 1990-01-08 0.339286   TRUE\n\n\nData frames are extremely flexible. You can subset them, filter rows, or add new columns:\n\n# Subset rows where the stock closed higher than it opened\n\nhigher_close &lt;- aapl_prices[aapl_prices$up_day == TRUE, ]\nhead(higher_close)\n\n   symbol       date     open     high      low    close    volume  adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.2427781\n11   AAPL 1990-01-16 0.299107 0.312500 0.292411 0.311384 214244800 0.2454167\n14   AAPL 1990-01-19 0.301339 0.308036 0.299107 0.305804 265137600 0.2410190\n17   AAPL 1990-01-24 0.290179 0.305804 0.287946 0.303571 169792000 0.2392588\n     weekday price_change up_day\n1    Tuesday           up   TRUE\n5     Monday           up   TRUE\n9     Friday           up   TRUE\n11   Tuesday           up   TRUE\n14    Friday           up   TRUE\n17 Wednesday           up   TRUE\n\n\n\n\n2.7.3.3 Lists: Combining Multiple Data Types\nWhile data frames organize tabular data, lists allow us to group together objects of different types and structures. This makes lists a powerful tool for storing heterogeneous data.\nFor example, let’s create a list to summarize key information about Apple’s stock prices:\n\nstock_summary &lt;- list(\n  ticker = \"AAPL\",\n  price_summary = summary(aapl_prices$close),\n  highest_price = max(aapl_prices$high, na.rm = TRUE),\n  date_range = range(aapl_prices$date, na.rm = TRUE)\n)\nstock_summary\n\n$ticker\n[1] \"AAPL\"\n\n$price_summary\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  0.1155   0.3834   3.3300  30.5265  28.8950 259.0200 \n\n$highest_price\n[1] 260.1\n\n$date_range\n[1] \"1990-01-02\" \"2024-12-27\"\n\n\nYou can access elements in a list using double brackets ([[):\n\nstock_summary[[\"highest_price\"]]\n\n[1] 260.1\n\n\nLists can also contain nested lists or data frames, enabling complex data structures. For example:\n\nnested_list &lt;- list(\n  summary = stock_summary,\n  recent_data = head(aapl_prices, n = 5)\n)\nnested_list\n\n$summary\n$summary$ticker\n[1] \"AAPL\"\n\n$summary$price_summary\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  0.1155   0.3834   3.3300  30.5265  28.8950 259.0200 \n\n$summary$highest_price\n[1] 260.1\n\n$summary$date_range\n[1] \"1990-01-02\" \"2024-12-27\"\n\n\n$recent_data\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n    weekday price_change up_day\n1   Tuesday           up   TRUE\n2 Wednesday         down  FALSE\n3  Thursday         down  FALSE\n4    Friday    unchanged  FALSE\n5    Monday           up   TRUE\n\n\n\n\n2.7.3.4 Factors, Data Frames, and Lists in Practice\nTo see how these structures work together, consider this small example. We’ll summarize Apple’s stock price movements over a week and organize the results in a list:\nHere’s the adjusted example using your actual dataset aapl_prices. This example simulates weekly price movements while working within the context of your existing data structure:\nWe will create a weekly summary based on simulated daily price movements and analyze the results using a summary list. This demonstrates how to summarize factor data combined with numerical summaries.\n\n# Extract the first five rows of the dataset to simulate one week of data\nweekly_data &lt;- head(aapl_prices, 5)\n\n# Add a factor for days of the week\nweekly_data$day &lt;- factor(\n  c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"),\n  levels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"),\n  ordered = TRUE\n)\n\n# Simulate price changes (-1 = down, 0 = unchanged, 1 = up)\nset.seed(42)  # For reproducibility\nweekly_data$price_change &lt;- sample(\n  c(-1, 0, 1), \n  size = nrow(weekly_data), \n  replace = TRUE, \n  prob = c(0.3, 0.4, 0.3)\n)\n\n# Summarize the data\nsummary_list &lt;- list(\n  week_data = weekly_data,\n  positive_days = sum(weekly_data$price_change &gt; 0),\n  total_change = sum(weekly_data$price_change)\n)\n\n# Display the summary list\nsummary_list\n\n$week_data\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n    weekday price_change up_day       day\n1   Tuesday           -1   TRUE    Monday\n2 Wednesday           -1  FALSE   Tuesday\n3  Thursday            0  FALSE Wednesday\n4    Friday           -1  FALSE  Thursday\n5    Monday            1   TRUE    Friday\n\n$positive_days\n[1] 1\n\n$total_change\n[1] -2\n\n\nHere is an explanation of the individual steps:\n\nExtract Weekly Data: The head function selects the first five rows from aapl_prices to simulate a single trading week.\nAdd Days as Factors: A new column day assigns ordered factors representing the days of the week.\nSimulate Price Movements: Random price movements (-1, 0, 1) are generated using sample, with probabilities for down, unchanged, and up movements.\nCreate a Summary List:\n\nweek_data holds the weekly data frame.\npositive_days calculates the number of days with an upward price movement.\ntotal_change sums the net price movements across the week.\n\n\nThis example combines:\n\nA factor (days) to represent the days of the week.\nA data frame (weekly_data) to organize daily price changes.\nA list (summary_list) to store the data frame and summary statistics.\n\nFactors, data frames, and lists are essential for organizing and analyzing real-world data. In financial analysis, they allow us to:\n\nCategorize data efficiently (factors).\nAnalyze structured datasets (data frames).\nIntegrate heterogeneous data (lists).\n\nArmed with these tools, you’re ready to perform deeper analyses and connect R concepts to practical financial problems. Next, we’ll use this knowledge to explore probabilistic questions with our stock price data.\n\n\n\n2.7.4 Example: Will the Stock Price of Apple Move Up or Down?\nLet us revisit the first lines of our dataset:\n\nhead(aapl_prices, n = 10)\n\n   symbol       date     open     high      low    close    volume  adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2    AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3    AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4    AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6    AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n7    AAPL 1990-01-10 0.335938 0.335938 0.319196 0.321429 199718400 0.2533336\n8    AAPL 1990-01-11 0.323661 0.323661 0.308036 0.308036 211052800 0.2427781\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.2427781\n10   AAPL 1990-01-15 0.308036 0.319196 0.305804 0.305804 161739200 0.2410190\n     weekday price_change up_day\n1    Tuesday           up   TRUE\n2  Wednesday         down  FALSE\n3   Thursday         down  FALSE\n4     Friday    unchanged  FALSE\n5     Monday           up   TRUE\n6    Tuesday         down  FALSE\n7  Wednesday         down  FALSE\n8   Thursday         down  FALSE\n9     Friday           up   TRUE\n10    Monday         down  FALSE\n\n\n\n2.7.4.1 Understanding the Structure of the Data\nTo begin, let’s confirm the type and structure of aapl_prices:\n\ntypeof(aapl_prices)\n\n[1] \"list\"\n\nclass(aapl_prices)\n\n[1] \"data.frame\"\n\n\nAs expected, the object is a list with the class data.frame. This means the dataset organizes data in rows and columns. We can use the dim() function to see how many trading days are recorded:\n\ndim(aapl_prices)\n\n[1] 8815   11\n\n\nThe dataset contains price information for 8815 trading days.\n\n\n\n2.7.4.2 Subsetting Data: Accessing Specific Elements\nTo analyze the dataset, we need to extract specific values or subsets of data. R provides a powerful and flexible notation system for subsetting:\naapl_prices[row_indices, column_indices]\nHere are the six main subsetting methods in R:\n\nPositive Integers:\n\nExample: Select the closing price on the first trading day:\n\n\n\naapl_prices[1, \"close\"]\n\n[1] 0.332589\n\n\n\nSelect the first 5 closing prices:\n\n\naapl_prices[1:5, \"close\"]\n\n[1] 0.332589 0.334821 0.335938 0.337054 0.339286\n\n\n\nNegative Integers:\n\nExclude the first observation (show first 3 entries after this operation):\n\n\n\n     head(aapl_prices[-1, \"close\"], 3)\n\n[1] 0.334821 0.335938 0.337054\n\n\n\nZero:\n\nCreates an empty object:\n\n\n\n     aapl_prices[0, 0]\n\ndata frame with 0 columns and 0 rows\n\n\n\nBlank Spaces:\n\nSelect all values in a dimension:\n\n\n\n     sel &lt;- aapl_prices[, \"close\"]\n     length(sel)\n\n[1] 8815\n\n\n\nLogical Values:\n\nExample: Use a logical vector to select the first closing price:\n\n\n\n     aapl_prices[1, c(FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE)]\n\n      high    close\n1 0.334821 0.332589\n\n\n\nNames:\n\nSelect using column names:\n\n\n\naapl_prices[1, \"close\"]\n\n[1] 0.332589\n\n\n\n\n2.7.4.3 Calculating Daily Price Differences\nLet us compute the day-to-day differences in closing prices. Using indexing, we can calculate these differences manually:\n\naux_1 &lt;- aapl_prices[2:8044, \"close\"]\naux_2 &lt;- aapl_prices[1:8043, \"close\"]\ndiff_close &lt;- aux_1 - aux_2\nhead(diff_close, n = 10)\n\n [1]  0.002231985  0.001117021  0.001116008  0.002231985 -0.003347993\n [6] -0.014508992 -0.013393015  0.000000000 -0.002231985  0.005579978\n\n\nAlternatively, the built-in diff() function simplifies this task:\n\naapl_prices$diff &lt;- c(NA, diff(aapl_prices$close))\nhead(aapl_prices, n = 5)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n    weekday price_change up_day        diff\n1   Tuesday           up   TRUE          NA\n2 Wednesday         down  FALSE 0.002231985\n3  Thursday         down  FALSE 0.001117021\n4    Friday    unchanged  FALSE 0.001116008\n5    Monday           up   TRUE 0.002231985\n\n\nThe first observation is NA because there is no prior day to calculate a difference.\n\n\n2.7.4.4 Frequency-Based Probability of Upward Moves\nTo determine the probability of the stock moving up on any given day, we create a logical column that indicates whether the price difference is positive:\n\naapl_prices$diff_pos &lt;- aapl_prices$diff &gt; 0\nhead(aapl_prices, n = 5)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n    weekday price_change up_day        diff diff_pos\n1   Tuesday           up   TRUE          NA       NA\n2 Wednesday         down  FALSE 0.002231985     TRUE\n3  Thursday         down  FALSE 0.001117021     TRUE\n4    Friday    unchanged  FALSE 0.001116008     TRUE\n5    Monday           up   TRUE 0.002231985     TRUE\n\n\nUsing the relative frequency approach, we calculate the probability of an upward move as:\n\nmean(aapl_prices$diff_pos, na.rm = TRUE)\n\n[1] 0.5073746\n\n\nor about 51 %.\nThis one line of code does several important things that are worth unpacking.\nFirst, R has an internal behavior known as type coercion, where logical values (TRUE and FALSE) are automatically converted into numerical values when used in numerical operations. Specifically:\n\nTRUE is coerced to 1\nFALSE is coerced to 0\n\nBy applying the mean() function to a logical vector, R computes the average of its numerical representation, effectively calculating the proportion of TRUE values. In this case, the mean gives the relative frequency of upward price movements in the dataset.\nAdditionally, this approach relies on the fact that the mean() function automatically divides the sum of the values by their count. Since TRUE values are coerced to 1, the sum of the vector corresponds to the total count of upward movements. 6\n6 In fact, using sum(aapl_prices$diff_pos, na.rm = TRUE) would provide the total number of days where the stock price increased, as sum() adds up the 1s in the binary vector.This step also highlights the flexibility and power of R’s vectorized operations, where simple functions like mean() and sum() can directly compute meaningful results for logical or binary data without additional loops or transformations.\n\n\n2.7.4.5 Applying Probability Concepts\nAssume that price movements are independent of one another. This means that the direction of the stock price movement today does not influence tomorrow’s movement. With this assumption, we can calculate compound probabilities over multiple days.\n\nProbability of Consecutive Increases: What is the probability that the stock price increases every day over a week (5 trading days)?\n\\(P(U \\cap U \\cap U \\cap U \\cap U) = P(U)^5 = 0.51^5 = 0.035\\)\nProbability of One Decrease and Four Increases: Consider the probability of a decrease on any one day, and increases on the other four:\n\\(P(D \\cap U \\cap U \\cap U \\cap U) = 0.49 \\cdot 0.51^4 = 0.033\\)\nSince there are 5 such (mutually exclusive) scenarios (one for each trading day), the total probability is:\n\\(5 \\cdot 0.033 = 0.132\\)\n\n\n\n2.7.4.6 Reflecting on Assumptions\nIs this analysis any good? How could we possibly judge this? Interestingly the relative frequencies of up and down moves look similar to a random experiment of a few thousand tosses of a fair coin. But can we learn anything from this? Are the up and down moves independent? Independence - of course - does not follow from the result we just got.\nThe idea that stock prices may fluctuate randomly was first discussed systematically by Louis Bachelier (1870 - 1946), a French mathematician who studied stock price movements mathematically. In 1965 the economist Samuelson (1965) published an article with the title “Proof that stock prices fluctuate randomly”. He argues in this paper that randomness comes about through the active participation of traders seeking to maximize their wealth. A huge army of investors would aggressively use the smallest informational advantage and in doing so, they incorporate the information into market prices, which quickly eliminates this profit opportunity. This lead to a cornerstone of modern Finance theory called the random walk hypothesis of stock price fluctuations.\nIf this theory was true, it would give an argument, why we might look at the up and down movements in the stock price of apple as if it was the outcome of tossing a fair coin. In this case the probability of an up or a down movement should be 1/2 and with the number of trials approaching infinity the frequency of ups and downs should approach this probability.\nThe literature on stock price fluctuations which came later, however, presented evidence that stock prices are predictable to a certain degree and do not fluctuate randomly. A good reference summarizing this evidence is Lo and MacKinlay (2019) In this case our approach would perhaps produce a misleading answer.\nWe cannot give a clear cut answer here. The point of this brief discussion is that you just cannot apply a theoretical machinery mechanically without giving it further thought and without maintaining a healthy amount of skepticism. It is fascinating that there are situations where abstract theories, like the theory of probability, show a robust relation to real world phenomena. But the nature, the precise meaning and the robustness of this relation has to be investigated for each single case.\nAs Louis Bachelier and Paul Samuelson noted, randomness in financial markets may emerge from the interplay of rational traders, yet this randomness is not absolute. A nuanced understanding of market dynamics is crucial for drawing meaningful conclusions.\n\n\n\n2.7.5 Example: Benford’s Law and Trading Volumes\nLet us conclude this lecture with an example that ties together our understanding of empirical probabilities and relative frequencies while also showcasing a surprising pattern in real-world data. This example will further strengthen your R skills and prepare you for the assigned project.\n\n2.7.5.1 The Curious Case of Leading Digits\nFirst, let us define leading digits: the leading digit of a number is simply its first non-zero digit. For instance: - The leading digit of 7829 is 7. - The leading digit of 0.00453 is 4. - The leading digit of 10892 is 1.\nYou might expect that in a random dataset, all leading digits from 1 to 9 occur with roughly equal frequencies (approximately 11% each). However, real-world datasets often show a very different pattern: smaller digits like 1 appear more frequently than larger digits like 9. This pattern follows a logarithmic distribution known as Benford’s Law, which states: \\(P(d) = \\log_{10}\\left(1 + \\frac{1}{d}\\right)\\)\nwhere \\(d \\in \\{1, 2, \\dots, 9\\}\\).\n\n\n2.7.5.2 Benford’s Law in Trading Volumes\nLet’s see if trading volumes follow Benford’s Law. First, we extract the trading volumes from the aapl_prices dataset:\n\nvolumes &lt;- aapl_prices$volume\n\nNext, we filter out invalid values (e.g., zeros or missing values) and extract the leading digits:\n\nvalid_volumes &lt;- volumes[volumes &gt; 0 & !is.na(volumes)]\n\nleading_digits &lt;- as.numeric(substr(as.character(valid_volumes), 1, 1))\n\nLet’s break down this code:\nHere’s an explanation of the two code lines tailored for students:\n1. Filter Valid Trading Volumes\n\nvalid_volumes &lt;- volumes[volumes &gt; 0 & !is.na(volumes)]\n\nThis line ensures that only valid trading volumes are included in the analysis. Let’s break it down:\n\nvolumes &gt; 0:\n\nThis condition filters out any non-positive values (e.g., 0 or negative numbers).\nThese values are not meaningful for analyzing leading digits because they do not have valid non-zero digits.\n\n!is.na(volumes):\n\nThe is.na() function checks if a value is missing (NA).\nThe ! operator negates this, keeping only non-missing values.\n\nCombining Conditions:\n\nThe & operator combines the two conditions. Only entries that satisfy both (i.e., are greater than 0 and not missing) are retained.\n\nSubsetting:\n\nvolumes[...] applies these conditions to the volumes vector. The result is a new vector, valid_volumes, containing only the valid trading volumes.\n\n\n2. Extract Leading Digits\n\nleading_digits &lt;- as.numeric(substr(as.character(valid_volumes), 1, 1))\n\nThis line extracts the leading digit of each valid trading volume. Here’s how it works:\n\nConvert to Character Strings:\n\n\nhead(as.character(valid_volumes),5)\n\n[1] \"183198400\" \"207995200\" \"221513600\" \"123312000\" \"101572800\"\n\n\n\nSince trading volumes are numeric, we need to treat them as text to extract specific characters.\nas.character() converts each number into a string representation.\n\n\nExtract the First Character:\n\n\nhead(substr(as.character(valid_volumes), 1, 1),5)\n\n[1] \"1\" \"2\" \"2\" \"1\" \"1\"\n\n\n\nThe substr() function extracts substrings from each string.\n1, 1 specifies that we extract the substring starting at position 1 and ending at position 1 (i.e., the first character of the string). This gives the leading digit as a character.\n\n\nConvert Back to Numeric:\nas.numeric(...): The extracted leading digit is initially a character. as.numeric() converts it back into a numeric value, allowing us to perform calculations like counting frequencies.\n\nNow we compute the empirical frequencies of the leading digits and compare them to the theoretical probabilities:\n\n# Tabulate empirical frequencies\nemp_freq &lt;- table(leading_digits) / length(leading_digits)\n\n# Create a data frame with empirical and Benford probabilities\nbenford &lt;- data.frame(\n  Digit = 1:9,\n  Empirical_Freq = as.numeric(emp_freq[1:9]), # Match digits 1 to 9\n  Benford_Prob = log10(1 + 1 / (1:9))\n)\n\nHere’s an explanation of the code chunk, step by step:\n1. Tabulate Empirical Frequencies\n\nemp_freq &lt;- table(leading_digits) / length(leading_digits)\n\n\ntable(leading_digits):\n\nThis function counts how many times each unique value appears in leading_digits.\nIn our case, leading_digits contains numbers between 1 and 9, so the output will have counts for each of these digits.\n\nlength(leading_digits):\n\nThis calculates the total number of entries in leading_digits, i.e., the total number of leading digits observed.\n\ntable(...) / length(...):\n\nDividing the counts from table(leading_digits) by the total count gives the relative frequencies of each digit.\nThese are the empirical probabilities of each digit appearing as the leading digit.\n\n\n\n\n2.7.5.3 2. Create a Data Frame for Comparison\n\nDigit = 1:9:\n\nThis creates a column in the data frame with the digits 1 through 9. These are the leading digits we’re analyzing.\n\nEmpirical_Freq = as.numeric(emp_freq[1:9]):\n\nSubsetting emp_freq[1:9]:\n\nThe table() function includes entries for all unique values in leading_digits. If there are missing digits (e.g., if leading_digits doesn’t include a certain number), emp_freq may not have all 9 entries.\nBy explicitly subsetting with [1:9], we ensure the frequencies for digits 1 through 9 are aligned with the Digit column, ignoring any gaps or extra entries (e.g., 0).\n\nas.numeric():\n\nThe table() output is a special type of object, not a standard numeric vector. Converting it ensures that the values can be used for computations and stored in the data frame.\n\n\nBenford_Prob = log10(1 + 1 / (1:9)):\n\nThis calculates the theoretical probabilities for digits 1 through 9 according to Benford’s Law: \\(P(d) = \\log_{10}\\left(1 + \\frac{1}{d}\\right)\\)\n1:9 provides the range of digits for which we compute probabilities.\n\nCombining into a Data Frame:\n\nThe data.frame() function organizes the information into a table with three columns:\n\nDigit: The digits 1 through 9.\nEmpirical_Freq: The observed relative frequencies of these digits from emp_freq.\nBenford_Prob: The theoretical probabilities predicted by Benford’s Law.\n\n\n\nWe can now display the results\n\n# Display the comparison\nknitr::kable(benford)\n\n\n\n\nDigit\nEmpirical_Freq\nBenford_Prob\n\n\n\n\n1\n0.3072036\n0.3010300\n\n\n2\n0.1674419\n0.1760913\n\n\n3\n0.1185479\n0.1249387\n\n\n4\n0.0976744\n0.0969100\n\n\n5\n0.0713556\n0.0791812\n\n\n6\n0.0670448\n0.0669468\n\n\n7\n0.0620533\n0.0579919\n\n\n8\n0.0584231\n0.0511525\n\n\n9\n0.0502552\n0.0457575\n\n\n\n\n\n\n\n2.7.5.4 Interpreting the Results\nThe table above compares the observed frequencies of leading digits in Apple’s trading volumes with the predictions of Benford’s Law. The match is stunningly close. The example illustrates how this surprising regularity applies even in financial datasets like trading volumes.\n\n\n2.7.5.5 Broader Applications\nBenford’s Law has been observed across diverse datasets, such as: - Market capitalization of companies, - GDP of countries or regions, - Population data of cities or counties.\nIts applicability hinges on datasets spanning multiple orders of magnitude and not being artificially bounded. Deviations from Benford’s Law have even been used to detect fraud in tax filings and financial records.\nFor your project, you will explore whether Benford’s Law holds for another financial dataset, allowing you to test this “law of leading digits” in a real-world context.\n\n\n\n2.7.6 Enhancing Your Learning with an LLM: Debugging and Exploring Data Analysis Pitfalls\nOne of the most powerful ways to use a Large Language Model (LLM), like ChatGPT, is as a debugging companion and a tool for uncovering potential pitfalls in your data analysis. This can be particularly valuable as you learn new concepts, such as the ones covered in this lecture on probability and data analysis with R.\nUse Case: Debugging R Code and Avoiding Common Mistakes\nWhen working with R, especially as a beginner, you might encounter errors, unexpected outputs, or difficulties understanding how certain functions behave. Here’s how you can use an LLM to enhance your learning:\n1. Explaining Error Messages\nWhen you run into an error message in R, an LLM can help you interpret it and suggest solutions. For example:\nError Message:\nError in table(leading_digits): all arguments must have the same length\nHow to Use an LLM: - Copy the error message and your code into the LLM, and ask:\n\n\n\n\n\n\nPrompt\n\n\n\nI encountered this error in R. Here is my code: [paste your code]. Can you explain what went wrong and suggest how to fix it?”\n\n\n2. Exploring Alternatives and Best Practices\nLLMs can also suggest alternative ways to achieve the same task or point out best practices you might not be aware of.\nExample: You might ask:\n\n\n\n\n\n\nPrompt\n\n\n\nI used substr(as.character(valid_volumes), 1, 1) to extract leading digits. Are there other, possibly better ways to do this in R?”\n\n\n3. Checking Your Understanding with “What-If” Questions\nLLMs are great for exploring “what-if” scenarios that test your understanding of concepts.\nExample Questions:\n\n\n\n\n\n\nPrompt\n\n\n\nWhat happens if I include zero or negative values in the dataset when applying Benford’s Law?“*\n\n\nor you might ask\n\n\n\n\n\n\nPrompt\n\n\n\nIf my dataset has fewer than 100 observations, how reliable are the empirical probabilities in approximating theoretical probabilities?”\n\n\nThis type of interaction helps you connect the theoretical content (e.g., the weak law of large numbers) with practical considerations.\n4. Summarizing and Synthesizing Knowledge\nYou can use an LLM to review and consolidate your learning by asking it to summarize key concepts or connect them in new ways.\n\n\n\n\n\n\nPrompt\n\n\n\nCan you summarize the main differences between empirical probabilities and theoretical probabilities, and explain how they relate to Benford’s Law?\n\n\nThis exercise reinforces your understanding while revealing gaps or misconceptions.\nDebugging and exploring potential pitfalls are essential skills for any data scientist or analyst. By leveraging an LLM, you not only solve immediate problems but also gain deeper insights into your learning process. This approach fosters independence and critical thinking, preparing you to tackle more complex challenges in the future.\nAs you work on the project for this lecture, try using an LLM to explain error messages, explore alternative solutions, and answer your “what-if” questions. You might be surprised at how much you can learn from this collaborative process!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#summary",
    "href": "02-lecture2.html#summary",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.8 Summary",
    "text": "2.8 Summary\nIn this lecture, we explored foundational concepts in probability theory and developed practical skills for working with R. Here’s a summary of what we covered:\n\n2.8.0.1 Probability Concepts\n\nPrecise definitions of:\n\nSample space: The set of all possible outcomes of an experiment.\nBasic outcomes: Individual elements of the sample space.\nEvents: Subsets of the sample space.\nProbability: A numerical measure of the likelihood of an event, satisfying the key properties of non-negativity, normalization, and additivity.\n\nDiscrete probability:\n\nHow probabilities are assigned to events in a finite or countably infinite sample space.\nThe construction of events using set operations like union, intersection, and complement.\nThe concept of mutually exclusive events and its implications for probabilities.\n\nEmpirical probability (or frequency probability):\n\nThe relative frequency of an event in a series of repeated experiments.\nHow empirical probabilities approximate theoretical probabilities as the number of trials increases, as demonstrated by the weak law of large numbers.\n\nIndependence of events:\n\nDefinition: Two events (A) and (B) are independent if (P(A B) = P(A) P(B)).\nImplications for computing probabilities in repeated or combined experiments.\n\n\n\n\n2.8.0.2 R Concepts\n\nR Objects:\n\nOverview of atomic vectors and their six types: double, integer, character, logical, complex, and raw.\nAttributes of atomic vectors, such as names, dimensions, and classes.\nIntroduction to data frames, the primary data structure for organizing and analyzing tabular data in R.\n\nSubsetting in R:\n\nMethods for subsetting R objects using positive integers, negative integers, zero, blank spaces, logical vectors, and names.\nPractical examples to manipulate data frames and vectors efficiently.\n\nString manipulation in R:\n\nExtracting and processing information from character data.\n\n\n\n\n\n2.8.0.3 Applications\n\nSimulating coin tosses:\n\nUsing R to simulate repeated trials and compute relative frequencies.\n\nReading data:\n\nLoading datasets from stored CSV files and directly from online sources.\n\nStock price analysis:\n\nUsing subsetting techniques to analyze up and down movements in stock prices.\nComputing empirical probabilities of price movements and using independence assumptions to estimate probabilities of sequences of moves.\n\nBenford’s Law:\n\nExploring the surprising regularity of leading digits in real-world datasets.\nApplying Benford’s Law to trading volumes and comparing empirical distributions to theoretical predictions.\n\n\nThrough this lecture, we reinforced the connection between probability theory and data analysis, demonstrating how abstract concepts like empirical probabilities and independence can be applied in real-world scenarios using R. The integration of theoretical ideas with computational tools prepares you for more advanced analysis and real-world problem-solving.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#project-2-financial-data-forensics-investigating-financial-reports-using-benfords-law",
    "href": "02-lecture2.html#project-2-financial-data-forensics-investigating-financial-reports-using-benfords-law",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.9 Project 2: Financial Data Forensics – Investigating Financial Reports Using Benford’s Law**",
    "text": "2.9 Project 2: Financial Data Forensics – Investigating Financial Reports Using Benford’s Law**\n\n2.9.1 Overview\nThis project challenges you to apply Benford’s Law to detect potential anomalies in financial data. By analyzing the leading digits of revenue and expenditure data for a set of companies, you will explore whether these datasets follow the natural logarithmic distribution predicted by Benford’s Law. Through this project, you will reinforce your understanding of empirical probabilities and their applications, while also practicing critical data analysis skills in R.\n\n\n2.9.2 Objectives\n\nAnalyze the distribution of leading digits in revenue and expenditure data.\nCompare empirical distributions with the theoretical predictions of Benford’s Law.\nIdentify and interpret deviations from Benford’s Law.\nReflect on the implications of your findings in the context of financial forensics.\n\n\n\n2.9.3 Steps\nStep 1: Understand the Research Question Your main tasks are: 1. To determine if the leading digits of revenues and expenditures conform to Benford’s Law. 2. To interpret deviations, particularly in expenditure data, which may suggest anomalies such as fraud or manipulation.\nStep 2: Obtain and Inspect the Dataset\n\nDownload the Dataset:\n\nMock Dataset: You will receive a CSV file named company_financials.csv, containing simulated data for revenues and expenditures of 200 companies. This dataset includes some subtle anomalies in the expenditures.\n\nInspect the Data:\n\nLoad the dataset in R and examine its structure using functions like head(), summary(), and str().\nEnsure you understand the data columns:\n\nCompanyID: A unique identifier for each company.\nRevenue: The revenue of the company (in dollars).\nExpenditure: The expenditure of the company (in dollars).\n\n\n\nStep 3: Prepare the Data\n\nFilter Valid Data:\n\nExclude invalid entries:\n\nNon-positive values (e.g., 0 or negative numbers).\nMissing values (NA).\n\n\nExtract Leading Digits:\n\nUse string manipulation to extract the first digit from each valid value:\n\n\nStep 4: Analyze the Data\n\nCompute Empirical Frequencies:\n\nTabulate the frequencies of the leading digits for revenues and expenditures:\n\nCompare with Benford’s Law:\n\nCreate data frames for comparison:\n\nVisualize the Results:\n\nPlot bar charts comparing empirical and theoretical distributions for revenues and expenditures.\n\n\nStep 5: Interpret the Results\n\nEvaluate Conformity:\n\nDoes the revenue data closely match Benford’s predictions?\nDo expenditures show significant deviations?\n\nHypothesize Causes:\n\nWhat might explain deviations in expenditures? Consider:\n\nRounded or artificial values.\nAnomalies such as fraud.\n\n\nProbability Context:\n\nRelate empirical frequencies to probabilities and discuss the implications of large sample sizes.\n\n\nStep 6: Submit Your Work\n\nA short report summarizing:\n\nYour approach to data preparation and analysis.\nA comparison of the revenue and expenditure distributions with Benford’s Law.\nYour interpretation of any anomalies.\n\nWell-commented R code for your analysis.\n\nDeliverables\n\nAnalysis Report:\n\nInclude plots and tables summarizing your findings.\n\nR Script:\n\nSubmit a clear and well-commented R script.\n\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nLo, Andrew, and Craig MacKinlay. 2019. A Non-Random Walk down Wallstreet. Princeton University Press.\n\n\nSamuelson, Paul. 1965. “Proof That Properly Anticipated Prices Fluctuate Randomly.” Industrial Management Review 6.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  }
]