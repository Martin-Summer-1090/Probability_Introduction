[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Probability",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee (knuth84?) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html#what-are-these-lecture-notes-about",
    "href": "index.html#what-are-these-lecture-notes-about",
    "title": "An Introduction to Probability",
    "section": "What are these lecture notes about",
    "text": "What are these lecture notes about\nThese lecture notes support the course An Introduction to Probability - with Applications to Computational Finance using R. The course introduces essential probability concepts that every finance practitioner must understand. As probability is crucial for solving financial problems computationally, we will use computational finance as the context to develop these ideas. The focus will be hands-on learning, enabling you to understand and internalize concepts by applying them yourself.\nTo make abstract concepts like randomness, chance, and probability more accessible, we will leverage computational tools, in our course this will be R, and visualizations built by using R. By actively engaging with these tools, you’ll not only deepen your understanding of probability but also enhance your ability to apply it to real-world finance problems.\n\n\n\n\n\n\nKey learning objectives\n\n\n\n\nUnderstand foundational probability concepts.\nApply probability principles to solve computational finance problems.\nGain hands-on experience using R for probability and finance applications.\nDevelop proficiency in interpreting and visualizing probabilistic data.\n\n\n\nBut how can we build abstract concepts such as probability, random phenomena and chance by our own hands? While probability is a mathematical theory, it gains practical value and an intuitive meaning in connection with real or conceptual experiments such as, the future change in a stock price index, the future value of a portfolio of securities, the chance that a creditor is not going to be able to pay back your loan.\nMany of these experiments we can nowadays simulate on the computer. We can can construct, build and simulate a huge variety of random phenomena. We can - for instance - implement models of random fluctuations of asset prices. We can model financial risks and contemplate possible future scenarios through simulation.\nThe arrival of the computer has been indeed a sea change for the mathematics of probability because it allows both reflecting about probability through theorems and proofs as well as by making use of computer simulation. We can approach our understanding of abstract concepts by building them with our own hands on the computer. This is the approach to teaching you probability in this course.\nLet me give you one example right away. Don’t worry if you do not understand precisely what the following snippet of code is doing. We will learn the syntax of the R language as we go along. This is just to demonstrate you what I explained in words before by a simple example highlighting simulation of random processes and visualization.\n\n\n\n\n\n\nAn R example for illustration\n\n\n\n\n# Simulate daily returns for a stock\n\nset.seed(123)  # Ensure reproducibility\nn &lt;- 100       # Number of days\ndaily_returns &lt;- rnorm(n, mean = 0.001, sd = 0.02)  # Normal distribution\n\n# Compute cumulative returns\nprice &lt;- cumprod(1 + daily_returns) * 100  # Starting price = 100\n\n# Visualize the stock price\nplot(price, type = \"l\", col = \"blue\", lwd = 2,\n     main = \"Simulated Stock Price\",\n     xlab = \"Days\", ylab = \"Price\")\n\n\n\n\n\n\n\n\n\n\nThis example highlights:\n\nHow randomness influences stock price movements\nR’s usefulness in simulations and visualization.\n\nIt is, however important to always keep in mind that the real world is different from the theory world as well as from the simulated world. The real world is both richer and more complex than both theory and simulation. It may contain features which need to be be taken into account for the explanation of phenomena which are absent in theory or in the simulation. We will try our best to develop your feeling for the difference, between theoretical concepts, simulations and applications as well as for the interdependence of these three areas.\nThere are many ways to build virtual objects and to run simulations to manipulate them. For this we will need a programming language. The language we choose for this course is R and the integrated development environment RStudio. This is one of the main languages used in data analysis, statistics and data science and is widely used in industry and academia. It will be our tool to do probability in this course.\nSince this course was being taught the first time new AI tools such as large language models (LLMs) like chatGPT, Claude or Gemini have been introduced. These new powerful tools create new opportunities for learning and reinforcing knowledge such as the concepts learned in this course. In this year I will show you throughout the course, how you can leverage AI tools like ChatGPT for enhancing your learning experience. I encourage you to use these tools to generate examples, explain concepts, and debug R code or perhaps translate R code into a language you might be more familiar with on the fly to help you understand how things are expressed in R.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#downloading-and-installing-r",
    "href": "index.html#downloading-and-installing-r",
    "title": "An Introduction to Probability",
    "section": "Downloading and installing R",
    "text": "Downloading and installing R\nSo lets start by downloading and installing R first. R is an open source project maintained by an international team of developers. The software is made available through a website called the comprehensive R archive network (http://cran.r-project.org).\nAt the top of this website, in a box named “Download and install R” you will find three links for downloading R. Choose the link that describes your operating system, Windows, Mac or Linux. These links will lead you to the necessary information you need to install a current version of R. The easiest install option is to install R from precompiled binaries. There is also the option to built R from source on all operating systems if you have the tools and the expertise to do so. R also comes both in a 32-bit and a 64-bit version. It does not make a substantial difference which version you use. 64-bit versions can handle larger files and data sets with fewer memory management problems.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#downloading-and-installing-rstudio",
    "href": "index.html#downloading-and-installing-rstudio",
    "title": "An Introduction to Probability",
    "section": "Downloading and installing RStudio",
    "text": "Downloading and installing RStudio\nRStudio is an application that helps you write and develop R code. It makes using R much easier for you than using it in isolation. The interface of RStudio looks the same across all operating systems.\nYou can download RStudio for free from https://www.rstudio.com/products/rstudio/. Select the box RStudio Desktop and follow the download instructions. RStudio Desktop is free. Note that you need to have a version of R installed to use RStudio.\nIf you have successfully installed R and RStudio, we are ready to start.\nThroughout this course I will use R with RStudio and when I do code demonstrations I will use this environment. R can be used in many other ways of course. Some of you might be accustomed to work with Jupyter Notebooks. I will just mention here that with some minor tweaks R can be used with Jupyter notebooks as well. I will not go into the details here. Those of you who are interested in such a setup are encouraged to ask chatGPT:\n\n“Please give me a step by step instruction how to set up the system to use R through Jupyter notebooks instead of RStudio.”",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "An Introduction to Probability",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course is built as an intuitive and elementary introduction to the field. No knowledge of probability or R is assumed and all concepts are built from scratch. You should be able to study for this course by relying on the slides and these lecture notes only without having to consult a textbook or further literature.\nMany of you will have heard or studied probability at some stage in college or at university. I hope the course will appeal to the newcomers as well as to the experienced. While it should allow the first group to quickly learn some of the most important concepts and raise your curiosity and excitement about a field of knowledge which is both beautiful and immensely practical, I hope it will open a new perspective for the more experienced among you and thus help deepening your understanding of concepts you might already have heard elsewhere.\nThe lectures develop probability concepts and R concepts in parallel. This follows the idea underlying this course to develop your knowledge of probability concepts by building them on the computer yourself and play with them. In this way we hope to build your R skills more thoroughly because they appear in a natural context and that this context also reinforces your understanding of probability.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-notes",
    "href": "index.html#structure-of-the-notes",
    "title": "An Introduction to Probability",
    "section": "Structure of the Notes",
    "text": "Structure of the Notes\nThe notes are structured into five main chapters. The idea is to cover each main chapter topic in one of the five double units allocated to this course.\nEach of the main chapters or lectures are followed by a so called project. The projects are meant to be worked on by you in the assigned groups between the lectures and should help you to practice and to deepen what you have learned. We will discuss the project questions at the end of each lecture. We will also distribute a worked problem after you have worked on it yourself.\nWe begin the first lecture, lecture 1 by discussing some of the main basic probability fundamentals by putting them in their historical context where the first ideas appear of how you could measure something such elusive as chance at all. Surprisingly these first simple ideas are already very rich and lead us quite far into the field and its problems.\nIn this lecture we will also give a first panoramic run down of what you can do with R. In this lecture the first application will be a simple game of chance, the tossing of a coin. Already this very elementary example has direct relevance for computational financial modelling of security prices as we will learn throughout the course.\nEven the very basic ideas of probability and R enable us, already at this early stage, to take on a serious real world problem as well. In fact, with only a few intuitive concepts we can study and understand problems of the chance of coincidences. This class of problems is at the heart of the science of constructing cryptographically safe finger prints, so called hash-functions, which are at the heart of crypto-currencies such as Bitcoin.\nThe first project will deal with engineering secure transaction indentifiers for a digital payment system and study how the construction of such identifiers has to take into account key system paramaters such as transaction volume and number of participants in the payment system.\nLecture 2 will introduce the first basic intuitive ideas a bit more formally and also generalize them. In particular it explains the basic ideas of how probability and frequencies of chance events are related. We will discuss the power as well as the limits of these ideas for applied work in probability.\nIn this lecture we will explain in detail how you can use R to read, store and manipulate data. We also start working with stock market data and randomly fluctuating asset prices and discuss how we can deepen our understanding of them using probability and R.\nProject 2 will help you to train and practice your understanding of data manipulation in R by analyzing a real world supervisory data set from the European Banking Authority EBA. We will use a really cool empirical probability pattern on the empirical frequency of leading digits - Benford’s law - for checking whether the data could be trusted or do look fishy on purely probabilistic grounds.\nIn Lecture 3 we will learn about the key concept of conditional probability. This lecture contains lots of concepts but it is also of key importance for understanding many problems in Finance and the management of financial risks. We will explain how the misunderstanding or perhaps willful ignorance of the concept of dependence is at the root of the big financial crisis of 2007-2008. We will also learn, how updating your probability assessments in the light of new data is important for investment decisions.\nOn top of this directly practical considerations, we will also learn how this particular problem of updating probability assessments in the light of new information helps us close a conceptual gap in the pure frequency interpretations of probability.\nIn terms of R this lecture gives you a bit of a break in the sense that it applies what we have learned before and thereby reinforces what we have already learned. It will introduce some useful techniques and functions along the way.\nIn project 3 you will deepen your understanding of how probability updating works and how its sound understanding can make a real difference for the success of investment decisions.\nLecture 4 will introduce you more precisely to the key probabilistic model of random returns and other random phenomena in Finance: The concept of a random variable and the concepts of expected value, variance, standard deviation, covariance and correlation. We will discuss at length the modelling of random returns and learn about the basic workhorse model of asset price dynamics, the binomial lattice.\nThis discussion gives us also the appropriate context to discuss how to write programs in R and some of the principles and constructions that are most useful for this task.\nIn the project for lecture 4 you will learn about how you can apply the ideas of random variables and the binomial distribution to deepen your understanding of credit risk.\nThe final Lecture 5 discusses the concept of continuous random variables, which is very important for a large body of applied work. You will be introduced to the normal distribution and its power and limits for modelling random returns and financial risks.\nIn terms of R this lecture will provide a good context for discussing what matters when you want to make your R code fast and efficient.\nThe last project will then allow us to connect all the concepts of the five lectures with the project following lecture 4 to get a somewhat realistic and practically useful model for analyzing and managing credit risk.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "An Introduction to Probability",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to thank my friend and colleague Branko Urošević that he entrusted me to develop this course for his new and innovative program in computational finance.\nI am very grateful for the opportunity to make a contribution to this effort and for all the encouragement, support and advice I got from him for this project. Of course he bears no responsibility whatsoever for any shortcomings of these lecture notes.\nI would also like to sincerely thank the former students of the bootcamp probability course who gave me feedback and studied from the lecture. They were a fantastic and enthusiastic audience from who I learned a lot.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "An Introduction to Probability",
    "section": "References",
    "text": "References\nWhen you teach such a fundamental field as probability theory and its applications in Finance you teach from a core knowledge that has been built by many pioneers, experts and other teachers in this area. In this sense nothing contained in this course is original work. The only contribution a teacher can make is the way how the material is presented and combined. In this sense these lecture notes are nothing but a collection of well known and time tested material as well as an experiment in yet one more, hopefully innovative exposition.\nBut even in this field I have copied and recycled a lot from the teaching materials and collective wisdom of other colleagues whose examples and problems I found particularly helpful. Without any claim to completeness let me briefly go through the sources I have relied on most strongly when developing this course.\nWith respect to probability theory two sources have influenced me most and I have followed their example extensively in this course: William Fellers “An introduction to probability theory and its applications volume 1.” which first appeared in 1957. (Feller (1968)). While really old by now it is still in my view one of the most outstanding reference with respect to exposition and clarity of explanation as well as an inspiring source of enthusiasm for the subject. I just would like to mention as an aside that Feller (born Vilibald Srećko Feller) was a Croatian emigrant to first Germany and then the US in the 1930ies. You might claim him as a citizen of Croatia, former Yugoslavia or even Austria, since when he was born in 1906, what is now Croatia was still a part of the Austrian-Hungarian empire.\nMy second most important source for this course in terms of probability was Karl Schmedder’s course “An intuitive introduction to probabiliy”, which Karl developed for the online platform Coursera. I consider this course an outstanding didactical achievement and masterpiece. If you find this lectures fun, I can only encourage you to also do the course on Coursera, when you find time. It is for a reason that Karl’s course is the mostly viewed Coursera course from Switzerand.1\n1 See https://www.coursera.org/learn/introductiontoprobabilityIn terms of R and R programming I have followed and copied a lot from Garett Grolemund’s excellent book “Hands on programming with R”.(Grolemund (2014)) I encourage you to study this book. From it I have not only learned about one of the most excellent explanations of R concepts but also the insight that these concepts are best explained within a very concrete and non trivial application context rather than in a patchy series of toy examples.\nWith respect to Finance and investments I have relied on the excellent book by David Luenberger, “Investment Science”. (Luenberger (2009)) If you can get a hold of this fantastic work, I can only encourage you to get one and have it on your shelf. It is a role model of clarity of exposition and has much more to offer than we can cover in this book.\nFinally with respect to the history and the interpretations of probability, I have learned the most form the excellent book “10 great ideas about chance”, by Persi Diaconis and Brian Skyrms. (Diaconis and Skyrms (2019)) If you find probability interesting or even fascinating I encourage you to read this book at some stage, if you find time. It is full of highly interesting philosophical, mathematical, historical and fun facts and ideas about this very rich subject.\nBut now, lets get straight down to the matter at hand.\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nGrolemund, Garrett. 2014. Hands on Programming with R. O’Reilly. https://rstudio-education.github.io/hopr/.\n\n\nLuenberger, David. 2009. Investment Science. Oxford University Press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#leveraging-chatgpt-free-version",
    "href": "index.html#leveraging-chatgpt-free-version",
    "title": "An Introduction to Probability",
    "section": "Leveraging ChatGPT (Free Version)",
    "text": "Leveraging ChatGPT (Free Version)\n\nUsing an LLM to Enhance Your Learning\nThroughout this course, you’ll learn how to use ChatGPT to support your studies. As a free tool, it offers capabilities that can make learning probability more interactive and engaging. Here’s how you can leverage it:\n\nClarify Concepts:\nAsk ChatGPT to explain concepts in simple terms or elaborate on topics you find difficult:\n\n“Explain the concept of a probability distribution with an example.”\n\nGenerate Examples:\nUse it to create additional examples for practice:\n\n“Can you give me an example of a random variable and how it applies in finance?”\n\nDebug R Code:\nIf you encounter issues with your R code, paste the code and ask:\n\n“Why does this R code not run, and how can I fix it?”\n\nPractice Exercises:\nRequest custom exercises:\n\n“Create three exercises to practice calculating probabilities for dice rolls.”\n\nSimulate Interactive Discussions:\nSimulate discussions with ChatGPT to test your understanding:\n\n“I think the variance of a constant is zero. Am I correct? Explain why or why not.”\n\nLearn Best Practices in R:\nAsk for tips to improve your coding practices:\n\n“What are the best practices for writing clean and efficient R code?”\n\n\nImportant Note:\nWhile ChatGPT is a powerful learning tool, remember to verify the outputs, especially for complex calculations, and always critically evaluate its suggestions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#signing-up-for-chatgpt-free-version",
    "href": "index.html#signing-up-for-chatgpt-free-version",
    "title": "An Introduction to Probability",
    "section": "Signing up for ChatGPT (Free Version)",
    "text": "Signing up for ChatGPT (Free Version)\nTo access the free version of ChatGPT, follow these steps:\n\nGo to https://chat.openai.com in your web browser.\nClick on the “Creat a free account” button to create an account. You can sign up using an email address or your existing Google or Microsoft account.\nFollow the prompts to verify your email address and complete the registration process.\nOnce registered, log in to access ChatGPT. You can start using it immediately for your learning needs.\n\nUsing ChatGPT is free, but note that certain features or advanced versions (e.g., GPT-40) may require a paid subscription. The free version (GPT-3.5) is sufficient for getting a feeling of what the LLM can do. They payed version is more powerful.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#using-jupyter-notebooks-with-r",
    "href": "index.html#using-jupyter-notebooks-with-r",
    "title": "An Introduction to Probability",
    "section": "Using Jupyter Notebooks with R",
    "text": "Using Jupyter Notebooks with R\nIf you prefer working in Jupyter notebooks instead of RStudio, follow these steps to set up your system:\n\nInstall Jupyter Notebook:\nInstall Jupyter using Python’s pip package manager:\npip install notebook\nInstall R and the IRkernel:\nInstall R on your system, then install the IRkernel within R:\ninstall.packages(\"IRkernel\")\nIRkernel::installspec()\nStart Jupyter Notebook:\nLaunch Jupyter from your terminal:\njupyter notebook\nCreate a New R Notebook:\nIn Jupyter, click “New” and select “R” to start an R notebook.\nInstall R Packages in Jupyter:\nUse the usual R syntax to install packages:\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nJupyter provides a flexible, interactive environment for running R code, making it an excellent alternative to RStudio.\n\nWith these resources and tools, you are ready to dive into the fascinating world of probability and computational finance!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#leveraging-the-possibilities-of-chatgpt",
    "href": "index.html#leveraging-the-possibilities-of-chatgpt",
    "title": "An Introduction to Probability",
    "section": "Leveraging the possibilities of ChatGPT",
    "text": "Leveraging the possibilities of ChatGPT\nIn these lectures I try to help you building experience how to integrate large language models or LLMs into your learning and study process as well as to support you in coding and programming. There are several such models currently on the market. For this course I will use openAI’s chatGPT as a tool. chatGPT is available in a free version and in an enhanced pay-version. For the purposes of this course the free version is sufficient.\n\nUsing an LLM to Enhance Your Learning\nThroughout this course, you’ll learn how to use ChatGPT to support your studies. As a free tool, it offers capabilities that can make learning probability more interactive and engaging. Here’s how you can leverage it:\n\nClarify Concepts:\nAsk ChatGPT to explain concepts in simple terms or elaborate on topics you find difficult:\n\n“Explain the concept of a probability distribution with an example.”\n\nGenerate Examples:\nUse it to create additional examples for practice:\n\n“Can you give me an example of a random variable and how it applies in finance?”\n\nDebug R Code:\nIf you encounter issues with your R code, paste the code and ask:\n\n“Why does this R code not run, and how can I fix it?”\n\nPractice Exercises:\nRequest custom exercises:\n\n“Create three exercises to practice calculating probabilities for dice rolls.”\n\nSimulate Interactive Discussions:\nSimulate discussions with ChatGPT to test your understanding:\n\n“I think the variance of a constant is zero. Am I correct? Explain why or why not.”\n\nLearn Best Practices in R:\nAsk for tips to improve your coding practices:\n\n“What are the best practices for writing clean and efficient R code?”\n\n\nImportant Note:\nWhile ChatGPT is a powerful learning tool, remember to verify the outputs, especially for complex calculations, and always critically evaluate its suggestions. One such tool you might consider for cross checking is Wolfram | Alpha, which you can find here: https://www.wolframalpha.com/ in a free version-",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#using-an-llm-to-enhance-your-learning",
    "href": "index.html#using-an-llm-to-enhance-your-learning",
    "title": "An Introduction to Probability",
    "section": "Using an LLM to Enhance Your Learning",
    "text": "Using an LLM to Enhance Your Learning\nIn these lectures I try to help you building experience how to integrate large language models or LLMs into your learning and study process as well as to support you in coding and programming. There are several such models currently on the market. For this course I will use openAI’s chatGPT as a tool. ChatGPT is available in a free version and in an enhanced pay-version. For the purposes of this course the free version is sufficient.\nThroughout this course, you’ll learn how to use ChatGPT to support your studies. As a free tool, it offers capabilities that can make learning probability more interactive and engaging. Here’s how you can leverage it:\n\nClarify Concepts:\nAsk ChatGPT to explain concepts in simple terms or elaborate on topics you find difficult:\n\n“Explain the concept of a probability distribution with an example.”\n\nGenerate Examples:\nUse it to create additional examples for practice:\n\n“Can you give me an example of a random variable and how it applies in finance?”\n\nDebug R Code:\nIf you encounter issues with your R code, paste the code and ask:\n\n“Why does this R code not run, and how can I fix it?”\n\nPractice Exercises:\nRequest custom exercises:\n\n“Create three exercises to practice calculating probabilities for dice rolls.”\n\nSimulate Interactive Discussions:\nSimulate discussions with ChatGPT to test your understanding:\n\n“I think the variance of a constant is zero. Am I correct? Explain why or why not.”\n\nLearn Best Practices in R:\nAsk for tips to improve your coding practices:\n\n“What are the best practices for writing clean and efficient R code?”\n\n\nImportant Note:\nWhile ChatGPT is a powerful learning tool, remember to verify the outputs, especially for complex calculations, and always critically evaluate its suggestions. One such tool you might consider for cross checking is Wolfram | Alpha, which you can find here: https://www.wolframalpha.com/ in a free version-",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-lecture1.html",
    "href": "01-lecture1.html",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1 Tossing a fair coin: First Probability ideas.\nLets start with a classic and basic example of a probability model: Tossing a fair coin. When probability theorists talk of a fair coin they mean that the coin has equal chances of coming up heads as coming up tails. When the coin is tossed it is uncertain whether it will ultimately come up Heads or Tails.\nFigure 1.1: Figure 1: Tossing a coin\nFigure 1.1 visualizes such a process.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#rolling-a-die",
    "href": "01-lecture1.html#rolling-a-die",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1.1 Random experiments, sample space, events\nIn the theory of probability a process leading to an uncertain outcome is called a random experiment. The example of throwing a die helps us to give a precise meaning to the notion of an uncertain outcome. While we can imagine in principle that the die lands on one of its corners and this is one outcome, we agree on the outset that we are going to consider the (practically relevant cases) that it will land on one of the faces.\nThe collection of all possible outcomes of a practical or conceptual random experiment is called in probability theory a sample space. While the sample space of throwing a die is an idealization it is exactly this idealization which simplifies the theory without affecting its applicability. The basic outcomes in the random experiment of throwing a die are that the die lands such that the upward showing face shows a 1 a 2, 3, 4, 5 or a 6. In the theory the collection of basic outcomes is denoted as a set. Thus the sample space of throwing a die is given as the set \\({\\cal S} = \\{1,2,3,4,5,6\\}\\).\nThe sample space \\({\\cal S}\\) is the set of all basic outcomes. The subsets of the sample space are often called events in probability theory. An event could be - for example - an outcome where the die shows an upward looking face with an even number \\(A = \\{2,4,6\\}\\).\n\n\n1.1.2 The role and nature of idealizations in applications\nIdealizations of the kind we discussed for the throw of a die are standard in probability and we will encounter them again and again. For example the movement of a stock price is often though of as a conceptual random experiment. When we try to agree on what is the appropriate sample space of this experiment, we can say that the price can not fall below 0 but it is hard to agree on what will be the highest possible price. In the probabilistic treatment of stock prices, which we will discuss later in our course, it is for instance common to assume that the sample space of this random experiment is the entire interval of non-negative real numbers \\({\\cal S} = [0, \\infty)\\).\nMany of us would hesitate to claim that the price might rise without bound. Yet many models in applied Finance are based on such an assumption. The models allow arbitrary price hikes but with arbitrary small probability as the price gets higher and higher. Practically it does not make sense to believe that a security price can become arbitrarily high. The use of arbitrarily small probabilities in a financial model might seem absurd but it does no practical harm and makes the model simple and convenient to use. Moreover, if we seriously introduced an upper bound on a security price at \\(x\\) it would be also awkward to assume that it is impossible that it could be just a cent higher, an assumption equally unappealing than assuming it can get in principle arbitrarily high.\n\n\n1.1.3 Classical Probability: Measuring uncertainty\nProbability is a measure of how likely an event of an experiment is. But how could we measure chance? Here is the first big idea of probability theory and how the originators thought about it.1 How do you measure anything? If you think of length, for example, you take an arbitrary standard you agree on and then count. The originators of probability theory pursued the same idea with chance: To measure probability you choose equally probable cases and then count.\n1 In this discussion I use chapter 1 in Diaconis and Skyrms (2019)The probability of an event \\(A\\) according to the originators of the theory of probability is then \\[\\begin{equation}\nP(A) = \\frac{\\text{Number of cases where $A$ occurs}}{\\text{Total number of cases}}\n\\end{equation}\\]\nThus, if we want to know the probability of the die ending up on a face such that an even number is shown (2,4,6) we have to compute according to this notion \\(3/6\\) (since the total number of cases are six basic outcomes) which is a chance of \\(50 \\%\\).\nNote that this classical notion of probability has a few interesting consequences, which we will discuss in more detail later but which already now naturally flow from this basic idea of measuring chance.\n\nProbability is never negative.\nIf an event \\(A\\) occurs in all cases \\(P(A)=1\\).\nIf \\(A\\) and \\(B\\) never occur in the same case, then \\(P(A \\,\\text{or}\\, B) = P(A) + P(B)\\).\n\nIn particular then the probability that an event does not occur is 1 less the probability that it occurs: \\(P(\\text{not}\\, A) = 1 - P(A)\\).\nLet us interrupt here our discussion of probability for a moment and ask how we can make good on our promise to make these ideas tangible, so we can play with them.\nFor this we will need the computer. Since we will talk to the computer in R, it is now the right time to look at the die rolling example from the perspective of R and dive into some of its basic concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#rolling-die-on-computer",
    "href": "01-lecture1.html#rolling-die-on-computer",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.2 Rolling a die on the computer: First steps in R",
    "text": "1.2 Rolling a die on the computer: First steps in R\n\n1.2.1 The R User Interface\nBefore we can ask our tool to do anything for us, we need to know how to talk to it. In our case RStudio allows us to talk to our computer. It works like any other application. When you launch RStudio on your computer, in figure @ref(fig:rstudio-start-screen) you see a screen looking like this:\n\n\n\n\n\nThe RStudio startup screen\n\n\n\n\nIn this picture you see a screenshot of my RStudio screen. Interacting with the app is easy. You type commands via your keyboard at the prompt, which is the &gt; symbol. You find this symbol in the RStudio pane called Console. You can see it in the left pane in the screenshot. You send the command to the computer by pressing enter. After you have pressed enter, RStudio sends the command to R and displays the result of your command with a new prompt to enter new commands, like this:\n\n1+1\n\n[1] 2\n\n\nLet me pause to explain what you see here in this code example. First you see a light-gray box containing the command 1+1. This is an instance of a so called code chunk. Moving the cursor to the right upper corner of the chunk, will display a copy icon and you can click this icon to copy the code to the computer’s clipboard and then paste it at the prompt of your console (or for that matter anywhere else) and run it in R studio, if you wish. In the code-chunk the R-prompt &gt; is not displayed. This symbol of the prompt is only shown in the Console itself.\nWhen the code is executed, you see the result in the second light-gray box, just below the first one. It starts with a double hash ##, indicating that it shows an output of running the above code chunk and then displays the output as it would appear in the command window [1] 2.\nThe [1] means that the line begins with the first value of your result. For example, if you enter the command 20:60 at the prompt of your console which means in the R language, “list all the integers from 20 to 60” and press enter you get:\n\n20:60\n\n [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[26] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nmeaning that 20 is the first value displayed in your result. Then there is a line break because not all values can be displayed on the same line and R tells you that 45 is the 26-th value of the result.\nThe colon operator : is a very useful function in R which we will need often. It allows us to create sequences of every integer between two given integers.\nR needs a complete command to be able to execute it, when the return key is pressed. Lets see what happens, if a command is incomplete, like for instance &gt; 5*.\nIn this case R will show the expression followed by a + instead of showing a new prompt. This means that the expression is incomplete. It expects more input. If we complete the expression, say like\n&gt; 5*\n+ 4\nthe expression can be evaluated and a new prompt is shown in the console.\nIf you type a command that R does not understand, you will be returned an error message. Don’t worry if you see an error message. It just is a way the computer tells you that he does not understand what you want him to do.\nFor instance, if you type 5%3 you will get an error message like this\n&gt; 5%3\nError: unexpected input in \"5%3\"\n&gt;\nSometimes it is obvious why a mistake occurred. In this case, that R just does not know what to do with the symbol %. It has no meaning in this context. Sometimes it is not so obvious what the error message actually means and what you might do about it.\nA useful strategy in this case is to type the error message into a search engine and see what you can find. The chance is very high that others encountered the same problem before you and got helpful advice how to fix it from other users on the internet. One site, we find particularly helpful for all kinds of questions related to R and R programming is https://stackoverflow.com/. Try it at the next opportunity.\nNow with this basic knowledge, we can already make the first step to create a die on the computer using R. If you think of a physical die, the essential thing that matters are the points on its six sides. If you throw the die it will usually land on one of these sides and the upward showing side of the die shows the number of points. The colon operator : gives us a way to create a group of numbers from 1 to 6. R gives us the result as a one dimensional set of numbers.\n\n1:6\n\n[1] 1 2 3 4 5 6\n\n\nLets use these first steps in R to recap the probability concepts we have learned using this example of the six sided die: A basic outcome of rolling a six-sided die is for example 6 if the upper side after rolling the die happens to be the side with 6 points. The sample space of the experiment of rolling a six-sided die is the set \\({\\cal S} = \\{1,2,3,4,5,6\\}\\). In probability theory we often use the symbol \\({\\cal S}\\) or \\(S\\) for sample space. In many probability texts the sample space is also often denoted by the symbol \\(\\Omega\\) the Greek letter for (big) Omega. A random experiment in this example is the rolling of the die. The outcome is uncertain but once the die is rolled the outcome can be determined precisely. The event that the outcome is a display of 10 points is the empty set \\(A = \\emptyset\\). The symbol \\(\\emptyset\\) comes from set theory and means the set containing no elements. This event can contain no elements because we can not get a score of 10 by rolling a six sided die.\n\n\n1.2.2 Objects\nYou can save data in R by storing them in objects. An object is a name, you can choose yourself to store data. For example, if you choose to store the value 6 in an object called point_six, you would type:\n\npoint_six &lt;- 6\n\nat the prompt. R will the store the value 6 in the object called point_six, which you can use to refer to the value. If you type the name of your object at the prompt, R will display the value you have assigned. A useful key combination for typing the assignment operator &lt;- is to use the key combination ALT _. At the R prompt R will automatically print an assignment operator.\nNow if you type the name of the object and press enter, R will display the value or values stored in it, like this:\n\npoint_six\n\n[1] 6\n\n\nNow you can use the name of the object to refer to its value. For instance, you could divide point_sixby 2and get a meaningful result\n\npoint_six/2\n\n[1] 3\n\n\nNow to make our die more tangible and useful, let us store it in an R object by typing the following command at the prompt. This command creates an object with name die and assigns the vector 1,2,3,4,5, 6 to it.\n\ndie &lt;- 1:6\n\n\n\n\n\n\nThe RStudio Environment pane keeps track of the objects you have created\n\n\n\n\nYou can now see in the right upper Environment pane in figure @ref(fig:rstudio-screen-with-command) that R shows you that there is an object with the name die that it consists of integers 1,2,3,4,5. As you create more objects they will be stored in the Environment pane and are ready for your reference, unless you delete them. You can remove or delete an object by typing rm(object) or by assigning the value die &lt;- NULL which would also remove the object from your environment or workspace.\nYou can name your objects almost anything with a few exceptions. An object name must not start with a number. There are some special symbols which can also not be used in object names, like ^, !, $, @, +, -, /, *. Note that R is case sensitive and distinguishes small and big letters. If you assign a new value for an object you have already created, R will overwrite the object without warning.\nYou can see which objects are currently created and available for you in the Environment pane of your session of by typing ls(). The UNIX users among you will recognize this command from the unix shell, where it displays the files in a directory.\nBefore we learn how we can actually roll our die and perform a random experiment with it, let us briefly use the opportunity to explain a few things about how R does computations. We have already explained that we can use the object name to refer to the value. So for instance if we type\n\ndie*die\n\n[1]  1  4  9 16 25 36\n\n\nThis might irritate some of you because we have called the object a vector. In linear algebra multiplication of vectors is only allowed if there is an inner product. What happens here, if we use * the multiplication operator is that R does an element-wise multiplication of the six numbers of our die. Of course R allows to take an inner product as well, but this needs a different operator. To compute an inner product, we would type\n\ndie %*% die\n\n     [,1]\n[1,]   91\n\n\nNow R displays the result as a vectors with one row and one column, which is denoted in the output by [ , 1] for the column and [1, ] for the row. We will learn later more about the use and the meaning of this notation in R.\nThe element wise execution R usually uses also means that when you, for example type\n\ndie - 1\n\n[1] 0 1 2 3 4 5\n\n\nR would subtract 1 from every component in the vector die.\nAnother specific behavior of R, you need to know about is called recycling. If you give R two vectors of different length in an operation, R will repeat the shorter vector as long as it is of equal length with the longer one. For example, if you have: ::: {.cell}\ndie + 1:2\n\n[1] 2 4 4 6 6 8\n\n:::\nyou see that R adds 1 to 1 and 2 to 2 and then starts over again by adding 1 to 3 and 2 to 4 and then starts over once again by adding 1 to 5 and 2 to 6. If the longer vectors is not a multiple of the shorter one, R recycles but the cuts off. It will give you a warning though. ::: {.cell}\ndie + 1:4\n\nWarning in die + 1:4: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 2 4 6 8 6 8\n\n:::\nWhile this might seem awkward to some of you, we will see that for data manipulation element-wise execution is often extremely useful. It allows to manipulate groups of values in a systematic yet easy way.\n\n\n1.2.3 Functions\nR contains many functions which we can use to manipulate data and compute things. The syntax for using a function is very simple: You type the function name and put the value of the function argument in parentheses. Here we use for illustrations the function of the square root sqrt(): ::: {.cell}\nsqrt(4)\n\n[1] 2\n\n::: or rounding a number: ::: {.cell}\nround(3.1415)\n\n[1] 3\n\n::: The data you write in the parentheses are called the function arguments. Arguments can be all sorts of things: raw data, R objects, results from other functions.\nIf functions are nested, R evaluates the innermost function first and then goes on to the outer functions. To see examples of all these instances you can take ::: {.cell}\nmean(1:6)\n\n[1] 3.5\n\nmean(die)\n\n[1] 3.5\n\nround(mean(die))\n\n[1] 4\n\n::: for example.\nFor simulating random experiments, R has the very useful function sample(). With this function we can roll our die on the computer and conduct actual random experiments.\nThe function takes as arguments a vector names x and a number named size. sample will return size elements randomly chosen from the vector x. Lets say: ::: {.cell}\nsample(x = 1:4, size = 2)\n\n[1] 2 1\n\n:::\nIn this case sample has chosen 4,1 from the vector x = (1,2,3,4) at random.\nIf we want to roll the die in our computer we can thus pass the die as an argument to sample and use the number 1 for the size argument. Lets do a few rolls with our die ::: {.cell}\nsample(x = die, size = 1)\n\n[1] 3\n\nsample(x = die, size = 1)\n\n[1] 2\n\nsample(x = die, size = 1)\n\n[1] 6\n\nsample(x = die, size = 1)\n\n[1] 5\n\n::: These are the random outcomes of our consecutive rolls. It is as if we had thrown an actual die but in this case we have done the same thing on the computer. Isn’t it cool that this is possible at all? The sample() function will remain our good friend throughout this course.\nR functions can have many arguments, but they need to be separated by a comma.\nEvery argument in every function has a name. We specify which data are assigned to the arguments by setting a mathematical equal sign = between the name and the data. Names help us to avoid passing the wrong data and thereby mixing up things or committing errors. But using names is not necessary. If we just wrote ::: {.cell}\nsample(die,1)\n\n[1] 3\n\n::: R would also know what to do. It is not always clear which names to use for a function. If you are not sure, you can use the function args() to look it up. Here we take the function roundas one example. ::: {.cell}\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n::: Note that the digits argument in the round function is already set to 0. Frequently R functions come with optional arguments. These arguments are optional because the come with a default value, which is 0 in case of the round function.\nWe recommend that you write out argument names as a rule. It gives clearer code and avoids errors. If you don’t write argument names, R matches your values to the arguments of the function by order.\n\n\n1.2.4 Writing your own functions\nNow we are ready to write our own function to roll the die in our computer. Each function in R has the same elements: A name, a function body of code and a set of arguments. To write your own function, you have to write up all of these parts and save them in an R object. The syntax is:\nmy_function &lt;- function() {}\n\nThe name here is my_function, next comes the expression function() which needs to be assigned. The names of the function arguments have to be written between the parentheses. Then we have to write the actual code within the braces {}.\nTo do this for the die, lets write a function named roll_die. ::: {.cell}\nroll_die &lt;- function(){die &lt;- 1:6 \n                         sample(die, size = 1)}\n:::\nNow we can roll our die for a few times to show how the function works ::: {.cell}\nroll_die()\n\n[1] 3\n\nroll_die()\n\n[1] 1\n\nroll_die()\n\n[1] 1\n\nroll_die()\n\n[1] 6\n\nroll_die()\n\n[1] 5\n\n::: Note that in our function roll_die() has no arguments, just the function body. This is perfectly legitimate in R. It is important that when we call the function we have to call it with the parenthesis like roll_die(). If we only call the name roll_die, R will display the code in the function body.\nA final remark in the sample function is in place here. If we look at the arguments of sample using the args function we see ::: {.cell}\nargs(sample)\n\nfunction (x, size, replace = FALSE, prob = NULL) \nNULL\n\n::: Lets do not discuss all the details of this output but concentrate for a moment on the replace argument. What does this mean?\nAs we saw previously we can use the sample function to model the rolling of our die. If we set the size argument to 1 we get the roll of one die. If we set the size argument to n, we would simulate the rolling of n dies. But now the replace argument becomes crucial. As we can see in the output of the args function replace has a default value FALSE. This is a logical argument. It tells R, for example, that if we set size = 2, meaning that two dice are rolled, if the first dice shows, say a value of 3, the second die cannot show 3 as well.\nThis is clearly not what we have in mind when we model the rolling of 2 dice. It should be possible that both dies show the same value. To enable this behavior of the sample function, we have to change the default value of the replace argument to TRUE. Then R chooses a random draw from all of the six possible values for all dice rolled.\nCongratulations ! You have written your first R function for conducting a simple random experiment. Let me remind you once again: Think of the parentheses as a trigger that tells R to run the function. If you omit the trigger R just prints the body of the function. When you run a function, all the code in the function body is executed and R returns the result of the last line of code. If the last line of code does not return a value neither will R.\n\n\n1.2.5 Arguments\nImagine we remove the first line of code in our function body and changed the name die in the sample function to “ball”. ::: {.cell}\nroll_die2 &lt;- function(){sample(ball, size = 1)}\n::: If we call the function now, we will get an error. The function call roll_die2() will result in the error message Error in sample(ball, size = 1) : object 'ball' not found (try it!)\nWe could supply ball when we call roll_die2 if we make ball an argument of the function. Lets do this: ::: {.cell}\nroll_die2 &lt;- function(ball){sample(ball, size = 1)}\n::: Now the function will work as long as we supply ball when we call the function. ::: {.cell}\nroll_die2(ball = 1:6)\n\n[1] 3\n\n::: Note that we still get an error, if we forget to supply ball argument. This could be avoided if we give the function a default argument ::: {.cell}\nroll_die2 &lt;- function(ball= 1:6){sample(ball, size = 1)}\n::: Now if we type: ::: {.cell}\nroll_die2()\n\n[1] 1\n\n::: everything works, just as intended.\n\n\n1.2.6 Scripts\nSo far we have worked by interacting with the console. But what if you want to edit your functions? It would be much easier, if you could use a draft of your code and work form there. This can be done by using a script.\nYou create a script by going to File &gt; New File &gt; R script in the menu bar of RStudio. Using scripts is the standard way to write code in R. It not only helps you to keep track of your code, save it and edit it later. It also makes your work reproducible. You can edit and proofread your code and share it with others. To save your script go to File &gt; Save As in the menu bar.\n\n\n\n\n\nThe RStudio Script\n\n\n\n\nRStudio has many useful features to help you work with scripts. You can for instance automatically execute a line in a code by using the run button. You can also execute sections of code or the entire script. The entire script is executed by running the Source button. For all these commands there are key short cuts which you will learn as you work more with RStudio and R.\nFrom now on you should write all your R-code for this course in scripts. Make a new folder for the scripts you write and store your R-code there from now on.\n\n\n1.2.7 Using packages and finding Help\nWe have now a function which we can use to simulate the rolling of a die, roll_die(). If the die is fair it should be the case that if we roll the die often, all numbers should occur about equally often. The die should not be weighted in favor of a particular value.\nOne way to learn whether our die is fair are repetition and visualization. These are tools we will need all the time, when working with data and when doing probability. While R has many useful functions, one of the great powers is that R is constantly extended by a huge community of users by providing packages.\nPackages are add on functions, which will not be available when you install R. They need to be installed and loaded before you can use them. Since packages are such a powerful tool in R we need to introduce what they are and how to use them in the beginning.\n\n\n1.2.8 Packages\nThere are many visualization tools in R that come with the basic installation. Since the point we want to make here is about packages, we will use a visualization function which is part of the add on package ggplot2, a very popular package for making all kinds of graphs. ggplot2 is not part of the base-R distribution. It has to be therefore loaded before we can use it.\nThere are many additional functions provided with this package. Indeed you could run a whole course on just that 2 For the point we want to make here we will use just one of them, called qplot(), a shorthand for for quick plot.\n2 A very good starting point is for instance Healy (2019), in case you are interested. Also a good source is the first chapter in Wickham and Grolemund (2017).Since qplot() is a function in the package ggplot2, we first need to install this package. To install a package you need to be connected to the internet. If you have internet connection go to the command line and run at the command line: install.packages(\"ggplot2\").\nR displays what is happening while executing the command. Don’t worry if you don not know what all of these messages exactly mean and don’t panic that they are displayed in red. All packages can be installed like this. You have just to enter the correct name in the function install.packages(). The lower right pane in the RStudio software alternatively provides a tab called Packages. Using this tab, you can also install R packages interactively by clicking the install button on the upper left corner of the Packages tab.\nAfter installation the package is on our hard-drive but it can not yet be used. To use the package it has to be loaded. This is done with the command library. To load the ggplot2 package we type ::: {.cell}\nlibrary(\"ggplot2\")\n::: and hit the return key. Many things could be said about the R package system and you will learn it in more detail as we go along in our course. For the moment the most important thing to remember is that a package needs to be newly loaded whenever you want to use it in a new R session.\nTo check whether our dies is fair, we need to roll it many times. R provides a function, that does this for us. This function is called replicate() and provides an easy way to repeat a command many times. The number of times we want to repeat something is given as an argument to the replicate function.\nNow lets roll our die 100 times and save the result in an object we call rolls: ::: {.cell}\nrolls &lt;- replicate(100, roll_die())\n::: We now use the qplot()function from the ggplot2 library to make a quick visualization, by typing the command qplot(rolls). I include the resulting picture in the following figure ::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: ::: If you try this and your picture looks a bit different from mine, don’t worry. This is as things should be. I have stored the picture after one particular run of the replicate function and saved it in my figures folder to these lecture notes. Each time you run the replicate function anew a new random outcome is stored which will look not exactly the same as any other or course. This is what randomness does to the outcome of rolling a die.\nIt looks like every value occurred roughly 16 times but there is still quite some variation. For instance, the value 3 seems to occur more than 20 times whereas the value 2 occurs less than 15 times.\nMaybe we have to give it another trial with more replications. With the computer we can do this with a fingertip. Let us roll our die 10000 times and plot the result.\n&gt; rolls2 &lt;- replicate(10000, roll_die())\n&gt; qplot(rolls2m bindwidth = 1)\nNow the picture looks better.\n\n\n\n\n\nFrequencies of 1, 2, 3, 4, 5, 6 after rolling our virtual die 10000 times\n\n\n\n\nWe will learn in our course why the picture get better as we increase the number of rolls of the die. For the moment, let us just state that at least for this case it seems that there is no significant visual evidence that our virtual die is loaded.\n\n\n1.2.9 Getting Help\nWe have now learned a tiny number of R functions and we have written one function ourselves. We have learned how to make use of functions provided by packages.\nIt would be overwhelming to memorize and learn them all. In R, fortunately, every function comes with a detailed documentation and with its own help page. You need to learn how to use this source right from the beginning.\nTo access the help page of a function you type the function name preceded by a question mark at the prompt, like this ::: {.cell}\n?sample\n:::\nThen, after you have pressed the return key, a help page is opened in the right lower pane under the help tab as shown in @ref(fig:helpinr) ::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\nThis help page has a particular structure that you will find for every other R function no matter whether it is provided by the base installation or by a package. In the upper left corner you see the name of the function (sample) and in curly brackets next to it the term base, which means that this is a function in the R base installation. Then you see a headline about what the function does.\nFrom the top of the page, you then first see the field Description. This is a short description what the function does. Here it says\nDescription\nsample takes a sample of the specified size from the elements of x using either with or without replacement.\nThe next field is Usage. It gives you the function description with the arguments. Here for example\nUsage\nsample(x, size, replace = FALSE, prob = NULL)\n\nsample.int(n, size = n, replace = FALSE, prob = NULL,\n           useHash = (!replace && is.null(prob) && size &lt;= n/2 && n &gt; 1e7))\n           \nThe first line in Usage should by now be familiar. Don’t worry about the second line. The function can obviously do more than we know so far.\nNext comes a list of arguments the function takes and what type of information R expects you to provide, as well as what R will do with this information. Here it says for example\nArguments\nx   \neither a vector of one or more elements from which to choose, or a positive integer. See ‘Details.’\n\nn   \na positive number, the number of items to choose from. See ‘Details.’\n\nsize    \na non-negative integer giving the number of items to choose.\n\nreplace \nshould sampling be with replacement?\n\nprob    \na vector of probability weights for obtaining the elements of the vector being sampled.\n\n...\n\nWe omit some information here.\nThen comes a field called Details which gives a more in-depth description of the function. The next field is called Value. It describes what the function returns when you run it. Then we have a reference to related R functions under the field See. Finally there is a field called Examples. This field contains example code that is guaranteed to work. It shows a couple of different cases how you can use the function in practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#coincidences",
    "href": "01-lecture1.html#coincidences",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.3 Application: Coincidences and the Blockchain",
    "text": "1.3 Application: Coincidences and the Blockchain\n\n1.3.1 The birthday problem\nIn this section, we will apply our foundational knowledge of probability - which is at the moment just the informal and intuitive notion of counting equally probably cases - to a fascinating real-world problem known as the birthday problem:\nThe starting question in the birthday problem is: What is the probability that at least two people in a room share the same birthday, when we neglect things like leap years, and when we assume that birthdays on any day of the year are equiprobable and the birthdays of the people in the room are independent. We have no twins for example.\nPerhaps some of you have already seen this problem before. If not, the result may surprise you. Even if you saw the birthday problem before, perhaps not many of you are aware that the significance of this problem reaches far beyond recreational math puzzles. This is because the birthday story is of course accidential. The mathematical formulation of the problem will reveal through the process of abstraction that this problem is at the heart a problem about the probability of coincidences. As such it has real world connections to cryptography and the blockchain security.4\n4 We follow here the discussion in Diaconis and Skyrms (2019)Through this problem, we will:\n\nExplore how to frame probability questions mathematically.\nUse basic R functions to compute probabilities.\nLearn how to simulate experiments to approximate probabilities. This problem is not only an entertaining mathematical puzzle but also a stepping stone to understanding its applications in cryptography and blockchain security.\n\nFirst, observe that we took our assumptions such that we are in the frame of classical probability like the originators of the field thought about it.\nNow for the sake of familiarizing ourselves with the new concepts, let us try to map the birthday problem into the probability notions we learned so far.\nThe sample space is the set of possible outcomes of the experiment. The sample space for our problem includes all possible combinations of birthdays for the group. For example, if there are 3 people, each with a birthday on any of 365 days, the sample space contains all possible sequences of three birthdays:\n\\((1,1,1),(1,1,2),…,(365,365,365)\\)\nIn general, for \\(n\\) people, there are \\(365^n\\) possible combinations, as each person’s birthday is independent of the others.5\n5 Mathematically you can describe the sample space for the birthday problem as a set with \\(365^n\\) ordered \\(n\\)-tuples or sequences of birthdays: \\({\\cal S} = \\{x | x \\in 365^n \\}\\) This is the sample space of this experiment, written as in set theoretic notation. It means the sample space is the set of all ordered \\(n-tuples\\) from the Cartesian product \\(365^n\\). Remember that in mathematics, a Cartesian product is a mathematical operation that returns a set (or product set or simply product) from multiple sets (in this case the sample space, \\({\\cal S}\\). That is, for sets \\(A\\) and \\(B\\), the Cartesian product \\(A \\times B\\) is the set of all ordered pairs \\((a,b)\\) where \\(a \\in A\\) and \\(b \\in B\\).Now given this sample space we can assign a probability to the event that two people chosen at random will have the same birthday. The denominator will this be \\(365^n\\). The nominator for this probability will be much more straightforward to figure out, if we compute the complement. We discussed before that the probability of the complement of an event is 1 minus the probability of the event: \\(P(\\text{not} A) = 1 - P(A)\\).\nLet \\(A\\) denote the event that no two people share the same brithday. We can then use the complement rule to compute the probability of at least two people sharing a birthday: Compute the probability that no two people share a birthday. Subtract this probability from 1 to get the probability of at least one match.\nHere’s how we compute the probability of no matches:\n\nThe first person can have any birthday \\((365/365)\\).\nThe second person must avoid the first person’s birthday \\((364/365)\\).\nThe third person must avoid the first two birthdays \\((363/365)\\).\n\nNow continue this reasoning for \\(n\\) people.\nThe final probability is:\n\\[\\begin{equation*}\nP(\\text{at least two people share birthday}) =\n1 - \\frac{365 \\times 364 \\times 363 \\cdots \\times 365 - n +1}{365^n}\n\\end{equation*}\\]\nNow there is an additional thing we did not yet introduce and which we will learn about in the next lecture. This refers to our assumption that the individual birthdays are independent. For the moment you need to take the following fact on faith: The probability of independent events is the product of the probability these events.\nNow comes the surprise for those of you who did not already see the birthday problem. Assume the group size of people is 23. Let us compute the birthday coincidence probability. For this computation we make use of the R function prod() which takes a vector of numbers as input and computes their product. For \\(n=23\\) we need the list of numbers from 365 to 343. Now we can use the : operator and take advantage from one of its cool properties. If we formulate the command as 365:343 then the colon operator will give us a descending list of integers starting at 365 and ending at 343. Then the probability can be computed as\n\n1 - prod(365:343)/365^23\n\n[1] 0.5072972\n\n\nWe have used the operator ^ which is needed to compute powers. Now we see from our computation that with 23 people the probability is already larger than 50 %. Quite stunning. You can now verify yourself that with 50 people this probability is already at 97 %.\nLet’s visualize how the probability of at least one shared birthday grows with the size of the group. This time I will use base R’s plotting functionality and not use the ggplot2 package.6\n6 I used ggplot2 before to explain the concept of a package and how to load it to enhance R’s functionality. Base R containts a lot of powerful plotting functions which work independently of gglot2.The plot below shows that the probability exceeds 50% with just 23 people and quickly approaches 100% as the group size increases.\n\n# Define the birthday_collisions function\n\nbirthday_collisions &lt;- function(n) {\n  1 - prod(365:(365 - n + 1)) / 365^n\n}\n\n# Generate data for group sizes\n\ngroup_sizes &lt;- 1:50\ncollision_probs &lt;- sapply(group_sizes, birthday_collisions)\n\n# Plot the probability of at least one shared birthday\n\nplot(group_sizes, collision_probs, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Group Size\", ylab = \"Collision Probability\",\n     main = \"Probability of At Least One Shared Birthday\")\nabline(h = 0.5, col = \"red\", lty = 2)  # Highlight the 50% threshold\n\n\n\n\n\n\n\n\nI will not go in detail into the explanation of the code at this stage. For those of you who crave for an explanation, please look at the marginal note here or use the LLM for a step by step explanation.7\n7 birthday_collisions Function: This function calculates the probability of at least one shared birthday for a group of nn people using the formula derived earlier. group_sizes and collision_probs: group_sizes defines a range of group sizes (1 to 50). sapply applies the birthday_collisions function to each group size, computing the collision probabilities. Visualization: A line plot (type = \"l\") is used to visualize how the collision probability increases with group size. A horizontal red dashed line (abline) highlights the 50% threshold for better visual context.\n\n1.3.2 Determining birthday collisions by simulation\nInstead of deriving the probability analytically, we can simulate the birthday experiment if we use the relative frequency notion of probability which we informally had used in our visualization of coin tosses. Here’s how:\n\nGenerate random birthdays for nn people using sample(1:365, n, replace = TRUE).\nCount how often at least two people share a birthday using the duplicated() functionin combination with the logical function any().\nRepeat the simulation many times (e.g., 10,000) using replicate() to estimate the probability.\n\n\nsim_birthday &lt;- function(n, trials = 10000) {\n  results &lt;- replicate(trials, {\n    birthdays &lt;- sample(1:365, n, replace = TRUE)\n    any(duplicated(birthdays))\n  })\n  mean(results)\n}\n\nLet me give a step-by-step breakdown of this piece of code.\n\nDefining the Function:\n\nThe function sim_birthday has two arguments\n\nn: The number of people in the room (group size).\ntrials: The number of simulation repetitions (default is 10,000). These parameters allow us to customize the simulation for different group sizes and levels of precision.\n\n\nUsing replicate to Run Simulations:\nThe replicate() function repeats an experiment a specified number of times (trials). Each experiment simulates one instance of \\(n\\) people in a room.\nSimulating Birthdays:\nThe sample() function generates \\(n\\) random birthdays (integers between 1 and 365) with replacement, representing the assumption that birthdays are independent and uniformly distributed. This step directly reflects the assumptions made in the analytical solution:\n\nThe sample space consists of 365 possible birthdays.\nSampling is done with replacement because multiple people can share the same birthday.\n\nChecking for Duplicates:\nThe duplicated() function identifies repeated birthdays in the sample. The any() function checks if there is at least one duplicate. If duplicates exist, this corresponds to a “birthday collision.” This step operationalizes the concept of finding the complement (at least one collision) rather than calculating the probability of all unique birthdays.\nCalculating the Probability:\nThe results vector contains TRUE (collision occurred) or FALSE (no collision) for each trial. Taking the mean() of this logical vector calculates the proportion of trials with a collision, which corresponds to the simulated probability. This step ties back to the definition of relative frequency probability introduced earlier: the probability of an event is the ratio of favorable outcomes to the total number of trials.\n\nThis step requires some explanation of how R uses data types. We will learn about this more systematically in the next lecture. Here I give a quick and superficial explanation. R can force logical data types automatically to a numerical type when it applies arithmetic functions to a logical vector, forcing TRUE to 1 and FALSE to 0. Now, when you take the average (mean()) of a vector containing only 1 and 0 values you will get the proportion of 1’s or TRUE values. We will use such tricks often during the course of the lecture. For the moment I ask you to bear with this quick and superficial explanation of why this way of proceeding does what we want.\nLet’s compute the birthday collision probability for 23 (as in our analytical solution) and 50 people:\n\nsim_birthday(23)\n\n[1] 0.5111\n\nsim_birthday(50)\n\n[1] 0.9718\n\n\n\nFor \\(n = 23\\), the output should be close to the analytically derived probability \\(\\approx 0.507\\).\nFor \\(n = 50\\), the output should approximate \\(\\approx 0.970\\).\n\nWhat are the advantages of simulations. Here are three main advantages:\n\nFlexibility: This method works even when the assumptions (e.g., uniformity or independence) are adjusted.\nVerification: We can confirm analytical results with experimental data.\nReal-World Application: Simulation methods are widely used in practice when exact formulas are unavailable or complex.\n\nBy understanding and applying this simulation, you now have a practical tool to explore probability problems beyond purely mathematical derivations.\nThe birthday problem has practical implications beyond recreational math. In cryptography, the same underlying principles are used to analyze the likelihood of hash collisions in blockchain systems and digital signatures. Let’s explore this connection further.\n\n\n1.3.3 Hash-functions and the blockchain\nThe birthday problem provides the foundation for studying coincidences in various contexts. A critical application is in cryptography, where the concept of hash functions relies on similar probabilistic principles to ensure data security.\nA hash function is a mathematical tool that converts an input (e.g., a text, file, or number) of any size into a fixed-length output, often called a “hash value” or “digest.” For example, the widely used hash function SHA-256 maps its input to a 256-bit string. You can think of this hash value as a digital fingerprint: any change to the input, no matter how small, results in a completely different hash.\nHash functions have two critical properties that make them essential in cryptography:\n\nOne-wayness: It is computationally unfeasible to determine the original input from the hash value. A hash function is easy to evaluate or compute but it is practically impossible to learn from the value the initial argument by computing the inverse.\nCollision resistance: It is highly unlikely for two different inputs to produce the same hash value.\n\nIf the range of the hash-function is \\(M\\) and the hash-function maps into a 256 bit string then there are \\(2^{256}\\) basic outcomes. Since the hash-function maps a large string onto a smaller string it is possible that there are two different strings \\(x \\neq y\\) mapping to the same value \\(\\text{hash}(x)=\\text{hash}(y)\\). This would be a problem for message authentication because it would give the same “fingerprint” for two different strings.\nFor a cryptographically secure hash function it is therefore required that the probability of such a collision should be small enough to exclude a collision in all practically relevant circumstances.\nThe collision problem for hash functions is analogous to the birthday problem. Instead of asking how many people are needed in a room for a shared birthday, we ask: how many inputs (e.g., messages or files) need to be hashed before two of them produce the same hash value? This probability depends on the size of the hash space, which is determined by the number of bits8 in the hash function’s output.\n8 A bit, short for binary digit, is defined as the most basic unit of data in telecommunications and computing. Each bit is represented by either a 1 or a 0Let \\(M = 2^{256}\\) the number of possible hash values. The probability of no collisions after \\(n\\) hashes is\n\\[\\begin{equation*}\nP_{nc}(n) = 1 \\times \\left(1 - \\frac{1}{M}\\right)\\times\n\\left(1 - \\frac{2}{M} \\right) \\times \\cdots \\times \\left(1 - \\frac{(n-1)}{M} \\right)\n\\end{equation*}\\]\nFor \\(n\\) large this probability can be approximated by a factorial expansion as \\[\\begin{equation*}\nP_{nc}(n) \\approx \\prod_{k=0}^{n-1} \\left( 1 - \\frac{k}{M} \\right)\n\\end{equation*}\\]\nNow convert the product into a sum using the natural logarithm: \\[\\begin{equation}\n\\ln P_{nc}(n) \\approx \\sum_{k=0}^{n -1} \\ln \\left( 1 - \\frac{k}{M} \\right)\n\\end{equation}\\]\nNow to do the estimation of the threshold we use an approximation, which works with a simple property of the logarithm: \\(\\log(1-x)\\approx -x\\) when \\(x\\) is small. Thus for \\(n\\) much smaller than \\(M\\), or \\(n \\ll M\\), we can write the no-collision probability as \\[\\begin{equation*}\n\\ln P_{nc}(n) \\approx - \\sum_{k=0}^{n-1} \\frac{k}{M}\n\\end{equation*}\\] Simplify the summation by usin the fact that \\(\\sum_{k=0}^{n-1} = \\frac{n(n-1)}{2}\\) we get \\[\\begin{equation*}\n\\ln P_{nc}(n) \\approx -\\frac{n(n-1)}{2\\, M}\n\\end{equation*}\\] By taking exponentials on both sides this gives us \\[\\begin{equation*}\nP_{nc}(n) \\approx \\exp \\left( - \\frac{n(n-1)}{2 \\, M} \\right)\n\\end{equation*}\\]\nThus the probability of a hash-collision as a function of \\(n\\) is then approximately\n\\[\\begin{equation*}\nP_{c}(n) = 1- P_{nc}(n) \\approx \\frac{n^2}{M}\n\\end{equation*}\\]\nLet’s try a visualization to show how \\(P_{c}(n)\\) increases with \\(n\\):\n\nn &lt;- 1:100\nM &lt;- 2^256\n\ncollision_prob &lt;- (n^2) / (2 * M)\n\nplot(n, collision_prob, type = \"l\", log = \"y\", col = \"blue\",\n     xlab = \"Number of Inputs (n)\", ylab = \"Collision Probability (log scale)\",\n     main = \"Hash Collision Probability (SHA-256)\")\nabline(h = 0.5, col = \"red\", lty = 2)  # 50% threshold\n\n\n\n\n\n\n\n\nThese results show that for SHA-256:\n\nTo have a \\(50\\)% chance of a collision, an attacker would need to compute approximately \\(2^{128}\\) hashes.\nAt \\(10000\\) hashes per second, this would take longer than the age of the universe.\n\nThis enormous computational difficulty ensures the security of modern cryptographic systems. Even with the combined computing power of all devices on Earth, finding a collision is practically impossible.\nThe birthday problem demonstrates how fundamental probability concepts can inform critical applications like cryptographic hash functions. By understanding the collision probabilities, we see why hash functions like SHA-256 are effective for ensuring data integrity and security.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#summary",
    "href": "01-lecture1.html#summary",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.4 Summary",
    "text": "1.4 Summary\nIn this lecture we have taken a first step towards some very basic probability notions and some basic steps in R. Isn’t it amazing how much territory we could cover with so few concepts? You have learned about how to think probabilistically about collision probabilities and how to solve for them analytically, by simulation and for large numbers by approximation.\nWe have convinced ourselves using this knowledge only and taking on faith that the probability of independent events is the product of their individual probabilities, that the cryptographic Hash-function SHA-256, while it can produce collisions in theory, practically the number of hashes to make such a collision occur would be so large that we can be confident that hashing bit strings with SHA-256 gives us a unique fingerprint practically with certainty.\nThese are the basic probability concepts we have covered in this lecture:\n\nA random experiment: A process leading to an uncertain outcome.\nSample space: The collection of all possible outcomes of a random experiment.\nBasic outcome: A possible outcome of a random experiment.\nEvent: An event is a subset of basic outcomes. Any event which contains a single outcome is called a simple event.\nClassical probability find or make equally probably cases and the count them. The probability of an event \\(A\\) is the number of cases when \\(A\\) occurs divided by the total number of cases.\nRelative frequency probability defined as the number of times an event \\(A\\) occurs in a repeated number of trials divided by the total number of trials in a random experiment.\n\nThese are the R concepts we have covered in this lecture:\n\nobjects arbitrary names that can store different values and data types.\nfunctions ab R object that can accept other R objects as arguments, operate on them and return a new object.\nscripts files that store sequences of R commands and can be saved, repopened and allow the execution of commands.\nusing packages\nfinding help\nthe functions sample and replicate\n\nThese are the Applications we have covered in the lecture:\n\nConstructing a coin on the computer and toss it an arbitrary number of times\nThe birthday problem and how to solve it analytically as well as through simulation\nExtrapolating the birthday problem to analyze cryptographic collision resistance of hash-functions by using ideas from probability",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#project-1-craps-a-casion-dice-game.",
    "href": "01-lecture1.html#project-1-craps-a-casion-dice-game.",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.5 Project 1: Craps: A casion dice game.",
    "text": "1.5 Project 1: Craps: A casion dice game.\nThe first project is about one of the very few dice games offered at a Casino: Craps.8\n8  This will be the only project in this course where we do not go into a direct financial application. Before we can do so, we need to build still a bit more knowledge of probability and skills in R.In Craps people throw a pair of dice and bet on outcomes of these throws. In this project we practice our new knowledge by implementing the dice throws of craps and the automatic summation of points on the computer. We try to find out the probability of the possible outcomes and check the fairness of the dice.\n\nYour first task in this project will be to write a function which allows you to virtually throw a pair of dice and sum up the points shown after the throw.\nSimulate and plot the result of your simulation using qplot() with the binwidth argument set to 1.\nAre these dice fair? Why?\nAssume the dice were unfair in the following sense: Numbers 1,2,3,4 and 5 have a probability of \\(1/8\\) while the 6 has a probability of \\(3/8\\). Study the help page of the samplefunction and find out how you could give these new probability weights to the function. If you redo your simulation analysis with the loaded dice, how does the picture change?\nWrite a script for this random experiment.\nNow look at the random experiment of throwing two dice with the concepts from probability theory: What is the sample space of this experiment. What are the probabilities of the basic outcomes? What is the probability of getting sum 7, what is the probability of getting sum 2 for the case of the fair dice, for the case of the unfair dice?\nImagine now that you are at the casino which uses fair dice and you can spend 150 Euro for chips. Since you have figured out that 7 is the most likely outcome you would like to buy bets on 7. The casino offers you a bet for 15 cent. When you win you get 1 Euro for the bet, when you loose you get nothing. Every bet costs 15 cents, so you can make 1000 bets in total on 7 from your budget of 150. Is this a good or a bad deal for you? Try to think about this problem in terms of a simulation.\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html",
    "href": "01-lecture1_updated.html",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1 Rolling a die: First Probability ideas.\nLets start with a classic and old example of a probability model, which occupied the attention of Pascal and Fermat as well as their gambling friend the Chevalier de Méré: Rolling a six sided die.\nWhen you role a die or several dice like shown in the following picture @ref(fig:dice), it is uncertain on which face it will finally end up after the throw.\nRolling dice: An old game of chance\nWhat seems clear though is, that it will turn out that it is either one of the six possible faces of the die.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html#rolling-a-die",
    "href": "01-lecture1_updated.html#rolling-a-die",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1.1 Random experiments, sample space, events\nIn the theory of probability a process leading to an uncertain outcome is called a random experiment. The example of throwing a die helps us to give a precise meaning to the notion of an uncertain outcome. While we can imagine in principle that the die lands on one of its corners and this is one outcome, we agree on the outset that we are going to consider the (practically relevant cases) that it will land on one of the faces.\nThe collection of all possible outcomes of a practical or conceptual random experiment is called in probability theory a sample space. While the sample space of throwing a die is an idealization it is exactly this idealization which simplifies the theory without affecting its applicability. The basic outcomes in the random experiment of throwing a die are that the die lands such that the upward showing face shows a 1 a 2, 3, 4, 5 or a 6. In the theory the collection of basic outcomes is denoted as a set. Thus the sample space of throwing a die is given as the set \\({\\cal S} = \\{1,2,3,4,5,6\\}\\).\nThe sample space \\({\\cal S}\\) is the set of all basic outcomes. The subsets of the sample space are often called events in probability theory. An event could be - for example - an outcome where the die shows an upward looking face with an even number \\(A = \\{2,4,6\\}\\).\n\n\n1.1.2 The role and nature of idealizations in applications\nIdealizations of the kind we discussed for the throw of a die are standard in probability and we will encounter them again and again. For example the movement of a stock price is often though of as a conceptual random experiment. When we try to agree on what is the appropriate sample space of this experiment, we can say that the price can not fall below 0 but it is hard to agree on what will be the highest possible price. In the probabilistic treatment of stock prices, which we will discuss later in our course, it is for instance common to assume that the sample space of this random experiment is the entire interval of non-negative real numbers \\({\\cal S} = [0, \\infty)\\).\nMany of us would hesitate to claim that the price might rise without bound. Yet many models in applied Finance are based on such an assumption. The models allow arbitrary price hikes but with arbitrary small probability as the price gets higher and higher. Practically it does not make sense to believe that a security price can become arbitrarily high. The use of arbitrarily small probabilities in a financial model might seem absurd but it does no practical harm and makes the model simple and convenient to use. Moreover, if we seriously introduced an upper bound on a security price at \\(x\\) it would be also awkward to assume that it is impossible that it could be just a cent higher, an assumption equally unappealing than assuming it can get in principle arbitrarily high.\n\n\n1.1.3 Classical Probability: Measuring uncertainty\nProbability is a measure of how likely an event of an experiment is. But how could we measure chance? Here is the first big idea of probability theory and how the originators thought about it.1 How do you measure anything? If you think of length, for example, you take an arbitrary standard you agree on and then count. The originators of probability theory pursued the same idea with chance: To measure probability you choose equally probable cases and then count.\n1 In this discussion I use chapter 1 in Diaconis and Skyrms (2019)The probability of an event \\(A\\) according to the originators of the theory of probability is then \\[\\begin{equation}\nP(A) = \\frac{\\text{Number of cases where $A$ occurs}}{\\text{Total number of cases}}\n\\end{equation}\\]\nThus, if we want to know the probability of the die ending up on a face such that an even number is shown (2,4,6) we have to compute according to this notion \\(3/6\\) (since the total number of cases are six basic outcomes) which is a chance of \\(50 \\%\\).\nNote that this classical notion of probability has a few interesting consequences, which we will discuss in more detail later but which already now naturally flow from this basic idea of measuring chance.\n\nProbability is never negative.\nIf an event \\(A\\) occurs in all cases \\(P(A)=1\\).\nIf \\(A\\) and \\(B\\) never occur in the same case, then \\(P(A \\,\\text{or}\\, B) = P(A) + P(B)\\).\n\nIn particular then the probability that an event does not occur is 1 less the probability that it occurs: \\(P(\\text{not}\\, A) = 1 - P(A)\\).\nLet us interrupt here our discussion of probability for a moment and ask how we can make good on our promise to make these ideas tangible, so we can play with them.\nFor this we will need the computer. Since we will talk to the computer in R, it is now the right time to look at the die rolling example from the perspective of R and dive into some of its basic concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html#rolling-die-on-computer",
    "href": "01-lecture1_updated.html#rolling-die-on-computer",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.2 Rolling a die on the computer: First steps in R",
    "text": "1.2 Rolling a die on the computer: First steps in R\n\n1.2.1 The R User Interface\nBefore we can ask our tool to do anything for us, we need to know how to talk to it. In our case RStudio allows us to talk to our computer. It works like any other application. When you launch RStudio on your computer, in figure @ref(fig:rstudio-start-screen) you see a screen looking like this:\n\n\n\n\n\nThe RStudio startup screen\n\n\n\n\nIn this picture you see a screenshot of my RStudio screen. Interacting with the app is easy. You type commands via your keyboard at the prompt, which is the &gt; symbol. You find this symbol in the RStudio pane called Console. You can see it in the left pane in the screenshot. You send the command to the computer by pressing enter. After you have pressed enter, RStudio sends the command to R and displays the result of your command with a new prompt to enter new commands, like this:\n\n1+1\n\n[1] 2\n\n\nLet me pause to explain what you see here in this code example. First you see a light-gray box containing the command 1+1. This is an instance of a so called code chunk. Moving the cursor to the right upper corner of the chunk, will display a copy icon and you can click this icon to copy the code to the computer’s clipboard and then paste it at the prompt of your console (or for that matter anywhere else) and run it in R studio, if you wish. In the code-chunk the R-prompt &gt; is not displayed. This symbol of the prompt is only shown in the Console itself.\nWhen the code is executed, you see the result in the second light-gray box, just below the first one. It starts with a double hash ##, indicating that it shows an output of running the above code chunk and then displays the output as it would appear in the command window [1] 2.\nThe [1] means that the line begins with the first value of your result. For example, if you enter the command 20:60 at the prompt of your console which means in the R language, “list all the integers from 20 to 60” and press enter you get:\n\n20:60\n\n [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[26] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nmeaning that 20 is the first value displayed in your result. Then there is a line break because not all values can be displayed on the same line and R tells you that 45 is the 26-th value of the result.\nThe colon operator : is a very useful function in R which we will need often. It allows us to create sequences of every integer between two given integers.\nR needs a complete command to be able to execute it, when the return key is pressed. Lets see what happens, if a command is incomplete, like for instance &gt; 5*.\nIn this case R will show the expression followed by a + instead of showing a new prompt. This means that the expression is incomplete. It expects more input. If we complete the expression, say like\n&gt; 5*\n+ 4\nthe expression can be evaluated and a new prompt is shown in the console.\nIf you type a command that R does not understand, you will be returned an error message. Don’t worry if you see an error message. It just is a way the computer tells you that he does not understand what you want him to do.\nFor instance, if you type 5%3 you will get an error message like this\n&gt; 5%3\nError: unexpected input in \"5%3\"\n&gt;\nSometimes it is obvious why a mistake occurred. In this case, that R just does not know what to do with the symbol %. It has no meaning in this context. Sometimes it is not so obvious what the error message actually means and what you might do about it.\nA useful strategy in this case is to type the error message into a search engine and see what you can find. The chance is very high that others encountered the same problem before you and got helpful advice how to fix it from other users on the internet. One site, we find particularly helpful for all kinds of questions related to R and R programming is https://stackoverflow.com/. Try it at the next opportunity.\nNow with this basic knowledge, we can already make the first step to create a die on the computer using R. If you think of a physical die, the essential thing that matters are the points on its six sides. If you throw the die it will usually land on one of these sides and the upward showing side of the die shows the number of points. The colon operator : gives us a way to create a group of numbers from 1 to 6. R gives us the result as a one dimensional set of numbers.\n\n1:6\n\n[1] 1 2 3 4 5 6\n\n\nLets use these first steps in R to recap the probability concepts we have learned using this example of the six sided die: A basic outcome of rolling a six-sided die is for example 6 if the upper side after rolling the die happens to be the side with 6 points. The sample space of the experiment of rolling a six-sided die is the set \\({\\cal S} = \\{1,2,3,4,5,6\\}\\). In probability theory we often use the symbol \\({\\cal S}\\) or \\(S\\) for sample space. In many probability texts the sample space is also often denoted by the symbol \\(\\Omega\\) the Greek letter for (big) Omega. A random experiment in this example is the rolling of the die. The outcome is uncertain but once the die is rolled the outcome can be determined precisely. The event that the outcome is a display of 10 points is the empty set \\(A = \\emptyset\\). The symbol \\(\\emptyset\\) comes from set theory and means the set containing no elements. This event can contain no elements because we can not get a score of 10 by rolling a six sided die.\n\n\n1.2.2 Objects\nYou can save data in R by storing them in objects. An object is a name, you can choose yourself to store data. For example, if you choose to store the value 6 in an object called point_six, you would type:\n\npoint_six &lt;- 6\n\nat the prompt. R will the store the value 6 in the object called point_six, which you can use to refer to the value. If you type the name of your object at the prompt, R will display the value you have assigned. A useful key combination for typing the assignment operator &lt;- is to use the key combination ALT _. At the R prompt R will automatically print an assignment operator.\nNow if you type the name of the object and press enter, R will display the value or values stored in it, like this:\n\npoint_six\n\n[1] 6\n\n\nNow you can use the name of the object to refer to its value. For instance, you could divide point_sixby 2and get a meaningful result\n\npoint_six/2\n\n[1] 3\n\n\nNow to make our die more tangible and useful, let us store it in an R object by typing the following command at the prompt. This command creates an object with name die and assigns the vector 1,2,3,4,5, 6 to it.\n\ndie &lt;- 1:6\n\n\n\n\n\n\nThe RStudio Environment pane keeps track of the objects you have created\n\n\n\n\nYou can now see in the right upper Environment pane in figure @ref(fig:rstudio-screen-with-command) that R shows you that there is an object with the name die that it consists of integers 1,2,3,4,5. As you create more objects they will be stored in the Environment pane and are ready for your reference, unless you delete them. You can remove or delete an object by typing rm(object) or by assigning the value die &lt;- NULL which would also remove the object from your environment or workspace.\nYou can name your objects almost anything with a few exceptions. An object name must not start with a number. There are some special symbols which can also not be used in object names, like ^, !, $, @, +, -, /, *. Note that R is case sensitive and distinguishes small and big letters. If you assign a new value for an object you have already created, R will overwrite the object without warning.\nYou can see which objects are currently created and available for you in the Environment pane of your session of by typing ls(). The UNIX users among you will recognize this command from the unix shell, where it displays the files in a directory.\nBefore we learn how we can actually roll our die and perform a random experiment with it, let us briefly use the opportunity to explain a few things about how R does computations. We have already explained that we can use the object name to refer to the value. So for instance if we type\n\ndie*die\n\n[1]  1  4  9 16 25 36\n\n\nThis might irritate some of you because we have called the object a vector. In linear algebra multiplication of vectors is only allowed if there is an inner product. What happens here, if we use * the multiplication operator is that R does an element-wise multiplication of the six numbers of our die. Of course R allows to take an inner product as well, but this needs a different operator. To compute an inner product, we would type\n\ndie %*% die\n\n     [,1]\n[1,]   91\n\n\nNow R displays the result as a vectors with one row and one column, which is denoted in the output by [ , 1] for the column and [1, ] for the row. We will learn later more about the use and the meaning of this notation in R.\nThe element wise execution R usually uses also means that when you, for example type\n\ndie - 1\n\n[1] 0 1 2 3 4 5\n\n\nR would subtract 1 from every component in the vector die.\nAnother specific behavior of R, you need to know about is called recycling. If you give R two vectors of different length in an operation, R will repeat the shorter vector as long as it is of equal length with the longer one. For example, if you have: ::: {.cell}\ndie + 1:2\n\n[1] 2 4 4 6 6 8\n\n:::\nyou see that R adds 1 to 1 and 2 to 2 and then starts over again by adding 1 to 3 and 2 to 4 and then starts over once again by adding 1 to 5 and 2 to 6. If the longer vectors is not a multiple of the shorter one, R recycles but the cuts off. It will give you a warning though. ::: {.cell}\ndie + 1:4\n\nWarning in die + 1:4: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 2 4 6 8 6 8\n\n:::\nWhile this might seem awkward to some of you, we will see that for data manipulation element-wise execution is often extremely useful. It allows to manipulate groups of values in a systematic yet easy way.\n\n\n1.2.3 Functions\nR contains many functions which we can use to manipulate data and compute things. The syntax for using a function is very simple: You type the function name and put the value of the function argument in parentheses. Here we use for illustrations the function of the square root sqrt(): ::: {.cell}\nsqrt(4)\n\n[1] 2\n\n::: or rounding a number: ::: {.cell}\nround(3.1415)\n\n[1] 3\n\n::: The data you write in the parentheses are called the function arguments. Arguments can be all sorts of things: raw data, R objects, results from other functions.\nIf functions are nested, R evaluates the innermost function first and then goes on to the outer functions. To see examples of all these instances you can take ::: {.cell}\nmean(1:6)\n\n[1] 3.5\n\nmean(die)\n\n[1] 3.5\n\nround(mean(die))\n\n[1] 4\n\n::: for example.\nFor simulating random experiments, R has the very useful function sample(). With this function we can roll our die on the computer and conduct actual random experiments.\nThe function takes as arguments a vector names x and a number named size. sample will return size elements randomly chosen from the vector x. Lets say: ::: {.cell}\nsample(x = 1:4, size = 2)\n\n[1] 3 1\n\n:::\nIn this case sample has chosen 4,1 from the vector x = (1,2,3,4) at random.\nIf we want to roll the die in our computer we can thus pass the die as an argument to sample and use the number 1 for the size argument. Lets do a few rolls with our die ::: {.cell}\nsample(x = die, size = 1)\n\n[1] 3\n\nsample(x = die, size = 1)\n\n[1] 1\n\nsample(x = die, size = 1)\n\n[1] 2\n\nsample(x = die, size = 1)\n\n[1] 1\n\n::: These are the random outcomes of our consecutive rolls. It is as if we had thrown an actual die but in this case we have done the same thing on the computer. Isn’t it cool that this is possible at all? The sample() function will remain our good friend throughout this course.\nR functions can have many arguments, but they need to be separated by a comma.\nEvery argument in every function has a name. We specify which data are assigned to the arguments by setting a mathematical equal sign = between the name and the data. Names help us to avoid passing the wrong data and thereby mixing up things or committing errors. But using names is not necessary. If we just wrote ::: {.cell}\nsample(die,1)\n\n[1] 4\n\n::: R would also know what to do. It is not always clear which names to use for a function. If you are not sure, you can use the function args() to look it up. Here we take the function roundas one example. ::: {.cell}\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n::: Note that the digits argument in the round function is already set to 0. Frequently R functions come with optional arguments. These arguments are optional because the come with a default value, which is 0 in case of the round function.\nWe recommend that you write out argument names as a rule. It gives clearer code and avoids errors. If you don’t write argument names, R matches your values to the arguments of the function by order.\n\n\n1.2.4 Writing your own functions\nNow we are ready to write our own function to roll the die in our computer. Each function in R has the same elements: A name, a function body of code and a set of arguments. To write your own function, you have to write up all of these parts and save them in an R object. The syntax is:\nmy_function &lt;- function() {}\n\nThe name here is my_function, next comes the expression function() which needs to be assigned. The names of the function arguments have to be written between the parentheses. Then we have to write the actual code within the braces {}.\nTo do this for the die, lets write a function named roll_die. ::: {.cell}\nroll_die &lt;- function(){die &lt;- 1:6 \n                         sample(die, size = 1)}\n:::\nNow we can roll our die for a few times to show how the function works ::: {.cell}\nroll_die()\n\n[1] 5\n\nroll_die()\n\n[1] 4\n\nroll_die()\n\n[1] 2\n\nroll_die()\n\n[1] 6\n\nroll_die()\n\n[1] 1\n\n::: Note that in our function roll_die() has no arguments, just the function body. This is perfectly legitimate in R. It is important that when we call the function we have to call it with the parenthesis like roll_die(). If we only call the name roll_die, R will display the code in the function body.\nA final remark in the sample function is in place here. If we look at the arguments of sample using the args function we see ::: {.cell}\nargs(sample)\n\nfunction (x, size, replace = FALSE, prob = NULL) \nNULL\n\n::: Lets do not discuss all the details of this output but concentrate for a moment on the replace argument. What does this mean?\nAs we saw previously we can use the sample function to model the rolling of our die. If we set the size argument to 1 we get the roll of one die. If we set the size argument to n, we would simulate the rolling of n dies. But now the replace argument becomes crucial. As we can see in the output of the args function replace has a default value FALSE. This is a logical argument. It tells R, for example, that if we set size = 2, meaning that two dice are rolled, if the first dice shows, say a value of 3, the second die cannot show 3 as well.\nThis is clearly not what we have in mind when we model the rolling of 2 dice. It should be possible that both dies show the same value. To enable this behavior of the sample function, we have to change the default value of the replace argument to TRUE. Then R chooses a random draw from all of the six possible values for all dice rolled.\nCongratulations ! You have written your first R function for conducting a simple random experiment. Let me remind you once again: Think of the parentheses as a trigger that tells R to run the function. If you omit the trigger R just prints the body of the function. When you run a function, all the code in the function body is executed and R returns the result of the last line of code. If the last line of code does not return a value neither will R.\n\n\n1.2.5 Arguments\nImagine we remove the first line of code in our function body and changed the name die in the sample function to “ball”. ::: {.cell}\nroll_die2 &lt;- function(){sample(ball, size = 1)}\n::: If we call the function now, we will get an error. The function call roll_die2() will result in the error message Error in sample(ball, size = 1) : object 'ball' not found (try it!)\nWe could supply ball when we call roll_die2 if we make ball an argument of the function. Lets do this: ::: {.cell}\nroll_die2 &lt;- function(ball){sample(ball, size = 1)}\n::: Now the function will work as long as we supply ball when we call the function. ::: {.cell}\nroll_die2(ball = 1:6)\n\n[1] 1\n\n::: Note that we still get an error, if we forget to supply ball argument. This could be avoided if we give the function a default argument ::: {.cell}\nroll_die2 &lt;- function(ball= 1:6){sample(ball, size = 1)}\n::: Now if we type: ::: {.cell}\nroll_die2()\n\n[1] 6\n\n::: everything works, just as intended.\n\n\n1.2.6 Scripts\nSo far we have worked by interacting with the console. But what if you want to edit your functions? It would be much easier, if you could use a draft of your code and work form there. This can be done by using a script.\nYou create a script by going to File &gt; New File &gt; R script in the menu bar of RStudio. Using scripts is the standard way to write code in R. It not only helps you to keep track of your code, save it and edit it later. It also makes your work reproducible. You can edit and proofread your code and share it with others. To save your script go to File &gt; Save As in the menu bar.\n\n\n\n\n\nThe RStudio Script\n\n\n\n\nRStudio has many useful features to help you work with scripts. You can for instance automatically execute a line in a code by using the run button. You can also execute sections of code or the entire script. The entire script is executed by running the Source button. For all these commands there are key short cuts which you will learn as you work more with RStudio and R.\nFrom now on you should write all your R-code for this course in scripts. Make a new folder for the scripts you write and store your R-code there from now on.\n\n\n1.2.7 Using packages and finding Help\nWe have now a function which we can use to simulate the rolling of a die, roll_die(). If the die is fair it should be the case that if we roll the die often, all numbers should occur about equally often. The die should not be weighted in favor of a particular value.\nOne way to learn whether our die is fair are repetition and visualization. These are tools we will need all the time, when working with data and when doing probability. While R has many useful functions, one of the great powers is that R is constantly extended by a huge community of users by providing packages.\nPackages are add on functions, which will not be available when you install R. They need to be installed and loaded before you can use them. Since packages are such a powerful tool in R we need to introduce what they are and how to use them in the beginning.\n\n\n1.2.8 Packages\nThere are many visualization tools in R that come with the basic installation. Since the point we want to make here is about packages, we will use a visualization function which is part of the add on package ggplot2, a very popular package for making all kinds of graphs. ggplot2 is not part of the base-R distribution. It has to be therefore loaded before we can use it.\nThere are many additional functions provided with this package. Indeed you could run a whole course on just that 2 For the point we want to make here we will use just one of them, called qplot(), a shorthand for for quick plot.\n2 A very good starting point is for instance Healy (2019), in case you are interested. Also a good source is the first chapter in Wickham and Grolemund (2017).Since qplot() is a function in the package ggplot2, we first need to install this package. To install a package you need to be connected to the internet. If you have internet connection go to the command line and run at the command line: install.packages(\"ggplot2\").\nR displays what is happening while executing the command. Don’t worry if you don not know what all of these messages exactly mean and don’t panic that they are displayed in red. All packages can be installed like this. You have just to enter the correct name in the function install.packages(). The lower right pane in the RStudio software alternatively provides a tab called Packages. Using this tab, you can also install R packages interactively by clicking the install button on the upper left corner of the Packages tab.\nAfter installation the package is on our hard-drive but it can not yet be used. To use the package it has to be loaded. This is done with the command library. To load the ggplot2 package we type ::: {.cell}\nlibrary(\"ggplot2\")\n::: and hit the return key. Many things could be said about the R package system and you will learn it in more detail as we go along in our course. For the moment the most important thing to remember is that a package needs to be newly loaded whenever you want to use it in a new R session.\nTo check whether our dies is fair, we need to roll it many times. R provides a function, that does this for us. This function is called replicate() and provides an easy way to repeat a command many times. The number of times we want to repeat something is given as an argument to the replicate function.\nNow lets roll our die 100 times and save the result in an object we call rolls: ::: {.cell}\nrolls &lt;- replicate(100, roll_die())\n::: We now use the qplot()function from the ggplot2 library to make a quick visualization, by typing the command qplot(rolls). I include the resulting picture in the following figure ::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: ::: If you try this and your picture looks a bit different from mine, don’t worry. This is as things should be. I have stored the picture after one particular run of the replicate function and saved it in my figures folder to these lecture notes. Each time you run the replicate function anew a new random outcome is stored which will look not exactly the same as any other or course. This is what randomness does to the outcome of rolling a die.\nIt looks like every value occurred roughly 16 times but there is still quite some variation. For instance, the value 3 seems to occur more than 20 times whereas the value 2 occurs less than 15 times.\nMaybe we have to give it another trial with more replications. With the computer we can do this with a fingertip. Let us roll our die 10000 times and plot the result.\n&gt; rolls2 &lt;- replicate(10000, roll_die())\n&gt; qplot(rolls2m bindwidth = 1)\nNow the picture looks better.\n\n\n\n\n\nFrequencies of 1, 2, 3, 4, 5, 6 after rolling our virtual die 10000 times\n\n\n\n\nWe will learn in our course why the picture get better as we increase the number of rolls of the die. For the moment, let us just state that at least for this case it seems that there is no significant visual evidence that our virtual die is loaded.\n\n\n1.2.9 Getting Help\nWe have now learned a tiny number of R functions and we have written one function ourselves. We have learned how to make use of functions provided by packages.\nIt would be overwhelming to memorize and learn them all. In R, fortunately, every function comes with a detailed documentation and with its own help page. You need to learn how to use this source right from the beginning.\nTo access the help page of a function you type the function name preceded by a question mark at the prompt, like this ::: {.cell}\n?sample\n:::\nThen, after you have pressed the return key, a help page is opened in the right lower pane under the help tab as shown in @ref(fig:helpinr) ::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\nThis help page has a particular structure that you will find for every other R function no matter whether it is provided by the base installation or by a package. In the upper left corner you see the name of the function (sample) and in curly brackets next to it the term base, which means that this is a function in the R base installation. Then you see a headline about what the function does.\nFrom the top of the page, you then first see the field Description. This is a short description what the function does. Here it says\nDescription\nsample takes a sample of the specified size from the elements of x using either with or without replacement.\nThe next field is Usage. It gives you the function description with the arguments. Here for example\nUsage\nsample(x, size, replace = FALSE, prob = NULL)\n\nsample.int(n, size = n, replace = FALSE, prob = NULL,\n           useHash = (!replace && is.null(prob) && size &lt;= n/2 && n &gt; 1e7))\n           \nThe first line in Usage should by now be familiar. Don’t worry about the second line. The function can obviously do more than we know so far.\nNext comes a list of arguments the function takes and what type of information R expects you to provide, as well as what R will do with this information. Here it says for example\nArguments\nx   \neither a vector of one or more elements from which to choose, or a positive integer. See ‘Details.’\n\nn   \na positive number, the number of items to choose from. See ‘Details.’\n\nsize    \na non-negative integer giving the number of items to choose.\n\nreplace \nshould sampling be with replacement?\n\nprob    \na vector of probability weights for obtaining the elements of the vector being sampled.\n\n...\n\nWe omit some information here.\nThen comes a field called Details which gives a more in-depth description of the function. The next field is called Value. It describes what the function returns when you run it. Then we have a reference to related R functions under the field See. Finally there is a field called Examples. This field contains example code that is guaranteed to work. It shows a couple of different cases how you can use the function in practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html#coincidences",
    "href": "01-lecture1_updated.html#coincidences",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.3 Application: Coincidences and the Blockchain",
    "text": "1.3 Application: Coincidences and the Blockchain\n\n1.3.1 The birthday problem\nWe learned now some basic notions of probability and of R with the example of the rolling of a fair die. It is really surprising what already can be done by just applying the simple ideas we have just learned. The application we want to show you now is known to probability theorists as the birthday problem. It originates in a piece of recreational maths and math-puzzles but it reaches out until cryptography, computer security and blockchain architecture. It also gives us an opportunity to apply some of our newly acquired R-knowledge.\nThe starting question in the birthday puzzle is: What is the probability that at least two people in a room share the same birthday, when we neglect things like leap years, and when we assume that birthdays on any day of the year are equiprobable and the birthdays of the people in the room are independent. We have no twins for example. Perhaps some of you have already seen this problem before. If not, the result may surprise you. Even if you saw the birthday problem before, perhaps not many of you have seen the connection to the cryptography and the blockchain.3\n3 We follow here the discussion in Diaconis and Skyrms (2019)First, observe that we took our assumptions such that we are in the frame of classical probability like the originators of the field thought about it.\nNow for the sake of familiarizing ourselves with the new concepts, let us try to map the birthday problem into the probability notions we learned so far.\nThe sample space is the set of possible outcomes of the experiment. Assume we have \\(n\\) people in the room. Since each person can have a birthday at any of the 365 days in the year (note that we assumed we exclude leap years) and we have \\(n\\) people in the room, the possible basic outcomes are \\(365 \\times 365 \\times 365 \\cdots 365\\) taken \\(n\\) times. This will produce a set with \\(365^{n}\\) ordered \\(n-tuples\\) or sequences of birthdays. This is the sample space of this experiment, written as \\({\\cal S} = \\{x | x \\in 365^n\\}\\) in set theoretic notation. It means the sample space is the set of all ordered \\(n-tuples\\) from the Cartesian product \\(365^n\\).\nRemember that in mathematics, a Cartesian product is a mathematical operation that returns a set (or product set or simply product) from multiple sets (in this case the sample space, \\({\\cal S}\\). That is, for sets \\(A\\) and \\(B\\), the Cartesian product \\(A \\times B\\) is the set of all ordered pairs \\((a, b)\\) where \\(a \\in A\\) and \\(b \\in B\\).\nNow given this sample space we can assign a probability to the event that two people chosen at random will have the same birthday. The denominator will this be \\(365^n\\). The nominator for this probability will be much more straightforward to figure out, if we compute the complement. We discussed before that the probability of the complement of an event is 1 minus the probability of the event. Translated to the birthday problem, this means we look for the 1 minus the probability that that all birthdays are different. The probability that the second person has a different birthday from the first is \\(\\frac{364}{365}\\). If they are different the probability that the third person has a different birthday from them is \\(\\frac{363}{365}\\) and so on for all \\(n\\) in the room. Thus the probability of a shared birthday in the room is: \\[\\begin{equation*}\nP(\\text{at least two people share birthday}) =\n1 - \\frac{365 \\times 364 \\times 363 \\cdots \\times 365 - n +1}{365^n}\n\\end{equation*}\\] Now there is an additional thing we did not yet introduce and which we will learn about in the next lecture. This refers to our assumption that the individual birthdays are independent. For the moment you need to take the following fact on faith: The probability of independent events is the product of the probability these events.\nNow comes the surprise for those of you who did not already see the birthday problem. Assume the group size of people is 23. Let us compute the birthday coincidence probability. For this computation we make use of the R function prod() which takes a vector of numbers as input and computes their product. For \\(n=23\\) we need the list of numbers from 365 to 343. Now we can use the : operator and take advantage from one of its cool properties. If we formulate the command as 365:343 then the colon operator will give us a descending list of integers starting at 365 and ending at 343. Then the probability can be computed as ::: {.cell}\n1 - prod(365:343)/365^23\n\n[1] 0.5072972\n\n::: We have used the operator ^ which is needed to compute powers. Now we see from our computation that with 23 people the probability is already larger than 50 %. Quite stunning. You can now verify yourself that with 50 people this probability is already at 97 %.\n\n\n1.3.2 An R function to compute birthday probabilities\nNow let us use our knowledge about how to write R functions to write a function to compute birthday probabilities. The function argument is the number \\(n\\) of people in class. The coincidence probability is given by the formula we have derived. Now here is how we could go about writing this function: ::: {.cell}\nbirthday_collisions &lt;- function(n){\n  1 - prod(365:(365-n+1))/365^n\n}\n::: Now lets verify what I have claimed before about the group size 50. ::: {.cell}\nbirthday_collisions(50)\n\n[1] 0.9703736\n\n::: Voila, indeed with 50 people in the group the collision probability is already at 97 %.\n\n\n1.3.3 Determining birthday collisions by simulation\nNow this was an argument derived analytically and then we just packed the derived result into a function to compute the exact birthday-collision probabilities. Here is how we could figure out these probabilities by simulation, even with the minimal tools we have learned so far.\nLet us simulate first birthdays for 23 people in a room using the sample function. We stick to the assumption that our year has 365 days (no leap years), and that all birthdays are equally likely, so we can sample from the list of days from 1 to 365 with the sample function which we have already encountered.\nWe need to take care to set the replace argument in the sample function to the value TRUE. This is because we always draw for every person in the room from the same list of values. If one birthday is drawn it is possible in principle that later in the process the same birthday is drawn again. Before, when we worked with only one die we did not need to care whether sample drew with replacement or not. ::: {.cell}\nb_23 &lt;- sample(1:365, 23, replace = TRUE)\n::: This little piece of code is to be read in the same way as before when we built the process of rolling a die on the computer. We sample 23 values from the vector \\(1,2,3,\\cdots,365\\) without replacement. Think about a situation where I use uniform random numbers to call 23 people in your group and write the birthdays I get in this way into the object b_23.\nHere is how I can find the number of matches in b_23. R has a built in function, which is called unique(). So if there were two people with the same birthday in b_23, applying unique() to b_23 would only keep one value. Now clearly if we subtract the length of the resulting object from 23 we get the number of collisions in b_23. Fortunately we can use the R function length() to achieve this task. length() computes the length of any object we give it as an argument. ::: {.cell}\nm_23 &lt;- 23 - length(unique(b_23))\n::: So for instance if sample() drew 23 different birthdays m_23 would be 0.\nLet’s pack this process into a function: ::: {.cell}\nsim_23 &lt;- function(){\n  b_23 &lt;- sample(1:365, 23, replace = TRUE)\n  (23 - length(unique(b_23)))\n}\n::: Now we can repeat this process many times - say 10000 times - using the replicate function. ::: {.cell}\nsimprobs_23 &lt;- replicate(10000, sim_23())\n:::\nNow how do we get from here to probabilities? Here we anticipate and introduce a new probability concept which we will discuss in more detail in the next lecture but which you perhaps already came across a few times.\nThis concept could be called the relative frequency probability . In this concept the probability of an event \\(A\\) is defined as \\[\\begin{equation*}\nP(A) = \\frac{\\text{Number of times $A$ occurs in repeated identical trials}}{\\text{Total number of trials in a random experiment}}\n\\end{equation*}\\] Applying this concept to our data generated by the replicate function and stored in b_23 would require two things. First we would have to count the number of times we saw a 1,2 or more matches, which in our case means 1,2 or more birthday collisions, add up the count and divide by the number of replications. Clearly it is also easier here to work the other way round. We count the number of no collisions and use the complement rule, by computing 1 minus the relative frequency of no collisions. Let us do that.\nNow this is clearly too tedious to do by hand. We will discuss systematically in the next lecture how to refer to individual values in an R object and select and operate on ranges of values etc.\nFor the moment, look at the next step just as a piece of magic and don’t worry if you do not exactly understand the syntax at the moment.\nIn the numerator I ask R to count the number of 0s (no birthday collisions) in the object simprobs_23 and then I divide it by 10000, just as in the definition of relative frequency probability. Then I compute 1 minus this relative frequency ::: {.cell}\n1 - length((simprobs_23[simprobs_23 == 0]))/10000\n\n[1] 0.5067\n\n::: Voila, this is pretty good, isn’t it? Remember that the analytical solution we just got a bit earlier was 0.5072972\nIsn’t this cool? Instead of using formal reasoning we built a random experiment on the computer and came up with a pretty good approximation of the collision probability.\nLet’s check the result with 50 for the sake of completeness and for the sake of practice, with an experiment with 10000 trials. I now stitch all steps together. ::: {.cell}\nsim_50 &lt;- function(){\n  b_50 &lt;- sample(1:365, 50, replace = TRUE)\n  (50 - length(unique(b_50)))}\n\nsimprobs_50 &lt;- replicate(100000, sim_50())\n1 - length((simprobs_50[simprobs_50 == 0]))/100000\n\n[1] 0.97027\n\n::: Pretty amazing! Before we got the exact value as 0.9703736.\nBut now let us stay still with the birthday problem but now get away from this playful application to something really serious: Cryptography.\n\n\n1.3.4 Hash-functions and the blockchain\nThe birthday problem is the canonical model for the probabilistic study of coincidences of all kinds. A particular important application of this idea is in cryptography, and its concept of so called hash-functions.\nA hash-function maps a string of arbitrary but finite length to a fixed length string of output.4 A very frequently used hash-function in practice is the function SHA-256, which maps its input to a string of 256 bits5. So, you could for instance give the text of these lecture notes as an input to SHA-256 and it would map this into a 256-bit string, which functions like a finger print of this text. This function is an instance of a special from of mappings called a one-way-function meaning that it is easy to evaluate or compute but it is practically impossible to learn from the value the initial argument by computing the inverse.\n4 We have taken this discussion from https://brilliant.org/wiki/birthday-paradox/5 A bit, short for binary digit, is defined as the most basic unit of data in telecommunications and computing. Each bit is represented by either a 1 or a 0Hash-Functions are key pillars of modern cryptography, where they play a major role in message authentication. This is because it is impossible to modify the input without significantly changing the output. So in our previous example, if you only deleted or added a comma to these notes and hash-them again they would hash into a completely different value than the previous version which still was without this minor change. Comparing the hash-values would reveal immediately that something has changed.\nThe collision problem for hash-functions is formally equivalent to the birthday problem. The event we are interested in is that at least two input strings hash-to the same value. Again it is easier to think about the complementary event that all inputs hash to a different value.\nIf the range of the hash-function is \\(M\\) and the hash-function maps into a 256 bit string then there are \\(2^{256}\\) basic outcomes. Since the hash-function maps a large string onto a smaller string it is possible that there are two different strings \\(x \\neq y\\) mapping to the same value \\(\\text{hash}(x)=\\text{hash}(y)\\). This would be a problem for message authentication because it would give the same “fingerprint” for two different strings.\nFor a cryptographically secure hash function it is therefore required that the probability of such a collision should be small enough to exclude a collision in all practically relevant circumstances.\nNote that this is a problem formally similar to the birthday problem. In the cryptography context we ask, how many values does a potential attacker have to hash through SHA-256 before the probability of a collision is larger than a certain threshold?\nThe probability of no-collision (\\(nc\\)) with \\(n\\) hashed-messages through a SHA-256 hash function is \\[\\begin{equation*}\nP_{nc}(n) = 1 \\times \\left(1 - \\frac{1}{2^{256}}\\right)\\times\n\\left(1 - \\frac{2}{2^{256}}\\right) \\times \\cdots \\times \\left(1 - \\frac{(n-1)}{2^{256}}\\right)\n\\end{equation*}\\]\nNow to do the estimation of the threshold we use an approximation, which works with a simple property of the logarithm: \\(\\log(1-x)\\approx -x\\) when \\(x\\) is small. Thus for \\(n\\) much smaller than \\(M\\), or \\(n \\ll M\\), we can write the no-collision probability approximately as \\[\\begin{align*}\nP_{nc}(n) &\\approx 1 \\times \\exp\\left(-\\frac{1}{2^{256}}\\right) \\times \\exp\\left(-\\frac{2}{2^{256}}\\right) \\times \\cdots\n\\times \\exp\\left(-\\frac{(n-1)}{2^{256}}\\right) \\\\\n&\\approx 1 \\times \\exp\\left(-\\frac{(1+2+3+...+(n-1))}{2^{256}} \\right) \\\\\n&\\approx \\exp\\left(-\\frac{n \\times (n-1)}{2 \\times 2^{256}}\\right)\n\\end{align*}\\] Now if \\(n \\ll 2^{256}\\) we can again use the approximation \\(\\exp(x) \\approx 1 + x\\) to get the probability of a hash-collision as \\[\\begin{align*}\nP_{nc}(n) &\\approx 1 - \\frac{n \\times (n-1)}{2\\times 2^{256}}\\\\\n       & \\approx 1 - \\frac{n^2}{2\\times 2^{256}}\n\\end{align*}\\] Thus the probability of a hash-collision as a function of \\(n\\) is then approximately \\[\\begin{equation*}\nP_{c}(n) = \\frac{n^2}{2 \\times 2^{256}}\n\\end{equation*}\\] Thus to have a collision probability with \\(99.8%\\) you need ::: {.cell}\nsqrt(2*2^256*0.998)\n\n[1] 4.807505e+38\n\n::: which is about \\(5*10^{38}\\) hashes. To have a collision probability of \\(50 \\%\\) you need ::: {.cell}\nsqrt(2^256)\n\n[1] 3.402824e+38\n\n::: which amounts to \\(2^{128}\\) hashes. This is, of course a really big number.\nLet’s put this into perspective: If a computer computes \\(10000\\) hashes per second, it would take more than \\(10^{27}\\) years to compute \\(2^{128}\\) hashes. Narayanan et al. (2016) formulate it like this: “…if every computer ever made by humanity had been computing since the beginning of the universe, the odds that they would have found a collision by now are still infinitesimally small. So small that it it’s far less than the odds that a meteor will land on your house.”.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html#summary",
    "href": "01-lecture1_updated.html#summary",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.4 Summary",
    "text": "1.4 Summary\nIn this lecture we have taken a first step towards some very basic probability notions and some basic steps in R. Isn’t it amazing how much territory we could cover with so few concepts? You have learned about how to think probabilistically about collision probabilities and how to solve for them analytically, by simulation and for large numbers by approximation.\nWe have convinced ourselves using this knowledge only and taking on faith that the probability of independent events is the product of their individual probabilities, that the cryptographic Hash-function SHA-256, while it can produce collisions in theory, practically the number of hashes to make such a collision occur would be so large that we can be confident that hashing bit strings with SHA-256 gives us a unique fingerprint practically with certainty.\nThese are the basic probability concepts we have covered in this lecture:\n\nA random experiment: A process leading to an uncertain outcome.\nSample space: The collection of all possible outcomes of a random experiment.\nBasic outcome: A possible outcome of a random experiment.\nEvent: An event is a subset of basic outcomes. Any event which contains a single outcome is called a simple event.\nClassical probability find or make equally probably cases and the count them. The probability of an event \\(A\\) is the number of cases when \\(A\\) occurs divided by the total number of cases.\nRelative frequency probability defined as the number of times an event \\(A\\) occurs in a repeated number of trials divided by the total number of trials in a random experiment.\n\nThese are the R concepts we have covered in this lecture:\n\nobjects arbitrary names that can store different values and data types.\nfunctions ab R object that can accept other R objects as arguments, operate on them and return a new object.\nscripts files that store sequences of R commands and can be saved, repopened and allow the execution of commands.\nusing packages\nfinding help\nthe functions sample and replicate\n\nThese are the Applications we have covered in the lecture:\n\nConstructing a die on the computer and roll it an arbitrary number of times\nThe birthday problem and how to solve it analytically as well as through simulation\nExtrapolating the birthday problem to analyze cryptographic collision resistance of hash-functions by using ideas from probability",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html#project-1-craps-a-casion-dice-game.",
    "href": "01-lecture1_updated.html#project-1-craps-a-casion-dice-game.",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.5 Project 1: Craps: A casion dice game.",
    "text": "1.5 Project 1: Craps: A casion dice game.\nThe first project is about one of the very few dice games offered at a Casino: Craps.6\n6  This will be the only project in this course where we do not go into a direct financial application. Before we can do so, we need to build still a bit more knowledge of probability and skills in R.In Craps people throw a pair of dice and bet on outcomes of these throws. In this project we practice our new knowledge by implementing the dice throws of craps and the automatic summation of points on the computer. We try to find out the probability of the possible outcomes and check the fairness of the dice.\n\nYour first task in this project will be to write a function which allows you to virtually throw a pair of dice and sum up the points shown after the throw.\nSimulate and plot the result of your simulation using qplot() with the binwidth argument set to 1.\nAre these dice fair? Why?\nAssume the dice were unfair in the following sense: Numbers 1,2,3,4 and 5 have a probability of \\(1/8\\) while the 6 has a probability of \\(3/8\\). Study the help page of the samplefunction and find out how you could give these new probability weights to the function. If you redo your simulation analysis with the loaded dice, how does the picture change?\nWrite a script for this random experiment.\nNow look at the random experiment of throwing two dice with the concepts from probability theory: What is the sample space of this experiment. What are the probabilities of the basic outcomes? What is the probability of getting sum 7, what is the probability of getting sum 2 for the case of the fair dice, for the case of the unfair dice?\nImagine now that you are at the casino which uses fair dice and you can spend 150 Euro for chips. Since you have figured out that 7 is the most likely outcome you would like to buy bets on 7. The casino offers you a bet for 15 cent. When you win you get 1 Euro for the bet, when you loose you get nothing. Every bet costs 15 cents, so you can make 1000 bets in total on 7 from your budget of 150. Is this a good or a bad deal for you? Try to think about this problem in terms of a simulation.\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nNarayanan, Arvind, Joseph Bonneau, Edward Felten, Andrew Miller, and Steven Goldfeder. 2016. Bitcoin and Cryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#tossing-a-coin",
    "href": "01-lecture1.html#tossing-a-coin",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1.1 Random experiments, sample space, events\nIn the theory of probability a process leading to an uncertain outcome is called a random experiment. The example of tossing a coin helps us to give a precise meaning to the notion of an uncertain outcome. While we can imagine in principle that the coin lands on its rim and this is one outcome, we agree on the outset that we are going to consider the (practically relevant cases) that it will land on one of the faces.\nThe collection of all possible outcomes of a practical or conceptual random experiment is called in probability theory a sample space. While the sample space of tossing a coin is an idealization it is exactly this idealization which simplifies the theory without affecting its applicability. The basic outcomes in the random experiment of tossing a coin are that the die lands such that the upward showing face shows Heads or Tails. Let us encode this outcome by 1 for “Heads” and 0 for “Tails”. In the theory the collection of basic outcomes is denoted as a set. Thus the sample space of tossing a coin is given as the set \\({\\cal S} = \\{1,0\\}\\).\nThe sample space \\({\\cal S}\\) is the set of all basic outcomes, which are in our case only two: 1 or 0.\nIn more complex random experiments there will usually be more than two outcomes. Subsets of the sample space containing more than one basic outcome are called events in the theory of probability.\n\n\n\n\n\n\nNow you try\n\n\n\nConsider another random experiment from games of chance: Throwing a six sided die. What are the basic outcomes in this random experiment? What is the sample space in this case? How would you describe the event that the outcome of the die shows an odd number in terms of a subset of the sample space? Consider yet a new random experiment where you throw two coins, instead of one. What are the basic outcomes in this random experiment? What is the sample space in this case? How do these concepts look like when you throw two six sided dice instead of two coins?\n\n\n\n\n1.1.2 The role and nature of idealizations in applications\nIdealizations of the kind we discussed for the toss of a coin are standard in probability and we will encounter them again and again. For example the movement of a stock price is often though of as a conceptual random experiment. When we try to agree on what is the appropriate sample space of this experiment, we can say that the price can not fall below 0 but it is hard to agree on what will be the highest possible price. In the probabilistic treatment of stock prices, which we will discuss later in our course, it is for instance common to assume that the sample space of this random experiment is the entire interval of non-negative real numbers \\({\\cal S} = [0, \\infty)\\).\nMany of us would hesitate to claim that the price might rise without bound. Yet many models in applied Finance are based on such an assumption. The models allow arbitrary price hikes but with arbitrary small probability as the price gets higher and higher. Practically it does not make sense to believe that a security price can become arbitrarily high. The use of arbitrarily small probabilities in a financial model might seem absurd but it does no practical harm and makes the model simple and convenient to use. Moreover, if we seriously introduced an upper bound on a security price at \\(x\\) it would be also awkward to assume that it is impossible that it could be just a cent higher, an assumption equally unappealing than assuming it can get in principle arbitrarily high.\n\n\n1.1.3 Classical Probability: Measuring uncertainty\nProbability is a measure of how likely an event of an experiment is. But how could we measure chance? Here is the first big idea of probability theory and how the originators thought about it.1 How do you measure anything? If you think of length, for example, you take an arbitrary standard you agree on and then count. The originators of probability theory pursued the same idea with chance: To measure probability you choose equally probable cases and then count.\n1 In this discussion I use chapter 1 in Diaconis and Skyrms (2019)The probability of an event \\(A\\) according to the originators of the theory of probability is then \\[\\begin{equation}\nP(A) = \\frac{\\text{Number of cases where $A$ occurs}}{\\text{Total number of cases}}\n\\end{equation}\\]\nThus, if we want to know the probability of the coin ending up Heads, we have to compute according to this notion \\(1/2\\), since the total number of cases is 2 (Heads or Tails).\n\n\n\n\n\n\nNow you try\n\n\n\nConsider another random experiment of throwing a six sided die: According to the probability notion explained above what would be the chance that the die lands on on a face such that an even number is shown?. What would be the chance that in the random experiment of throwing two dice the sum of points would be 7? What is the chance that when rolling two six sided dice the sum is 2?\n\n\nNote that this classical notion of probability has a few interesting consequences, which we will discuss in more detail later but which already now naturally flow from this basic idea of measuring chance.\n\nProbability is never negative.\nIf an event \\(A\\) occurs in all cases \\(P(A)=1\\).\nIf \\(A\\) and \\(B\\) never occur in the same case, then \\(P(A \\,\\text{or}\\, B) = P(A) + P(B)\\).\n\nWhile rule 1. and 2. are intuitively fairly obvious (try to think of examples to convince yourself!), rule 3 might need some concrete explanation. The case described by rule 3 is often referred to as mutually exclusive events, i.e. they can not occur together.\n\n\n\n\n\n\nExample for rule 3:\n\n\n\nSuppose you have a standard deck of 52 playing cards, and you are interested in the probability of drawing a King \\(A\\) or a Queen \\(B\\).\n\nThe probability of drawing a King is \\(P(A) = \\frac{4}{52}\\), since there are 4 Kings in the deck.\nThe probability of drawing a Queen is \\(P(B) = \\frac{4}{52}\\), since there are also 4 Queens.\n\nSince a card cannot be both a King and a Queen, \\(A\\) and \\(B\\) are mutually exclusive events.\nThus, the probability of drawing a King or a Queen is: \\(P(A \\,\\text{or}\\, B) = P(A) + P(B) = \\frac{4}{52} + \\frac{4}{52} = \\frac{8}{52} = \\frac{2}{13}\\)\nThis illustrates how probabilities of mutually exclusive events add up.\n\n\nIn particular then the probability that an event does not occur is 1 less the probability that it occurs: \\(P(\\text{not}\\, A) = 1 - P(A)\\).\nNow let me show you how you can make use of a large language model, using ChatGPT as a use case, to support learning new concepts such as we have just introduced them here.\nWe have discussed new two classical and famous random experiments you will probably find in every textbook on probability. Assume you would like to deepen your understanding of these concepts and also consider more complex examples in particular from Finance. This is something the LLM is very good at. You could - for instance - ask ChatGPT at the prompt something like shown in Figure 1.2 :\n\n\n\n\n\n\n\n\nFigure 1.2: Figure 2: The ChatGPT prompt\n\n\n\n\n\nHere is an answer I would get for this prompt:\n\n\n\n\n\n\nAnswer by ChatGPT40\n\n\n\nHere are three detailed examples from finance to illustrate the concepts of a random experiment, sample space, basic outcomes, and events:\n\nExample 1: Daily Price Movement of a Stock\nRandom Experiment: Observe the daily price movement of a particular stock (e.g., Apple Inc.) at the close of the trading day.\n\nSample Space \\({\\cal S}\\): The set of all possible outcomes of the price movement. For simplicity, we can represent the sample space as: \\({\\cal S} = \\{ \\text{price increase, price decrease, no change} \\}\\)\nBasic Outcomes:\n\n“Price increases” (e.g., the stock closes higher than it opened).\n“Price decreases” (e.g., the stock closes lower than it opened).\n“No change” (e.g., the stock closes at the same price it opened).\n\nEvents:\n\nEvent \\(A\\): “The stock price increases.” \\(\\{ \\text{price increase} \\}\\)\nEvent \\(B\\): “The stock price either increases or remains unchanged.” \\(\\{ \\text{price increase, no change} \\}\\)\n\n\n\nExample 2: Credit Card Default Risk\nRandom Experiment: Assess whether a randomly selected credit card holder defaults on their monthly payment.\n\nSample Space \\({\\cal S}\\):\n\\(S = \\{\\text{default, no default} \\}\\)\nBasic Outcomes:\n\n“Default” (e.g., the cardholder fails to make the minimum payment).\n“No default” (e.g., the cardholder pays at least the minimum required).\n\nEvents:\n\nEvent \\(A\\): “The cardholder defaults.” ({default})\nEvent \\(B\\): “The cardholder does not default.” ({no default})\nEvent \\(C\\): “Default occurs for a customer from a specific demographic (e.g., under 30 years old).” This is a subset of the sample space but is conditioned on additional information.\n\n\n\nExample 3: Bond Ratings Transition\nRandom Experiment: Observe the change in the credit rating of a corporate bond over a year.\n\nSample Space \\({\\cal S}\\): The set of possible credit rating transitions. For example, using a simple rating scale: \\({\\cal S} = \\{\\text{AAA}, \\text{AA}, \\text{A}, \\text{BBB}, \\text{BB}, \\text{B}, \\text{CCC}, \\text{default} \\}\\)\nOver a year, the bond can transition from its initial rating to any of these, or stay the same.\nBasic Outcomes:\n\n“AAA to AAA” (no change in the highest credit quality).\n“BBB to BB” (downgrade).\n“A to default” (significant deterioration).\n\nEvents:\n\nEvent \\(A\\): “The bond is downgraded by one or more notches.” (Any outcome where the final rating is lower than the initial rating, e.g., {AAA to AA, A to BB}).\nEvent \\(B\\): “The bond defaults.” ({default}).\nEvent \\(C\\): “The bond remains investment-grade.” (All transitions where the final rating is AAA, AA, A, or BBB).\n\n\n\nThese examples show how the concepts apply to finance, helping to bridge the theoretical framework of probability with practical applications in the field. Would you like to dive deeper into any of these examples or extend them to computations like probabilities of events?\n\n\nLet us interrupt here our discussion of probability for a moment and ask how we can make good on our promise to make these ideas tangible, so we can play with them.\nFor this we will need the computer. Since we will talk to the computer in R, it is now the right time to look at the die rolling example from the perspective of R and dive into some of its basic concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#tossing-coin-on-computer",
    "href": "01-lecture1.html#tossing-coin-on-computer",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.2 Tossing a coin on the computer: First steps in R",
    "text": "1.2 Tossing a coin on the computer: First steps in R\n\n1.2.1 The R User Interface\nBefore we can ask our tool to do anything for us, we need to know how to talk to it. In our case RStudio allows us to talk to our computer. It works like any other application. When you launch RStudio on your computer, in figure Figure 1.3 you see a screen looking like this:\n\n\n\n\n\n\n\n\nFigure 1.3: The RStudio startup screen\n\n\n\n\n\nIn this picture you see a screenshot of my RStudio screen. Interacting with the app is easy. You type commands via your keyboard at the prompt, which is the &gt; symbol. You find this symbol in the RStudio pane called Console. You can see it in the left pane in the screenshot. You send the command to the computer by pressing enter. After you have pressed enter, RStudio sends the command to R and displays the result of your command with a new prompt to enter new commands, like this:\n\n1+1\n\n[1] 2\n\n\nLet me pause to explain what you see here in this code example. First you see a light-gray box containing the command 1+1. This is an instance of a so called code chunk. Moving the cursor to the right upper corner of the chunk, will display a copy icon and you can click this icon to copy the code to the computer’s clipboard and then paste it at the prompt of your console (or for that matter anywhere else) and run it in R studio, if you wish. In the code-chunk the R-prompt &gt; is not displayed. This symbol of the prompt is only shown in the Console itself.\nThe [1] means that the line begins with the first value of your result. For example, if you enter the command 20:60 at the prompt of your console which means in the R language, “list all the integers from 20 to 60” and press enter you get:\n\n20:60\n\n [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[26] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nmeaning that 20 is the first value displayed in your result. Then there is a line break because not all values can be displayed on the same line and R tells you that 45 is the 26-th value of the result.\nThe colon operator : is a very useful function in R which we will need often. It allows us to create sequences of every integer between two given integers.\nR needs a complete command to be able to execute it, when the return key is pressed. Lets see what happens, if a command is incomplete, like for instance &gt; 5*.\nIn this case R will show the expression followed by a + instead of showing a new prompt. This means that the expression is incomplete. It expects more input. If we complete the expression, say like\n&gt; 5*\n+ 4\nthe expression can be evaluated and a new prompt is shown in the console.\nIf you type a command that R does not understand, you will be returned an error message. Don’t worry if you see an error message. It just is a way the computer tells you that he does not understand what you want him to do.\nFor instance, if you type 5%3 you will get an error message like this\n&gt; 5%3\nError: unexpected input in \"5%3\"\n&gt;\nSometimes it is obvious why a mistake occurred. In this case, that R just does not know what to do with the symbol %. It has no meaning in this context. Sometimes it is not so obvious what the error message actually means and what you might do about it.\nA useful strategy in this case is to type the error message into a search engine and see what you can find. The chance is very high that others encountered the same problem before you and got helpful advice how to fix it from other users on the internet. One site, we find particularly helpful for all kinds of questions related to R and R programming is https://stackoverflow.com/. Try it at the next opportunity. You can also show the error to the LLM prompt and ask how to fix it. In most cases chatGPT will give you a useful answer.\nNow with this basic knowledge, we can already make the first step to create a coin on the computer using R. If you think of a physical coin, the essential thing that matters is that it can show Heads or Tails. Lets encode Heads by 1 and Tails by 0. If you toss the coin it will usually land on one of these faces and the upward showing side of the coin shows the outcome of the toss. The colon operator : gives us a way to create a group of numbers from 0 to 1. R gives us the result as a one dimensional set of numbers.\n\n0:1\n\n[1] 0 1\n\n\nLets use these first steps in R to recap the probability concepts we have learned using this example of the six sided die: A basic outcome of tossing the coin is for example 1 if the upper side after tossing the coin happens to be Heads. The sample space of the experiment of tossing a coin is the set \\({\\cal S} = \\{0,1\\}\\). In probability theory we often use the symbol \\({\\cal S}\\) or \\(S\\) for sample space. In many probability texts the sample space is also often denoted by the symbol \\(\\Omega\\) the Greek letter for (big) Omega. A random experiment in this example is the tossing of the coin. The outcome is uncertain but once the coin is tossed the outcome can be determined precisely. The event that the outcome is a display of 2 is the empty set \\(A = \\emptyset\\). The symbol \\(\\emptyset\\) comes from set theory and means the set containing no elements. This event can contain no elements because we can not get any other number than 0 or 1 by tossing the coin.\n\n\n1.2.2 Objects\nYou can save data in R by storing them in objects. An object is a name, you can choose yourself to store data. For example, if you choose to store the value 1 in an object called Heads, you would type:\n\nHeads &lt;- 1\n\nat the prompt. R will the store the value 1 in the object called Heads, which you can use to refer to the value. If you type the name of your object at the prompt, R will display the value you have assigned. A useful key combination for typing the assignment operator &lt;- is to use the key combination ALT _. At the R prompt R will automatically print an assignment operator.\nNow if you type the name of the object and press enter, R will display the value or values stored in it, like this:\n\nHeads\n\n[1] 1\n\n\nNow you can use the name of the object to refer to its value. For instance, you could divide Heads by 2 and get a meaningful result\n\nHeads/2\n\n[1] 0.5\n\n\nNow to make our coin more tangible and useful, let us store it in an R object by typing the following command at the prompt. This command creates an object with name coin and assigns the vector 0,1 to it.\n\ncoin &lt;- 0:1\n\n\n\n\n\n\n\n\n\nFigure 1.4: The RStudio Environment pane keeps track of the objects you have created\n\n\n\n\n\nYou can now see in the right upper Environment pane in figure Figure 1.4 that R shows you that there is an object with the name coin that it consists of integers 0,1. As you create more objects they will be stored in the Environment pane and are ready for your reference, unless you delete them. You can remove or delete an object by typing rm(object) or by assigning the value coin &lt;- NULL which would also remove the object from your environment or workspace.\nYou can name your objects almost anything with a few exceptions. An object name must not start with a number. There are some special symbols which can also not be used in object names, like ^, !, $, @, +, -, /, *. Note that R is case sensitive and distinguishes small and big letters. If you assign a new value for an object you have already created, R will overwrite the object without warning.\nYou can see which objects are currently created and available for you in the Environment pane of your session of by typing ls(). The UNIX users among you will recognize this command from the unix shell, where it displays the files in a directory.\nBefore we learn how we can actually roll our die and perform a random experiment with it, let us briefly use the opportunity to explain a few things about how R does computations. We have already explained that we can use the object name to refer to the value. So for instance if we type\n\ncoin*coin\n\n[1] 0 1\n\n\nThis might irritate some of you because we have called the object a vector. In linear algebra multiplication of vectors is only allowed if there is an inner product. What happens here, if we use * the multiplication operator is that R does an element-wise multiplication of the six numbers of our die. Of course R allows to take an inner product as well, but this needs a different operator. To compute an inner product, we would type\n\ncoin %*% coin\n\n     [,1]\n[1,]    1\n\n\nNow R displays the result as a vectors with one row and one column, which is denoted in the output by [ , 1] for the column and [1, ] for the row. We will learn later more about the use and the meaning of this notation in R.\nThe element wise execution R usually uses also means that when you, for example type\n\ncoin + 1\n\n[1] 1 2\n\n\nR would subtract 1 from every component in the vector coin.\nAnother specific behavior of R, you need to know about is called recycling. If you give R two vectors of different length in an operation, R will repeat the shorter vector as long as it is of equal length with the longer one. For example, if you have:\n\ncoin + 1\n\n[1] 1 2\n\n\nyou see that R adds 1 to 0 and then starts over again by adding 1 to 1.\nIf the longer vectors is not a multiple of the shorter one, R recycles but the cuts off.2\n2 As of R version 4.2.0 (released in April 2022), the behavior regarding the recycling rule has changed. In older versions of R, a warning was issued when the length of one vector was not a multiple of the length of the other vector during arithmetic operations. The reasoning behind this change was to reduce unnecessary noise in the output. Many users found the warnings redundant in cases where the recycling rule was intentional and understood. However, this change means you need to be more cautious, as unexpected behavior might go unnoticed without the warning.\ncoin + 1:4\n\n[1] 1 3 3 5\n\n\nWhile this might seem awkward to some of you, we will see that for data manipulation element-wise execution is often extremely useful. It allows to manipulate groups of values in a systematic yet easy way.\n\n\n1.2.3 Functions\nR contains many functions which we can use to manipulate data and compute things. The syntax for using a function is very simple: You type the function name and put the value of the function argument in parentheses. Here we use for illustrations the function of the square root sqrt():\n\nsqrt(4)\n\n[1] 2\n\n\nor rounding a number:\n\nround(3.1415)\n\n[1] 3\n\n\nThe data you write in the parentheses are called the function arguments. Arguments can be all sorts of things: raw data, R objects, results from other functions.\nIf functions are nested, R evaluates the innermost function first and then goes on to the outer functions. To see examples of all these instances you can take\n\nnumbers &lt;- 1:7\nmean(numbers)\n\n[1] 4\n\nround(mean(numbers))\n\n[1] 4\n\n\nfor example.\nFor simulating random experiments, R has the very useful function sample(). With this function we can - for example - toss a coin on the computer and conduct actual random experiments.\nThe function takes as arguments a vector names x and a number named size. sample will return size elements randomly chosen from the vector x. Lets say:\n\nsample(x = 0:1, size = 1)\n\n[1] 0\n\n\nIn this case sample has chosen either 0 or 1 from the vector x = (0,1) at random.\nIf we want to toss the coin in our computer we can thus pass the coin as an argument to sample and use the number 1 for the size argument. Lets do a few tosses with our die\n\nsample(x = coin, size = 1)\n\n[1] 1\n\nsample(x = coin, size = 1)\n\n[1] 1\n\nsample(x = coin, size = 1)\n\n[1] 0\n\nsample(x = coin, size = 1)\n\n[1] 1\n\n\nThese are the random outcomes of our consecutive tosses. It is as if we had thrown an actual coin but in this case we have done the same thing on the computer. Isn’t it cool that this is possible at all? The sample() function will remain our good friend throughout this course.\nR functions can have many arguments, but they need to be separated by a comma.\nEvery argument in every function has a name. We specify which data are assigned to the arguments by setting a mathematical equal sign = between the name and the data. Names help us to avoid passing the wrong data and thereby mixing up things or committing errors. But using names is not necessary. If we just wrote\n\nsample(coin,1)\n\n[1] 1\n\n\nR would also know what to do. It is not always clear which names to use for a function. If you are not sure, you can use the function args() to look it up. Here we take the function roundas one example.\n\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n\nNote that the digits argument in the round function is already set to 0. Frequently R functions come with optional arguments. These arguments are optional because the come with a default value, which is 0 in case of the round function.\nWe recommend that you write out argument names as a rule. It gives clearer code and avoids errors. If you don’t write argument names, R matches your values to the arguments of the function by order.\n\n\n1.2.4 Writing your own functions\nNow we are ready to write our own function to toss the coin in our computer. Each function in R has the same elements: A name, a function body of code and a set of arguments. To write your own function, you have to write up all of these parts and save them in an R object. The syntax is:\nmy_function &lt;- function() {}\n\nThe name here is my_function, next comes the expression function() which needs to be assigned. The names of the function arguments have to be written between the parentheses. Then we have to write the actual code within the braces {}.\nTo do this for the die, lets write a function named toss_coin.\n\ntoss_coin &lt;- function(){coin &lt;- 0:1 \n                         sample(coin, size = 1)}\n\nNow we can toss our coin for a few times to show how the function works\n\ntoss_coin()\n\n[1] 1\n\ntoss_coin()\n\n[1] 0\n\ntoss_coin()\n\n[1] 0\n\ntoss_coin()\n\n[1] 0\n\ntoss_coin()\n\n[1] 1\n\n\nNote that in our function toss_coin() has no arguments, just the function body. This is perfectly legitimate in R. It is important that when we call the function we have to call it with the parenthesis like toss_coin(). If we only call the name toss_coin, R will display the code in the function body.\nA final remark in the sample function is in place here. If we look at the arguments of sample using the args function we see\n\nargs(sample)\n\nfunction (x, size, replace = FALSE, prob = NULL) \nNULL\n\n\nLets do not discuss all the details of this output but concentrate for a moment on the replace argument. What does this mean?\nAs we saw previously we can use the sample function to model the rolling of our die. If we set the size argument to 1 we get the toss of one coin. If we set the size argument to n, we would simulate the tossing of n coins. But now the replace argument becomes crucial. As we can see in the output of the args function replace has a default value FALSE. This is a logical argument. It tells R, for example, that if we set size = 2, meaning that two coins are tossed, if the first coin shows, say Heads - a value of 1- the coin die cannot show Heads as well.\nThis is clearly not what we have in mind when we model the tossing of two coins. It should be possible that both coins show the same value. To enable this behavior of the sample function, we have to change the default value of the replace argument to TRUE. Then R chooses a random draw from all of the six possible values for all dice rolled.\nCongratulations ! You have written your first R function for conducting a simple random experiment. Let me remind you once again: Think of the parentheses as a trigger that tells R to run the function. If you omit the trigger R just prints the body of the function. When you run a function, all the code in the function body is executed and R returns the result of the last line of code. If the last line of code does not return a value neither will R.\n\n\n\n\n\n\nNow you try\n\n\n\nConsider another random experiment of throwing a six sided die. Use the concepts learned in this section to write a function which simulates the rolling of a six sided die. Write a function to simulate the throwing of two dice. Think about how to set the replace argument in the sample function in this case. Use R’s help function to learn about the role of the replace argument. Alternatively ask the LLM to explain the use of replace in R’s sample function. You can also ask for examples or demonstrations what goes wrong when this argument is parametrized in the wrong way.\n\n\n\n\n1.2.5 Arguments\nImagine we remove the first line of code in our function body and changed the name die in the sample function to “ball”.\n\ntoss_coin2 &lt;- function(){sample(ball, size = 1)}\n\nIf we call the function now, we will get an error. The function call roll_die2() will result in the error message Error in sample(ball, size = 1) : object 'ball' not found (try it!)\nWe could supply ball when we call toss_coin2 if we make ball an argument of the function. Lets do this:\n\ntoss_coin2 &lt;- function(ball){sample(ball, size = 1)}\n\nNow the function will work as long as we supply ball when we call the function.\n\ntoss_coin2(ball = 0:1)\n\n[1] 1\n\n\nNote that we still get an error, if we forget to supply ball argument. This could be avoided if we give the function a default argument\n\ntoss_coin2 &lt;- function(ball= 0:1){sample(ball, size = 1)}\n\nNow if we type:\n\ntoss_coin2()\n\n[1] 0\n\n\neverything works, just as intended.\n\n\n1.2.6 Scripts\nSo far we have worked by interacting with the console. But what if you want to edit your functions? It would be much easier, if you could use a draft of your code and work form there. This can be done by using a script.\nYou create a script by going to File &gt; New File &gt; R script in the menu bar of RStudio. Using scripts is the standard way to write code in R. It not only helps you to keep track of your code, save it and edit it later. It also makes your work reproducible. You can edit and proofread your code and share it with others. To save your script go to File &gt; Save As in the menu bar.\n\n\n\n\n\nThe RStudio Script\n\n\n\n\nRStudio has many useful features to help you work with scripts. You can for instance automatically execute a line in a code by using the run button. You can also execute sections of code or the entire script. The entire script is executed by running the Source button. For all these commands there are key short cuts which you will learn as you work more with RStudio and R.\nFrom now on you should write all your R-code for this course in scripts. Make a new folder for the scripts you write and store your R-code there from now on.\n\n\n1.2.7 Using Quarto Documents\nQuarto documents provide an elegant way to combine explanatory text, R code, and the resulting output in one cohesive file. This makes them especially useful for documenting your learning process, conducting reproducible research, or preparing assignments.\nLet’s walk through how to use a Quarto document to write down and experiment with your coin-tossing function.\n\nCreating a New Quarto Document: To begin, create a new Quarto document in RStudio by navigating to File &gt; New File &gt; Quarto Document.... You will see a popup window allowing you to type a title for the document and your name. It also allows you to select a format for your document. Choose the HTML format for now, as it allows easy preview in your browser, and click the Create button The popup window will look something like this:\n\n\n\n\n\n\nThe quarto pop up window\n\n\n\n\n\nUnderstanding the Quarto Layout: The new Quarto document will have a YAML header at the top, which might look like this:\n\n\n\n\n\n\nThe quarto yaml header\n\n\n\n\nBelow this header, you will see some example text and R code chunks. Replace the example content with your own text and code.\n\nAdding the Coin Tossing Function: In your Quarto document, you can explain your approach to creating the coin-tossing function, and include the code in a code chunk.\nHere’s an example:\n\n\n\n\n\n\nCombining text and code in quarto documents\n\n\n\n\n\nRunning and Compiling the Document: Once you have written your text and code, save your document. You can execute the R code directly within the Quarto document. To run a code chunk, click the green play button in the upper-right corner of the chunk or use the shortcut Ctrl + Shift + Enter (Windows) or Cmd + Shift + Enter (Mac).\nAfter executing your code, you can compile the document into a readable output format by clicking the Render button or using the shortcut Ctrl + Shift + K (Windows) or Cmd + Shift + K (Mac).\n\nThere is of course a lot you can do to enhance your quarto documents. For example the lecture notes you are reading now are all written and composed in quarto as are the lecture slides.\nBy using Quarto documents, you can create well-documented, interactive code files that are both functional and easy to share with others. You can use quarto indpendently from R and combine it with other languages like phyton or julia. Ceck out: https://quarto.org/\n\n\n1.2.8 Using packages and finding Help\nWe have now a function which we can use to simulate the tossing of a coin, coin_toss(). If the coin is fair it should be the case that if we toss the coin often, Heads and Tails should occur about equally often. The coin should not be weighted in favor of a particular value.\nOne way to learn whether our coin is fair are repetition and visualization. These are tools we will need all the time, when working with data and when doing probability. While R has many useful functions, one of the great powers is that R is constantly extended by a huge community of users by providing packages.\nPackages are add on functions, which will not be available when you install R. They need to be installed and loaded before you can use them. Since packages are such a powerful tool in R we need to introduce what they are and how to use them in the beginning.\n\n\n1.2.9 Packages\nThere are many visualization tools in R that come with the basic installation. Since the point we want to make here is about packages, we will use a visualization function which is part of the add on package ggplot2, a very popular package for making all kinds of graphs. ggplot2 is not part of the base-R distribution. It has to be therefore loaded before we can use it.\nThere are many additional functions provided with this package. Indeed you could run a whole course on just that 3\n3 A very good starting point is for instance Healy (2019), in case you are interested. Also a good source is the first chapter in Wickham and Grolemund (2017).Since we want to use functios now which are not in the base R distribution. we need to load the package which provides the functions we want. In this example we use the package ggplot2.\nWhen we use a package for the first time it needs to be installed. Once installed it does not have to be reinstalled when we use it at later coding sessions again.\nTo install a package you need to be connected to the internet. If you have internet connection go to the command line and run at the command line: install.packages(\"ggplot2\").\nR displays what is happening while executing the command. Don’t worry if you don not know what all of these messages exactly mean and don’t panic that they are displayed in red. All packages can be installed like this. You have just to enter the correct name in the function install.packages(). The lower right pane in the RStudio software alternatively provides a tab called Packages. Using this tab, you can also install R packages interactively by clicking the install button on the upper left corner of the Packages tab.\nAfter installation the package is on our hard-drive but it can not yet be used. To use the package it has to be loaded. This is done with the command library. To load the ggplot2 package we type\n\nlibrary(\"ggplot2\")\n\nand hit the return key. Many things could be said about the R package system and you will learn it in more detail as we go along in our course. For the moment the most important thing to remember is that a package needs to be newly loaded whenever you want to use it in a new R session.\nTo check whether our coin is fair, we need to toss it many times. R provides a function, that does this for us. This function is called replicate() and provides an easy way to repeat a command many times. The number of times we want to repeat something is given as an argument to the replicate function.\nNow lets roll our die 100 times and save the result in an object we call tosses:\n\nset.seed(123)\ntosses &lt;- replicate(100, toss_coin())\n\nThe function set.seed() I used here with the argument 123 is just a device to make this experiment reproduceable. If this function is not used, each new run of the 100 replications would show a different outcome.\nI now show you a code for visualizing this outcome. The code contains some concepts that we did not yet discuss. Don’t worry. We will learn them soon. The point here is to shwo you how we use the functions of the package to produce a visualization of the outcome of our 100 coin tosses.\n\n# Create a data frame for plotting\ntosses_df &lt;- data.frame(\n  Outcome = factor(tosses, levels = c(0, 1), labels = c(\"Tails\", \"Heads\"))\n)\n\n# Plot the results using ggplot2\nggplot(tosses_df, aes(x = Outcome)) +\n  geom_bar(fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Occurrences of Heads and Tails\",\n    x = \"Outcome\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt looks like every value occurred roughly 50 times but there is still quite some variation. Tail occured fewer times than Heads.\nMaybe we have to give it another trial with more replications. With the computer we can do this with a fingertip. Let us roll our die 1000 times and plot the result.\nHere is how we would code this by recycling the code used before.\n\nset.seed(123)\ntosses &lt;- replicate(1000, toss_coin())\n\n# Create a data frame for plotting\ntosses_df &lt;- data.frame(\n  Outcome = factor(tosses, levels = c(0, 1), labels = c(\"Tails\", \"Heads\"))\n)\n\n# Plot the results using ggplot2\nggplot(tosses_df, aes(x = Outcome)) +\n  geom_bar(fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Occurrences of Heads and Tails\",\n    x = \"Outcome\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNow it looks much better: Each face comes up approximately 500 times. We will learn in our course why the picture get better as we increase the number of rolls of the die. For the moment, let us just state that at least for this case it seems that there is no significant visual evidence that our virtual die is loaded.\nBut wait a minute. We have introduced the idea of classical probability by constructing equally probable cases and then count. Here we have repeated a random experiment over and over again to draw some conclusion on the underlying probabilities by visualizing the frequency polts of outcomes? These ideas are connected. We have just informally introduced a new idea in probability, the relative frequency notion of probability. How the two concepts are related and why this notion makes sense we will learn more formally in the next lecture. For the moment let’s just stick with the idea that practitioners have used at all times to assess probabilities by looking at frequencies of particular events occurring. When and why this kind of reasoning about probability works we learn in the next lecture.\n\n\n\n\n\n\nTip for R novices: How to leverage the LLM to enhance understanding\n\n\n\nLet me give you a hint here how you could use the LLM to enhance your learning experience in another way. The students usually taking this bootcamp class have very different backgrounds. Some have much experience with programming but perhaps not necessarily in R. Many of you will - for example - know Python, because most of the MQF is taught and run with Python. In such a case you could use the LLM to translate the R code into Python and then compare.\nOr assume you are impatient and want to jump ahead by having somebody explaining you the ggplot2 code we used for the plotting function. Ask the LLM to explain you the R code and what it does, and I assure you you will get a pretty good and well explained answer.\nTry it!\n\n\n\n\n1.2.10 Getting Help\nWe have now learned a tiny number of R functions and we have written one function ourselves. We have learned how to make use of functions provided by packages.\nIt would be overwhelming to memorize and learn them all. In R, fortunately, every function comes with a detailed documentation and with its own help page. You need to learn how to use this source right from the beginning.\nTo access the help page of a function you type the function name preceded by a question mark at the prompt, like this\n\n?sample\n\nThen, after you have pressed the return key, a help page is opened in the right lower pane under the help tab as shown in Figure 1.5\n\n\n\n\n\n\n\n\nFigure 1.5: The Help-pane in RStudio\n\n\n\n\n\nThis help page has a particular structure that you will find for every other R function no matter whether it is provided by the base installation or by a package. In the upper left corner you see the name of the function (sample) and in curly brackets next to it the term base, which means that this is a function in the R base installation. Then you see a headline about what the function does.\nFrom the top of the page, you then first see the field Description. This is a short description what the function does. Here it says\nDescription\nsample takes a sample of the specified size from the elements of x using either with or without replacement.\nThe next field is Usage. It gives you the function description with the arguments. Here for example\nUsage\nsample(x, size, replace = FALSE, prob = NULL)\n\nsample.int(n, size = n, replace = FALSE, prob = NULL,\n           useHash = (!replace && is.null(prob) && size &lt;= n/2 && n &gt; 1e7))\n           \nThe first line in Usage should by now be familiar. Don’t worry about the second line. The function can obviously do more than we know so far.\nNext comes a list of arguments the function takes and what type of information R expects you to provide, as well as what R will do with this information. Here it says for example\nArguments\nx   \neither a vector of one or more elements from which to choose, or a positive integer. See ‘Details.’\n\nn   \na positive number, the number of items to choose from. See ‘Details.’\n\nsize    \na non-negative integer giving the number of items to choose.\n\nreplace \nshould sampling be with replacement?\n\nprob    \na vector of probability weights for obtaining the elements of the vector being sampled.\n\n...\n\nWe omit some information here.\nThen comes a field called Details which gives a more in-depth description of the function. The next field is called Value. It describes what the function returns when you run it. Then we have a reference to related R functions under the field See. Finally there is a field called Examples. This field contains example code that is guaranteed to work. It shows a couple of different cases how you can use the function in practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#introduction-real-world-importance-of-unique-identifiers-in-finance",
    "href": "01-lecture1.html#introduction-real-world-importance-of-unique-identifiers-in-finance",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.1 Introduction: Real-World Importance of Unique Identifiers in Finance",
    "text": "2.1 Introduction: Real-World Importance of Unique Identifiers in Finance\nIn modern financial systems, every transaction is assigned a unique identifier, allowing stakeholders to track, verify, and manage transaction details efficiently. These identifiers are critical for maintaining transparency, accountability, and security across high-volume transaction systems. However, as the volume of transactions grows, the risk of identifier collisions—where two different transactions are assigned the same identifier—increases.\nThis issue is analogous to the birthday problem: as the number of transactions increases, the likelihood of collisions rises due to the combinatorial nature of the problem. A collision could result in processing errors or even security vulnerabilities, such as fraudulent transactions or misattribution of funds.\nTo mitigate such risks, many financial systems employ hash functions, such as the widely used SHA-256, to map data to a large space of possible outputs. These hash functions, with their vast output range (e.g., $2^{256}), make collisions extremely unlikely, even for billions of transactions. By understanding the principles behind collision probabilities, financial institutions can design systems that maintain transaction integrity even under high workloads.\nIn this project, we will explore this critical aspect of financial systems by simulating transaction identifiers and analyzing collision probabilities. This exercise demonstrates how theoretical probability concepts translate directly to practical challenges in computational finance.8\n8 For more information and details you might want to check this source: https://www.hostmerchantservices.com/articles/what-are-transaction-ids/?utm_source=chatgpt.comIn this project, you will:\n\nSimulate a system assigning transaction identifiers.\nInvestigate the probability of identifier collisions under different scenarios.\nDiscuss implications for system design and security.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#simulate-transaction-identifiers",
    "href": "01-lecture1.html#simulate-transaction-identifiers",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.2 Simulate transaction identifiers",
    "text": "2.2 Simulate transaction identifiers\n\nWrite a function to simulate transaction identifiers, where each identifier is randomly chosen from a pool of size \\(M\\) (e.g. \\(M = 10^6\\) or \\(M = 10^9\\))\nUsing your function, simulate \\(n\\) transactions and check for duplicates using R’s duplicated() function.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#compute-collision-probabilities",
    "href": "01-lecture1.html#compute-collision-probabilities",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.3 Compute collision probabilities",
    "text": "2.3 Compute collision probabilities\n\nSimulate nn transactions multiple times and estimate the collision probability as the fraction of simulations with at least one collision.\nUse this function to compute the collision probability for \\(n = 10^3, n = 10^6, n = 10^9\\) and \\(M = 10^6, M = 10^9\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#visualize-the-results",
    "href": "01-lecture1.html#visualize-the-results",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.4 Visualize the results",
    "text": "2.4 Visualize the results\nPlot the collision probability as a function of nn for different values of \\(M\\). We did not cover Rs plotting functions in the lecture so far. We only showed some examples which we did not explain. But we explained how you could research R’s help system and how you could ask an LLM for help. With any of these resources you should be able to come up with some reasonable visualization.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#investigate-the-system-design",
    "href": "01-lecture1.html#investigate-the-system-design",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.5 Investigate the system design",
    "text": "2.5 Investigate the system design\n\nReflect on how the size of the pool \\(M\\) influences the collision risk.\nWhat happens when the transaction volume \\(n\\) increases significantly?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#financial-implications",
    "href": "01-lecture1.html#financial-implications",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.6 Financial implications",
    "text": "2.6 Financial implications\n\nImagine a financial institution processing transactions with \\(M=10^6\\) identifiers.\nIf the daily transaction volume grows to \\(n=10^5\\), what is the risk of a collision?\nDiscuss potential consequences of a collision (e.g., failed transactions, fraud risks) and propose ways to mitigate these risks (e.g., increasing \\(M\\), using hash functions).\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#project-1-detecting-collisions-in-financial-transactions",
    "href": "01-lecture1.html#project-1-detecting-collisions-in-financial-transactions",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.5 Project 1: Detecting collisions in financial transactions",
    "text": "1.5 Project 1: Detecting collisions in financial transactions\n\n1.5.1 Introduction: Real-World Importance of Unique Identifiers in Finance\nIn modern financial systems, every transaction is assigned a unique identifier, allowing stakeholders to track, verify, and manage transaction details efficiently. These identifiers are critical for maintaining transparency, accountability, and security across high-volume transaction systems. However, as the volume of transactions grows, the risk of identifier collisions—where two different transactions are assigned the same identifier—increases.\nThis issue is analogous to the birthday problem: as the number of transactions increases, the likelihood of collisions rises due to the combinatorial nature of the problem. A collision could result in processing errors or even security vulnerabilities, such as fraudulent transactions or misattribution of funds.\nTo mitigate such risks, many financial systems employ hash functions, such as the widely used SHA-256, to map data to a large space of possible outputs. These hash functions, with their vast output range (e.g., $2^{256}), make collisions extremely unlikely, even for billions of transactions. By understanding the principles behind collision probabilities, financial institutions can design systems that maintain transaction integrity even under high workloads.\nIn this project, we will explore this critical aspect of financial systems by simulating transaction identifiers and analyzing collision probabilities. This exercise demonstrates how theoretical probability concepts translate directly to practical challenges in computational finance.8\n8 For more information and details you might want to check this source: https://www.hostmerchantservices.com/articles/what-are-transaction-ids/?utm_source=chatgpt.comIn this project, you will:\n\nSimulate a system assigning transaction identifiers.\nInvestigate the probability of identifier collisions under different scenarios.\nDiscuss implications for system design and security.\n\n\n\n1.5.2 Simulate transaction identifiers\n\nWrite a function to simulate transaction identifiers, where each identifier is randomly chosen from a pool of size \\(M\\) (e.g. \\(M = 10^6\\) or \\(M = 10^9\\))\nUsing your function, simulate \\(n\\) transactions and check for duplicates using R’s duplicated() function.\n\n\n\n1.5.3 Compute collision probabilities\n\nSimulate nn transactions multiple times and estimate the collision probability as the fraction of simulations with at least one collision.\nUse this function to compute the collision probability for \\(n = 10^3, n = 10^6, n = 10^9\\) and \\(M = 10^6, M = 10^9\\)\n\n\n\n1.5.4 Visualize the results\nPlot the collision probability as a function of nn for different values of \\(M\\). We did not cover Rs plotting functions in the lecture so far. We only showed some examples which we did not explain. But we explained how you could research R’s help system and how you could ask an LLM for help. With any of these resources you should be able to come up with some reasonable visualization.\n\n\n1.5.5 Investigate the system design\n\nReflect on how the size of the pool \\(M\\) influences the collision risk.\nWhat happens when the transaction volume \\(n\\) increases significantly?\n\n\n\n1.5.6 Financial implications\n\nImagine a financial institution processing transactions with \\(M=10^6\\) identifiers.\nIf the daily transaction volume grows to \\(n=10^5\\), what is the risk of a collision?\nDiscuss potential consequences of a collision (e.g., failed transactions, fraud risks) and propose ways to mitigate these risks (e.g., increasing \\(M\\), using hash functions).\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#exercises-and-project",
    "href": "01-lecture1.html#exercises-and-project",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.5 Exercises and Project:",
    "text": "1.5 Exercises and Project:\n\n1.5.1 Exercises\n\n1.5.1.1 Exercise 1: Simulate Rolling Two Dice\n\nWrite a function to simulate rolling a pair of dice. Each die produces a random number between 1 and 6. The function should return the sum of the two dice.\nSimulate 10,000 rolls and store the results. Use table() to compute the frequencies of each sum and visualize the results with a histogram.\nCreate a histogram. Use the help function of R to find out how to do this.\n\n\n\n1.5.1.2 Exercise 2: Analyze the Fairness of Dice\n\nCompute the theoretical probabilities of all possible outcomes (sums from 2 to 12) for fair dice. Recall that each outcome corresponds to different combinations of the two dice.\nCompare the theoretical probabilities to the simulated frequencies. Are the dice fair? Justify your conclusion.\n\n\n\n1.5.1.3 Exercise 3: Simulate Loaded Dice\n\nAssume the dice are “loaded” such that numbers 1, 2, 3, 4, and 5 have probabilities of ( ), while 6 has a probability of ( ).\nModify your simulation using the prob argument in sample() to reflect these probabilities.\nSimulate 10,000 rolls of the loaded dice and visualize the results. How does the distribution of sums change compared to fair dice?\n\n\n\n1.5.1.4 Exercise 4: The Probability of Specific Sums\n\nUsing your theoretical understanding of the sample space, calculate the probabilities of:\n\nRolling a sum of 7.\nRolling a sum of 2.\nRepeat for both fair and loaded dice.\n\nVerify your theoretical results with your simulations.\n\n\n\n1.5.1.5 Exercise 5: Evaluating a Betting Strategy\n\nImagine you are at a casino with fair dice. You have €150 to spend, and each bet on the sum of 7 costs €0.15. If you win, the casino pays €1. Otherwise, you lose your bet.\nSimulate 1,000 bets on the sum of 7:\n\nCalculate the total amount you win or lose.\nRepeat the simulation 10,000 times to estimate the expected profit or loss.\n\n\n\nIs betting on 7 a good strategy? Why, why not?\n\n\n\n1.5.1.6 Exercise 6: Explore Alternative Scenarios\n\nAssume the casino uses loaded dice (as defined earlier) instead of fair dice. How does this change your betting strategy and expected profit? Perform the simulation again under these conditions.\n\n-Reflect on the importance of fairness in games of chance and how loaded dice impact the house advantage.\n\n\n\n1.5.2 Project 1: Detecting collisions in financial transactions\n\n1.5.2.1 Introduction: Real-World Importance of Unique Identifiers in Finance\nIn modern financial systems, every transaction is assigned a unique identifier, allowing stakeholders to track, verify, and manage transaction details efficiently. These identifiers are critical for maintaining transparency, accountability, and security across high-volume transaction systems. However, as the volume of transactions grows, the risk of identifier collisions—where two different transactions are assigned the same identifier—increases.\nThis issue is analogous to the birthday problem: as the number of transactions increases, the likelihood of collisions rises due to the combinatorial nature of the problem. A collision could result in processing errors or even security vulnerabilities, such as fraudulent transactions or misattribution of funds.\nTo mitigate such risks, many financial systems employ hash functions, such as the widely used SHA-256, to map data to a large space of possible outputs. These hash functions, with their vast output range (e.g., $2^{256}), make collisions extremely unlikely, even for billions of transactions. By understanding the principles behind collision probabilities, financial institutions can design systems that maintain transaction integrity even under high workloads.\nIn this project, we will explore this critical aspect of financial systems by simulating transaction identifiers and analyzing collision probabilities. This exercise demonstrates how theoretical probability concepts translate directly to practical challenges in computational finance.9\n9 For more information and details you might want to check this source: https://www.hostmerchantservices.com/articles/what-are-transaction-ids/?utm_source=chatgpt.comIn this project, you will:\n\nSimulate a system assigning transaction identifiers.\nInvestigate the probability of identifier collisions under different scenarios.\nDiscuss implications for system design and security.\n\n\n\n1.5.2.2 Simulate transaction identifiers\n\nWrite a function to simulate transaction identifiers, where each identifier is randomly chosen from a pool of size \\(M\\) (e.g. \\(M = 10^6\\) or \\(M = 10^9\\))\nUsing your function, simulate \\(n\\) transactions and check for duplicates using R’s duplicated() function.\n\n\n\n1.5.2.3 Compute collision probabilities\n\nSimulate nn transactions multiple times and estimate the collision probability as the fraction of simulations with at least one collision.\nUse this function to compute the collision probability for \\(n = 10^3, n = 10^6, n = 10^9\\) and \\(M = 10^6, M = 10^9\\)\n\n\n\n1.5.2.4 Visualize the results\nPlot the collision probability as a function of nn for different values of \\(M\\). We did not cover Rs plotting functions in the lecture so far. We only showed some examples which we did not explain. But we explained how you could research R’s help system and how you could ask an LLM for help. With any of these resources you should be able to come up with some reasonable visualization.\n\n\n1.5.2.5 Investigate the system design\n\nReflect on how the size of the pool \\(M\\) influences the collision risk.\nWhat happens when the transaction volume \\(n\\) increases significantly?\n\n\n\n1.5.2.6 Financial implications\n\nImagine a financial institution processing transactions with \\(M=10^6\\) identifiers.\nIf the daily transaction volume grows to \\(n=10^5\\), what is the risk of a collision?\nDiscuss potential consequences of a collision (e.g., failed transactions, fraud risks) and propose ways to mitigate these risks (e.g., increasing \\(M\\), using hash functions).\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "index.html#using-an-llm-to-enhance-your-learning-experience",
    "href": "index.html#using-an-llm-to-enhance-your-learning-experience",
    "title": "An Introduction to Probability",
    "section": "Using an LLM to Enhance Your Learning Experience",
    "text": "Using an LLM to Enhance Your Learning Experience\nIn these lectures I try to help you building experience how to integrate large language models or LLMs into your learning and study process as well as to support you in coding and programming. There are several such models currently on the market. For this course I will use openAI’s chatGPT as a tool. ChatGPT is available in a free version and in an enhanced pay-version.\nThroughout this course, you’ll learn how to use ChatGPT to support your studies. As a tool, it offers capabilities that can make learning probability more interactive and engaging. Here’s how you can leverage it:\n\nClarify Concepts:\nAsk ChatGPT to explain concepts in simple terms or elaborate on topics you find difficult:\n\n“Explain the concept of a probability distribution with an example.”\n\nGenerate Examples:\nUse it to create additional examples for practice:\n\n“Can you give me an example of a random variable and how it applies in finance?”\n\nDebug R Code:\nIf you encounter issues with your R code, paste the code and ask:\n\n“Why does this R code not run, and how can I fix it?”\n\nPractice Exercises:\nRequest custom exercises:\n\n“Create three exercises to practice calculating probabilities for dice rolls.”\n\nSimulate Interactive Discussions:\nSimulate discussions with ChatGPT to test your understanding:\n\n“I think the variance of a constant is zero. Am I correct? Explain why or why not.”\n\nLearn Best Practices in R:\nAsk for tips to improve your coding practices:\n\n“What are the best practices for writing clean and efficient R code?”\n\n\nImportant Note:\nWhile ChatGPT is a powerful learning tool, remember to verify the outputs, especially for complex calculations, and always critically evaluate its suggestions. One such tool you might consider for cross checking is Wolfram | Alpha, which you can find here: https://www.wolframalpha.com/ in a free version.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-lecture1.html#project-designing-transaction-identifyers-for-digital-payment-systems",
    "href": "01-lecture1.html#project-designing-transaction-identifyers-for-digital-payment-systems",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.5 Project: Designing transaction identifyers for digital payment systems",
    "text": "1.5 Project: Designing transaction identifyers for digital payment systems\n\n1.5.1 Introduction: Real-World Importance of Unique Identifiers in Finance\nIn modern financial systems, every transaction is assigned a unique identifier, allowing stakeholders to track, verify, and manage transaction details efficiently. These identifiers are critical for maintaining transparency, accountability, and security across high-volume transaction systems. However, as the volume of transactions grows, the risk of identifier collisions—where two different transactions are assigned the same identifier—increases.\nThis issue is analogous to the birthday problem: as the number of transactions increases, the likelihood of collisions rises due to the combinatorial nature of the problem. A collision could result in processing errors or even security vulnerabilities, such as fraudulent transactions or misattribution of funds.\nTo mitigate such risks, many financial systems employ hash functions, such as the widely used SHA-256, to map data to a large space of possible outputs. These hash functions, with their vast output range (e.g., \\(2^{256}\\)), make collisions extremely unlikely, even for billions of transactions. By understanding the principles behind collision probabilities, financial institutions can design systems that maintain transaction integrity even under high workloads.\nIn this project, we will explore this critical aspect of financial systems by simulating transaction identifiers and analyzing collision probabilities. This exercise demonstrates how theoretical probability concepts translate directly to practical challenges in computational finance.9\n9 For more information and details you might want to check this source: https://www.hostmerchantservices.com/articles/what-are-transaction-ids/?utm_source=chatgpt.comIn this project, you will:\n\nSimulate a system assigning transaction identifiers.\nInvestigate the probability of identifier collisions under different scenarios.\nDiscuss implications for system design and security.\n\n\n\n1.5.2 Simulate transaction identifiers\n\nWrite a function to simulate transaction identifiers, where each identifier is randomly chosen from a pool of size \\(M\\) (e.g. \\(M = 10^6\\) or \\(M = 10^9\\))\nUsing your function, simulate \\(n\\) transactions and check for duplicates using R’s duplicated() function.\n\n\n\n1.5.3 Compute collision probabilities\n\nSimulate nn transactions multiple times and estimate the collision probability as the fraction of simulations with at least one collision.\nUse this function to compute the collision probability for \\(n = 10^3, n = 10^6, n = 10^9\\) and \\(M = 10^6, M = 10^9\\)\n\n\n\n1.5.4 Visualize the results\nPlot the collision probability as a function of nn for different values of \\(M\\). We did not cover Rs plotting functions in the lecture so far. We only showed some examples which we did not explain. But we explained how you could research R’s help system and how you could ask an LLM for help. With any of these resources you should be able to come up with some reasonable visualization.\n\n\n1.5.5 Investigate the system design\n\nReflect on how the size of the pool \\(M\\) influences the collision risk.\nWhat happens when the transaction volume \\(n\\) increases significantly?\n\n\n\n1.5.6 Financial implications\n\nImagine a financial institution processing transactions with \\(M=10^6\\) identifiers.\nIf the daily transaction volume grows to \\(n=10^5\\), what is the risk of a collision?\nDiscuss potential consequences of a collision (e.g., failed transactions, fraud risks) and propose ways to mitigate these risks (e.g., increasing \\(M\\), using hash functions).\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html",
    "href": "02-lecture2.html",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "",
    "text": "2.1 Terminology\nProbability is a mathematical theory that provides a rigorous framework for understanding uncertainty. It is particularly useful when analyzing real-world phenomena, such as random sampling in surveys or fluctuations in stock prices.\nLet us now formalize the foundational concepts of this theory:\nTo clarify this concept, we define uncertain outcomes in advance. We pin down the possible outcomes by agreeing on the outset what we want to consider as the possible outcomes. Take the simple example of considering whether the price of a stock is going to rise or fall at the next day. In a practical situations the outcome of a move in the stock price can be that it rises or falls but it could in principle also stay the same. Still when we think about the experiment of observing the stock price tomorrow in many applications in Finance we usually agree that rise and fall are the only possible outcomes of this experiment. When we want to consider the random experiment that includes the case that the price stays the same we need to agree upfront the the possible outcomes are rise, fall and unchanged.\nThis collection of all possible outcomes in probability theory is called the\nIn the example before when we look at rises and falls in the stock price we would have \\({\\cal S} = \\{rise, fall \\}\\)\nExample: For {S} = {rise,fall}$, the event \\(\\{\\text{rise}\\}\\) is a simple event, while the event \\(\\{\\text{rise, fall}\\}\\) includes all outcomes and corresponds to the entire sample space.\nIn the first lecture we learned about two approaches to measure probability. But the theory of probability actually does not depend on how we measure it precisely. In the theory of probability this measure is an abstract concept.\nThus when we talk about probability in a precise and meaningful sense we can only do so in relation to a given sample space or to a certain conceptual experiment. In theory, probability is an abstract measure of uncertainty that does not depend on how it is observed or estimated. While practical applications may involve approximating probabilities using data, the mathematical theory assumes probabilities are predefined and satisfy specific rules.\nI have itroduced here probabilities with relation to a discrete sample space. The sample spaces discussed in Lecture 1 include finite sets, which can be small or large, such as the extensive output space of the SHA-256 hash function.\nThere are also more complicated discrete sample spaces: Think of the random experiment of tossing a coin as often as necessary to see Heads for the first time. We can begin writing down the basic outcomes as: \\(E_1=H, E_2=TH, E_3 = TTH, E_4 = TTTH, ...\\). An event where Heads never appear, denoted \\(E0\\), may also be considered. In this case, when the basic events can be arranged into a simple sequence. A sample space is called discrete if it contains only finitely many points, or infinitely many points which can be arranged into a simple sequence.\nNot all sample spaces are discrete. Except for the technical tools required there is no essential difference between the two cases. In our discussion of probability in this lecture we consider mostly discrete sample spaces, however we will also discuss some basic non-discrete sample spaces later in the lectures. Now let us state more formally what we mean by probability.\nBefore we go on let me further clarify what we mean when we say that in the theory, probability is an abstract measure of uncertainty which we take as given.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#terminology",
    "href": "02-lecture2.html#terminology",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "",
    "text": "Random Experiment\n\nA process with a set of possible outcomes, where the specific outcome cannot be predicted with certainty beforehand.\n\n\n\n\n\nSample space\n\nThe collection of all possible outcomes of an experiment is called the sample space and is denoted as the set \\({\\cal S}\\)\n\n\n\n\nBasic outcome, event, simple event\n\nA basic outcome is a single possible result of a random experiment. An event is a subset of the sample space, representing one or more outcomes. A simple event is an event containing exactly one basic outcome.\n\n\n\n\n\nProbability\n\nFor a given (discrete) sample space ${S}, Probability is a function that assigns a value to each event, representing its likelihood. The function has to fulfill three properties: 1. \\(P({\\cal S}) = 1\\). 2. For any event \\(A \\in {\\cal S}\\), \\(0 \\leq P(A) \\leq 1\\). The probability of an event can never be negative or larger than 1. 3. For mutually exclusive events \\(A\\) and \\(B\\), \\(P(A \\cup B) \\leq P(A) + P(B)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#probability-theory-and-applications",
    "href": "02-lecture2.html#probability-theory-and-applications",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.2 Probability in theory and applications of probability",
    "text": "2.2 Probability in theory and applications of probability\nProbabilities are expressed as numbers between 0 and 1. As mentioned by Feller (1968) in his famous probability textbook, these numbers are of the same nature as distances in geometry. In the theory we assume they are given to us.\nFrom the viewpoint of probability theory, we need not assume anything about how they are measured. In this sense probabilities in the theory of probability are an abstract measure of uncertainty.\nIn practical applications, determining probabilities or applying theory often requires sophisticated statistical methods. So, while the mathematical as well as the intuitive meaning of probability are clear only as we proceed with the theory we will get a better ability to see how we can apply this concept.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#basic-rules-of-probability",
    "href": "02-lecture2.html#basic-rules-of-probability",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.3 Basic rules of probability",
    "text": "2.3 Basic rules of probability\nWe already saw that the classical notion of probability directly lead to a few simple rules. In fact in the theory of probability we require these rules to hold more generally no matter how probability is measured in actual applications.\nWhen we talk of probabilities we mostly consider probabilities with relation to a discrete sample space. The sample spaces discussed in Lecture 1 include finite sets, which can be small or large, such as the extensive output space of the SHA-256 hash function..\nThere are also more complicated discrete sample spaces: Think of the random experiment of tossing a coin as often as necessary to see Heads for the first time. We can begin writing down the basic outcomes as: \\(E_1=H, E_2=TH, E_3 = TTH, E_4 = TTTH, ...\\). An event where Heads never appear, denoted \\(E0\\), may also be considered. In this case, when the basic events can be arranged into a simple sequence. A sample space is called discrete if it contains only finitely many points, or infinitely many points which can be arranged into a simple sequence.\nNot all sample spaces are discrete. Except for the technical tools required there is no essential difference between the two cases. In our discussion of probability in this lecture we consider mostly discrete sample spaces, however we will also discuss some basic non-discrete sample spaces later in the lectures. Now let us state more formally what we mean by probability.\n\nProbability\n\nGiven a (discrete) sample space \\({\\cal S}\\) the probability assigned to events in this sample space must always fulfill three rules:\n\n\n\n\\(P({\\cal S}) = 1\\), where \\({\\cal S}\\) is the sample space.\nFor any event \\(A \\in {\\cal S}\\), \\(0 \\leq P(A) \\leq 1\\). The probability of an event can never be negative or larger than 1.\nThe probability of the union of two events \\(A\\) and \\(B\\) is always smaller or equal to the sum of the probability of these events looked at in isolation: \\(P(A \\cup B) \\leq P(A) + P(B)\\).\n\nNote that these three properties resulted naturally from the classic idea to make equally probable cases and count. Property 3 looks slightly different but we will shortly see that it amounts to the same thing.\nIn the theory of probability we use the language of sets and set theory to describe relations among event. We have used one of these relations in rule three, where we expressed the relation between two sets as their union \\(\\cup\\). It is useful in studying probability to know a few of these set theoretic definitions.\nLet’s go through them and illustrate the concepts in the context of the examples we have already developed in lecture 1.\n\nUnion\n\nThe union of two events \\(A\\) and \\(B\\) is the set that contains all events that are either in \\(A\\) or in \\(B\\) or in both sets. Set union is written as \\(A \\cup B\\).\n\n\nLet us use the example of the experiment of rolling a die. The sample space \\({\\cal S}\\) is the set of all possible outcomes of rolling the dice \\({\\cal S}=\\{1,2,3,4,5,6\\}\\). Assume one event is that the outcome is 1, 2 or 3. In set notation, we would write \\(A = \\{1,2,3\\}\\). Let us also assume that the second event is that the outcome is some even number, i.e. 2, 4, 6. Again using set notation we would write \\(B = \\{2,4,6\\}\\). The event \\(A \\cup B\\) is then the set of all outcomes such that the outcome is in \\(A\\) or in \\(B\\) or in both or these sets. In such simple examples it is sometimes helpful to depict the situation graphically.\n\n\n\n\n\nThe meaning of set union\n\n\n\n\nThe sample space \\({\\cal S}\\) is the gray set containing all possible outcomes of our random experiment. Graphically the union of \\(A\\) and \\(B\\), \\(A \\cup B\\) is a subset of the sample space, the entire colored area.\nBy the way and as an aside: R provides functions for computing set operations. Let us use the occasion to show you briefly how to use these functions in the context of this example: We define the sets \\(A\\) and \\(B\\) first using the assignment operator:\n\nA &lt;- c(1,2,3)\nB &lt;- c(2,4,6)\n\nWe compute the union by using the function union()\n\nunion(A,B)\n\n[1] 1 2 3 4 6\n\n\nwhich gives us the result we have already derived graphically.\n\nIntersection\n\nThe intersection of two events is the set that contains all events that are both in \\(A\\) and in \\(B\\). Set intersection is written as \\(A \\cap B\\).\n\n\nWe can again illustrate this concept graphically\n\n\n\n\n\nThe meaning of set intersection\n\n\n\n\nThe intersection of \\(A\\) and \\(B\\), \\(A \\cap B\\) is the orange area containing the dice face with two points. Indeed two is both in \\(A\\) and in \\(B\\), which is exactly the meaning of set intersection.\nThe R function for computing intersections is called intersect(). We call this function for our example\n\nintersect(A,B)\n\n[1] 2\n\n\nwhich gives us the result we have already derived graphically.\n\nComplement\n\nThe complement of an event \\(A\\) with respect to an event \\(B\\) is the set of all elements that are in \\(B\\) but not in \\(A\\). Set difference is written as \\(A \\setminus B\\).\n\n\nLets say we want to get the complement, or the set difference of \\(A \\cup B\\) with respect to the sample space \\({\\cal S}\\). Let us show \\({\\cal S} \\setminus (A \\cup B)\\).\n\n\n\n\n\nThe meaning of complement\n\n\n\n\nThis complement is the dice shown in the light redish area, i.e. all the elements of \\({\\cal S}\\) which are not in \\(A \\cup B\\).\nThe R function for computing set complements is called setdiff(). You can try it with our example:\n\nS &lt;- c(1,2,3,4,5,6)\n\nsetdiff(S, union(A,B))\n\n[1] 5\n\n\nwhich is 5, as expected.\n\nMutually Exclusive\n\nTwo events \\(A\\) and \\(B\\) are said to be mutually exclusive if they have no basic outcomes in common. The notation is \\(A \\cap B = \\emptyset\\)\n\n\nAn example in our context is the set of even outcomes \\(B=\\{2,4,6\\}\\) and the set of odd outcomes, let us call it \\(C=\\{1,3,5\\}\\). If we intersect these sets\n\nB &lt;- c(2,4,6)\nC &lt;- c(1,3,5)\n\nintersect(B,C)\n\nnumeric(0)\n\n\nwe get the empty set, which is expressed by R by giving the data type, in this case numeric, because we are intersecting sets of numeric values, followed by (0). This means, there is no numeric value in the intersection of \\(B\\) and \\(C\\).\nLet us discuss probability rule 3 a bit further: Go to the picture we drew to illustrate the meaning of set union in figure @ref(fig:set-union). To compute the probability \\(P(A \\cup B)\\) we have to add the probabilities of all sample points that are contained either in \\(A\\) or in \\(B\\) but each point is to be counted only once. Therefore, probability rule 3, has a (weak) inequality: \\(P(A \\cup B) \\leq P(A) + P(B)\\).\nNow let \\(E\\) be a point contained both in \\(A\\) and in \\(B\\), in our example this would be \\(E = \\{2\\}\\), then \\(P(E)\\) occurs twice on the right hand side of our inequality but only once on the left. Therefore the right hand side exceeds the left by the amount \\(P(A \\cap B\\)). Thus we have \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).\nThis reasoning holds for arbitrary pairs of events and not only in our example. In the case \\(A\\) and \\(B\\) are arbitrary events and \\(A\\) and \\(B\\) are mutually exclusive, i.e. \\(A \\cap B = \\emptyset\\) then our inequality in rule 3 becomes an equality and we have \\(P(A \\cup B) = P(A) + P(B)\\).\nNow you recognize property 3 in lecture 1, where we said that: ” If \\(A\\) and \\(B\\) never occur in the same case, then \\(P(A \\,\\text{or}\\, B) = P(A) + P(B)\\).” The meaning of \\(A\\) and \\(B\\) not occurring in the same case is in set theoretic terms exactly the requirement that they are mutually exclusive events.\nTherefore in many treatments of probability you will find rule 3 formulated as:\n3-a: The probability of the union of two mutually exclusive events \\(A\\) and \\(B\\) is \\(P(A \\cup B) =  P(A) + P(B)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#frequency",
    "href": "02-lecture2.html#frequency",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.5 Probability and Frequency",
    "text": "2.5 Probability and Frequency\nNow it is time to explain a connection we made so far informally when we looked at visualizations and simulations, where we used connection between relative frequencies of probabilities in repetions of a random experiment.\nThe frequency interpretation of probability is a practical approach to understanding uncertainty. It defines the probability of an event \\(A\\) as:\n\\[\\begin{equation*}\nP(A) = \\frac{\\text{Number of times $A$ occurs in repeated identical trials}}{\\text{Total number of trials}}\n\\end{equation*}\\]\nThis interpretation is intuitive and often used in fields like engineering, finance, and natural sciences. However, it raises important questions about the connection between observed frequencies and theoretical probabilities.\nThe origins of this discussion reach back to the seventeenth century. The philosophers Gottfried Wilhelm Leibnitz (1646 - 1716) and Jacob Bernoulli (1655 - 1705) had great hopes for the new field of probability to find applications in fields like medicine, law, commerce and finance. This interest in exploring new fields of potential applications drove them to study frequency evidence of events. They felt that relying on intuitively equally probable cases might not be enough for these ambitious application attempts.\nJacob Bernoulli gave an answer which is among the great ideas in probability theory (see Diaconis and Skyrms (2019)), the weak law of large numbers. It establishes one of the most important connections between frequency and probability.\n\n2.5.1 The Weak Law of Large Numbers (WLLN)\nThe Weak Law of Large Numbers provides a rigorous mathematical foundation for the frequency interpretation of probability. It states:\nAs the number of independent and identically distributed (i.i.d.) trials increases, the relative frequency of an event converges to its true probability with high probability.\nNow let us pause here and restate what the weak law says: The law says that\n\nOver many trials, the observed frequencies of outcomes will get closer to their theoretical probabilities.\nThis convergence occurs with high likelihood as the number of trials increases.\n\nObserve also what the weak law does not say:\n\nFrequencies are not probabilities. Instead frequencies approximate probabilities as trials increase.\nThe weak law does not guarantee exact convergence in finite samples - it describes long run outcomes of repeating identical experiments many times.\n\nLet#s go back to the exmple of the fair coin we played with in lecture 1: There we constructed a fair coin toss where \\(P(\\text{Heads})=0.5\\). When we have just a few tosses, say 10 times, you might observe 6 Heads (60%), which is close to but not exactly 50%. As you increase the tosses, say to 100, the frequency might be 52 Heads (52%), closer to 50%. if you go to even more tosses, say 10,000 tosses, the frequency approaches 50%.\nThis demonstrates that, as the number of trials increases, the relative frequency converges to the theoretical probability.\nIt is crucial to understand that Bernoulli’s Law of Large Numbers does not assert that frequencies are probabilities. Rather, it describes how, under certain conditions, frequencies fall within specific bounds relative to probabilities. Specifically, given a theoretical probability, a desired margin of error, and a confidence level, Bernoulli’s result provides an upper bound on the number of trials required for the relative frequency of an event to approximate its true probability.\nThis distinction highlights an important limitation: the Weak Law of Large Numbers addresses the problem of inference from probabilities to frequencies, not the reverse. That is, it shows how probabilities can predict the behavior of frequencies in repeated trials, but it does not justify using observed frequencies as definitive probabilities. Instead, the theorem ensures that as the number of trials grows, the relative frequency of an event will converge to its probability, within specified bounds.\nAlthough we will frequently use frequency-based measures of probability in practical applications, it is essential to recognize the conceptual gap between the idealized mathematical theory and the empirical realities it seeks to describe. Probability theory relies on the concept of limiting relative frequencies, which exist only in an idealized framework of infinite trials. Real-world applications, however, involve finite data and inherently require interpretation, judgment, and domain expertise.\nThis distinction between theory and practice is more than a technical nuance—it addresses a fundamental challenge for any theory with practical ambitions: How does the idealized framework relate to the messy complexities of reality? In probability, as in other fields, there is no direct or naive application of theory to practice. Bridging this gap demands a sound understanding of the theory, along with careful consideration of real-world conditions and context.1\n1 For a formal statement of the weak law of large numbers, we need more concepts which we have not yet introduced, in particular the notion of a random variable. Let me give the formal statement here for those of you who are interested and know the concept of a random variable already: Let \\(X_1, X_2, \\dots, X_n\\) be \\(n\\) independently, identically distributed random variables with expected value \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\bar{X}_n\\) denote the sample mean. Then the weak law of large numbers states: \\(P(|\\bar{X}_n - \\mu | \\geq \\epsilon) \\to 0 \\quad \\text{as } n \\to \\infty\\). This means that the probability of the sample mean deviating significantly from the expected value diminishes as the number of trials increases.\n\n2.5.2 Exploring the Weak Law of Large Numbers with R\nIn the following code chunk we first define a coin as we did in lecture 1 with the understanding that 1 represents Heads and 0 represents Tails. Then we use the replicate function and the sample function to toss the coin 1000 times.\nNow in the next step we use the logical condition results == 1 to create a logical vector indicating whether each toss resulted in Heads. cumsum()then calculates the cumulative count of Heads after each toss. Dividing this cumulative count by (1:n) (the toss number) gives the cumulative relative frequency of Heads at each step.\nThen we plot these frequencies againts the number of tosses. The \\(x\\)-axis represents the number of tosses, while the \\(y\\)-axis represents the relative frequency of heads. We draw a red horizontal line where the relative frequency is identical to the theoretical probability of \\(0.5\\).\n\n# Define the coin\ncoin &lt;- c(1, 0)\n\n# Toss the coin n times\nn &lt;- 1000\nresults &lt;- replicate(n, sample(coin, size = 1))\n\n# Calculate cumulative frequency of Heads\n\nheads_freq &lt;- cumsum(results == 1) / (1:n)\n\n# Plot the convergence\nplot(1:n, heads_freq, type = \"l\", ylim = c(0.4, 0.6),\n     xlab = \"Number of Tosses\", ylab = \"Frequency of Heads\",\n     main = \"Convergence of Relative Frequency to True Probability\")\nabline(h = 0.5, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nThis plot illustrates how the relative frequency of Heads approaches \\(P(\\text{Heads})=0.5\\) as the number of tosses increases.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#independence",
    "href": "02-lecture2.html#independence",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.6 Independence",
    "text": "2.6 Independence\nThe idealized thought experiment behind the Weak Law of Large Numbers assumes the idea of independence. Let us define this concept more precisely and explain its implications. While we have already used independence implicitly in earlier examples, it is important to formalize and understand it carefully.\n\n2.6.1 Intuition Behind Independence\nTwo events are said to be independent if the occurrence of one event does not influence the probability of the other event occurring. In other words, knowing that one event has occurred provides no information about the likelihood of the other event.\nFor example, consider rolling a fair six-sided die twice. The outcome of the first roll does not affect the outcome of the second roll because the rolls are independent. The probability of any number appearing on the second roll remains \\(1/6\\), regardless of what happened on the first roll.\n\n\n2.6.2 A Worked Example\nLet us calculate the probability of rolling a 5 on the first roll and a 6 on the second roll.\nThis is the probability of the event “5 on the first roll and 6 on the second roll,” which we write as \\(P(5 \\cap 6)\\).\nSince the rolls are independent: \\(P(5 \\cap 6) = P(5) \\times P(6)\\).\n\nThe probability of rolling a 5 on a fair six-sided die is \\(P(5) = 1/6\\).\nThe probability of rolling a 6 on a fair six-sided die is \\(P(6) = 1/6\\).\n\nThus: \\(P(5 \\cap 6) = \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{1}{36}\\).\nThis calculation uses the multiplication rule for independent events, which states that if two events \\(A\\) and \\(B\\) are independent, then the probability of both occurring is the product of their individual probabilities.\n\n\n2.6.3 Definition of Independence\nWe now formalize this concept:\nIndependence:\nTwo events \\(A\\) and \\(B\\) are independent if and only if: \\(P(A \\cap B) = P(A) \\times P(B)\\).\nThis definition formalizes the idea that the occurrence of one event does not affect the likelihood of the other. Independence is an assumption we often make in theoretical models like the Weak Law of Large Numbers and many other probabilistic frameworks.\n\n\n2.6.4 Important Note: Independence vs. Multiplication Rule\nWhile independence allows us to use the multiplication rule \\(P(A \\cap B) = P(A) \\times P(B)\\), it is critical to remember that the reverse is not true:\n- Just because you can multiply probabilities does not necessarily mean the events are independent.\n- For independence to hold, the probabilities of the events must truly be unaffected by one another.\n\n\n2.6.5 Why Is Independence Important Here?\nIndependence is a key assumption in the Weak Law of Large Numbers, where we consider a series of independent and identically distributed (i.i.d.) trials. Without independence, the law’s guarantees about the behavior of frequencies would not hold. As we move forward, we will see independence as a foundational concept in many probabilistic models and calculations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#moreR",
    "href": "02-lecture2.html#moreR",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.7 Some more concepts from R: Reading Data, R Objects, Subsetting and Modifying Values",
    "text": "2.7 Some more concepts from R: Reading Data, R Objects, Subsetting and Modifying Values\nBut now enough of dices and coins. Let us try out and apply some of the new ideas we just have learned and use the opportunity to learn some more important R concepts in the context of an example with stock price data.\n\n2.7.1 Reading data in R\nBefore we can do anything with data, we need to learn how to load data into R and how to save them. We will discuss now how to do this with comma separated text files. R provides packages for reading and writing from almost any other format, like Excel, STATA, SAS or MATLAB. In all those cases the same principles apply as in the csv case.\nI have prepared a data-file recording daily information on the stock price of Apple in csv, which I have downloaded from the internet. This file is called aapl_prices.csv. Let me at this stage skip the details of how I did that exactly. We will discuss getting financial data from the internet later.\nTo read a plain text or csv file, R provides the function read.csv(). If the csv file comes with a European instead of an US decimal format (, instead of . for the decimal sign.) you need read.csv2().\nIn the simplest form you read the data and write them to an object you can work with in R. For this you need to type something like this:\n\naapl_prices &lt;- read.csv(\"data/aapl_prices.csv\")\n\nThe function needs as an argument the file name. If the file is in a sub-folder of the current directory you need to also specify the path. To specify the correct path to the file you need to know in which part of your directory tree you are currently working.\nIn my case I am working in the project folder for my lecture notes, which has a sub-folder called data and thus I specify the path relative to this location. To find out what is your current R working directory, R provides the function getwd(). If I type this in my case, I will get\n\ngetwd()\n\n[1] \"/home/martinsummer/Code/R/Probability_Introduction\"\n\n\nthe path of my project folder for this lecture notes. So if I type the string \"data/aapl_prices.csv\" this specifies the path relative to my working directory.\nIf you read the file on your computer, you need to specify the path appropriately from where you are working in R at the moment to where you have stored the csv file.\nNow read.csv() has many additional arguments, which provide you with lots of flexibility. I encourage you to check it out and play with it using the help function and the examples given therein.\nWe have now read the apple stock price data and written it to the R-object aapl_prices. Lets inspect the object a bit to see what we’ve got. I use the function head() with the parameter value n = 10. This will show me the first 10 rows of the datafile.\n\nhead(aapl_prices, n = 10)\n\n   symbol       date     open     high      low    close    volume adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.266423\n2    AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.268211\n3    AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.269106\n4    AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.270000\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.271788\n6    AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.269106\n7    AAPL 1990-01-10 0.335938 0.335938 0.319196 0.321429 199718400 0.257483\n8    AAPL 1990-01-11 0.323661 0.323661 0.308036 0.308036 211052800 0.246755\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.246755\n10   AAPL 1990-01-15 0.308036 0.319196 0.305804 0.305804 161739200 0.244967\n\n\nWithout going into the details, we see that we get a data table, which records for every day beginning in January 1990 the opening, the highest and the lowest price, the closing price, the volume (number of shares) and the closing price adjusted for dividend and stock splits. Let’s not worry for the moment what the exact technical meaning of these financial terms is. Now we are ready to work with these data.\n\n\n2.7.2 R objects\nThe most basic type of R objects are atomic vectors. Most structures in R are built from atomic vectors. The stock data-file we have just loaded is an example of such a more complex structure built from atomic vectors. We have already encountered a few of those in our previous lecture.\n\n2.7.2.1 Atomic vectors\nAn atomic vector is just a simple vector of data. For example our die constructed in the first lecture is an instance of this.\n\ndie &lt;- 1:6\n\nR has a function, which allows you to check whether an object is an atomic vector or not. This function is called is.vector(). It takes the object name as an argument and returns TRUE if the object is an atomic vector and FALSE if it is not. For example:\n\nis.vector(die)\n\n[1] TRUE\n\n\ndoes indeed return TRUE.\nEach atomic vector stores values in a one-dimensional vector, and each atomic vector can only store one type of data. The length of the atomic vector can be determined by the function length() This function takes an R object, which is an atomic vector, as an argument and returns the number of elements in this vector. Here is the example of the die\n\nlength(die)\n\n[1] 6\n\n\nwhich is 6 as it should be. An atomic vector could also have only one element, in which case lenght()would return 1.\nNow altogether R has implemented six basic types of atomic vectors:\n\ndouble\nintegers\ncharacters\nlogical\ncomplex\nraw\n\nWe will not encounter complex and raw data-types in this course, so let us skip those and discuss only the first 4 types.\nIf yo u go back to our stock data and look at the first three lines\n\nhead(aapl_prices, n=3)\n\n  symbol       date     open     high      low    close    volume adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.266423\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.268211\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.269106\n\n\nThe variables open, high, close, volume and adjusted are all stored as the first R data type: Doubles. A double vector stores regular numbers. It seems natural to store such quantities as opening and closing prices as doubles. Doubles can be positive or negative, they can have digits right of the decimal place. If not explicitly told otherwise, R saves any number you give it as a double. Note that some functions refer to doubles with the more intuitive term numeric. It has an equivalent meaning in R.\nFor example the number of traded shares in our apple stock price file is saved as a double. But arguably the number of shares could also have been saved as an Integer. This is the second data type in R for number which can be written without decimals. If you explicitly specify number as integers, you need to append the letter L to the number. Say, for example, we would want to specifie the points shown on our die explicitly as integers, then we would type\n\ndie_int &lt;- c(1L,2L,3L,4L,5L,6L)\n\nNow R will save the die with integer values. Before, since we did not explicitly specify it, the values were stored probably as doubles. You can check the type always with the function typeof():\n\ntypeof(die_int)\n\n[1] \"integer\"\n\n\nNow why should we care for distinguishing integers from doubles? This has to do with the way a computer does computations. Sometimes a difference in precision can have surprising effects. In your computer 64bits of memory are allocated for each double in an R program. While this allows for a very precise representation of numbers not all numbers can be exactly represented with 64-bits. The famous candidates are \\(\\pi\\), which has an infinite sequence of digits and must therefore be rounded by the computer. Usually the rounding error introduced into your computations will go unnoticed but sometimes surprises can occur. Take for instance:\n\nsqrt(2)^2 - 2\n\n[1] 4.440892e-16\n\n\nWhy is that? The square root of 2 can not be expresses precisely because, as already the old Greeks knew, it is not a rational number. And voila, you have a small rounding error. Such errors are called floating point errors in computer science lingo and computing with such numbers is called floating-point-arithmetic.\nWith integers floating point errors are avoided, but for many applications this is not an option. Luckily for most cases floating-point arithmetic provides sufficient precision for most of the applications we encounter in practice.\nWhen you look at the first column of our stock price data you see the ticker symbol of the apple stock AAPL. This is the symbol of the third data type: Character. A character vector stores strings of text, which have to be put between quotation marks \"\". Strings are the individual elements of a character vector. We have encountered character vectors already in this lecture, where we represented the fair coin with the vector c(\"H\", \"T\") with the string \"H\" for head and the string \"T\" for tail.\nNote that a string can be more than just letters. If you type, for instance the number 1 with quotation marks, like \"1\" R would interpret the value as a string not as a number. Sometimes one can get confused in R because both objects and characters appear as text in R code. Object names are without quotation marks strings always are between quotation marks.\nThe next data type in the list are: Logicals. Logical vectors store TRUEs and FALSES. They are extremely useful for doing comparisons and - as we will see shortly - also for subsetting.\nFor example, if you type:\n\n0 &gt; 1\n\n[1] FALSE\n\n\nR tells you that this statement is False.\nWhenever you type TRUE of FALSE without quotation marks, R will treat the input as logical data. For instance, the following statement yields:\n\nlogic &lt;- c(TRUE, FALSE, TRUE)\nlogic\n\n[1]  TRUE FALSE  TRUE\n\n\nYou can check the type:\n\ntypeof(logic)\n\n[1] \"logical\"\n\n\nOne important R-fact which you need to know about atomic vectors is that atomic vectors can have attributes. Attributes won’t affect the values of an object but can hold and store object metadata. Normally we do not look at these metadata, but many R functions check for attributes and then do special things with the object depending on these attributes.\nAttributes can be checked with the function attributes() using an R object as an argument.\n\nattributes(die)\n\nNULL\n\n\nSince diehas no attributes R returns NULL. This value is often returned by functions whose values are undefined.\nThe most common attributes for atomic vectors are names, dimensions and classes. Each of these attributes has its own helper function that you can use to give attributes to the object.\nWhen you look up the value for the die you will get\n\nnames(die)\n\nNULL\n\n\nWe see that our die object has no names, but we could assign some. For example:\n\nnames(die) &lt;- c(\"one\", \"two\", \"three\", \"four\", \"five\", \"six\")\n\nNow the die has a name attribute:\n\nattributes(die)\n\n$names\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\"  \"six\"  \n\n\nNote that the values are not affected nor will the names be affected when the values are manipulated. For instance, if you type\n\ndie + 1\n\n  one   two three  four  five   six \n    2     3     4     5     6     7 \n\n\nthe names stay what they were and all values are increased by 1. To remove the names you can type:\n\nnames(die) &lt;- NULL\n\nOne very important attribute, we will encounter all the time is dimension, with the helper function dim(). For example we can look at our data object of stock prices to get:\n\ndim(aapl_prices)\n\n[1] 8044    8\n\n\nwhich returns two numbers, which mean that the object has 8044 rows and 8 columns.\nBut with dim we can also transform our die into a matrix, with say two rows and three columns:\n\ndim(die) &lt;- c(2,3)\ndie\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nR will always use the first value in dim for the rows and the second for the columns. dim can be generalized to higher dimensional array structures.\nIf you want to have more control on how R fills up the values, you can use a richer helper function, which gives you more control. For two dimensional data arrays this is provide by the function matrix(). Say you type\n\nm &lt;- matrix(die, nrow = 2)\nm\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nthe matrix will be filled up by column. You can change that behavior by setting the byrow argument to TRUE.\n\nm &lt;- matrix(die, nrow = 2, byrow = TRUE)\nm\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n\nNotice that changing the dimensions of an object, will not change its type. But it will change the object’s class attribute:\n\ndim(die) &lt;- c(2,3)\ntypeof(die)\n\n[1] \"integer\"\n\nclass(die)\n\n[1] \"matrix\" \"array\" \n\n\nA class in R is a special case of an atomic vector.\nWith the attribute system R allows you to represent more data types. R uses, for example a special class to represent dates and times. The data variable in our stock data is - for example - represents as a type of this kind.\nTo illustrate this we take the R-function Sys.time(). This function returns the current time on your computer. It looks like a character string when you display it but it is actually a double with class POSIXct, POSIXt (it has two classes):\n\nnow &lt;- Sys.time()\nnow\n\n[1] \"2024-12-28 23:16:57 CET\"\n\ntypeof(now)\n\n[1] \"double\"\n\nclass(now)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nPOSIXct is a widely used framework for representing dates and times. But we will skip the details here.\nR stores categorical data, such as nationality, sex etc. by using factors. If you take for instance, sex, it can have only two values - male or female - and these values may have their idiosyncratic order, for example that females go always first. To make a factor in R you have to pass an atomic vector to the factor() function. This function works by recoding the values in the vector as integers and store the results in an integer vector. R also adds a level attribute which contains the set of labels and their order and a class attribute that says the vector is a factor. Example:\n\nsex &lt;- factor(c(\"m\", \"f\", \"f\", \"m\"))\ntypeof(sex)\n\n[1] \"integer\"\n\nattributes(sex)\n\n$levels\n[1] \"f\" \"m\"\n\n$class\n[1] \"factor\"\n\n\nFactors can be confusing since they look like characters but behave like integers.\nNote that R will often try to convert character strings to factors when you load and create data. I recommend that you do not allow R to make factors unless you explicitly ask for it. This can usually be controlled by an argument to whatever the data reader function is. For instance you can give the read.csv() function the argument stringsAsFactors = FALSE.\nR has an internal coercion behavior for data types, which you should know about if you work with R. With this knowledge you can do many useful things.\nIf a character string is present in an atomic vector, R will automatically convert every other component in this vector to a character string. If a vector contains only logicals and numbers, R will convert the logicals to numbers. In this case every TRUE becomes a 1 and every FALSE becomes a 0.\nR also uses the coercion rules, when we do math with logicals, like for example\n\nsum(c(TRUE, TRUE, FALSE, FALSE))\n\n[1] 2\n\n\nWhat happens here is that R coerces the vector c(TRUE, TRUE, FALSE, FALSE) to the vector c(1, 1, 0, 0) and sums the components.\nGoing back to our data set on the apple stock price, we see that this dataset stores values of different types, characters, dates and doubles. How does R achieve this?\nThe answer is that this is achieved by a data structure called a list. List are like atomic vectors, because the group data into a one-dimensional set. However, lists do no group together individual values. List group together R objects, such as atomic vectors or even other lists.\nFor example, you can create a list, which contains a numeric vector of length 31 in its first element, a character vector of length 1 in its second element and a new list of length 2 in its third. This is done by using the list()function of R, like this:\n\nlist_example &lt;- list(100:130, \"R\", list(TRUE, FALSE))\nlist_example\n\n[[1]]\n [1] 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n[20] 119 120 121 122 123 124 125 126 127 128 129 130\n\n[[2]]\n[1] \"R\"\n\n[[3]]\n[[3]][[1]]\n[1] TRUE\n\n[[3]][[2]]\n[1] FALSE\n\n\nThe double bracketed indices tell you which element of the list is being displayed. The single bracketed indices tell you which sub-element of the list is being displayed. For example 100 is the first subelement of the first element in the list. “R” is the first subelement of the second list element.\nData Frames are the two dimensional version of a list. They are by far the most useful storage structure for data analysis. Indeed, our dataset on the Apple stock price we have loaded before is an instance of a data frame. Data frames group vectors together in a two dimensional table. As a consequence each variable can, i.e. each column of the data frame can contain a different data type. Within a column, however, we can have only one data type.\nif you look at the type of a data frame, you will see that it is indeed a list and the class is a data frame.\n\ntypeof(aapl_prices)\n\n[1] \"list\"\n\nclass(aapl_prices)\n\n[1] \"data.frame\"\n\n\nNow armed with our new knowledge of R and probability lets ask a question to our new data set.\n\n\n\n2.7.3 Example: Will the stock price of Apple move up or down?\nLet us look at the first lines of our data once again:\n\nhead(aapl_prices, n = 10)\n\n   symbol       date     open     high      low    close    volume adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.266423\n2    AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.268211\n3    AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.269106\n4    AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.270000\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.271788\n6    AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.269106\n7    AAPL 1990-01-10 0.335938 0.335938 0.319196 0.321429 199718400 0.257483\n8    AAPL 1990-01-11 0.323661 0.323661 0.308036 0.308036 211052800 0.246755\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.246755\n10   AAPL 1990-01-15 0.308036 0.319196 0.305804 0.305804 161739200 0.244967\n\n\nLet us check what kind of object aapl_prices is.\n\ntypeof(aapl_prices)\n\n[1] \"list\"\n\nclass(aapl_prices)\n\n[1] \"data.frame\"\n\n\nAs expected the object is a list. The class shows that our object is a data.frame. Since the object is of class data-frame we can use the dim function to see the number of daily observations in our data set.\n\ndim(aapl_prices)\n\n[1] 8044    8\n\n\nWe see that our data store price information about 8044 trading days.\nWe could now approach the question about the probability of the Apple stock price moving up tomorrow by taking a frequency-approach. This would require us first to count the number of up-movements in our data. How do we do that?\nNow we need to know know how we address and work with individual values in our data-set. Let’s say we measure the movement of the close price. R has a notation system to extract a value or subset of values from an object: You write the name of the object first, followed by a pair of hard brackets. Between the brackets goes a ,separating row and column indices. The notation is thus like aapl_prices[ , ].\nWhen it comes to writing the indices you have six different ways to do this, all of them very simple. You can use:\n\nPositive integers\nNegative integers\nZero\nBlank spaces\nLogical values\nNames\n\nThe simplest are positive integers. For example if you choose\n\naapl_prices[1,6]\n\n[1] 0.332589\n\n\nyou see that the first value in the sixth column is selected. You thus get the first closing price of the Apple stock recorded in our dataset. You can extract more than one value, for instance by typing:\n\naapl_prices[1:10, 6]\n\n [1] 0.332589 0.334821 0.335938 0.337054 0.339286 0.335938 0.321429 0.308036\n [9] 0.308036 0.305804\n\n\nwhich gives you the first 10 closing prices.\nNote that R’s notation system is not limited to data frames. The same syntax can be used to select values from any R object, provided you supply an index for each dimension of the object. Two things have to be kept in mind. In R indexing begins at 1. In some other programming languages indexing begins at 0. The indexing convention in R is just like in linear algebra. The second thing to note is that if you select two or more columns from a data frame, R will return a new data frame, like in\n\naapl_prices[1:10,1:8]\n\n   symbol       date     open     high      low    close    volume adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.266423\n2    AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.268211\n3    AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.269106\n4    AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.270000\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.271788\n6    AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.269106\n7    AAPL 1990-01-10 0.335938 0.335938 0.319196 0.321429 199718400 0.257483\n8    AAPL 1990-01-11 0.323661 0.323661 0.308036 0.308036 211052800 0.246755\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.246755\n10   AAPL 1990-01-15 0.308036 0.319196 0.305804 0.305804 161739200 0.244967\n\n\nHowever, if you select a single column, R will return a vector:\n\naapl_prices[1:10, 6]\n\n [1] 0.332589 0.334821 0.335938 0.337054 0.339286 0.335938 0.321429 0.308036\n [9] 0.308036 0.305804\n\n\nIf you prefer to get returned a data frame in this case, you have to add the argument drop = FALSE, like:\n\naapl_prices[1:10, 6, drop = FALSE]\n\n      close\n1  0.332589\n2  0.334821\n3  0.335938\n4  0.337054\n5  0.339286\n6  0.335938\n7  0.321429\n8  0.308036\n9  0.308036\n10 0.305804\n\n\nNegative integers work exactly opposite to positive integers. If you type:\n\nhead(aapl_prices[-1,6], n = 10)\n\n [1] 0.334821 0.335938 0.337054 0.339286 0.335938 0.321429 0.308036 0.308036\n [9] 0.305804 0.311384\n\n\nR will return the sixth column of the data frame except the first row. We just display the first 10 values.\nIf you try to pair a negative and a positive integer in an index, R will return an error. However, you can use both negative and positive integers if you use them in different indices.\nZero is neither positive nor negative, If you use 0 as an index, R will return nothing from a dimension with index 0. The following syntax for instance creates an empty object\n\naapl_prices[0,0]\n\ndata frame with 0 columns and 0 rows\n\n\nBlank spaces are used if you want to ask R to select every value in a dimension. So for instance, if you type:\n\nsel &lt;- aapl_prices[ , 6]\n\nR will select the entire column of closing prices. You can check that the length of this vector is\n\nlength(sel)\n\n[1] 8044\n\n\nas expected.\nLogical Values can also be used for subsetting. If you type for instance\n\naapl_prices[1, c(F,F,F,F,F,T,F,F)]\n\n[1] 0.332589\n\n\nR will select the first closing price. Note that here we used the R convention that TRUEand T as well as FALSEand F have an equivalent meaning.\nFinally, you can ask for the elements, you want by name. On our case, you could select the first closing price by\n\naapl_prices[1, \"close\"]\n\n[1] 0.332589\n\n\nFinally, note that two types of object in R obey an optional second system of notation. You can extract values from data frames and lists with th $syntax. It works as follows: For example\n\nmean(aapl_prices$close)\n\n[1] 16.39576\n\n\nwould select the column of closing prices and compute their mean.\nNow lets try to solve the counting problem for our data set. To see whether the price went up we need to subtract the closing price of the first data from the second, and see whether it is larger than zero. This we would have to do for every observation in the whole frame.\nNow let us bring our indexing rules into action. We create two new objects, one ranging from observation 2 of the close price until the end of the data frame, the other ranging from 1 to but the last element, then we subtract and write the result in a new object. Finally we inspect the first 10 elements of the result\n\naux_1 &lt;- aapl_prices[2:8044, \"close\"]\naux_2 &lt;- aapl_prices[1:8043 , \"close\"]\ndiff_close &lt;- (aux_1 - aux_2)\ndiff_close[1:10]\n\n [1]  0.002232  0.001117  0.001116  0.002232 -0.003348 -0.014509 -0.013393\n [8]  0.000000 -0.002232  0.005580\n\n\nNow this works but it is not very elegant. Actually R has a built in function, called diff() which will do the job for us. We can even append the result to the frame we have using the $ notation as follows: Let us call the new variable diff. But now we have to be careful. Since we take first differences we will looe one observation. This will create a mismatch in the dimenion of the dataframe. Since the data frame starts at some date (in our case 1990-01-02) we have no difference for the first observation. R’s notation for a missing observation is NA. So we neet to put an NAahead of our diff column:\n\naapl_prices$diff &lt;- c(NA, diff(aapl_prices$close, lag = 1))\nhead(aapl_prices, n= 5)\n\n  symbol       date     open     high      low    close    volume adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.266423\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.268211\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.269106\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.270000\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.271788\n      diff\n1       NA\n2 0.002232\n3 0.001117\n4 0.001116\n5 0.002232\n\n\nLet’s check whether we get the same thing and compare with the values in diff_close. They do indeed match.\nNow we can use a really cool feature to count the up moves, which shows you the power of R in a very nice example. We can test conditions by logical operators. In our case we want to know whether the diff between two consecutive closing prices is positive. We add another column to the dataframe which will then contain TRUE or FALSEvalues, when the condition holds. To see what happens, let us do this and look at the first 5 rows of our new data frame:\n\naapl_prices$diff_pos &lt;- aapl_prices$diff &gt; 0\nhead(aapl_prices, n = 10)\n\n   symbol       date     open     high      low    close    volume adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.266423\n2    AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.268211\n3    AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.269106\n4    AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.270000\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.271788\n6    AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.269106\n7    AAPL 1990-01-10 0.335938 0.335938 0.319196 0.321429 199718400 0.257483\n8    AAPL 1990-01-11 0.323661 0.323661 0.308036 0.308036 211052800 0.246755\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.246755\n10   AAPL 1990-01-15 0.308036 0.319196 0.305804 0.305804 161739200 0.244967\n        diff diff_pos\n1         NA       NA\n2   0.002232     TRUE\n3   0.001117     TRUE\n4   0.001116     TRUE\n5   0.002232     TRUE\n6  -0.003348    FALSE\n7  -0.014509    FALSE\n8  -0.013393    FALSE\n9   0.000000    FALSE\n10 -0.002232    FALSE\n\n\nNow according to the frequency notion of probability we could attach the probability of an inter day up move of the Apple stock price by computing the share of up moves (diff positive) among all our change observations.\nNow we can use the R coercion rules. If we apply the mean function to our diff_pos column we get the share of up moves, since R will coerce the TRUE values to 1 and the FALSE values to 0. This is a new opportunity to practice our newly acquired subsetting rules:\n\nmean(aapl_prices$diff_pos, na.rm = T)\n\n[1] 0.5051598\n\n\nWe see that the result is that the stock price has moved up about half of the days and not up at the other days.\nLet me explain what is going on in the syntax of our R statement. We selected the column from our data frame that contained the differences. Then we apply the mean() function to this column. But since the column contains one value which is NA the mean would return NA if we not correct for this fact. This is what the argument na.rm = T does. It tells R to first remove NA form the observations and then compute the mean.\nNow from a frequency point of view \\(P(U) = 0.51\\) and \\(P(D) = 0.49\\) accordingly.\nNow assume in addition that the direction of a price move of the Apple stock on the current trading day is independent of the direction of the Apple stock price move on the previous trading day for all days. This means that the probability of \\(U\\)-movements and of \\(D\\)-movements is unaffected by the number of previous \\(U\\) and \\(D\\) movements. This is - of course - an assumption. You may think about whether this assumption is reasonable.\nPostponing this important discussion, consider now a week from Monday to Friday and ask: “What is the probability that the price of Apple will increase on each of the consecutive days?”\nThere are five trading days, so we need to compute \\(P(U \\cap U \\cap U \\cap U \\cap U \\cap U)\\). This is by our assumption of independence equal to \\(P(U) \\cdot P(U) \\cdot P(U) \\cdot P(U) \\cdot P(U)\\). By our probability estimate from relative frequency this amounts to \\(0.51^5\\), which amounts to \\(0.035\\).\n“What is the probability that the stock price will decrease either on Monday, Tuesday, Wednesday, Thursday or Friday and will increase on the other four days?”\nThe probability that the \\(D\\) movement happens, say on a Monday, is \\(P(D \\cap U \\cap U \\cap U \\cap U)\\) or \\(0.49*0.51^4\\) which is \\(0.033\\). We have in total five mutually exclusive scenarios: \\(P(D \\cap U \\cap U \\cap U \\cap U)\\), \\(P(U \\cap D \\cap U \\cap U \\cap U)\\), \\(P(U \\cap U \\cap D \\cap U \\cap U)\\), \\(P(U \\cap U \\cap U \\cap D \\cap U)\\), \\(P(U \\cap U \\cap U \\cap U \\cap D)\\). Thus we have \\(0.033 + 0.033 + 0.033 + 0.033 + 0.033\\) as the final probability of this event, which is \\(0.132\\).\nIs this analysis any good? How could we possibly judge this? Interestingly the relative frequencies of up and down moves look similar to a random experiment of a few thousand tosses of a fair coin. But can we learn anything from this? Are the up and down moves independent? Independence - of course - does not follow from the result we just got.\nThe idea that stock prices may fluctuate randomly was first discussed systematically by Louis Bachelier (1870 - 1946), a French mathematician who studied stock price movements mathematically. In 1965 the economist Samuelson (1965) published an article with the title “Proof that stock prices fluctuate randomly”. He argues in this paper that randomness comes about through the active participation of traders seeking to maximize their wealth. A huge army of investors would aggressively use the smallest informational advantage and in doing so, they incorporate the information into market prices, which quickly eliminates this profit opportunity. This lead to a cornerstone of modern Finance theory called the random walk hypothesis of stock price fluctuations.\nIf this theory was true, it would give an argument, why we might look at the up and down movements in the stock price of apple as if it was the outcome of tossing a fair coin. In this case the probability of an up or a down movement should be 1/2 and with the number of trials approaching infinity the frequency of ups and downs should approach this probability.\nThe literature on stock price fluctuations which came later, however, presented evidence that stock prices are predictable to a certain degree and do not fluctuate randomly. A good reference summarizing this evidence is Lo and MacKinlay (2019) In this case our approach would perhaps produce a misleading answer.\nWe cannot give a clear cut answer here. The point of this brief discussion is that you just cannot apply a theoretical machinery mechanically without giving it further thought and without maintaining a healthy amount of skepticism. It is fascinating that there are situations where abstract theories, like the theory of probability, show a robust relation to real world phenomena. But the nature, the precise meaning and the robustness of this relation has to be investigated for each single case.\n\n\n2.7.4 Example: Benfords law: The empirical probabilities of leading digits\nLet us discuss last example in this lecture which will fortify both our probability knowledge as well as our newly gained R skills. It will also prepare you for the project assigned in this lecture, which will familiarize you with yet another financial dataset.\nThe example we are going to discuss here is again an instance of empirical probabilities for a rather surprising instance: Leading digits. What are leading digits? Well the concept is actually simple: Say we have the number 7829, then the leading digit in this number is 7, the first digit in 7829. If the number is 10892, it would be 1, if it is 4 it would be 4.\nNow we could ask the following question: let’s look at datasets we can find in real life, any datasets. How often in these datasets is the leading digit a 1, a 2, etc. or a 9? You might think that they all occur with about the same frequency. Indeed, what would be special about any particular digit? So in this case each number would occur with probability \\(1/9\\) or with a frequency of about \\(11 \\%\\).\nBut here is an interesting fact: They occur with unequal frequency. They are actually quite unequal and follow a peculiar logarithmic pattern; you can give a formula for the empirical probabilities. This formula looks as follows:\n\nBenfords Law\n\nIn a dataset the frequency of \\(d\\) being the first digit is \\[\\begin{equation*}\nP(d) = \\log_{10}\\left(1+ \\frac{1}{d}\\right)\n\\end{equation*}\\] where \\(d \\in \\{1,\\cdots, 9 \\}\\)\n\n\nLet us look at how this distribution looks like. Let us build a data frame where we see the digits in one column and the Benford-Probabilities in the second one:\n\nbenford &lt;- data.frame(Digit = 1:9)\nbenford$Prob_Benford &lt;- log10(1 + 1/benford$Digit)\nknitr::kable(benford)\n\n\n\n\nDigit\nProb_Benford\n\n\n\n\n1\n0.3010300\n\n\n2\n0.1760913\n\n\n3\n0.1249387\n\n\n4\n0.0969100\n\n\n5\n0.0791812\n\n\n6\n0.0669468\n\n\n7\n0.0579919\n\n\n8\n0.0511525\n\n\n9\n0.0457575\n\n\n\n\n\nNow here comes the really surprising fact. This law has been observed in credit card bills, stock market prices, in the market valuation of listed companies, in population data, the length of rivers.\nThere are some requirements on these datasets: They should not have an arbitrary maximum or minimum for example. The numbers also must not be assigned, like grades of exams, assigned phone numbers or assigned identities.\nNow let us check this with a real data set. We choose a dataset of US-county population data. This give us an opportunity to show you a second way to read a csv file into R by giving the internet link to the file directly as a string to the read.csv() function. We do this with the population data and store the data in an object we call population_data.\n\npopulation_data &lt;-  read.csv(\"https://www2.census.gov/programs-surveys/popest/datasets/2010-2020/counties/totals/co-est2020-alldata.csv\")\n\nSo you, if you know the internet address of the file and the file name you can read data also directly from an url without first downloading them and then read it from your local copy.\nNow we want to use these data to check or illustrate Benford’s law. Now you will encounter something typical for real data situations. If you inspect the data, you will see that there is much more than we need. Let us therefore select the data we want to look at, using the R tools we learned in this lecture. We create a new data frame which contains only the variables COUNTY STNAME, CTYNAME and CENSUS2010POP, and we discard all the other ones.\n\npop_us &lt;- data.frame(County_ID = population_data$COUNTY, State = population_data$STNAME, County = population_data$CTYNAME, Population = population_data$CENSUS2010POP)\n\nThis looks already pretty good, but we would like to have the data purely on county level. The current data set also contains State aggregates, which have County_Id == 0. There are also a few counties which get value “X”. We also want to filter out those. Let’s use logical sub-setting to filter these data:\n\npop_us_filtered &lt;- pop_us[((pop_us$County_ID != 0) & (pop_us$Population != \"X\")), ]\n\nThis will give us the pop_us data frame with all rows except those where the variable County_ID is equal to zero (which are the state aggregates).\nNow we extract the leading digits from the population numbers. Notice that in our data set the population numbers are stored as characters, as you can see from typing:\n\ntypeof(pop_us_filtered$Population)\n\n[1] \"character\"\n\n\nSo we can use R’s string functions. This is a vast area which could fill a lecture on it own. We just mention one of the many R-functions operating on strings. One way to implement selection of a leading digit is using one of R’s string functions, substr(). You can give any string to this function and cut a substring, by specifying where it should begin and where it should end. Now in our case the population number. Let us use this to add a new column of leading digits to the data frame.\n\npop_us_filtered$LD &lt;- substr(pop_us_filtered$Population, 1, 1)\n\nNow we just need to tabulate the leading digits. This means, we count the frequency of 1, 2 et. until 9 and divide by the total number of observations. This tabulation functionality is implemented in R by its function table. So lets tabulate the respective column and divide by the total number of observations:\n\nemp_dist &lt;- (table(pop_us_filtered$LD)/length(pop_us_filtered$LD))\nemp_dist\n\n\n         1          2          3          4          5          6          7 \n0.30340656 0.18911175 0.11907036 0.09805794 0.06781280 0.06685769 0.05794333 \n         8          9 \n0.04839223 0.04934734 \n\n\nIt would be nicer if we could show both the empirical frequencies and the bendford probabiliities in one dataframe. We first transform the emp_dist into a data frame and merge with the benford$Prob_Benford vector. This can be done by using the R function cbind() which can merge two data framews\n\ndf &lt;- cbind(as.data.frame(emp_dist), benford$Prob_Benford)\n\nNow lets give the new data frame nicer names using the name function and display the result in a nice table:\n\nnames(df) &lt;- c(\"Digit\", \"Emp_Freq\", \"Benf_Freq\")\nknitr::kable(df)\n\n\n\n\nDigit\nEmp_Freq\nBenf_Freq\n\n\n\n\n1\n0.3034066\n0.3010300\n\n\n2\n0.1891117\n0.1760913\n\n\n3\n0.1190704\n0.1249387\n\n\n4\n0.0980579\n0.0969100\n\n\n5\n0.0678128\n0.0791812\n\n\n6\n0.0668577\n0.0669468\n\n\n7\n0.0579433\n0.0579919\n\n\n8\n0.0483922\n0.0511525\n\n\n9\n0.0493473\n0.0457575\n\n\n\n\n\nThe match is amazingly close. This is quite cool, isn’t it ?\nOne application of this interesting regularity has been to screen data sets, for instance tax filings, to find evidence whether they might have been manipulated by looking for deviations from Benford’s law. In the second project you are asked to do just that. Investigate the first digit distribution in a real financial data-set and compare to Benford’s law. For those of you who got interested in this “magic” law, let me mention that there is a monograph length treatment in Berger and Hill (2015)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#summary-lecture-2",
    "href": "02-lecture2.html#summary-lecture-2",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.8 Summary",
    "text": "2.8 Summary\nIn this lecture we have dived a bit deeper into probability. We made some of our introductory concepts more precise and more general and learned about the important concepts of independence and how probabilities and frequencies of events in series of identical independent experiments are and are not related. We also learned about tools to go back and forth between probability theory and data and encountered our first financial dataset, a data frame for the stock pirce of Apple.\nThese are the probability concepts we have covered in this lecture:\nIn this lecture we have given precise definitions of our basic concepts of sample space, basic outcome, events and probability. This time we gave an abstract definition of probability which shares the basic properties we required for classical probability. We explained what discrete probability means and explained how we can build events by set theoretic operations of union, intersection, complements. We have also explained the meaning of mutually exlusive. We have explained the notion of empirical probability or frequency probability. We have discussed the weak law of large numbers which establishes for a given probability the number of identical independent trials that are needed such that the relative frequency of an event falls within any given bound around this probability. While relative frequencies are not probabilities they approach given probabilities in the limit of a sequence of identical independent trials. We have finally learned about the definition of independence of events. If two events \\(A\\) and \\(B\\) are independent the probability that \\(A\\) and \\(B\\) occur is the product of their individual probabilities.\nThese are the R concepts covered in this lecture: We learned what R objects are and that R has six types of atomic vectors, namely double, integer, character, logical, complex and raw. We have learned how to subset R objects by positive integers, negative integers, zero, blank spaces, logicals and names.\nThese are the applications we have covered in this lecture. Tossing a fair coin by simulation. reading data from a stored csv files and also reading a csv file directly from the internet. Using R’s subsetting functions to analyze up and down movements of stock price data and use empirical probabilities together with an independence assumption to compute the probability of certain sequences of up and down moves. Benford’s law of leading digits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#project2",
    "href": "02-lecture2.html#project2",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.9 Project 2: Should we trust European supervisory bank data?",
    "text": "2.9 Project 2: Should we trust European supervisory bank data?\nThe European Banking Authority EBA conducts a biannual stress testing exercise for the biggest European banks every two years. In the stress test bank exposure data are collected and then particular stress scenarios are applied to the reported data. The goal is to find out whether the banks would have enough capital to withstand an adverse scenario. EBA makes a huge effort to publish the data that are the base of the stress test on its web site: https://www.eba.europa.eu/risk-analysis-and-data/eu-wide-stress-testing\nIn this project you are asked to use the knowledge you learned about empirical probabilities, R and Benford’s law to check whether the EBA stress testing data show the distribution of leading digits, you would expect from Benford’s law.\n\nGo to the EBA website https://www.eba.europa.eu/risk-analysis-and-data/eu-wide-stress-testing and download the file Credit Risk IRB (https://www.eba.europa.eu/assets/st21/full_database/TRA_CRE_IRB.csv). You can do so by downloading the file, storing it locally and then read it into R or you can read it directly from the web.\nWe want to study the distribution of leading digits in the Amounts reported in the EBA file of credit risk exposures. Now the csv file contains many different data that all somehow belong to the EBA stress test. We would not like to check all data but only exposure data. We need first to filter the data to make sure we have a meaningful collection of exposure data for all the banks. The description of the data and the data dimensions is in the files Metadata_TR.xlsx and Data_Dictionary.xlsx. You are welcome to study these data in detail. It will probably need more time than you have available. They are also quite complicated. Since the aim of this project is not directly to understand the eba-data but to work with your R-concepts and probability concepts, let me guide you here how to filter these data in 10 sequential steps. Note that this sequencing is for didactical reasons only and for the purpose not to loose oversight. With routine and experience all these steps can be done in one go as well:\n\nExtract all variables names, using the names()function.\nSelect all rows where the Scenario variable has value 1. Note that the symbol you need in the R syntax for equal is ==, the syntax is therefore Scenario == 1. You might check out the R-help entry Comparison for further details.\nFrom the resulting data-frame select all rows where the Country variable is not equal to 0. (hint: The not equal operator in the R syntax is !=). If you look into the Metadata-File you will see that 0 are all the aggregate exposures not broken down by country. Excluding these will give us country exposures.\nFrom the resulting data frame select all rows where the Portfolio variable has value 1 or 2.These codes describe the accounting rules under which the exposure values are reported, internal rating based (IRB) or standard approach (SA). As a hint you can use R’s subset operator %in% here so Portfolio %in% c(1,2) written with the approprate subsetting rule will select all rows where the Porfolio variable has value 1 or 2.\nFrom the resulting data frame choose all the rows where the Exposure variable is not 0.This gives again disaggregated numbers.\nFrom the resulting data frame choose all the rows where the Status variable has value 0.\nFrom the resulting data frame choose all the rows where the IFRS9_Stages variable has value 1,2, or 3.\nFrom the resulting data frame choose all the rows where the CR_guarantees variable is 0\nFrom the resulting data frame choose all the rows where the CR_exp_moratoria variable is 0.\nFrom the resulting data frame, drop all rows where the Amount variable is 0.\n\nCheck the type of the Amount variable.\nTransform the Amount variable to type numeric()\nCheck for NA in the Amount variable in the resulting data frame and if you find any, remove them.\nChange the Amount variable from the actual unit of Million Euros to the unit of 1 Euro 1 Euro and throw away data smaller than 1 after this transformation.\nSelect the leading digits from the Amount variable, using R’s string functions and add a variable with name LD to your data frame.\nCompare the empirical frequencies in the data with the theoretical frequencies from Benford’s law by creating an appropriate data frame. What can you say from this evidence?\n\n\n\n\n\nBerger, Arno, and Theodore Hill. 2015. An Introduction to Benford’s Law. Princeton University Press.\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nLo, Andrew, and Craig MacKinlay. 2019. A Non-Random Walk down Wallstreet. Princeton University Press.\n\n\nSamuelson, Paul. 1965. “Proof That Properly Anticipated Prices Fluctuate Randomly.” Industrial Management Review 6.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#probability-and-the-language-of-sets",
    "href": "02-lecture2.html#probability-and-the-language-of-sets",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.3 Probability and the language of sets",
    "text": "2.3 Probability and the language of sets\nProbability theory relies on the language of sets to describe relationships between events. Understanding key set operations is essential for working with probabilities effectively.\nLet’s go through them and illustrate the concepts in the context of the examples we have already developed in lecture 1.\n\nUnion\n\nThe union of two events \\(A\\) and \\(B\\) represents all outcomes that belong to \\(A\\), \\(B\\) or in both. It is written \\(A \\cup B\\).\n\n\nLet us use the example of the experiment of rolling a die. The sample space \\({\\cal S}\\) is the set of all possible outcomes of rolling the dice \\({\\cal S}=\\{1,2,3,4,5,6\\}\\). Assume one event is that the outcome is 1, 2 or 3. In set notation, we would write \\(A = \\{1,2,3\\}\\). Let us also assume that the second event is that the outcome is some even number, i.e. 2, 4, 6. Again using set notation we would write \\(B = \\{2,4,6\\}\\). The event \\(A \\cup B\\) is then the set of all outcomes such that the outcome is in \\(A\\) or in \\(B\\) or in both or these sets.\nYou can implement set operations in R because R provides functions for computing set operations. Let us use the occasion to show you briefly how to use these functions in the context of this example: We define the sets \\(A\\) and \\(B\\) first using the assignment operator:\n\nA &lt;- c(1,2,3)\nB &lt;- c(2,4,6)\n\nWe compute the union by using the function union()\n\nunion(A,B)\n\n[1] 1 2 3 4 6\n\n\nwhich gives us the union of both sets.\nTo understand this operation better we can visualize the set union in this example by looking at Figure 2.1\n\n\n\n\n\n\n\n\nFigure 2.1: The meaning of set union\n\n\n\n\n\nThe sample space \\({\\cal S}\\) is the gray set containing all possible outcomes of our random experiment. Graphically the union of \\(A\\) and \\(B\\), \\(A \\cup B\\) is a subset of the sample space, the entire colored area.\n\nIntersection\n\nThe intersection of two events are all outcomes that are both in \\(A\\) and in \\(B\\). It is written as \\(A \\cap B\\).\n\n\nIn R we would implement this operation by using the function intersect() and apply it to our sets \\(A\\) and \\(B\\) we have defined before.\n\nintersect(A,B)\n\n[1] 2\n\n\nFigure 2.2 visualizes this operation graphically\n\n\n\n\n\n\n\n\nFigure 2.2: The meaning of set intersection\n\n\n\n\n\nThe intersection of \\(A\\) and \\(B\\), \\(A \\cap B\\) is the orange area containing the dice face with two points. Indeed two is both in \\(A\\) and in \\(B\\), which is exactly the meaning of set intersection.\n\nComplement\n\nThe complement of an event \\(A\\) within the sample space \\({\\cal S}\\) is the set of all outcomes that are in \\({\\cal S}\\) but not in \\(A\\). It is written as \\({\\cal S} \\setminus A\\).\n\n\nLets say we want to get the complement, or the set difference of \\(A \\cup B\\) with respect to the sample space \\({\\cal S}\\).\nThe R implementation of the set differecne operation is the function setdiff(). This is how we would tell R to compute the setdifference of the union of \\(a\\) and \\(B\\) and the sample space \\({\\cal S}\\) in our example of the die:\n\nS &lt;- c(1,2,3,4,5,6)\n\nsetdiff(S, union(A,B))\n\n[1] 5\n\n\nThis can again be visualized in ?fig-set-minus\n\n\n\n\n\nThe meaning of complement\n\n\n\n\nThis complement is the dice shown in the light redish area, i.e. all the elements of \\({\\cal S}\\) which are not in \\(A \\cup B\\).\n\nMutually Exclusive\n\nTwo events \\(A\\) and \\(B\\) are mutually exclusive if they can not occur simultaneously. This means \\(A \\cap B = \\emptyset\\), their intersection is empty.\n\n\nAn example in our context is the set of even outcomes \\(B=\\{2,4,6\\}\\) and the set of odd outcomes, let us call it \\(C=\\{1,3,5\\}\\). If we intersect these sets\n\nB &lt;- c(2,4,6)\nC &lt;- c(1,3,5)\n\nintersect(B,C)\n\nnumeric(0)\n\n\nwe get the empty set, which is expressed by R by giving the data type, in this case numeric, because we are intersecting sets of numeric values, followed by (0). This means, there is no numeric value in the intersection of \\(B\\) and \\(C\\).\nLet us discuss set operations a bit further by thinking about the probability of the union of two events \\(A\\) and \\(B\\) within the context of our visual examples. Remember that we had \\(A = \\{1,2,3 \\}\\) and $B = { 2,4,6}. Look at Figure 2.3\n\n\n\n\n\n\n\n\nFigure 2.3: The meaning of set union\n\n\n\n\n\nWe would like to know what is the probability of \\(P(A \\cup B)\\)? Now if we add up \\(P(A)\\) and \\(P(B)\\) we would assign a probability to the outcome \\(2\\) twice. Such double counting must be avoided and thus we have to subtract \\(P(A \\cap B)\\) the intersection, so that: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\). Now you see what the qualification mutually exclusive does in our probability rule 3. When \\(A\\) and \\(B\\) are mutually exclusive \\(A \\cap B = \\emptyset\\) and in this case \\(P(A \\union B) = P(A) + P(B)\\). We can add up the probabilities because when events are mutually exclusive we can not double count events by adding up individual probabilities.\n\n\n\n\n\n\nNow you try\n\n\n\nIf you drop the qualification that \\(A\\) and \\(B\\) must be mutually exclusive in point 3 in the definition of probability. How would rule 3 have to be changed?\n\n\nHere’s a tip you could share with your students to encourage effective use of a Large Language Model (LLM) for deepening their understanding of the concepts and definitions introduced in this section:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#using-an-llm-to-deepen-your-understanding-of-set-theory-in-probability",
    "href": "02-lecture2.html#using-an-llm-to-deepen-your-understanding-of-set-theory-in-probability",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.4 Using an LLM to Deepen Your Understanding of Set Theory in Probability",
    "text": "2.4 Using an LLM to Deepen Your Understanding of Set Theory in Probability\nAn LLM like ChatGPT can be an incredibly helpful tool to explore concepts and solidify your understanding. Here are some example of what you could do:\n\n2.4.1 Ask for Clarifications\nIf a definition or concept isn’t clear, ask the LLM to explain it in simpler terms or using different examples. For instance:\n\n\n\n\n\n\nPrompt\n\n\n\n“What is the difference between the union and intersection of sets in probability? Can you give examples?”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“Can you compare this to a real-life scenario, like rolling a die or flipping a coin?”\n\n\n\n\n2.4.2 Generate Additional Examples\nUse the LLM to create new examples similar to the ones in the lecture. This will give you more practice applying the concepts.\n\n\n\n\n\n\nPrompt\n\n\n\n“Give me an example of mutually exclusive events involving sports outcomes.”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Can you show a sample space and events for tossing two coins?”\n\n\n\n\n2.4.3 Simulate Visualizations and Code Interpretation\nWhile the LLM doesn’t directly produce visuals, you can ask it to describe how a diagram or R output would look. This helps connect theoretical concepts to their graphical representations. For example:\n\n\n\n\n\n\nPrompt\n\n\n\n“Describe what a Venn diagram looks like for \\(A \\cup B\\), \\(A \\cap B\\), and \\(A \\setminus B\\).”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“What does the R function union(A, B) compute? How is it related to \\(A \\cup B\\)?”\n\n\n\n\n2.4.4 Practice Applying Definitions\nUse the LLM to test your understanding by quizzing yourself.\n\n\n\n\n\n\nPrompt\n\n\n\n“Ask me questions about the definitions of sample spaces, union, intersection, and complement.”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Give me a scenario and ask which set operation applies.”\n\n\n\n\n2.4.5 Explore Real-World Applications\nUse the LLM to explore how these concepts apply in real-world contexts beyond the lecture.\n\n\n\n\n\n\nPrompt\n\n\n\n“How is the concept of set intersection used in data science or finance?”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Explain how mutually exclusive events are important in designing experiments.”\n\n\n\n\n2.4.6 Learn R Through Step-by-Step Guidance\nIf you’re new to R, ask the LLM to guide you through using functions like union(), intersect(), and setdiff() step by step.\n\n\n\n\n\n\nPrompt\n\n\n\n“Explain how to use setdiff() in R with an example involving dice rolls.”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“How does this output relate to the complement of a set?”\n\n\n\nDive Deeper into Probability Rule 3:\n\nUse the LLM to generate explanations and examples that clarify how probabilities relate to set operations.\n\n\n\n\n\n\nFollow up\n\n\n\n“Explain why \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“Can you provide a numerical example to illustrate this rule?”\n\n\n\n\n2.4.7 Simulate Discussions\nAsk the LLM to take the role of a peer or instructor to simulate a conversation about the material.\n\n\n\n\n\n\nPrompt\n\n\n\n“Pretend you are my study partner. Let’s discuss the complement of events and its significance in probability.”\n\n\nBy actively engaging with the LLM through these kinds of prompts, you can practice, explore, and deepen your understanding of the material beyond the lecture. Try it alone or with your group.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  }
]