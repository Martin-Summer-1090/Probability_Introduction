[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Probability",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-these-lecture-notes-about",
    "href": "index.html#what-are-these-lecture-notes-about",
    "title": "An Introduction to Probability",
    "section": "What are these lecture notes about ?",
    "text": "What are these lecture notes about ?\nThese lecture notes support the course An Introduction to Probability - with Applications to Computational Finance using R. The course introduces essential probability concepts that every finance practitioner must understand. As probability is crucial for solving financial problems computationally, we will use computational finance as the context to develop these ideas. The focus will be hands-on learning, enabling you to understand and internalize concepts by applying them yourself.\nTo make abstract concepts like randomness, chance, and probability more accessible, we will leverage computational tools, in our course this will be R, and visualizations built by using R. By actively engaging with these tools, you’ll not only deepen your understanding of probability but also enhance your ability to apply it to real-world finance problems.\n\n\n\n\n\n\nNoteKey learning objectives\n\n\n\n\nUnderstand foundational probability concepts.\nApply probability principles to solve computational finance problems.\nGain hands-on experience using R for probability and finance applications.\nDevelop proficiency in interpreting and visualizing probabilistic data.\n\n\n\nBut how can we build abstract concepts such as probability, random phenomena and chance by our own hands? While probability is a mathematical theory, it gains practical value and an intuitive meaning in connection with real or conceptual experiments such as, the future change in a stock price index, the future value of a portfolio of securities, the chance that a creditor is not going to be able to pay back your loan.\nMany of these experiments we can nowadays simulate on the computer. We can can construct, build and simulate a huge variety of random phenomena. We can - for instance - implement models of random fluctuations of asset prices. We can model financial risks and contemplate possible future scenarios through simulation.\nThe arrival of the computer has been indeed a sea change for the mathematics of probability because it allows both reflecting about probability through theorems and proofs as well as by making use of computer simulation. We can approach our understanding of abstract concepts by building them with our own hands on the computer. This is the approach to teaching you probability in this course.\nLet me give you one example right away. Don’t worry if you do not understand precisely what the following snippet of code is doing. We will learn the syntax of the R language as we go along. This is just to demonstrate you what I explained in words before by a simple example highlighting simulation of random processes and visualization.\n\n\n\n\n\n\nTipAn R example for illustration\n\n\n\nSimulate stock prices with normally distributed random daily returns.\n\n# Simulate daily returns for a stock\n\nset.seed(123)  # Ensure reproducibility\nn &lt;- 100       # Number of days\ndaily_returns &lt;- rnorm(n, mean = 0.001, sd = 0.02)  # Normal distribution\n\n# Compute cumulative returns\nprice &lt;- cumprod(1 + daily_returns) * 100  # Starting price = 100\nprice\n\n  [1]  98.97905  98.62237 101.79547 102.04081 102.40670 106.02179 107.10516\n  [8] 104.50237 103.17132 102.35490 104.96307 105.82338 106.77742 107.12056\n [15] 106.03684 109.93245 111.13699 106.87684 108.48290 107.56558 105.37593\n [22] 105.02192 102.97188 101.57374 100.40557  97.11890  98.84332  99.24536\n [29]  97.08551  99.61714 100.56642 100.07351 101.96515 103.85789 105.66830\n [36] 107.22932 108.52447 108.49862 107.94319 107.22975 105.84711 105.51281\n [43] 102.94801 107.51675 110.22179 107.85619 107.09498 106.20254 107.96543\n [50] 107.89338 108.54790 108.59447 108.60996 111.69144 111.29880 114.78573\n [57] 111.34502 112.75824 113.15031 113.75214 114.72959 113.69169 113.04772\n [64] 110.85782 108.59235 109.36016 110.44984 110.67738 112.82954 117.56857\n [71] 116.53154 111.26625 113.61561 112.11770 110.68706 113.06810 112.53719\n [78] 109.90221 110.41062 110.21433 110.33725 111.29781 110.58403 112.11977\n [85] 111.73747 112.59066 115.17313 116.29072 115.64896 118.42177 120.89325\n [92] 122.34009 123.04656 121.62437 125.05577 123.67950 129.21375 133.30365\n [99] 132.80856 130.21502\n\n\nVisualize the distribution of daily returns:\n\nhist(daily_returns, \n     main = \"Distribution of daily stock returns\",\n     xlab = \"Returns\",\n     ylab = \"Number of Days\")\n\n\n\n\n\n\n\n\nNow visualize the price dynamics:\n\n# Visualize the stock price\nplot(price, type = \"l\", col = \"blue\", lwd = 2,\n     main = \"Simulated Stock Price\",\n     xlab = \"Days\", ylab = \"Price\")\n\n\n\n\n\n\n\n\n\n\nThis example highlights:\n\nHow randomness influences stock price movements\nR’s usefulness in simulations and visualization.\n\nIt is, however important to always keep in mind that the real world is different from the theory world as well as from the simulated world. The real world is both richer and more complex than both theory and simulation. It may contain features which need to be be taken into account for the explanation of phenomena which are absent in theory or in the simulation. We will try our best to develop your feeling for the difference, between theoretical concepts, simulations and applications as well as for the interdependence of these three areas.\nThere are many ways to build virtual objects and to run simulations to manipulate them. For this we will need a programming language. The language we choose for this course is R and the integrated development environment RStudio. This is one of the main languages used in data analysis, statistics and data science and is widely used in industry and academia. It will be our tool to do probability in this course.\nSince this course was being taught the first time in 2021, new AI tools such as large language models (LLMs) like chatGPT, Claude or Gemini have been introduced. These new powerful tools create new opportunities for learning and reinforcing knowledge such as the concepts learned in this course. In this year I will show you throughout the course, how you can leverage AI tools like ChatGPT for enhancing your learning experience. I encourage you to use these tools to generate examples, explain concepts, and debug R code or perhaps translate R code into a language you might be more familiar with on the fly to help you understand how things are expressed in R.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#using-an-llm-to-enhance-your-learning-experience",
    "href": "index.html#using-an-llm-to-enhance-your-learning-experience",
    "title": "An Introduction to Probability",
    "section": "Using an LLM to Enhance Your Learning Experience",
    "text": "Using an LLM to Enhance Your Learning Experience\nIn these lectures I try to help you building experience how to integrate large language models or LLMs into your learning and study process as well as to support you in coding and programming. There are several such models currently on the market. For this course I will use openAI’s chatGPT as a tool. ChatGPT is available in a free version and in an enhanced pay-version.\nThroughout this course, you’ll learn how to use ChatGPT to support your studies. As a tool, it offers capabilities that can make learning probability more interactive and engaging.\nHere’s are some example for interesting use cases:\n\nClarify Concepts:\nAsk ChatGPT to explain concepts in simple terms or elaborate on topics you find difficult:\n\n“Explain the concept of a probability distribution with an example.”\n\nGenerate Examples:\nUse it to create additional examples for practice:\n\n“Can you give me an example of a random variable and how it applies in finance?”\n\nDebug R Code:\nIf you encounter issues with your R code, paste the code and ask:\n\n“Why does this R code not run, and how can I fix it?”\n\nPractice Exercises:\nRequest custom exercises:\n\n“Create three exercises to practice calculating probabilities for dice rolls.”\n\nSimulate Interactive Discussions:\nSimulate discussions with ChatGPT to test your understanding:\n\n“I think the variance of a constant is zero. Am I correct? Explain why or why not.”\n\nLearn Best Practices in R:\nAsk for tips to improve your coding practices:\n\n“What are the best practices for writing clean and efficient R code?”\n\n\nImportant Note:\nWhile ChatGPT is a powerful learning tool, remember to verify the outputs, especially for complex calculations, and always critically evaluate its suggestions. One such tool you might consider for cross checking is Wolfram | Alpha, which you can find here: https://www.wolframalpha.com/ in a free version.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#downloading-and-installing-r",
    "href": "index.html#downloading-and-installing-r",
    "title": "An Introduction to Probability",
    "section": "Downloading and installing R",
    "text": "Downloading and installing R\nSo lets start by downloading and installing R first. R is an open source project maintained by an international team of developers. The software is made available through a website called the comprehensive R archive network (http://cran.r-project.org).\nAt the top of this website, in a box named “Download and install R” you will find three links for downloading R. Choose the link that describes your operating system, Windows, Mac or Linux. These links will lead you to the necessary information you need to install a current version of R. The easiest install option is to install R from precompiled binaries. There is also the option to built R from source on all operating systems if you have the tools and the expertise to do so. R also comes both in a 32-bit and a 64-bit version. It does not make a substantial difference which version you use. 64-bit versions can handle larger files and data sets with fewer memory management problems.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#downloading-and-installing-rstudio",
    "href": "index.html#downloading-and-installing-rstudio",
    "title": "An Introduction to Probability",
    "section": "Downloading and installing RStudio",
    "text": "Downloading and installing RStudio\nRStudio is an application that helps you write and develop R code. It makes using R much easier for you than using it in isolation. The interface of RStudio looks the same across all operating systems.\nYou can download RStudio for free from https://www.rstudio.com/products/rstudio/. Select the box RStudio Desktop and follow the download instructions. RStudio Desktop is free. Note that you need to have a version of R installed to use RStudio.\nIf you have successfully installed R and RStudio, we are ready to start.\nThroughout this course I will use R with RStudio and when I do code demonstrations I will use this environment. R can be used in many other ways of course. Some of you might be accustomed to work with Jupyter Notebooks. I will just mention here that with some minor tweaks R can be used with Jupyter notebooks as well. I will not go into the details here. Those of you who are interested in such a setup are encouraged to ask chatGPT:\n\n“Please give me a step by step instruction how to set up the system to use R through Jupyter notebooks instead of RStudio.”",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#signing-up-for-chatgpt-free-version",
    "href": "index.html#signing-up-for-chatgpt-free-version",
    "title": "An Introduction to Probability",
    "section": "Signing up for ChatGPT (Free Version)",
    "text": "Signing up for ChatGPT (Free Version)\nTo access ChatGPT, follow these steps:\n\nGo to https://chat.openai.com in your web browser.\nClick on the “Creat a free account” button to create an account. You can sign up using an email address or your existing Google or Microsoft account.\nFollow the prompts to verify your email address and complete the registration process.\nOnce registered, log in to access ChatGPT. You can start using it immediately for your learning needs.\n\nUsing ChatGPT is free, but note that certain features or advanced versions (e.g., GPT-40) may require a paid subscription. The free version (GPT-3.5) is sufficient for getting a feeling of what the LLM can do. They payed version is more powerful.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "An Introduction to Probability",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course is built as an intuitive and elementary introduction to the field. No knowledge of probability or R is assumed and all concepts are built from scratch. You should be able to study for this course by relying on the slides and these lecture notes only without having to consult a textbook or further literature.\nMany of you will have heard or studied probability at some stage in college or at university. I hope the course will appeal to the newcomers as well as to the experienced. While it should allow the first group to quickly learn some of the most important concepts and raise your curiosity and excitement about a field of knowledge which is both beautiful and immensely practical, I hope it will open a new perspective for the more experienced among you and thus help deepening your understanding of concepts you might already have heard elsewhere.\nThe lectures develop probability concepts and R concepts in parallel. This follows the idea underlying this course to develop your knowledge of probability concepts by building them on the computer yourself and play with them. In this way we hope to build your R skills more thoroughly because they appear in a natural context and that this context also reinforces your understanding of probability.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-notes",
    "href": "index.html#structure-of-the-notes",
    "title": "An Introduction to Probability",
    "section": "Structure of the Notes",
    "text": "Structure of the Notes\nThe notes are structured into five main chapters. The idea is to cover each main chapter topic in one of the five double units allocated to this course.\nEach of the main chapters or lectures are followed by a so called project. The projects are meant to be worked on by you in the assigned groups between the lectures and should help you to practice and to deepen what you have learned. We will discuss the project questions at the end of each lecture. We will also distribute a worked problem after you have worked on it yourself.\nWe begin the first lecture, lecture 1 by discussing some of the main basic probability fundamentals by putting them in their historical context where the first ideas appear of how you could measure something such elusive as chance at all. Surprisingly these first simple ideas are already very rich and lead us quite far into the field and its problems.\nIn this lecture we will also give a first panoramic run down of what you can do with R. In this lecture the first application will be a simple game of chance, the tossing of a coin. Already this very elementary example has direct relevance for computational financial modelling of security prices as we will learn throughout the course.\nEven the very basic ideas of probability and R enable us, already at this early stage, to take on a serious real world problem as well. In fact, with only a few intuitive concepts we can study and understand problems of the chance of coincidences. This class of problems is at the heart of the science of constructing cryptographically safe finger prints, so called hash-functions, which are at the heart of crypto-currencies such as Bitcoin.\nThe first project will deal with engineering secure transaction indentifiers for a digital payment system and study how the construction of such identifiers has to take into account key system paramaters such as transaction volume and number of participants in the payment system.\nLecture 2 will introduce the first basic intuitive ideas a bit more formally and also generalize them. In particular it explains the basic ideas of how probability and frequencies of chance events are related. We will discuss the power as well as the limits of these ideas for applied work in probability.\nIn this lecture we will go deeper into the R language, explain its many data types and how to reference and change data in more complext data structures.\nProject 2 will help you to train and practice your understanding of data manipulation in R by useing a really cool empirical probability pattern on the empirical frequency of leading digits - Benford’s law - for checking whether the data could be trusted or do look fishy on purely probabilistic grounds.\nIn Lecture 3 we will learn about the key concept of conditional probability. This lecture contains lots of concepts but it is also of key importance for understanding many problems in Finance and the management of financial risks. We will explain how the misunderstanding or perhaps willful ignorance of the concept of dependence is at the root of the big financial crisis of 2007-2008. We will also learn, how updating your probability assessments in the light of new data is important for investment decisions.\nOn top of this directly practical considerations, we will also learn how this particular problem of updating probability assessments in the light of new information helps us close a conceptual gap in the pure frequency interpretations of probability.\nIn terms of R this lecture gives you a bit of a break in the sense that it applies what we have learned before and thereby reinforces what we have already learned. It will introduce some useful techniques and functions along the way.\nLecture 4 will introduce you more precisely to the key probabilistic model of random returns and other random phenomena in Finance: The concept of a random variable and the concepts of expected value, variance, standard deviation, covariance and correlation. We will discuss at length the modelling of random returns and learn about the basic workhorse model of asset price dynamics, the binomial lattice.\nThis discussion gives us also the appropriate context to discuss how to write programs in R and some of the principles and constructions that are most useful for this task.\nIn terms of R this lecture will teach you how to write more complex programs by using R’s control structures and by modularizing code.\nThe final Lecture 5 discusses the concept of continuous random variables, which is very important for a large body of applied work. You will be introduced to the normal distribution and its power and limits for modelling random returns and financial risks.\nIn terms of R this lecture will provide a good context for discussing what matters when you want to make your R code fast and efficient.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "An Introduction to Probability",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to thank my friend and colleague Branko Urošević that he entrusted me to develop this course for his new and innovative program in computational finance.\nI am very grateful for the opportunity to make a contribution to this effort and for all the encouragement, support and advice I got from him for this project. Of course he bears no responsibility whatsoever for any shortcomings of these lecture notes.\nI would also like to sincerely thank the former students of the bootcamp probability course who gave me feedback and studied from the lecture. They were a fantastic and enthusiastic audience from who I learned a lot.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "An Introduction to Probability",
    "section": "References",
    "text": "References\nWhen you teach such a fundamental field as probability theory and its applications in Finance you teach from a core knowledge that has been built by many pioneers, experts and other teachers in this area. In this sense nothing contained in this course is original work. The only contribution a teacher can make is the way how the material is presented and combined. In this sense these lecture notes are nothing but a collection of well known and time tested material as well as an experiment in yet one more, hopefully innovative exposition.\nBut even in this field I have copied and recycled a lot from the teaching materials and collective wisdom of other colleagues whose examples and problems I found particularly helpful. Without any claim to completeness let me briefly go through the sources I have relied on most strongly when developing this course.\nWith respect to probability theory two sources have influenced me most and I have followed their example extensively in this course: William Fellers “An introduction to probability theory and its applications volume 1.” which first appeared in 1957. (Feller (1968)). While really old by now it is still in my view one of the most outstanding reference with respect to exposition and clarity of explanation as well as an inspiring source of enthusiasm for the subject. I just would like to mention as an aside that Feller (born Vilibald Srećko Feller) was a Croatian emigrant to first Germany and then the US in the 1930ies. You might claim him as a citizen of Croatia, former Yugoslavia or even Austria, since when he was born in 1906, what is now Croatia was still a part of the Austrian-Hungarian empire.\nMy second most important source for this course in terms of probability was Karl Schmedder’s course “An intuitive introduction to probabiliy”, which Karl developed for the online platform Coursera. I consider this course an outstanding didactical achievement and masterpiece. If you find this lectures fun, I can only encourage you to also do the course on Coursera, when you find time. It is for a reason that Karl’s course is the mostly viewed Coursera course from Switzerand.1\n1 See https://www.coursera.org/learn/introductiontoprobabilityIn terms of R and R programming I have followed and copied a lot from Garett Grolemund’s excellent book “Hands on programming with R”.(Grolemund (2014)) I encourage you to study this book. From it I have not only learned about one of the most excellent explanations of R concepts but also the insight that these concepts are best explained within a very concrete and non trivial application context rather than in a patchy series of toy examples.\nWith respect to Finance and investments I have relied on the excellent book by David Luenberger, “Investment Science”. (Luenberger (2009)) If you can get a hold of this fantastic work, I can only encourage you to get one and have it on your shelf. It is a role model of clarity of exposition and has much more to offer than we can cover in this book.\nFinally with respect to the history and the interpretations of probability, I have learned the most form the excellent book “10 great ideas about chance”, by Persi Diaconis and Brian Skyrms. (Diaconis and Skyrms (2019)) If you find probability interesting or even fascinating I encourage you to read this book at some stage, if you find time. It is full of highly interesting philosophical, mathematical, historical and fun facts and ideas about this very rich subject.\nBut now, lets get straight down to the matter at hand.\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nGrolemund, Garrett. 2014. Hands on Programming with R. O’Reilly. https://rstudio-education.github.io/hopr/.\n\n\nLuenberger, David. 2009. Investment Science. Oxford University Press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "05-lecture5.html",
    "href": "05-lecture5.html",
    "title": "5  Continuous random variables and Monte Carlo Simulation",
    "section": "",
    "text": "5.1 Continuous Random Variables and Probability\nIn this lecture we will introduce the most important probability distribution, the normal distribution. While we have discussed discrete random variables so far, where the number of possible outcomes for \\(X\\) is finite (or countably infinite), to discuss the normal distribution we need to deal with the case that the number of outcomes for \\(X\\) is uncountable infinite, or a continuum. This leads us to the concept of a continuous random variable.\nHere, we discuss the most important concepts for practical work with continuous random variables. For a mathematically rigorous treatment, advanced techniques such as measure theory are required. However, we will not delve into those here (see, for example, Billingsley (1995)). Instead, we focus on applied and practical aspects.\nRandom variables that can take on a continuum of values, rather than discrete values like the fair coin we discussed earlier, play a crucial role in practical applications. For instance, consider asset prices or returns. A stock’s price, in principle, could take any value in \\([0, \\infty)\\). Or could it?\nThis is a modeling assumption that can be debated in terms of realism. After all, stock prices are quoted in currency, which has a smallest unit (e.g., cents or pennies). In Lecture 1, we discussed the assumption of unbounded stock prices in the context of sample spaces. Similarly, for other practical cases—such as task completion times, lengths, or weights—a continuum of outcomes often provides a natural model.\nYou might argue that these examples are not truly continuous. For instance, time is measured in hours, minutes, or seconds. However, we can refine our measurements to a much finer scale, with the limit imposed only by our measuring instruments. Time itself is continuous—it does not jump.\nIn contrast, stock prices do have a smallest monetary unit (e.g., cents in the Eurozone). Yet, for practical modeling, treating prices as continuous simplifies computations and analysis.\nEven without delving into the mathematical machinery of measure theory, it is crucial to grasp the implications of continuous random variables. Let’s explore an example of a continuous random variable that can take any value in the interval \\([0,1]\\). In R, we can generate such numbers easily using the runif() function. Consider the following example:\nset.seed(123)\nrunif(10, 0, 1)\n\n [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 0.0455565 0.5281055\n [8] 0.8924190 0.5514350 0.4566147\nNow, consider the probability of this random variable taking on a specific value, say \\(0.4566147\\), one of the values in our list. To investigate, we simulate one million uniformly distributed random numbers in \\([0,1]\\) and calculate the relative frequency of \\(0.4566147\\) occurring:\nuniform_rv &lt;- runif(10^6, 0, 1)\nmean(uniform_rv == 0.4566147)\n\n[1] 0\nThe result is zero—literally zero. Even with one million draws, the random number generator in R produced unique values each time. The probability of hitting any specific value is zero.\nThis occurs because the interval \\([0,1]\\) contains an infinite number of points. For any number in this interval, there are infinitely many numbers both larger and smaller. Assigning a positive probability to any single point would result in probabilities summing to a value greater than 1, violating the laws of probability.\nThis is a fundamental shift from discrete random variables: for continuous random variables, we cannot assign positive probabilities to individual points. Instead, probabilities are associated with intervals of real numbers.\nFor example, consider the probability that a uniformly distributed random variable \\(X \\sim U[0,1]\\) takes a value between \\(0\\) and \\(1/4\\). Using the simulated numbers, we calculate:\nmean(0 &lt;= uniform_rv & uniform_rv &lt;= 1/4)\n\n[1] 0.250841\nThe result is 25%, as expected. Using R’s cumulative distribution function (CDF), we confirm:\npunif(1/4)\n\n[1] 0.25\nFor continuous random variables, probabilities are represented as areas under a curve. This is the major distinction from discrete random variables, where probabilities are assigned to individual points.\nConsider this density function example:\nWith continuous random variables, probabilities are areas under the density function\nMathematically, these areas are calculated using integrals. The probability density function (PDF) describes the distribution as follows:\nThe cumulative distribution function (CDF) provides another essential tool for continuous random variables:\nThis transition from discrete points to areas under a curve marks a crucial conceptual shift in working with continuous random variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables and Monte Carlo Simulation</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#continuous-random-variables-and-probability",
    "href": "05-lecture5.html#continuous-random-variables-and-probability",
    "title": "5  Continuous random variables and Monte Carlo Simulation",
    "section": "",
    "text": "TipDefinition: Continuous Random Variable\n\n\n\nA continuous random variable \\(X\\) can take on a continuum of possible values within a given range.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor every continuous random variable \\(X\\), we have \\(P(X = x) = 0\\) for all \\(x\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipProbability Density Function of a Continuous Random Variable\n\n\n\nFor a continuous random variable \\(X\\) with density function \\(f(x)\\):\n\n\\(f(x) \\geq 0\\), for all \\(x\\).\n\\(\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\).\nFor any \\(a &lt; b\\), the probability that \\(X\\) falls within \\((a, b)\\) is given by the integral:\n\\(P(a &lt; X &lt; b) = \\int_a^b f(x) \\, dx\\).\n\n\n\n\n\n\n\n\n\n\nTipCumulative Distribution Function of a Continuous Random Variable\n\n\n\nThe cumulative distribution function (CDF) shows the probability that \\(X\\) takes a value less than or equal to \\(x\\):\n\\(F(x) = P(X \\leq x)\\).\nFor any \\(a &lt; b\\):\n\\(P(a &lt; X &lt; b) = F(b) - F(a) = \\int_a^b f(x) \\, dx\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables and Monte Carlo Simulation</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#normal-distribution",
    "href": "05-lecture5.html#normal-distribution",
    "title": "5  Continuous random variables and Monte Carlo Simulation",
    "section": "5.2 Normal Distribution",
    "text": "5.2 Normal Distribution\nThe normal distribution is perhaps the most iconic and fundamental probability distribution in all of probability theory. Its bell-shaped curve has become synonymous with ideas of natural variability and randomness. From the heights of people to measurement errors, and from stock returns to the central limit theorem, the normal distribution underpins countless phenomena in the natural and social sciences.\nWhat makes the normal distribution truly remarkable is its simplicity and universality. With just two parameters—the mean (\\(\\mu\\)) and variance (\\(\\sigma^2\\))—it captures the essence of variability in a way that is mathematically elegant and empirically ubiquitous. This distribution lies at the heart of probability theory, serving as the cornerstone for much stochastic modeling. For the application of probability to the modelling of data, the field of statistics, the normal distribution is foundational.\n\n\n\n\n\n\nTipDefinition: Normal Distribution\n\n\n\nThe normal distribution is a continuous probability distribution that is centered around the mean, bell-shaped, symmetric, and completely determined by two parameters: the mean \\(\\mu\\) and the variance \\(\\sigma^2\\). The notation is \\(X \\sim N(\\mu, \\sigma^2)\\). Its probability density function is given by: \\[\nf(x, \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right)\n\\]\n\n\nOften referred to as the Gaussian distribution, after the German mathematician Karl Friedrich Gauss (1777–1855), it is also known as the Gauss-Laplace distribution, honoring Pierre-Simon Laplace (1749–1827). The shape of its probability density function has earned it the nickname bell curve, an image deeply ingrained in the language of science, education, and beyond.\nLet’s use R to make this image tangible.\n\n# Set up the sequence of x values\nx &lt;- seq(-4, 4, length.out = 1000)\n\n# Compute the probability density function of the standard normal distribution\ny &lt;- dnorm(x)\n\n# Create the plot\nplot(x, y, type = \"l\", lwd = 2, col = \"blue\", \n     main = \"The Bell Curve: Standard Normal Distribution\",\n     xlab = \"Value\", ylab = \"Density\")\ngrid()\n\n\n\n\n\n\n\n\nThe code above demonstrates how to create a simple visualization of the bell curve for the standard normal distribution using base R. Here’s a breakdown of the key steps:\nWe first define a range of values: The seq() function generates a sequence of values for the x-axis. In this case, it creates 1,000 equally spaced values between -4 and 4, which is sufficient to capture the central shape of the bell curve.\nIn a second step we compute the density.The dnorm() function computes the probability density of the standard normal distribution at each value in the sequence. If the mean and standard deviation are not explicitly specified in dnorm(), the function assumes the standard normal distribution by default, with a mean of 0 and a standard deviation of 1This function is part of a family of functions for working with random variables in R: - dnorm(x) computes the density at x. - pnorm(x) computes the cumulative distribution function at x. - qnorm(p) gives the quantile for a given probability p. - rnorm(n) generates n random samples from the normal distribution.\nWe need not discuss the basic syntax of the plotting function once more at this stage, because by now we have often done so.\n\n5.2.1 Standardization and the Standard Normal Distribution\nOne of the most powerful properties of the normal distribution is the ability to standardize it. By transforming any normally distributed random variable into a standard form, we unlock the ability to compare and analyze data across scales and contexts.\n\n\n\n\n\n\nTipDefinition: Standard Normal Distribution\n\n\n\nThe standard normal distribution is a normal distribution with a mean \\(\\mu = 0\\) and variance \\(\\sigma^2 = 1\\). Any normally distributed random variable \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\) can be rewritten as a standard normal random variable \\(Z\\) using the transformation: \\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\] By definition, \\(Z \\sim N(0,1)\\).\n\n\nHere’s a draft for discussing the probability mass within 1, 2, and 3 standard deviations of the mean for the normal distribution, along with its significance:\n\n\n5.2.2 The 68-95-99.7 Rule: Probability Mass in the Normal Distribution\nOne of the most useful properties of the normal distribution is that it has a predictable concentration of probability mass around the mean. This is often summarized by the 68-95-99.7 rule, which states:\n\nApproximately 68% of the data falls within 1 standard deviation of the mean.\nApproximately 95% of the data falls within 2 standard deviations of the mean.\nApproximately 99.7% of the data falls within 3 standard deviations of the mean.\n\nThis property is not only fundamental to understanding the normal distribution but also provides a quick and intuitive way to interpret variability in data, regardless of the units of measurement.\nWhy Is This Important?\n\nUniversal Applicability:\nThe percentages remain the same no matter the scale or units of the data. For instance, whether we measure test scores, heights, or stock returns, this property holds for all normally distributed data.\nQuick Validation of Normality: The 68-95-99.7 rule provides a straightforward diagnostic tool for assessing whether a dataset is approximately normal. If the proportions of data falling within 1, 2, and 3 standard deviations deviate significantly from these benchmarks, it is a strong indicator that the data may not be normally distributed.\nPractical Insight:\nIt allows us to quickly assess how “unusual” a value is. For example:\n\nA value more than 2 standard deviations from the mean is relatively rare (occurring in only 5% of cases).\nA value more than 3 standard deviations from the mean is exceptionally rare (occurring in only 0.3% of cases).\n\nDecision-Making:\nThis rule aids in many practical applications, such as quality control, where v alues falling outside of 3 standard deviations might indicate defects or anomalies. Similarly, in finance, it helps in risk assessment by estimating the likelihood of extreme losses or gains.\n\nTo illustrate this property, we can plot the standard normal distribution and shade the areas corresponding to 1, 2, and 3 standard deviations from the mean.\n\n# Define the x-axis range and density\nx &lt;- seq(-4, 4, length.out = 1000)\ny &lt;- dnorm(x)\n\n# Create the plot\nplot(x, y, type = \"l\", lwd = 2, col = \"blue\",\n     main = \"The 68-95-99.7 Rule\",\n     xlab = \"Standard Deviations from the Mean\", ylab = \"Density\")\n\n# Add shaded areas with distinct colors\n# Shade 3 SD region first (light pink)\npolygon(c(-3, seq(-3, 3, length.out = 100), 3), \n        c(0, dnorm(seq(-3, 3, length.out = 100)), 0),\n        col = \"#FBB4AE\", border = NA)\n\n# Shade 2 SD region (light green)\npolygon(c(-2, seq(-2, 2, length.out = 100), 2), \n        c(0, dnorm(seq(-2, 2, length.out = 100)), 0),\n        col = \"#CCEBC5\", border = NA)\n\n# Shade 1 SD region (light blue)\npolygon(c(-1, seq(-1, 1, length.out = 100), 1), \n        c(0, dnorm(seq(-1, 1, length.out = 100)), 0),\n        col = \"#B3CDE3\", border = NA)\n\n# Redraw the outline of the curve on top for clarity\nlines(x, y, lwd = 2, col = \"blue\")\n\n# Add legend positioned middle-left at y = 0.2\nlegend(-4, 0.2, \n       legend = c(\"68% (1 SD)\", \"95% (2 SDs, includes 1 SD)\", \"99.7% (3 SDs, includes 1 & 2 SDs)\"),\n       fill = c(\"#B3CDE3\", \"#CCEBC5\", \"#FBB4AE\"), \n       border = NA, box.lty = 0, bg = \"white\", x.intersp = 0.5, y.intersp = 1.5)\n\n\n\n\n\n\n\n\nTo confirm these proportions, we compute the probabilities using R’s cumulative distribution function (pnorm):\n\n# Probabilities for 1, 2, and 3 standard deviations\np1 &lt;- pnorm(1) - pnorm(-1)  # ~68%\np2 &lt;- pnorm(2) - pnorm(-2)  # ~95%\np3 &lt;- pnorm(3) - pnorm(-3)  # ~99.7%\n\ncat(\"Probability within 1 SD: \", p1, \"\\n\")\n\nProbability within 1 SD:  0.6826895 \n\ncat(\"Probability within 2 SDs: \", p2, \"\\n\")\n\nProbability within 2 SDs:  0.9544997 \n\ncat(\"Probability within 3 SDs: \", p3, \"\\n\")\n\nProbability within 3 SDs:  0.9973002 \n\n\nThese results reinforce the importance of the 68-95-99.7 rule as a tool for interpreting data variability and making informed decisions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables and Monte Carlo Simulation</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#lognormal-distribution",
    "href": "05-lecture5.html#lognormal-distribution",
    "title": "5  Continuous random variables and Monte Carlo Simulation",
    "section": "5.3 Lognormal Distribution",
    "text": "5.3 Lognormal Distribution\nWhile the normal distribution models many natural and financial phenomena, it is not always suitable for modeling certain quantities—such as stock prices—that are constrained to be positive. This is where the lognormal distribution becomes essential. It serves as a natural model for variables that are strictly positive and exhibit multiplicative growth, such as asset prices in financial markets.\n\n\n\n\n\n\nTipDefinition: Lognormal Distribution\n\n\n\nA lognormal random variable \\(Y\\) is one whose natural logarithm is normally distributed. If \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(Y = \\exp(X)\\) follows a lognormal distribution. The probability density function of \\(Y\\) is given by: \\[\nf(y, \\mu, \\sigma) = \\frac{1}{y \\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(\\ln y - \\mu)^2}{2 \\sigma^2}\\right), \\quad y &gt; 0.\n\\]\n\n\n\n5.3.1 Why the Lognormal Distribution for Stock Prices?\nStock prices, by their nature, cannot fall below zero and often grow in a multiplicative manner over time. If the logarithm of a stock price follows a normal distribution, then the stock price itself is lognormally distributed. This aligns with the widely used geometric Brownian motion model This aligns with the widely used geometric Brownian motion model for stock price dynamics.1\n1 Geometric Brownian Motion (GBM) is a stochastic process widely used to model stock prices. It follows the equation: \\[\ndS_t=μ\\,S_t \\,dt+σ\\,S_t\\,dW_t\n\\] where \\(\\mu\\) is the drift (expected return), \\(\\sigma\\) is the volatility, and \\(W_t\\) is a Wiener process (also called a Brownian motion). A Wiener process is a continuous-time stochastic process with independent, normally distributed increments and is fundamental in modeling randomness in finance. GBM ensures that stock prices remain strictly positive. For a more extensive discussion see for example Luenberger (2009)In finance, returns are a natural way to measure changes in stock prices over time. Remember that for a discrete time interval, the return is defined as: \\[\nR = \\frac{S_t - S_0}{S_0},\n\\] where \\(S_0\\) is the initial price and \\(S_t\\) is the price at time \\(t\\). However, this formulation has limitations for very short time intervals or when returns are compounded over time.\nInstead, logarithmic returns (or continuously compounded returns) are defined as: \\[\nr = \\ln\\left(\\frac{S_t}{S_0}\\right).\n\\] This definition arises naturally because it allows for: 1. Additivity in Continuous Time:\nOver small time intervals, the log of cumulative returns adds up, making it easy to model and sum returns over time. For instance, if a stock moves from \\(S_0\\) to \\(S_t\\) and then to \\(S_T\\), the total log return is: \\[\n   r = \\ln\\left(\\frac{S_t}{S_0}\\right) + \\ln\\left(\\frac{S_T}{S_t}\\right) = \\ln\\left(\\frac{S_T}{S_0}\\right).\n   \\] This property simplifies modeling in continuous time frameworks.\n\nConsistency with Compounding:\nFinancial returns often compound multiplicatively (e.g., reinvested dividends or reinvested profits). Logarithmic returns handle compounding naturally and ensure that the total return across intervals corresponds to the product of growth factors.\nSymmetry in Statistical Analysis:\nWhile absolute returns can grow unboundedly in a positive direction, logarithmic returns are symmetric around the mean, simplifying statistical analysis and aligning better with the assumptions of models like geometric Brownian motion.\n\nWhen stock prices follow geometric Brownian motion, their logarithmic returns \\(r\\) are normally distributed: \\[\nr \\sim N(\\mu, \\sigma^2),\n\\] where \\(\\mu\\) is the mean log return and \\(\\sigma^2\\) is the variance. As a result, the stock price itself, given by \\(S_t = S_0 \\exp(r)\\), follows a lognormal distribution. The lognormal distribution is particularly suitable for stock prices because:\n\nPositive Skewness: It allows for rare but extreme positive returns, reflecting real-world market behavior.\nNon-Negativity: Stock prices cannot fall below zero.\nCompounding Effects: It captures the multiplicative nature of price movements over time.\n\n\n\n5.3.2 Comparing Real Stock Data with the Lognormal Distribution\nTo connect the theoretical discussion with real-world data, we’ll analyze historical stock prices from the S&P 500 index. Using data from Yahoo Finance, we compute daily log returns, overlay the empirical distribution with a fitted lognormal distribution, and discuss the fit’s implications.\nLet’s use our R-tools fro overlaying empirical data with the theoretical model of the random variable.\nWe first load tidyquant:\n\nlibrary(tidyquant)\n\nThen we fit the model to actual stock market data for the SP500.\n\n\n\n\n\n\n\n\n\nThe R-code behind this visualization is somewhat involved and I will not go through it here because I want to focus on a more important point of potential confusion, which needs to be explained carefully. Those of you who want to look at the code just unfold the code chunk.\nSo what are we doing here? We started by assuming that stock prices follow a lognormal distribution\n- Prices are strictly positive, which makes the lognormal distribution a natural choice. - If \\(S_t\\) is a stock price and follows geometric Brownian motion:\n\\[\nS_t = S_0 \\exp(X_t)\n\\] where \\(X_t\\) is normally distributed.\nBy our assumption that stock prices are modeled by a lognormal distribution, log-returns follow a normal distribution\n\nReturns are often modeled as additive over time.\nLog-returns are defined as: \\[\nr_t = \\ln\\left(\\frac{S_t}{S_{t-1}}\\right)\n\\]\nIf stock prices follow a lognormal distribution, then log-returns must follow a normal distribution.\n\nWe work with log-returns because they simplify calculations, but prices are what we observe. It would be a mistake to fitting a lognormal distribution to log-returns (instead of prices). The correct modeling framework depends on whether we are working with prices or returns.\nLet me summarize these remarks in a side by side comparison table.\n\n\n\n\n\n\n\n\nVariable\nDistribution\nWhy?\n\n\n\n\nStock Prices \\(S_t\\)\nLognormal\nPrices can’t be negative, and returns compound multiplicatively.\n\n\nLog-Returns \\(r_t\\)\nNormal\nReturns add over time and often appear symmetric.\n\n\n\nNow let us go back to the discussion of what we see in the graph:\n\nFat Tails:\nThe empirical distribution may display so called fat tails, meaning extreme returns are more frequent in real data than predicted by the lognormal model. These events are crucial for risk assessment and portfolio stress testing.\nTo see more clearly whether we have fat tails in our daily log return data, let us visually zoom in to the region of more extreme negative returns.\n\n\n\n\n\n\n\n\n\n\nTo put this graph into perspective remember the 68-95-99.7 rule. It tells us that in a normal distribution:\n\nAbout 68% of observations fall within 1 standard deviation of the mean.\nAbout 95% fall within 2 standard deviations.\nAbout 99.7% fall within 3 standard deviations.\n\nIf daily log-returns followed a normal distribution, we would expect only 0.15% of observations to be more extreme than -3σ. However, in our empirical data, 0.99% of returns fall below this threshold—more than six times the expected frequency.\nThis gives us a critical insight for risk management:\n\nNormal models underestimate extreme downside risk—leading to potential miscalculations in risk measures like Value-at-Risk (VaR), a concept we will discuss later in more detail.\nMarket downturns often exhibit far worse losses than a normal distribution would predict.\nAlternative distributions, such as the t-distribution or generalized extreme value (GEV) models, may be better suited to capturing these extreme tail risks.\n\n\nAsymmetry (Skewness):\nReal stock returns often show negative skewness, where extreme negative returns (e.g., during crashes) are more pronounced than extreme positive returns.\n\nThis is another critical property of financial returns which is asymmetry, or skewness.\nWhat Does Skewness Mean? A normal distribution is symmetric, meaning that extreme positive and negative values are equally likely. However, real-world stock returns often show negative skewness, meaning that large negative returns occur more frequently than large positive returns. This asymmetry is particularly visible during market crashes, when prices tend to decline much faster than they rise during bull markets.\nIn our fat-tail visualization, we focused on the left tail of the distribution (extreme losses). If stock returns were truly symmetric, we would expect to see a similar excess probability mass on the right tail (large gains). However: The left tail extends much further and is more pronounced than the right. Large losses tend to be larger in magnitude than large gains.\nThis is why risk management focuses more on downside risk —investors care more about avoiding catastrophic losses than capturing rare, extreme gains.\nTo quantify this asymmetry, we can compute the skewness statistic of our dataset. A normal distribution has a skewness of 0, while: Negative skewness (&lt; 0) indicates that the left tail is heavier than the right. Positive skewness (&gt; 0) indicates the opposite. In our data the skewness is\n-1.05. For reference: A normal distribution has a skewness of 0 (perfect symmetry). A skewness of -1.05 indicates a strongly asymmetric distribution with heavier left tails.\n\n\n5.3.3 Applications and Limitations: A Balanced Perspective\nLooking at our empirical data, we see that the normal approximation to log-returns fits quite well in the center of the distribution. You can check for yourself that as you go to considering weekly or monthly returns instead of daily ones this fit in the center becomes actually quite good. The lognormal model remains a widely used and valuable framework for understanding stock price dynamics.\nHowever, while the center of the distribution aligns well with theory, the tails remain problematic. As we saw in the fat-tail visualization, extreme negative returns occur far more often than a normal model would suggest. This is a critical issue in risk management, where tail events—such as financial crises or sudden market drops—can have disproportionate consequences.\nThat said, in many other applications, where the focus is on general trends, valuation models, or portfolio optimization, a good fit in the center may be sufficient. The profession does not work with a “wrong” model—rather, different models are used depending on the question being asked. For example:\n\nIn option pricing (Black-Scholes), the lognormal assumption is a reasonable starting point, though corrections (e.g., stochastic volatility models) are often needed.\nIn long-term investment strategies, where extreme short-term fluctuations average out, the normal/lognormal framework remains quite effective.\n\nThus, while tail risks must be explicitly accounted for in risk management, the lognormal assumption remains a useful and practical tool in many areas of finance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables and Monte Carlo Simulation</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#inverse-normal-and-quantiles-in-risk-management",
    "href": "05-lecture5.html#inverse-normal-and-quantiles-in-risk-management",
    "title": "5  Continuous random variables and Monte Carlo Simulation",
    "section": "5.4 Inverse Normal and Quantiles in Risk Management",
    "text": "5.4 Inverse Normal and Quantiles in Risk Management\nIn risk management, a fundamental question is:\nWhat is the worst-case loss I should expect, given a certain probability threshold?\nThis is different from what we studied earlier. Previously, we were given a threshold and asked for the probability of falling below it. Now, we flip the question:\n\nWe are given a probability (e.g., 1%)\nWe want to find the threshold such that losses exceed it only with that probability.\n\nThis is known as the inverse problem in probability, and it plays a central role in Value at Risk (VaR) calculations.\nSuppose you manage a portfolio with uncertain (random) returns. A key risk management question is:\nHow large can losses be over a given time horizon, with a probability of only 1% (or another predefined risk threshold)?\nFor example, a bank may want to ensure that the probability of losing more than a certain percentage of its capital remains below 1%. In this case, the 1% quantile of portfolio returns (often called the 1% Value at Risk, or VaR) is the key statistic.\n\n5.4.1 The Inverse Normal Function in R\nThe quantile function (or inverse cumulative distribution function) helps solve this problem. For a normally distributed random variable \\(X\\), we want to find the threshold \\(x\\) such that: \\(P(X≤x)=p\\) where \\(p\\) is a given probability.\nIn R, we compute this using the qnorm() function. Let’s demonstrate how it works using our data from before:\n\nqnorm(0.01, mean = mean(log_returns), sd = sqrt(var(log_returns)))\n\n[1] -0.02709901\n\n\nThis function finds the 1% quantile of a normal distribution with a given mean and a given standard deviation derived from the logarithmic returns of the SP500. It answers the question:\nWhat is the worst-case daily return we should expect, such that losses exceed this level only 1% of the time?\nThis is often referred to as the inverse normal problem, since it inverts the cumulative distribution function (CDF). Definition: Quantile\n\n\n\n\n\n\nTipp-quantile\n\n\n\nThe \\(p\\)-th quantile (or percentile) of a probability distribution is the value \\(x\\) such that: \\[\nP(X≤x)=p\n\\] If \\(X\\) is normally distributed, i.e., \\(X\\sim N(\\mu,\\sigma^2)\\), then: \\[\nx=F^{−1}(p),\n\\]\nwhere \\(F^{−1}\\) is the inverse CDF (quantile function) of the normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\).\n\n\nThe median is the 50% quantile (\\(P(X≤x)=0.5\\)), dividing the distribution in half. The 1%-quantile (or 99% left-tail quantile) gives us a worst-case threshold, which is critical for risk management models, like value at risk. You can consider any other percentile you might be interested in in this way.\nThe inverse CDF is essential in many areas of finance, including risk management, stress testing, and capital adequacy planning.\nNow that we understand quantiles and the inverse normal, we can directly apply this concept to Value at Risk (VaR).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables and Monte Carlo Simulation</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#value-at-risk",
    "href": "05-lecture5.html#value-at-risk",
    "title": "5  Continuous random variables and Monte Carlo Simulation",
    "section": "5.5 Value at risk",
    "text": "5.5 Value at risk\nDesigning portfolios in a way that enable an acceptable trade off between risk of loss and the potential for profit is a key consideration in portfolio management. Quantitative risk measures are one way to achieve goals like this.\nQuantitative measures of risk a broad topic that we can not fully cover here. But one particular popular risk measure value at risk is directly based on the concept of quantiles of a normally distributed random variable and thus is the perfect application case for appreciating the significance of quantiles as an analytical tool in finance.\nLet us imagine a financial position modeled by a continuous random variable \\(X\\) denoting the change in value of a position at a given future time \\(T\\). In general the variable may take on either positive or negative values depending on its realization. We refer to the random variable \\(X\\) for convenience as position. From the risk perspective, we may focus on the associated loss, which is \\(-X\\).\nThe concept of value at risk (abbreviated VaR) is motivated by the concern about loss. We start by specifying a loss tolerance \\(h\\) between 0 and 1 and a companion confidence level equal to \\(1-h\\). For example, we could choose a loss tolerance \\(h=0.05\\) and a corresponding confidence level of \\(1-h = 0.95\\)\nFor a particular position \\(X\\) and a given loss tolerance \\(h\\), VaR is then the smallest number \\(V\\) such that the probability of a loss greater than \\(V\\) is no more than \\(h\\).\n\n\n\n\n\n\nTipDefinition: Value at risk\n\n\n\nFor a given position \\(X\\) and a given loss toleracne \\(h\\), VaR is the smallest number \\(V\\) such that the probability of a loss greater tan \\(V\\) is no more than \\(h\\):\n\\[\nVaR_h(X) = \\min_{h} \\{ V: P\\left[ - X &gt; V \\right] \\leq h \\}\n\\] Equivalently, VaR is the smallest number \\(V\\) such that the probability of the loss beeing no more than \\(V\\) is gretaer than \\(1-h\\) or:\n\\[\nVaR_h(X) = \\min_{h} \\{ V: P\\left[ - X \\leq V \\right] &gt; 1 - h \\}\n\\]\n\n\nHere is a visualization:\n\n\n\n\n\n95% Value at Risk (VaR) Visualization\n\n\n\n\nThe graph illustrates the 95% Value at Risk (VaR) concept using a normal distribution of daily portfolio log returns. The blue curve represents the probability density function of log-returns. The red-shaded area on the left highlights the 5% tail probability, indicating extreme negative returns that occur with only a 5% likelihood. The green-shaded area represents the complementary 95% probability mass, where returns are expected to fall under normal conditions.\nTwo vertical dashed lines mark key reference points:\n\nThe black dashed line represents the mean return (expected value).\nThe red dashed line represents the VaR threshold, the level of loss that is only exceeded 5% of the time.\n\nThis visualization helps quantify downside risk: A risk manager using VaR at 95% confidence would focus on the red-shaded region to assess the worst-case loss threshold. However, as we have seen with empirical stock return data, real-world distributions often exhibit fat tails, meaning extreme losses occur more frequently than the normal model predicts. This suggests that while VaR is a useful benchmark, adjustments may be needed for more accurate risk assessments.\nThe value at risk as defined here and in the literture comes with an implicit definition of a given time horizon \\(T\\) at which \\(X\\) is realized. If the position is liquid this horizon may be one or a few days. Often there are also regulatry requirements setting the rules how this horizon can or must be chosen.\nNow you can see how the concepts of the inverse normal can be directly brough to bear in the case of normally distributed log returns of stock prices.\n\n\n\n\n\n\nTipProposition: VaR for the normal distribution\n\n\n\nSuppose \\(X\\) follows a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Then\n\\[\nVaR_h(X) = - \\sigma \\, F^{-1}_N(h) - \\mu\n\\] where \\(F_N\\) is the cumulative probability distribution function of the standardized normal variable (with mean 0 and standard deviation 1).\n\n\nHere’s a structured draft covering the three Value at Risk (VaR) examples along with R code to illustrate each case.\nLet’s consider three examples:\nExample 1: Highly Liquid Portfolio with Small Mean Return\nA highly liquid portfolio consists of assets that can be easily bought or sold with minimal impact on price. Examples include:\n\nShort-term U.S. Treasury bills\nLarge-cap ETFs (e.g., SPY, QQQ)\nHighly traded currency pairs (EUR/USD, USD/JPY)\n\nFor such portfolios: Expected returns are very small over short time horizons. VaR is then mainly driven by portfolio variance (volatility) rather than the mean return. In this case the VaR can be approximated as: \\[\n  VaR_{95\\%} \\approx 1.65 \\times \\sigma\n\\] since the mean return is negligible over short periods and \\(-F^{-1}_N(0.05) = 1.65\\). Check using the quantile function of the normal distribution.\n\nqnorm(0.05)*(-1)\n\n[1] 1.644854\n\n\nHere is a numerical R-example for typical values of such a portfolio\n\n# Define parameters\nsigma_liquid &lt;- 0.015  # 1.5% daily volatility (assumption)\nmu_liquid &lt;- 0         # Negligible mean return\nalpha &lt;- 0.05          # 95% confidence level\n\n# Compute 1-day VaR\nVaR_liquid &lt;- qnorm(alpha, mean = mu_liquid, sd = sigma_liquid)\n\n# Output result\nsprintf(\"1-day 95%% VaR for a highly liquid portfolio: %.4f (or %.2f%%)\", VaR_liquid, VaR_liquid * 100)\n\n[1] \"1-day 95% VaR for a highly liquid portfolio: -0.0247 (or -2.47%)\"\n\n\nFor a highly liquid asset with daily volatility of 1.5%, the 1-day 95% VaR is approximately -2.47%, meaning that on 5% of days, the portfolio could lose at least 2.47%** under normal conditions.\nExample 2: A pension fund:\nLet’s consider next the example of a 10-Day VaR for a Pension Fund. A pension fund typically invests in a diversified mix of stocks, bonds, and alternative assets. Suppose a fund manager wants to compute 10-day VaR for a $500 million endowment.\nTo scale VaR from 1-day to N-days, we assume returns follow a normal distribution and use the square-root rule:\n\\[\nVaR_{N-\\text{day}} = VaR_{1-\\text{day}} \\times \\sqrt{N}\n\\] Here is a numerical R-example:\n\n# Define parameters\nsigma_fund &lt;- 0.02    # 2% daily volatility\nmu_fund &lt;- 0.0002     # 0.02% daily return (assumed)\nN &lt;- 10               # 10-day horizon\nportfolio_value &lt;- 500 # $500 million\n\n# Compute 1-day VaR\nVaR_fund_1d &lt;- qnorm(alpha, mean = mu_fund, sd = sigma_fund)\n\n# Compute 10-day VaR using square-root scaling\nVaR_fund_10d &lt;- VaR_fund_1d * sqrt(N) * portfolio_value\n\n# Output result\nsprintf(\"10-day 95%% VaR for a $500M pension fund: $%.2f million\", VaR_fund_10d)\n\n[1] \"10-day 95% VaR for a $500M pension fund: $-51.70 million\"\n\n\nFor a pension fund with 2% daily volatility, a $ 500 million portfolio, and a 10-day horizon, the 10-day 95% VaR is around $ X million. This means the fund can expect to lose at least this amount over a 10-day period with 5% probability.\nExample 3: Portfolio diversification: Finally, let’s look at the example of a diversified portfolio. Now, suppose the pension fund invests 50% in equities and 50% in bonds, with:\n\nStock volatility = 2.5%\nBond volatility = 1.0%\nNegative correlation (-0.3) between stocks and bonds\n\nUnder normal conditions, VaR satisfies subadditivity:\n\\[\nVaR(A + B) \\leq VaR(A) + VaR(B)\n\\]\nwhich means diversification reduces overall risk.\nHere is a numerical R-example_\n\n# Define portfolio components\nsigma_stocks &lt;- 0.025  # 2.5% daily volatility\nsigma_bonds &lt;- 0.01    # 1.0% daily volatility\nw_stocks &lt;- 0.5        # 50% allocation to stocks\nw_bonds &lt;- 0.5         # 50% allocation to bonds\ncorrelation &lt;- -0.3    # Negative correlation\n\n# Compute portfolio volatility\nportfolio_volatility &lt;- sqrt(\n  (w_stocks * sigma_stocks)^2 +\n  (w_bonds * sigma_bonds)^2 +\n  2 * w_stocks * w_bonds * sigma_stocks * sigma_bonds * correlation\n)\n\n# Compute portfolio VaR\nVaR_portfolio &lt;- qnorm(alpha, mean = 0, sd = portfolio_volatility) * portfolio_value\n\n# Compute individual VaRs\nVaR_stocks &lt;- qnorm(alpha, mean = 0, sd = sigma_stocks) * (w_stocks * portfolio_value)\nVaR_bonds &lt;- qnorm(alpha, mean = 0, sd = sigma_bonds) * (w_bonds * portfolio_value)\n\n# Output results\nsprintf(\"VaR without diversification: $%.2f million\", VaR_stocks + VaR_bonds)\n\n[1] \"VaR without diversification: $-14.39 million\"\n\nsprintf(\"VaR with diversification: $%.2f million\", VaR_portfolio)\n\n[1] \"VaR with diversification: $-9.86 million\"\n\n\nWithout diversification, the combined VaR of individual assets would be higher than the VaR of the diversified portfolio. This illustrates the subadditivity property of VaR, which states that risk should not increase when assets are combined. However, this property holds only when log-returns follow a normal distribution. If the normality assumption does not hold—such as in cases with fat tails, skewness, or extreme market events—VaR may no longer be subadditive, and diversification benefits could be overestimated. If you are interested in details of risk management the go to referecne is still McNeil, Embrechts, and Frey (2015).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables and Monte Carlo Simulation</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#empirical-value-at-risk-var-and-its-limitations",
    "href": "05-lecture5.html#empirical-value-at-risk-var-and-its-limitations",
    "title": "5  Continuous random variables and Monte Carlo Simulation",
    "section": "5.6 Empirical Value at Risk (VaR) and Its Limitations",
    "text": "5.6 Empirical Value at Risk (VaR) and Its Limitations\nSo far, we have used parametric VaR based on the normal distribution. However, we can also estimate VaR empirically, directly from historical data, without assuming a particular distribution.\nTo compute the empirical 95% VaR, we:\n\nSort the historical log returns in ascending order.\nFind the return at the 5th percentile of the empirical distribution.\n\nFor a 10-day VaR, we use weekly returns rather than daily data.\n\n# Convert daily log returns to 5-day log returns by summing \n#over non-overlapping 5-day periods\n\nlog_returns_10d &lt;- \n  colSums(matrix(log_returns, nrow = 5, \n                 byrow = TRUE), na.rm = TRUE)\n\nWarning in matrix(log_returns, nrow = 5, byrow = TRUE): data length [1509] is\nnot a sub-multiple or multiple of the number of rows [5]\n\n# Sort 10-day log returns in ascending order\nsorted_returns &lt;- sort(na.omit(log_returns_10d))\n\n# Compute empirical cumulative distribution function (ECDF)\nn &lt;- length(sorted_returns)\necdf_values &lt;- seq(1, n) / n  # Explicitly named for clarity\n\n# Identify empirical quantiles for 95% and 99% VaR\nVaR_95_empirical &lt;- sorted_returns[min(which(ecdf_values &gt;= 0.05))]\nVaR_99_empirical &lt;- sorted_returns[min(which(ecdf_values &gt;= 0.01))]\n\n# Output results\nsprintf(\"Empirical 95%% VaR: %.4f (10-day horizon)\", VaR_95_empirical)\n\n[1] \"Empirical 95% VaR: -0.0386 (10-day horizon)\"\n\nsprintf(\"Empirical 99%% VaR: %.4f (10-day horizon)\", VaR_99_empirical)\n\n[1] \"Empirical 99% VaR: -0.0866 (10-day horizon)\"\n\n\nThe 95% empirical VaR** suggests that, based purely on historical data, the worst 5% of observed weeks had returns of at least -4% or lower. The 99% empirical VaR tells us that in the worst 1% of historical weeks, losses exceeded -0.09.\nLet’s look at a visualization:\n\n\n\n\n\nEmpirical CDF of 5-Day Log Returns with 95% and 99% VaR Thresholds\n\n\n\n\nEmpirical VaRs are straightforward but they also have important limitations, particularly with small samples:\n\nIf we only have a few years of weekly returns, the 5th percentile may be based on very few observations.\nVaR then depends heavily on the worst few weeks, making it unreliable.\nThe 99% quantile requires even fewer observations, making it very sensitive to individual extreme weeks.\nMore advanced techniques (such as Extreme Value Theory (EVT)) can help estimate tail risks beyond observed data.\nIf we haven’t observed an extreme event, empirical VaR ignores it.\nThis happened in the case of Long-Term Capital Management (LTCM).\n\nExample: The LTCM Case: A Real-World Lesson In the late 1990s, the hedge fund Long-Term Capital Management (LTCM) collapsed due to ignoring large, low-probability tail risks. The fund’s risk models were based on historical market behavior, assuming that extreme losses were too unlikely to be of concern.\nHowever, during the 1998 Russian financial crisis, markets experienced far greater volatility than LTCM had anticipated. The fund suffered catastrophic losses, requiring a $3.6 billion bailout coordinated by the Federal Reserve to prevent wider market contagion.\nFor those of you who are interested in advanced risk modeling, a deeper discussion can be found in McNeil, Embrechts, and Frey (2015).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables and Monte Carlo Simulation</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#data-and-statistics",
    "href": "05-lecture5.html#data-and-statistics",
    "title": "5  Continuous random variables and Monte Carlo Simulation",
    "section": "5.7 Data and statistics",
    "text": "5.7 Data and statistics\nOver the course we have now so many times estimated moments for log-returns and then plugged theses estimates into our software provided functions. It seems necessary at this stage to clarify a few things about the statistics of return data.\nUnlike in probability, where we start from the assumption of a random model, typically one or many random variables and think about the consequences for the outcomes, like the properties shape and moments of the distribution and so on, in statistics we take the reverse perspective. We observe data and then try to find out what could be the random variables that might have generated these data, if there is a random process in the background of our observations.\nI would therefore like to discuss some key issues in the empirical analysis of return data. The key data source for estimation is historical returns data, which are today available on the internet at daily frequency. This approach is reasonably reliable fro some parameters such as variances and covariances. It is, however, decidedly unreliable for other parameters such as expected return. The reason why I want to discuss this problem is that the root cause is a fundamental limitation of the estimation process not the quality of the data or measurement.\n\n5.7.1 Peridod-Length Effects\nSuppose that the annual return of a stock is \\(1+r_y\\). It can be thought of as the result of 12 monthly returns and can be written as a product \\[\n1+r_y = (1+r_1)(1+r_2)(1+r_3)\\dots (1+r_{12})\n\\] Note that in this equation tge monthly returns are not measured per annum. They are the actual returns over a month. If the returns are small, we can expand the product and keep only the first order terms as a good approximation: \\[\n1+r_y \\approx 1 + r_1 + r_2 + r_3 + \\dots + r_{12}\n\\] In this approximation the compounding effects are ignored, which is for the purpose of a rough estimates of orders of magnitude of parameters good enough.\nNow let’s think about these returns from the perspective of probability theory and imagine that there is an underlying random variable model generating them. Let these random variables be mutually uncorrelated and each monthly return \\(r_i\\) has the same expected value \\(\\bar{r}\\) and the same variance \\(\\sigma^2\\). using our approximation we find that \\[\n\\bar{r_y} = 12\\, \\bar{r}\n\\] Likewise \\[\n\\sigma_y^2 = \\mathbb{E}\\left[ \\sum_{i=1}^{12}(r_i-\\bar{r}) \\right] = \\mathbb{E}\\left[ \\sum_{i=1}^{12}(r_i - \\bar{r})^2 \\right] = 12 \\sigma^2\n\\] where the pull of the exponent into the squared brackets is a consequence of the assumption that the returns are uncorrelated. Now turn these equations around and taking the suqare root of the variance, we obtain an expression for the monthly values in terms of annual values \\[\\begin{eqnarray*}\n\\bar{r} &=& \\frac{1}{12} \\bar{r}_y \\\\\n\\bar{\\sigma} &=& \\frac{1}{\\sqrt{12}} \\sigma_y\n\\end{eqnarray*}\\] This can be generalized to any length of period. If the period is \\(p\\) part of a year (expressed as a fraction of a year) then the expected return and the standard error of the 1-period rate of return can be found by generalizing from monthly periods \\(p = 1/12\\). This gives us\n\\[\\begin{eqnarray*}\n\\bar{r_p}&=& p \\, \\bar{r}_y \\\\\n\\bar{\\sigma_p}&=& \\sqrt{p} \\, \\sigma_y\n\\end{eqnarray*}\\]\nBecause the expected return decreases linearly with the period, the standard deviation is proportional to the square root of the length of the period. Therefore the ratio if the two increases dramatically as the length is reduced. In the limit, as the length goes to zero, this ratio diverges. Thus rates of return for small perios have high standard deviations compared to their expected values.\nLet#s put this into perspective. The mean annual return for stocks ranges from around \\(6%\\) to \\(30%\\) with a typical value at around \\(12%\\). These mean values change over time so any particual value is meaningful roughly for about 2 or three years. The standard deviation of yearly stock returns ranges from 10% to 60% with typically 15%.\nLet#s translate these numbers into monthly values, thus \\(p=1/12\\). With \\(\\bar{r}_y = 12 %\\) and and \\(\\sigma_y = 15%\\) this leds to $r_{1/12} = 1% and $_{1/12} = 4.33%. So while for the yearly figure the ratio is 1.25 it is 4.3 for the monthly.\nIf we assume the returns are generated through independent daily returns and assume 25o trading days then \\(\\r_{1/250} = 0.048\\) % and \\(\\sigma_{1/250} = 0.95\\) %. The ratio is now 19.8.\nNow we can show how this amplification effect makes the estimation of expected mean rates nearly impossible. Let’s select a basic period length \\(p\\) and try to estimate the mean for this period. We assume that the returns of each period are independent random variables with mean \\(\\bar{r}\\) and standard error \\(\\sigma\\). We also assume that individual returns are mtually uncorrelated.\nSuppose we have \\(n\\) samples of period returns The best estimate for the mean is \\[\n\\hat{\\bar{r}} = \\frac{1}{n} \\sum_{i=1}^n r_i\n\\]\nThe estimate is itself a random variables. If we used different samples, we got different v alues for this estimate. However the expected value of the estimate is the true value \\[\n\\mathbb{E}(\\hat{\\bar{r}}) = \\hat{\\mathbb{E}} \\left( \\frac{1}{n} \\sum_{i = 1}^n r_i \\right) = \\bar{r}\n\\] We compute the standard deviation of the estimate to asess the accuracy of the estimator for the mean returns. \\[\n\\sigma_{\\hat{\\bar{r}}}^2 = \\mathbb{E} \\left[ (\\hat{\\bar{r}} - \\bar{r})^2 \\right] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^n (r_i - \\bar{r}) \\right]^2 = \\frac{1}{n} \\sigma^2\n\\] Hence \\[\n\\sigma_{\\hat{\\bar{r}}} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nThis is a standard formula for the error in the estimate of a mean value.\nIf the period is 1 month, the monthly values used earlier ar \\(\\bar{r} = 1\\) % and \\(\\sigma = 4.33\\) %. If we use 12 month of data we get \\[\n\\sigma_{\\hat{\\bar{r}}} = \\frac{4.33}{\\sqrt{12}}\n\\] which is 1,25 %. The standard error of the mean return estimate is then larger than the mean return itself. If we use 4 years of data we can cut this standard deviation by a factor 2, which still must count as a very poor estimate. For an estimate to be considered good we need to be able to cut down the standard deviation to about 1/10th of the mean. This would require about \\(n = 43.3^2 = 1875\\) or about 156 years of data.\nThis is a well known problem in empirical finance which has even a name. It is called the historical blur problem for the measurement of \\(\\bar{r}\\). It is basically impossible to measure \\(\\bar{r}\\) to within workable accuracy using historical data. The problem can not be improved much by changing the period length. If longer periods are used, each sample is more reliable but fewer independent samples are obtained in any year. If the period is smaller, more samples are available but each is worse in terms of the ratio of standard deviation to mean value.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables and Monte Carlo Simulation</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#introdcution-to-monte-carlo-simulation-in-finance",
    "href": "05-lecture5.html#introdcution-to-monte-carlo-simulation-in-finance",
    "title": "5  Continuous random variables and Monte Carlo Simulation",
    "section": "5.8 Introdcution to Monte Carlo Simulation in Finance",
    "text": "5.8 Introdcution to Monte Carlo Simulation in Finance\nMonte Carlo simulation is a computational technique that uses repeated random sampling to estimate uncertain outcomes. It is particularly useful when dealing with probabilistic models and complex systems where analytical solutions are difficult or impossible to derive. It is also an excellent application context where we can discuss how to optimize speed and efficiency in R code.\nThe term Monte Carlo Simulation originates from the Manhattan Project during World War II, where scientists, including Stanislaw Ulam and John von Neumann, used random sampling techniques to model complex physical processes, such as neutron diffusion in nuclear reactions. The name “Monte Carlo” was inspired by the famous Monte Carlo Casino in Monaco, reflecting the method’s reliance on randomness and probability—just like games of chance in a casino.\nMonte Carlo methods are used extensively in computational Finance. Financial markets are inherently uncertain, and Monte Carlo methods allow us to model this uncertainty by simulating a large number of possible outcomes. Some key applications in finance include:\n\nRisk estimation (e.g., computing Value at Risk).\n\nPricing derivatives (e.g., options pricing using risk-neutral Monte Carlo methods).\n\nPortfolio optimization (e.g., estimating expected returns and volatility distributions).\n\nIn this lecture, we focus on estimating Value at Risk (VaR) which we have discussed from a conceptual viewpoint in this lecture and where we have focussed on cases of normally distributed log returns of stocks. We also briefly touched on the approach of historical simulation, where past return data are used to estimated the risk\nIn contrast to these methods Monte Carlo Simulation simulates future returns using random draws based on estimated statistical properties like mean and standard deviation. This is an apporach where the computer can shine.\n\n5.8.1 Simulating Portfolio Returns for VaR Calculation Step-by-Step Implementation in R\nWe will now implement a basic Monte Carlo simulation in R to estimate VaR.\nThe approach consists of:\n\nEstimating the portfolio’s mean return and standard deviation based on historical data.\n\nGenerating thousands of random return scenarios assuming a normal distribution.\n\nCalculating the simulated portfolio losses and extracting the VaR from the loss distribution.\n\n\n5.8.1.1 Step 1: Simulating Portfolio Returns\nWe begin by assuming that log-returns of a portfolio follow a normal distribution, which is a common assumption in risk modeling.\nGiven historical return data, we estimate the mean and standard deviation, then simulate thousands of potential returns. I do not do an explicit estimation in the follwoing code chunk because we have done so explicitly at many parts in this lecture. This could be done with historical data (keeping the blur problem in mind) and then - since the normal distribution is fully characterized by mean and standard deviation - we can use the normality assumption to pin down a simulation distribution. For the sake of demonstration I do 10000 simulations here fro somewhat typical returns parameter values. You can play with this. This can be in principle scaled up a lot. Note also that we set a random seed for reproduceability here. For the visualization I take the ggplot2 package, which I think, produced the visualy most appealing results in R.\n\n# Load necessary packages\nlibrary(ggplot2)  # For visualization\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Simulated historical daily log-returns (e.g., from a stock index)\nhistorical_returns &lt;- rnorm(250, mean = 0.0005, sd = 0.01)  # 250 trading days\n\n# Estimate parameters (mean and standard deviation)\nmu &lt;- mean(historical_returns)\nsigma &lt;- sd(historical_returns)\n\n# Simulate 10,000 future return scenarios using Monte Carlo\nn_sim &lt;- 10000\nsimulated_returns &lt;- rnorm(n_sim, mean = mu, sd = sigma)\n\n# Quick visualization of simulated returns\nggplot(data.frame(returns = simulated_returns), aes(x = returns)) +\n  geom_histogram(bins = 50, fill = \"blue\", alpha = 0.5) +\n  labs(title = \"Histogram of Simulated Portfolio Returns\",\n       x = \"Simulated Return\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n5.8.1.2 Step 2: Estimating Value at Risk (VaR)\nOnce we have simulated thousands of possible portfolio returns, we extract Value at Risk (VaR) from the distribution. Since VaR is the quantile of the loss distribution, we take the left-tail quantile (e.g., 5% or 1% quantile).\n\n# Compute portfolio losses (negative returns)\nsimulated_losses &lt;- -simulated_returns  # Losses are negative returns\n\n# Compute VaR at 95% and 99% confidence levels\nVaR_95 &lt;- quantile(simulated_losses, probs = 0.95)\nVaR_99 &lt;- quantile(simulated_losses, probs = 0.99)\n\n# Print results\ncat(\"Monte Carlo Estimated VaR:\\n\")\n\nMonte Carlo Estimated VaR:\n\ncat(\"95% VaR:\", round(VaR_95, 4), \"\\n\")\n\n95% VaR: 0.0151 \n\ncat(\"99% VaR:\", round(VaR_99, 4), \"\\n\")\n\n99% VaR: 0.0214 \n\n\nThe raw VaR numbers reported in our Monte Carlo simulation— e.g., 95% VaR = 0.0151—represent the potential daily loss as a fraction of the portfolio value. However, to make this result more intuitive, let’s express it in real monetary terms.\nSuppose we are managing a $10 billion USD portfolio. The 95% VaR of 0.0151 means:\n\nDaily Loss Interpretation:\n\nUnder normal market conditions, there is a 95% probability that the portfolio will lose no more than $ 151 million USD in a single trading day (1.51% of $ 10 billion).\nConversely, there is a 5% chance that losses will exceed this amount.\n\nYearly Exceedance Frequency:\n\nSince 5% of trading days (approximately 12–13 days per year in a 250-day trading year) fall in the worst-case loss category,\nwe expect to lose more than $151 million USD on about 12–13 days per year.\nYou can do this kind of translation for the 99% VaR for yourself. You can even support your intuition by leveraging the power of R to write a function yourself to make such interpretations dynamic for arbitrary portfolio sizes and let R do the interpretation for you. Let us call this function interpret_VaR:\n\n# Function to interpret Monte Carlo VaR results\ninterpret_var &lt;- function(VaR_95, VaR_99, portfolio_value, trading_days = 250) {\n  # Compute monetary losses\n  VaR_95_dollars &lt;- VaR_95 * portfolio_value\n  VaR_99_dollars &lt;- VaR_99 * portfolio_value\n  \n  # Compute expected exceedance frequency\n  # ~5% exceedance\n  days_exceeding_95 &lt;- trading_days * (1 - 0.95)\n  # ~1% exceedance\n  days_exceeding_99 &lt;- trading_days * (1 - 0.99)  \n  \n  # Print interpretation\n  cat(\"Monte Carlo Value at Risk (VaR) Interpretation:\\n\")\n  cat(\"-------------------------------------------------\\n\")\n  cat(\"Portfolio Value: $\", format(portfolio_value, big.mark = \",\"), \"\\n\\n\")\n  \n  cat(\"95% VaR Interpretation:\\n\")\n  cat(\"- Expected daily loss will not exceed\", \n      round(VaR_95 * 100, 2), \"% in 95% of cases.\\n\")\n  cat(\"- This corresponds to a daily loss limit of approximately $\", \n      format(round(VaR_95_dollars, 2), big.mark = \",\"), \".\\n\")\n  cat(\"- However, about\", \n      round(days_exceeding_95, 1), \"days per year, losses may exceed this level.\\n\\n\")\n  \n  cat(\"99% VaR Interpretation:\\n\")\n  cat(\"- Expected daily loss will not exceed\", \n      round(VaR_99 * 100, 2), \"% in 99% of cases.\\n\")\n  cat(\"- This corresponds to a daily loss limit of approximately $\", \n      format(round(VaR_99_dollars, 2), big.mark = \",\"), \".\\n\")\n  cat(\"- However, about\", \n      round(days_exceeding_99, 1), \"days per year, losses may exceed this level.\\n\")\n}\n\n# Example usage: Assume a portfolio of $10 billion\nportfolio_value &lt;- 10e9  # $10 billion\n\n# Call the function with Monte Carlo estimated VaR\ninterpret_var(0.0151 , 0.0214 , portfolio_value)\n\nMonte Carlo Value at Risk (VaR) Interpretation:\n-------------------------------------------------\nPortfolio Value: $ 1e+10 \n\n95% VaR Interpretation:\n- Expected daily loss will not exceed 1.51 % in 95% of cases.\n- This corresponds to a daily loss limit of approximately $ 1.51e+08 .\n- However, about 12.5 days per year, losses may exceed this level.\n\n99% VaR Interpretation:\n- Expected daily loss will not exceed 2.14 % in 99% of cases.\n- This corresponds to a daily loss limit of approximately $ 2.14e+08 .\n- However, about 2.5 days per year, losses may exceed this level.\n\n\n\n\n5.8.1.3 Step 3: Optimizing the Monte Carlo Simulation\nA naive Monte Carlo approach can be slow for large-scale simulations. To optimize performance, we introduce:\n\nVectorization: Using matrix operations instead of loops.\nParallel computing: Running simulations in parallel using the future.apply package.\nEfficient data handling: Using data.table instead of standard data frames.\n\nThese are excellent points! To expand and refine this section, I’ll structure it as follows:\n\n\n\n\n5.8.2 Step 3: Optimizing the Monte Carlo Simulation\nMonte Carlo simulations can become computationally expensive when scaled up, making performance optimization an important consideration.\nWe introduce three key techniques:\n1. Vectorization – Using efficient matrix operations instead of loops.\n2. Parallel Computing (future.apply) – Distributing tasks across multiple CPU cores.\n3. Efficient Data Handling (data.table) – Reducing memory overhead and improving speed.\nR has built in functionality for parallel computing. Here we will discuss parallel computing with future.apply.\nSo what us future.apply and how does t help? The future.apply package allows us to run apply-type functions in parallel across multiple CPU cores, improving computational speed when processing large datasets. Normally, R executes code sequentially (one operation at a time).\nWhat future.apply does is that it enables parallel execution, meaning tasks run simultaneously across multiple CPU cores, reducing computation time.\nParallelization is essential when:\n\nYou need to simulate millions of scenarios (e.g., high-frequency Monte Carlo simulations).\n\nYour computations involve nested loops (e.g., pricing derivatives across many strike prices).\n\nYou work with large financial datasets that slow down single-threaded processing.\n\nHowever, parallelization is not always beneficial for small-scale problems due to overhead costs of setting up parallel workers.\nLet us compare sequential vs. parallel Monte Carlo Simulation to demonstrate these points. We first run a single-core Monte Carlo simulation and then compare it to a parallelized version.\n\n# Load necessary package\nlibrary(future.apply)\n\nLoading required package: future\n\n# Set up parallel processing using all available CPU cores\nplan(multisession)\n\n# Define Monte Carlo function for Value at Risk (VaR)\nmonte_carlo_var &lt;- function(n_sim, mu, sigma, confidence = 0.95) {\n  simulated_losses &lt;- -rnorm(n_sim, mean = mu, sd = sigma)\n  return(quantile(simulated_losses, probs = confidence))\n}\n\n# Define number of simulations\nn_sim &lt;- 100000  # Large-scale simulation\n\n# Measure time for sequential computation (single core)\nstart_time_seq &lt;- Sys.time()\nVaR_95_seq &lt;- sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95))\nend_time_seq &lt;- Sys.time()\ntime_seq &lt;- as.numeric(difftime(end_time_seq, start_time_seq, units = \"secs\"))  # Convert to numeric seconds\n\n# Measure time for parallel computation (multiple cores)\nstart_time_par &lt;- Sys.time()\nVaR_95_par &lt;- future_sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95), future.seed = TRUE)\nend_time_par &lt;- Sys.time()\ntime_par &lt;- as.numeric(difftime(end_time_par, start_time_par, units = \"secs\"))  # Convert to numeric seconds\n\n# Print timing results\ncat(\"Execution Time Comparison:\\n\")\n\nExecution Time Comparison:\n\ncat(\"Sequential Execution Time: \", round(time_seq, 2), \" seconds\\n\")\n\nSequential Execution Time:  0.07  seconds\n\ncat(\"Parallel Execution Time: \", round(time_par, 2), \" seconds\\n\")\n\nParallel Execution Time:  1.49  seconds\n\ncat(\"Speedup Factor: \", round(time_seq / time_par, 2), \"x faster with parallel computing\\n\")\n\nSpeedup Factor:  0.05 x faster with parallel computing\n\n\nLet’s break down the key steps in our parallel Monte Carlo simulation for VaR and explain why each part matters.\n\nlibrary(future.apply)\nplan(multisession)\n\nfuture.apply is an enhanced version of apply() functions** that supports parallel execution. plan(multisession) tells R to use multiple CPU cores instead of executing code sequentially. Normally, R runs tasks one at a time (single-threaded). With multisession, R splits tasks across CPU cores, reducing execution time for large simulations.\n\nmonte_carlo_var &lt;- function(n_sim, mu, sigma, confidence = 0.95) {\n  simulated_losses &lt;- -rnorm(n_sim, mean = mu, sd = sigma)\n  return(quantile(simulated_losses, probs = confidence))\n}\n\nWe generate n_sim random portfolio losses from a normal distribution (negating returns to represent losses).\nWe extract the VaR quantile from the simulated loss distribution. This function encapsulates our Monte Carlo simulation into a modular, reusable block of code. Later, we parallelize multiple independent simulations using this function.\n\nstart_time_seq &lt;- Sys.time()\nVaR_95_seq &lt;- sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95))\nend_time_seq &lt;- Sys.time()\ntime_seq &lt;- as.numeric(difftime(end_time_seq, start_time_seq, units = \"secs\"))\n\nWe measure execution time for the sequential (single-core) simulation.\nsapply(1:10, function(x) ...) runs 10 independent Monte Carlo simulations.\nThis serves as our baseline for performance comparison. We need to measure the cost of computing independent Monte Carlo simulations one by one.\n\nstart_time_par &lt;- Sys.time()\nVaR_95_par &lt;- future_sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95), future.seed = TRUE)\nend_time_par &lt;- Sys.time()\ntime_par &lt;- as.numeric(difftime(end_time_par, start_time_par, units = \"secs\"))\n\nWe now run 10 Monte Carlo simulations in parallel, distributing them across CPU cores. future.seed = TRUE ensures statistically valid random number generation across parallel workers. Unlike sequential execution, multiple CPU cores now share the workload. This should reduce execution time, but the actual gain depends on the computational task and system configuration.\nThe last step just compares performance\nAt first glance, the results of our output seems counterintuitive—why is parallel execution slower than sequential execution here?\nIt is key to understand that parallelization does not always improve speed. Small tasks don’t benefit from parallel execution.\n\nThe overhead cost of setting up parallel workers (process spawning, memory sharing, inter-process communication) can exceed the time saved for small problems.\n\nParallelization shines for large-scale computations.\n\nLet’s increase the number of simulations to 10 million (1e7) per iteration to demonstrate when parallel computing becomes a necessity.\n\n# Define larger-scale simulation\nn_sim &lt;- 1e7  # 10 million simulations per iteration\n\n# Run the comparison again\nstart_time_seq &lt;- Sys.time()\nVaR_95_seq &lt;- sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95))\nend_time_seq &lt;- Sys.time()\ntime_seq &lt;- as.numeric(difftime(end_time_seq, start_time_seq, units = \"secs\"))\n\nstart_time_par &lt;- Sys.time()\nVaR_95_par &lt;- future_sapply(1:10, function(x) monte_carlo_var(n_sim, mu, sigma, 0.95), future.seed = TRUE)\nend_time_par &lt;- Sys.time()\ntime_par &lt;- as.numeric(difftime(end_time_par, start_time_par, units = \"secs\"))\n\ncat(\"Execution Time Comparison (Larger Simulations):\\n\")\n\nExecution Time Comparison (Larger Simulations):\n\ncat(\"Sequential Execution Time: \", round(time_seq, 2), \" seconds\\n\")\n\nSequential Execution Time:  6.83  seconds\n\ncat(\"Parallel Execution Time: \", round(time_par, 2), \" seconds\\n\")\n\nParallel Execution Time:  3.45  seconds\n\ncat(\"Speedup Factor: \", round(time_seq / time_par, 2), \"x faster with parallel computing\\n\")\n\nSpeedup Factor:  1.98 x faster with parallel computing\n\n\nNow we see a doubling of speed achieved by our prallelisation. You can play around yourself to get a feeling for the performance enhancements you may gain for computationally large problems. Maybe you can go back to the very first probelm we dealt with in this coures financial-transaction-identifyer collision probability ofr large transaction volume and large output space \\(M\\).\nLet’s visualize the execution time comparison of our example:\n\n# Load necessary package\nlibrary(ggplot2)\n\n# Simulated execution times (adjust these with actual results if needed)\nexecution_times &lt;- data.frame(\n  Simulation_Size = factor(c(\"100,000 Sims\", \"10,000,000 Sims\"), \n                           levels = c(\"100,000 Sims\", \"10,000,000 Sims\")),\n  Execution_Time = c(0.07 , 1.22, 6.17, 2.85),  # Replace with actual results\n  Method = rep(c(\"Sequential\", \"Parallel\"), each = 2)\n)\n\n# Create bar plot\nggplot(execution_times, aes(x = Simulation_Size, y = Execution_Time, fill = Method)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", alpha = 0.7) +\n  labs(title = \"Execution Time Comparison: Sequential vs. Parallel Monte Carlo Simulation\",\n       x = \"Simulation Size\",\n       y = \"Execution Time (seconds)\") +\n  scale_fill_manual(values = c(\"blue\", \"red\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere are the key Takeaways from the visualization:\nFor small-scale simulations (100,000 draws), parallelization adds overhead and is actually slower. For large-scale simulations (10 million draws), parallel computing significantly reduces execution time. The break-even point depends on system resources, but the lesson is clear: parallel computing is most useful for large problems.\nIf we need even more computational power, tools such as GPU-based parallelization can help:\n\ngpuR: Allows running matrix operations on GPUs in R.\ntorch (for R): Enables deep learning-like parallel computations on GPUs.\ncuda.ml: Uses NVIDIA’s CUDA for ultra-fast numerical processing.\n\nThese methods are beyond the scope of our discussion but become crucial for high-frequency trading, derivative pricing, and deep learning applications in finance.\n\n5.8.2.1 Efficient Data Handling with data.table\ndata.tableis an R package that increases the efficiency of data handling and becomes useful if you are dealing with big datasets. Monte Carlo Simulation is a typical context where such issues can arise quickly. In this case the use of data.tablebrings enhanced memory efficiency: Standard data.frame operations create copies of data, increasing memory usage. data.table avoids this by modifying objects in place.\ndata.table is optimized for speed. It can process millions of rows significantly faster than data.frame.\nLet’s discuss an example where we compare data frames and the data.tablepackage. Let’s compare performance for storing and manipulating simulated losses.\n\n# Load the data.table package\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:xts':\n\n    first, last\n\n# Generate a large dataset of simulated losses\nn_sim &lt;- 1e6  # 1 million simulations\nsimulated_losses &lt;- -rnorm(n_sim, mean = mu, sd = sigma)\n\n# Store as a standard data frame\ndf_losses &lt;- data.frame(losses = simulated_losses)\n\n# Store as a data.table\ndt_losses &lt;- data.table(losses = simulated_losses)\n\n# Measure time to compute quantiles using data.frame\nstart_df &lt;- Sys.time()\nVaR_95_df &lt;- quantile(df_losses$losses, probs = 0.95)\nend_df &lt;- Sys.time()\ntime_df &lt;- as.numeric(difftime(end_df, start_df, units = \"secs\"))  # Convert to numeric\n\n# Measure time to compute quantiles using data.table\nstart_dt &lt;- Sys.time()\nVaR_95_dt &lt;- dt_losses[, quantile(losses, probs = 0.95)]\nend_dt &lt;- Sys.time()\ntime_dt &lt;- as.numeric(difftime(end_dt, start_dt, units = \"secs\"))  # Convert to numeric\n\n# Print timing results\ncat(\"Execution Time Comparison for VaR Calculation:\\n\")\n\nExecution Time Comparison for VaR Calculation:\n\ncat(\"Data Frame Execution Time: \", round(time_df, 4), \" seconds\\n\")\n\nData Frame Execution Time:  0.0114  seconds\n\ncat(\"Data Table Execution Time: \", round(time_dt, 4), \" seconds\\n\")\n\nData Table Execution Time:  0.0196  seconds\n\ncat(\"Speedup Factor: \", round(time_df / time_dt, 2), \"x faster with data.table\\n\")\n\nSpeedup Factor:  0.58 x faster with data.table\n\n\n\n\n\n5.8.3 Reporting and Documentation of R Code\nMonte Carlo simulations often generate large volumes of data, and proper documentation and reporting are essential for:\n\nReproducibility: Ensuring others (or future you) can understand and replicate your work.\n\nClarity: Providing a clear summary of simulation results, assumptions, and methodology.\n\nCommunication: Making results accessible to different stakeholders (e.g., researchers, risk managers).\n\n\n\n5.8.3.1 Writing Readable and Reproducible R Code\nHere are three best practice tips:\n\nUse clear function names and comments\n\nAvoid cryptic variable names.\n\nAdd comments to explain key steps.\n\nInclude metadata in scripts\n\nDefine input parameters at the beginning.\n\nSet a random seed for reproducibility.\n\nUse docstrings with roxygen2 (for functions in packages).\n\nThe roxygen2 package is a widely used tool for automatically generating documentation for R functions, especially in package development. Instead of manually writing separate documentation files, roxygen2 allows you to write structured docstrings directly above function definitions using specially formatted comments (#’).\nWhy use roxygen2? Here are three reasons:\n\nConsistency – Ensures documentation is always in sync with the function code.\nEfficiency – Eliminates the need for manually maintaining help files.\nIntegration – Works seamlessly with RStudio, devtools, and pkgdown for package development.\n\nroxygen works by extracting special comments (#’) and converting them into R help files. You can specify parameters, return values, usage examples, and references. The devtools::document() function compiles the documentation.\nFor example, this is how a well documented Monte Carlo function could look like.\n\n#' Monte Carlo Simulation for Value at Risk (VaR)\n#'\n#' This function estimates Value at Risk using Monte Carlo simulations.\n#'\n#' @param n_sim Number of simulations\n#' @param mu Mean return of asset\n#' @param sigma Standard deviation of returns\n#' @param confidence Confidence level for VaR (default = 0.95)\n#' @return Estimated VaR\n#' @examples\n#' monte_carlo_var(10000, 0.0005, 0.01, 0.95)\nmonte_carlo_var &lt;- function(n_sim, mu, sigma, confidence = 0.95) {\n  simulated_losses &lt;- -rnorm(n_sim, mean = mu, sd = sigma)\n  return(quantile(simulated_losses, probs = confidence))\n}\n\n\nThis uses roxygen2 syntax for automatic documentation generation.\n\nCalling devtools::document() in a package project generates documentation from comments.\n\nOnce we run a Monte Carlo simulation, we need to summarize and present the results effectively.\nA structured summary table is often clearer than raw printouts.\n\n# Example results from Monte Carlo VaR estimation\nConfidence_Level &lt;- c(\"95%\", \"99%\")\nVaR_Percentage &lt;- c(0.0151, 0.0214)  # Example VaR values\nPortfolio_Value &lt;- 10e9  # Assume $10 billion portfolio\n\n# Compute VaR amount in monetary terms\nVaR_Amount &lt;- VaR_Percentage * Portfolio_Value\n\n# Create a data frame\nresults &lt;- data.frame(\n  Confidence_Level = Confidence_Level,\n  VaR_Percentage = VaR_Percentage,\n  Portfolio_Value = Portfolio_Value,\n  VaR_Amount = VaR_Amount\n)\n\n# Print formatted table\nprint(results)\n\n  Confidence_Level VaR_Percentage Portfolio_Value VaR_Amount\n1              95%         0.0151           1e+10   1.51e+08\n2              99%         0.0214           1e+10   2.14e+08\n\n\nInstead of manually copying outputs, Quarto/RMarkdown can generate reports dynamically.\nHere’s the Base R version of the Quarto (qmd) report snippet, avoiding ggplot2 and dplyr:\nThis is how a rewritten example report snippet in Quarto (.qmd) would - for instance - look like:\n\n---\ntitle: \"Monte Carlo Simulation Report\"\nauthor: \"Your Name\"\ndate: \"`r Sys.Date()`\"\nformat: html\n---\n  \n# Load necessary base R functions\n  \n# Print results as an HTML table using knitr::kable\nlibrary(knitr)\nkable(results, format = \"html\", caption = \"Monte Carlo Estimated VaR\")\n\n# Define execution times (example data)\nSimulation_Size &lt;- c(\"100,000 Sims\", \"10,000,000 Sims\")\nExecution_Time_Sequential &lt;- c(0.07, 25.3)  # Hypothetical values\nExecution_Time_Parallel &lt;- c(1.22, 5.8)\n\n# Create bar plot using base R\nbarplot(\n  height = rbind(Execution_Time_Sequential, Execution_Time_Parallel),\n  beside = TRUE,\n  col = c(\"blue\", \"red\"),\n  names.arg = Simulation_Size,\n  legend.text = c(\"Sequential\", \"Parallel\"),\n  main = \"Execution Time Comparison\",\n  xlab = \"Simulation Size\",\n  ylab = \"Execution Time (seconds)\"\n)\n\n# Save summary table as CSV\nwrite.csv(results, \"monte_carlo_var_results.csv\", row.names = FALSE)\n\n# Save entire workspace (useful for large simulations)\nsave.image(\"monte_carlo_workspace.RData\")\n\nIn a report you would of course interweave text and code, but the example chunk gives you an idea. Quarto is very versatily and visually appealing. For instance the lecture notes for this code as well as the slides and all other materials are entirely written in Quarto.\n\n\n\n\nBillingsley, Patrick. 1995. Probability and Measure. Wiley.\n\n\nLuenberger, David. 2009. Investment Science. Oxford University Press.\n\n\nMcNeil, Alexander, Paul Embrechts, and Rudiger Frey. 2015. Quantitative Risk Management. Princeton University Press.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables and Monte Carlo Simulation</span>"
    ]
  },
  {
    "objectID": "06-references.html",
    "href": "06-references.html",
    "title": "6  References",
    "section": "",
    "text": "Billingsley, Patrick. 1995. Probability and Measure. Wiley.\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about\nChance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its\nApplications. 3rd ed. Vol. 1. Wiley.\n\n\nGilboa, Ithak. 2009. Theory of Decision Under Uncertainty.\nCambridge University Press.\n\n\nGlasserman, Paul. 2003. Monte Carlo Methods in Financial\nEngineering. Springer.\n\n\nGrolemund, Garrett. 2014. Hands on Programming with\nR. O’Reilly. https://rstudio-education.github.io/hopr/.\n\n\nHalpern, Joseph. 2017. Reasoning about Uncertainty. 2nd ed. MIT\nPress.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical\nIntroduction. Princeton University Press.\n\n\nLiu, Jun S. 2001. Monte Carlo Strategies in Scientific\nComputing. Springer.\n\n\nLo, Andrew, and Craig MacKinlay. 2019. A Non-Random Walk down\nWallstreet. Princeton University Press.\n\n\nLuenberger, David. 2009. Investment Science. Oxford University\nPress.\n\n\nMcNeil, Alexander, Paul Embrechts, and Rudiger Frey. 2015.\nQuantitative Risk Management. Princeton University Press.\n\n\nSamuelson, Paul. 1965. “Proof That Properly Anticipated Prices\nFluctuate Randomly.” Industrial Management Review 6.\n\n\nTaleb, Nassim Nicolas. 2007. Fooled by Randomenss. Penguin.\n\n\nTooze, Adam. 2018. Crashed. How a Decade of Financial Crisis Changed\nthe World. Viking.\n\n\nWickham, Hadley. 2019. Advanced r. 2nd ed. Taylor; Francis.\n\n\nWickham, Hadley, and Jenifer Bryan. 2023. R Packages: Organize,\nTest, Document, and Share Your Code. O’Reilly. https://r-pkgs.org/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data\nScience. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "07-appendix.html",
    "href": "07-appendix.html",
    "title": "7  Appendix",
    "section": "",
    "text": "7.1 Further Reading\nThis is the end of this lecture and these lecture notes which have made an attempt to teach you introductory probability by building probability concepts with R in a finance application context. The field of probability and its applications is huge an rich and only a tiny part of it could be covered in this course. The hope is, of course, that the lecture has been able to teach you some tools and concepts which will enable those of you who found this subject interesting to explore it on your own, using your own interests and the literature on probability as a guide. For those of you who want to do so, let me give you a few pointers to the literature, which might cover for some of you the next steps.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "07-appendix.html#further-reading",
    "href": "07-appendix.html#further-reading",
    "title": "7  Appendix",
    "section": "",
    "text": "7.1.1 Probability theory and mathematics\nI have confessed already in the beginning of this lecture that I think that among the many probability books on the market the book by Feller (1968) is still an outstanding master piece in exposition. It is full of passion and enthusiasm for the subject and interesting throughout. Its orientation is clearly mathematical but the confinement to discrete sample spaces allows him to cover much ground with minimal machinery. Reading or studying this books is perhaps even more fun today since you can complement by using the computer to study and think about the many interesting examples contained in it, something you could not do when the book first appeared.\nAmong the big ideas in probability was the attempt to build a general, axiomatic framework for the field that would establish probability theory firmly as a field of mathematics. The modern mathematical formulation of probability is perhaps nowhere better presented than in Billingsley (1995). It is a very good source for those of you who have or had some more advanced training in mathematics.\n\n\n7.1.2 Further studies of R\nIf you are a beginner of R and if you found the language and what it can do useful and interesting I recommend to study the book by Grolemund (2014). It is a great read and I have relied very much on it for this course. From this book I tried to emulate the idea that R is best taught withing a concrete context instead of teaching the language as such perhaps with a few toy examples and then apply the machinery afterwards. The book is also available online and I recommend the beginners among you very warmly to study it. You can get the book on the internet at https://rstudio-education.github.io/hopr/\nMore advanced students in the group who want to learn and understand R seriously as a programming language, should at some stage study Wickham (2019). This book is also freely available in an online edition at https://adv-r.hadley.nz/.\nFinally let me point out to you an excellent guide to the huge number of useful books many of them freely available at the site https://www.bigbookofr.com/ which is a regularly updated list of books about R and applications of R.\n\n\n7.1.3 Probability, Philosophy and Concepts\nFor those of you who have an interest in conceptual and foundational discussions and reflections, I would like to point you to four sources in particular. First of all, I think you should at some stage study the outstanding discussion by Diaconis and Skyrms (2019). I made several references to it in these lecture notes. It is a truly good read.\nFor those of you who have the training and the interest in more formal discussions, I would like to recommend the monographs of Gilboa (2009) and Halpern (2017)\nAn outstanding and very famous book, which is more of a meditation about randomness also with respect to financial markets is Taleb (2007). If you had to choose one and only one book in this section, it is perhaps this one.\n\n\n7.1.4 Monte Carlo Simulation and Computing\nWe discussed a lot about simulation in this lecture. If you want to learn about Monte Carlo simulation seriously, you should at some stage study Liu (2001). A standard reference for Monte Carlo Simulation in Finance and Financial Engineering is Glasserman (2003)\n\n\n7.1.5 Finance and Risk Management\nThere are many books on Finance, some very technical other verbose and business like. I believe that among the Finance books I know the book by Luenberger (2009) is truly outstanding. A comprehensive overview on the methods and mathematics of risk management in a financial context is McNeil, Embrechts, and Frey (2015)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBillingsley, Patrick. 1995. Probability and Measure. Wiley.\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nGilboa, Ithak. 2009. Theory of Decision Under Uncertainty. Cambridge University Press.\n\n\nGlasserman, Paul. 2003. Monte Carlo Methods in Financial Engineering. Springer.\n\n\nGrolemund, Garrett. 2014. Hands on Programming with R. O’Reilly. https://rstudio-education.github.io/hopr/.\n\n\nHalpern, Joseph. 2017. Reasoning about Uncertainty. 2nd ed. MIT Press.\n\n\nLiu, Jun S. 2001. Monte Carlo Strategies in Scientific Computing. Springer.\n\n\nLuenberger, David. 2009. Investment Science. Oxford University Press.\n\n\nMcNeil, Alexander, Paul Embrechts, and Rudiger Frey. 2015. Quantitative Risk Management. Princeton University Press.\n\n\nTaleb, Nassim Nicolas. 2007. Fooled by Randomenss. Penguin.\n\n\nWickham, Hadley. 2019. Advanced r. 2nd ed. Taylor; Francis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  }
]