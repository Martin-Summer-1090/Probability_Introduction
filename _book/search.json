[
  {
    "objectID": "04-lecture4.html",
    "href": "04-lecture4.html",
    "title": "4  Random Variables",
    "section": "",
    "text": "4.1 Random Variables and Distributions\nA random variable is a numerical outcome of a random phenomenon. Formally, a random variable is a function that assigns a real number to each outcome in the sample space of a random experiment. More explicitly:\nA random variable is thus a function defined on the sample space of a random experiment. This formal definition allows us to generalize and analyze a wide variety of real-world scenarios. For instance:\nBy explicitly recognizing these as random variables, we can now apply a systematic framework to quantify their behavior and analyze them.\nNote that it is a widely held convention in probability theory to use capital letters such as \\(X\\) and \\(Y\\) etc. as symbols of a random variable.\nRandom variables come in two varieties, depending on the properties of the sample space \\({\\cal S}\\). If the sample space is a finite or countably finite set, the sample space is discrete and we talk of a discrete random variables.\nSometimes it is natural to consider continuous sample spaces. For example when we consider the return of an asset over a year or the price of a stock at a specific time. In this case we call a random variable continuous. With continuous sample spaces we will need tools from calculus. We will discuss continuous random variables in the next lecture. Here we stick with the concept of a discrete random variable.\nIn the case of a discrete sample space we can theoretically tabulate and random variable \\(X\\) by enumerating in some order all points in the sample space and associating with each the corresponding value of \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#random-variables-and-distributions",
    "href": "04-lecture4.html#random-variables-and-distributions",
    "title": "4  Random Variables",
    "section": "",
    "text": "Definition: Random Variable\n\n\n\nA random variable \\(X\\) is a function \\(X:\\mathcal{S} \\to \\mathbb{R}\\) from the sample space (the set of all possible outcomes of the random experiment) to the real numbers.\n\n\n\n\nIn the coin-flipping example from Lecture 2, the number of heads in 10 flips was a random variable.\nThe number of multiple brithdays in a group of \\(n\\) people is also an example of a random variable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#probability-distributions-and-cumulative-probability-distributions",
    "href": "04-lecture4.html#probability-distributions-and-cumulative-probability-distributions",
    "title": "4  Random Variables",
    "section": "4.2 Probability Distributions and Cumulative Probability Distributions",
    "text": "4.2 Probability Distributions and Cumulative Probability Distributions\nA random variable is characterized by its probability distribution, which describes how probabilities are assigned to its possible values.\n\n\n\n\n\n\nDefinition: Probability Mass Function of a Discrete Random Variable\n\n\n\nLet \\(X\\) be a random variable and \\(x_1, x_2, ...\\) the values which it assumes. The aggregate of all sample points on which \\(X\\) assumes a fixed value \\(x_i\\) is form the event \\(X = x_i\\). It’s probability is denoted by \\(P(X = x_i)\\). The function: \\[\\begin{equation*}\nP(X = x_i) = p(x_i), \\,\\, i=1,2,...\n\\end{equation*}\\] is called the probability distribution of the random variable \\(X\\).\nwhere \\(p(x_i)\\) satisfies:\n\n\\(0 \\leq p(x_i) \\leq 1 \\,\\, \\text{for}\\,\\, i = 1,2, ...\\)\n\\(\\sum_{i} p(x_i) = 1\\).\n\n\n\nWe can visualize the probability distribution of a discrete random variable using R in a standard example: Consider a random variable \\(X\\) that represents the outcome of rolling a fair six-sided die. The probability distribution is: \\(P(X = x_i) = \\frac{1}{6}, \\quad i = 1, 2, 3, 4, 5, 6\\).\nThis can be visualized as:\n\n# Plot bars\n\nplot(1:6, rep(1/6,6), type = \"h\", \n        main = \"Probability Distribution\", \n        xlab = \"Outcome\", \n        ylab = \"Probability\")\n\n# Add blue filled points at the top of each bar\n\npoints(1:6, rep(1/6, 6), pch = 16, col = \"blue\")\n\n\n\n\n\n\n\n\nThis plot shows that each outcome has an equal probability of \\(\\frac{1}{6}\\) symbolized as a bar. For better readability of the graph we symbolized the function values at \\(1/6\\) by a blue filled dot.\nLet me discuss some common confusions that arise often with understanding the concept of a random variable.\n\nRandom Variable vs. Outcome: A random variable is not the same as an individual outcome. It is a function that assigns values to outcomes. The confusion is partially created by the name. Maybe a better term would be a random mapping. Anyway, keep in mind that a random variable is a function defined on the sample space.\nProbability Distribution vs. Histogram: A probability distribution represents theoretical probabilities. Don’t mix this concept up with the concept of a histogram, known from statistics and data analysis, which shows frequencies of empirical data.\nDiscrete vs. Continuous: Discrete variables take specific values (e.g., dice outcomes), while continuous variables can take any value in a range. Dealing with continuous variables needs specifc tools which we discuss in lecture 5.\n\n\n\n\n\n\n\nNow You Try: Other Examples\n\n\n\nThink of scenarios where you could define a discrete random variable.\nFor instance:\n\nThe number of defective products in a batch of 20.\nThe number of rainy days in a week.\n\nSimulate and visualize the Probability distribution of these examples in R. You can take the visualization approach we took here. You could also try to make use of R’s barplot() function.\n\n\nA related concept to the probability distribution, is the cumulative distribution function (CDF). It can also be used to describe a discrete random variable. The CDF provides the probability that the random variable \\(X\\) takes a value less than or equal to a specific value \\(x\\):\n\n\n\n\n\n\nDefinition: Cumulative Distribution Function (CDF)\n\n\n\nThe cumulative distribution function - abbreviated CDF - shows the probability that a random variable \\(X\\) take a value less than or equal to a given value \\(x_i\\). It is usually denoted as \\(F(x_i) = P(X \\leq x_i)\\) where \\(F\\) is non-decreasing and \\(0 \\leq F(x_i) \\leq 1\\) for \\(i = 1,2,...\\)\n\n\nIn the case of a discrete random variable the cumulative distribution function (CDF) is a step function, increasing by jump discontinuities. The points where the jumps occur are exactly at the values the random variable can take.\nThe CDF, \\(F(x)\\), can be computed in this case as:\n\n\\(F(1) = P(X \\leq 1) = P(X = 1) = \\frac{1}{6}\\),\n\\(F(2) = P(X \\leq 2) = P(X = 1) + P(X = 2) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6}\\),\n\\(F(3) = P(X \\leq 3) = \\frac{3}{6}\\),\n…\n\\(F(6) = P(X \\leq 6) = \\frac{6}{6} = 1\\).\n\nThe CDF can be visualized as a step function, showing the cumulative probabilities:\n\nx &lt;- 1:6\ncdf &lt;- cumsum(rep(1/6, 6))\n\n# Plot the empty plot frame\nplot(0:6, c(0, cdf), type = \"n\", \n     main = \"Cumulative Distribution Function\", \n     xlab = \"Outcome\", \n     ylab = \"F(x)\", \n     xlim = c(0, 6.5), ylim = c(0, 1))\n\n# Draw the first horizontal bar\nsegments(0, 0, 1, 0, col = \"blue\", lwd = 2)\npoints(0, 0, pch = 16, col = \"blue\")  # Closed circle at (0, 0)\npoints(1, 0, pch = 1, col = \"blue\", cex = 1.2)  # Open circle at (1, 0)\n\n# Draw the stepwise CDF\nfor (i in 1:(length(x) - 1)) {\n  # Draw the horizontal bar for each step\n  segments(x[i], cdf[i], x[i + 1], cdf[i], col = \"blue\", lwd = 2)\n  \n  # Add the closed circle at the start of the segment\n  points(x[i], cdf[i], pch = 16, col = \"blue\")  # Filled circle\n  \n  # Add the open circle at the end of the segment\n  points(x[i + 1], cdf[i], pch = 1, col = \"blue\", cex = 1.2)  # Open circle\n}\n\n# Draw the last horizontal bar and closed circle at the end\nsegments(6, cdf[6], 6.5, cdf[6], col = \"blue\", lwd = 2)\npoints(6, cdf[6], pch = 16, col = \"blue\")  # Closed circle at (6, 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Remark on Constructing Complex Graphs in R\n\n\n\nThis visualization of the cumulative distribution function (CDF) is a great example of how graphs in R can be constructed step by step using layers and overlays. Let’s break it down to understand how the graph is built:\nWe start with a blank canvas and create an empty plot frame using plot() with type = \"n\". The type = \"n\" argument ensures that no points or lines are drawn initially, allowing us to control exactly what is added to the plot later. This blank canvas defines the axes, labels, and range, providing a foundation for the layers to come.\nThe first horizontal line is drawn using segments(), starting from (0, 0) to (1, 0). We add a filled circle (pch = 16) at the start (0, 0) and an open circle (pch = 1) at the end (1, 0) using points(). This step visually initializes the CDF at the correct starting point.\nEach step of the CDF is added one at a time using a loop. For each step: - A horizontal line is drawn to represent the constant value of the CDF over that interval. - A closed circle is placed at the beginning of the line to indicate that the value is included in the CDF at that point. - An open circle is added at the end to show that the value is not included yet, highlighting the jump discontinuity.\nThe final horizontal line is added separately to show the end of the CDF. A closed circle is placed at the end of the last step to signify the inclusion of the final value in the distribution.\n\n\nHere are some key points to keep in mind when working with cumulative distribution functions (CDFs):\n\nThe CDF gives cumulative probabilities (\\(P(X \\leq x)\\)), not individual probabilities.\nThe CDF is always non-decreasing and reaches 1 for the largest possible value of \\(X\\).\nThe CDF for discrete variables is a step-function with jumps at the values the random variable can take.\n\nThe concepts of a probability distribution function and cumulative distribution function (CDF) can be extended to more than one random variable. When working with multiple random variables, we are often interested in their joint distribution, which describes how they behave together, and conditional distributions, which describe how one variable behaves given specific information about another. I discuss here the generalization to two random variables. All of the following discussion generalizes to more than two random variables.\n\n\n\n\n\n\nJoint Probability Distribution\n\n\n\nConsider now two discrete random variables \\(X\\) and \\(Y\\) defined on the same sample space and let \\(x_1,x_2, ...\\) and \\(y_1,y_2,...\\) the values which they assume. Let \\(f(x_i)\\) and \\(g(y_j)\\) be the corresponding distribution functions. The aggregate of points in which the two condistions \\(X=x_i\\) and \\(Y=y_j\\) are satisfied forms an event whose probability is denoted by \\[P(X = x_i, Y = y_j) = p(x_i, y_j), \\,\\, i,j,=1,2,...\\] is called the joint probability distribution of \\(X\\) and \\(Y\\). where \\(p(x_i, y_j)\\) satisfies:\n\n\\(0 \\leq p(x_i, y_j) \\leq 1\\) for \\(i,j = 1,2, ...\\),\n\\(\\sum_{i,j} p(x_i, y_j) = 1\\) for \\(i,j=1,2,...\\)..\n\n\n\nLet me give an example: Imagine rolling two fair six-sided dice. Let \\(X\\) and \\(Y\\) represent the outcomes of the first and second dice, respectively. The joint PMF is: \\[P(X = x_i, Y = y_j) = \\frac{1}{36}, \\quad x_i, y_j = 1, 2, 3, 4, 5, 6.\\]\nThis joint probability distribution captures the probability of every possible pair of outcomes, such as \\((X = 2, Y = 5)\\).\nOne visualization tool for two dimensional probability distributions the heatmap. The \\(x\\) and \\(y\\) axis of the heat map symbolized the values of \\(X\\) and \\(Y\\) whereas the third dimension visualizes the probability of each pair of values \\((x_i,y_j)\\) by a color code. So, for example when the dice are fair we should have a probability of \\(1/36\\) for each basic outcome, so you should see only one uniform color.\nTo illustrate this visualization concept, imagine an example where the dice are biased and not fair, so the probabilities of outcomes can differ:\n\n# Define the outcomes and probabilities for two biased dice\noutcomes &lt;- 1:6\nprob_die1 &lt;- c(0.05, 0.1, 0.2, 0.25, 0.2, 0.2)  # Probabilities for die 1\nprob_die2 &lt;- c(0.1, 0.15, 0.2, 0.25, 0.2, 0.1)  # Probabilities for die 2\n\n# Compute the joint PMF as the outer product of the two probability vectors\njoint_pmf &lt;- outer(prob_die1, prob_die2)\n\n# Create a heatmap using the image() function\nimage(\n  1:6, 1:6, joint_pmf,\n  col = colorRampPalette(c(\"white\", \"blue\"))(100),\n  xlab = \"Outcome of Die 2\",\n  ylab = \"Outcome of Die 1\",\n  main = \"Heatmap of Joint PMF (Two Biased Dice)\",\n  axes = FALSE\n)\n\n# Add axis labels\naxis(1, at = 1:6, labels = outcomes)\naxis(2, at = 1:6, labels = outcomes)\n\n# Add a color legend\nlegend(\n  \"topright\", \n  legend = round(seq(min(joint_pmf), max(joint_pmf), length.out = 5), 3),\n  fill = colorRampPalette(c(\"white\", \"blue\"))(5),\n  title = \"Probability\"\n)\n\n\n\n\n\n\n\n\nHere you see immediately that something is fishy with the dice. Would the dice be fair there should be a uniform color all over the heatmap with a color at the value of \\(1/36\\) or \\(0.03\\). Try it!\nWith this notation we can also define the notion of a conditional probability for discrete random variables.\n\n\n\n\n\n\nDefinition: Conditional Probability\n\n\n\nThe conditional probability of an event \\(Y=y_j\\), given \\(X=x_i\\) with \\(f(x_i) &gt; 0\\) is defined as \\[\nP(Y = y_j | X = x_i) = \\frac{p(x_i,y_j)}{f(x_i)}\n\\]\n\n\nIn this way a number is associated with every value of \\(X\\) and so defines a function on \\(X\\). This function is called\n\n\n\n\n\n\nDefinition: Conditional Distribution\n\n\n\nThe conditional distribution of \\(Y\\) for given \\(X\\) and denoted by \\[\nP(Y = y_j|X)\n\\]\n\n\nLet’s note a few improtant points about conditional distributions:\n\nIn general, the conditional probability distribution of \\(Y\\) given \\(X = x_i\\) (denoted \\(p(y_j \\mid x_i)\\)) differs from the marginal probability distribution \\(g(y_j)\\). This means that knowing the value of \\(X\\) provides information about \\(Y\\), which indicates stochastic dependence between \\(X\\) and \\(Y\\). The strongest form of dependence occurs when \\(Y\\) is a deterministic function of \\(X\\), i.e., \\(Y = h(X)\\) for some function \\(h\\). In this case, \\(X\\) completely determines \\(Y\\), and their relationship is entirely predictable.\n\nOn the other hand, if the joint probability distribution \\(p(x_i, y_j)\\) factorizes as: \\[\np(x_i, y_j) = f(x_i) g(y_j) \\quad \\text{for all pairs } (x_i, y_j),\n\\] then \\(X\\) and \\(Y\\) are independent. This implies that the occurrence of one event (e.g., \\(X = x_i\\)) has no influence on the probability of the other (e.g., \\(Y = y_j\\)). The joint distribution in this case takes the form of a multiplication table, where the probabilities are products of the marginal probabilities. When \\(X\\) and \\(Y\\) are independent, their interaction is minimal, and no inference can be drawn about one variable from the other.\n\nThe joint distribution \\(p(x_i, y_j)\\) uniquely determines the marginal distributions \\(f(x_i)\\) and \\(g(y_j)\\), as these can be computed by summing over the appropriate dimensions: \\[\nf(x_i) = \\sum_j p(x_i, y_j), \\quad g(y_j) = \\sum_i p(x_i, y_j).\n\\] However, the reverse is not true: the marginal distributions \\(f(x_i)\\) and \\(g(y_j)\\) alone do not determine the joint distribution \\(p(x_i, y_j)\\). For example, different joint distributions can have the same marginals but encode different types of dependence or independence between \\(X\\) and \\(Y\\).\nNote that two random variables \\(X\\) and \\(Y\\) can have the same marginal distribution \\(f(x_i) = g(y_j)\\) but may or may not be independent. For instance:\n\n\nIf \\(X\\) and \\(Y\\) are independent, their joint distribution will factorize as described earlier.\nIf \\(X\\) and \\(Y\\) are dependent, their joint distribution will include nontrivial interactions between the variables.\n\nAt this stage it might be a good idea to recall the example on structured finance from lecture 3. The random variables hidden in this example are default indicators \\(X\\) for bond 1 and \\(Y\\) with \\(X: \\{D,N\\} \\mapsto \\{0,1\\}\\) and \\(Y: \\{D,N\\} \\mapsto \\{0,1\\}\\)\nIn the case of independence we had a contingency table like this:\n\n\nCode\n# Marginal probabilities for B_1\nP_N &lt;- 0.9  # Probability that B_1 does not default\nP_D &lt;- 0.1  # Probability that B_1 defaults\n\n# Conditional probabilities for B_2 given B_1\nP_N_given_N &lt;- 0.81  # Probability that B_2 does not default given B_1 does not default\nP_D_given_N &lt;- 0.09   # Probability that B_2 defaults given B_1 does not default\nP_N_given_D &lt;- 0.09    # Probability that B_2 does not default given B_1 defaults\nP_D_given_D &lt;- 0.01    # Probability that B_2 defaults given B_1 defaults\n\n\n# Load necessary library\nlibrary(kableExtra)\n\n# Define marginal probabilities\nP_N_total &lt;- P_N\nP_D_total &lt;- P_D\nP_N_given_total &lt;- round(P_N_given_N + P_D_given_N,2)\nP_D_given_total &lt;- round(P_N_given_D + P_D_given_D,2)\n\n# Create the contingency table\ncontingency_table &lt;- matrix(\n  c(\n    P_D_given_D, P_D_given_N, P_D_total,    # Row 1: Bond 1 Default\n    P_N_given_D, P_N_given_N, P_N_total,    # Row 2: Bond 1 No Default\n    P_D_given_total, P_N_given_total, 1  # Row 3: Column totals\n  ),\n  nrow = 3,\n  byrow = TRUE,\n  dimnames = list(\n    c(\"Bond 1: Default (D)\", \"Bond 1: No Default (N)\", \"Total\"),\n    c(\"Bond 2: Default (D)\", \"Bond 2: No Default (N)\", \"Total\")\n  )\n)\n\n# Create a styled table\nkable_output &lt;- knitr::kable(\n  contingency_table,\n  caption = \"Contingency Table of Joint and Marginal Probabilities: Independence\",\n  format = \"html\",\n  escape = FALSE,\n  col.names = c(\"\", \"Default (D)\", \"No Default (N)\", \"Total\")\n) %&gt;%\n  kableExtra::column_spec(1, bold = TRUE)\n\n# Render the table\nkable_output\n\n\n\nContingency Table of Joint and Marginal Probabilities: Independence\n\n\n\nDefault (D)\nNo Default (N)\nTotal\n\n\n\n\nBond 1: Default (D)\n0.01\n0.09\n0.1\n\n\nBond 1: No Default (N)\n0.09\n0.81\n0.9\n\n\nTotal\n0.10\n0.90\n1.0\n\n\n\n\n\n\n\nThe joint probabilities in this case are just the product of the marginal probabilities and the contingency table is similar to a multiplication table.\nIn the case of dependence this case changes. The new contingency table now looks like this:\n\n\nCode\n# Load necessary library\nlibrary(kableExtra)\n\n# Updated values from the table\nP_D_given_D &lt;- 0.06  # Probability that Y defaults given X defaults\nP_N_given_D &lt;- 0.04  # Probability that Y does not default given X defaults\nP_D_given_N &lt;- 0.04  # Probability that Y defaults given X does not default\nP_N_given_N &lt;- 0.86  # Probability that Y does not default given X does not default\nP_D_total &lt;- 0.1     # Marginal probability that Y defaults\nP_N_total &lt;- 0.9     # Marginal probability that Y does not default\n\n# Define totals\nP_D_given_total &lt;- P_D_given_D + P_D_given_N\nP_N_given_total &lt;- P_N_given_D + P_N_given_N\n\n# Create the contingency table\ncontingency_table &lt;- matrix(\n  c(\n    P_D_given_D, P_D_given_N, P_D_total,    # Row 1: X Default\n    P_N_given_D, P_N_given_N, P_N_total,    # Row 2: X No Default\n    P_D_given_total, P_N_given_total, 1     # Row 3: Column totals\n  ),\n  nrow = 3,\n  byrow = TRUE,\n  dimnames = list(\n    c(\"X: Default (d)\", \"X: No Default (n)\", \"Total\"),\n    c(\"Y: Default (d)\", \"Y: No Default (n)\", \"Total\")\n  )\n)\n\n# Create a styled table\nkable_output &lt;- knitr::kable(\n  contingency_table,\n  caption = \"Contingency Table of Joint and Marginal Probabilities: Dependence\",\n  format = \"html\",\n  escape = FALSE\n) %&gt;%\n  kableExtra::column_spec(1, bold = TRUE)\n\n# Render the table\nkable_output\n\n\n\nContingency Table of Joint and Marginal Probabilities: Dependence\n\n\n\nY: Default (d)\nY: No Default (n)\nTotal\n\n\n\n\nX: Default (d)\n0.06\n0.04\n0.1\n\n\nX: No Default (n)\n0.04\n0.86\n0.9\n\n\nTotal\n0.10\n0.90\n1.0\n\n\n\n\n\n\n\nNote that in both cases the marginal probabilities look the same. So in the case of structured finance, a superficial analysis looking at each bond in isolation and assuming independence might make a fatally wrong risk asessment.\nFor instance, consider \\(X\\) and \\(Y\\) that both represent the outcomes of rolling two fair dice. If \\(X\\) and \\(Y\\) are independent, the joint probabilities \\(p(x_i, y_j)\\) will simply be products of the marginal probabilities. However, if \\(X = Y\\) (e.g., the two dice always show the same value), the joint distribution will reflect perfect dependence, and \\(X\\) and \\(Y\\) are no longer independent despite having identical marginal distributions.\nHere is a visualization of this situation:\n\n# Define a joint PMF for dependent dice: one die matches the other\noutcomes &lt;- 1:6\njoint_pmf &lt;- diag(1/6, 6, 6)  # Probability of matching outcomes is 1/6 for each pair\n\n# Create heatmap for the dependent joint PMF\nimage(\n  1:6, 1:6, joint_pmf,\n  col = colorRampPalette(c(\"white\", \"blue\"))(100),\n  xlab = \"Outcome of Die 2\",\n  ylab = \"Outcome of Die 1\",\n  main = \"Heatmap of Joint PMF (Dependent Dice)\",\n  axes = FALSE\n)\naxis(1, at = 1:6, labels = outcomes)\naxis(2, at = 1:6, labels = outcomes)\n\n# Add a color legend\nlegend(\n  \"topright\",\n  legend = round(seq(min(joint_pmf), max(joint_pmf), length.out = 5), 3),\n  fill = colorRampPalette(c(\"white\", \"blue\"))(5),\n  title = \"Probability\"\n)\n\n\n\n\n\n\n\n\nKeep in mind:\n\nJoint Distributions describe how two random variables behave together.\nConditional Distributions refine our understanding of one variable based on information about another.\nJoint PMFs naturally generalize to describe both independent and dependent random variables. For dependent variables, the joint PMF captures the interaction and dependencies between the variables, often requiring conditional probabilities to explain the relationships.\nThese concepts are foundational for understanding dependencies in financial modeling, such as asset correlations or portfolio risk.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#the-binomial-distribution-a-fundamental-model-of-chance",
    "href": "04-lecture4.html#the-binomial-distribution-a-fundamental-model-of-chance",
    "title": "4  Random Variables",
    "section": "4.3 The Binomial Distribution: A Fundamental Model of Chance",
    "text": "4.3 The Binomial Distribution: A Fundamental Model of Chance\nIn our very first lecture we modeled the flipping of a fair coin using probability theory and R. When we modeled the coin flip with R and tried to visualize, we flipped the coin many times.\nSay we flip the coins 10 times. How many heads do you expect to see? What’s the likelihood of getting exactly three heads? These questions, simple yet profound, are at the heart of the binomial distribution —one of the most important and versatile models of a discrete random variable.\nThe binomial distribution arises whenever we repeat a simple experiment, known as a Bernoulli trial, multiple times under the same conditions. Each trial has two possible outcomes, often labeled as “success” and “failure.” By counting the number of successes in a fixed number of trials, the binomial distribution provides a complete picture of the probabilities associated with all possible outcomes.\nBut why is the binomial distribution so crucial? For one, it is ubiquitous in applications. From predicting election outcomes to evaluating the reliability of systems and understanding financial risks, the binomial model underpins countless real-world phenomena. More importantly, it serves as a foundational tool for understanding the behavior of discrete random variables, providing a framework to compute probabilities, analyze expectations, and quantify variability.\nIn this section, we’ll explore the binomial distribution in depth. We’ll derive its probability mass function, visualize its cumulative distribution function, and compute key statistics like the mean and variance. Through this concrete example, you’ll not only solidify your understanding of random variables but also gain a versatile tool for modeling uncertainty in diverse contexts.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#expected-value-variance-and-standard-error-understanding-random-variables",
    "href": "04-lecture4.html#expected-value-variance-and-standard-error-understanding-random-variables",
    "title": "4  Random Variables",
    "section": "4.4 Expected Value, Variance, and Standard Error: Understanding Random Variables",
    "text": "4.4 Expected Value, Variance, and Standard Error: Understanding Random Variables\nIn Lecture 1, we flipped a fair coin and asked: “What’s the likelihood of heads? How many heads would we expect in multiple flips?” Now, let’s deepen our understanding by connecting these questions to key concepts in probability: expected value, variance, and standard error.\nConsider a single flip of a fair coin. Let’s define a random variable \\(X\\), where: - \\(X = 1\\) if the coin lands heads (success), - \\(X = 0\\) if the coin lands tails (failure).\nThis is an example of a Bernoulli random variable, the simplest discrete random variable.\n\n\n\n\n\n\nDefinition: Expected value\n\n\n\nLet \\(X\\) be a random variable assuming the values \\(x_1,x_2,x_3, \\dots\\) with probabilities \\(p(x_1), p(x_2), p(x_3), \\dots\\). The expected value of \\(X\\) is defined by \\[\n\\mathbb{E}[X] = \\sum_{x_i} x_i \\cdot p(x_i)\n\\] provided the series converges absolutely. In this case, we say that \\(X\\) has finite expectation. If \\(\\sum |x_i| p(x_i)\\) diverges, we say that \\(X\\) has no finite expectation.\n\n\nIt is sometimes convenient - and we have done so repeatedly in this lecture - to interpret probabilities as limits of observable frequencies in repeated, independent random experiments.\nThis would lead to the following intuitive interpretation of expectation. Let the experiment be repeated \\(n\\) times under identical conditions and denote by \\(X_1,X_2,...,X_n\\) the values of \\(X\\) that were actually observed, then for large \\(n\\) the average of these values should be close to \\(\\mathbb{E}[X]\\).\nWhile the terms mean, average and expectation are synonymous, expectation is usually used in relation to random variables, whereas mean and average is used in relation to empirical data.\nLet us compute the expected value of the Bernoulli random variable \\(X\\): \\[\n\\mathbb{E}[X] = 1 \\cdot p + 0 \\cdot (1-p) = p.\n\\] Thus, the expected value of \\(X\\) is \\(p\\). For a fair coin (\\(p = 0.5\\)), \\(\\mathbb{E}[X] = 0.5\\). This means that in the long run, half of the flips are expected to result in heads.\nWhile the expected value gives the central tendency, the variance quantifies the spread of a random variable’s possible outcomes around its expected value.\n\n\n\n\n\n\nDefinition: Expected value\n\n\n\nLet \\(X\\) be a random variable assuming the values \\(x_1,x_2,x_3, ...\\) with probabilities \\(p(x_1), p(x_2, p(x_3), ...\\). The variance of \\(X\\) is defined by \\[\n\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2].\n\\]\n\n\nFor discrete random variables, this can be written as: \\[\n\\text{Var}(X) = \\sum_{x_i} (x_i - \\mathbb{E}[X])^2 \\cdot p(x_i).\n\\] Let’s look again at how this applies to the Bernoulli variable. For our Bernoulli random variable \\(X\\), substituting \\(\\mathbb{E}[X] = p\\): \\[\n\\text{Var}(X) = (1 - p)^2 \\cdot p + (0 - p)^2 \\cdot (1-p) = p(1-p).\n\\] For a fair coin (\\(p = 0.5\\)): \\[\n\\text{Var}(X) = 0.5 \\cdot 0.5 = 0.25.\n\\]\nThe standard error (SE) is the square root of the variance: \\[\n\\text{SE}(X) = \\sqrt{\\text{Var}(X)}.\n\\] For our fair coin: \\[\n\\text{SE}(X) = \\sqrt{0.25} = 0.5.\n\\] The standard error is often more convenient than the variance because it is expressed in the same units as the expected value. It thus gives as a clear sense how much the random variable is spread around the mean in the units of the mean. Here is an example.\nSuppose a stock’s daily return, \\(X\\), is modeled as a random variable with an expected return of 0.002 (0.2%) and a variance of \\(\\text{Var}(X)=0.0004\\) (0.04%). The variance is expressed in squared units of the return. While mathematically precise, it is not immediately interpretable because we don’t think of returns in squared terms. The standard error is 0.02 (2%) is expressed in the same units as the return. This tells us that daily returns typically deviate by about 2% from the expected return of 0.2%. This demonstrates why the standard error is often preferred when communicating uncertainty—it translates mathematical variability into a form that aligns with practical understanding.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#extending-to-the-binomial-random-variable",
    "href": "04-lecture4.html#extending-to-the-binomial-random-variable",
    "title": "4  Random Variables",
    "section": "4.5 Extending to the Binomial Random Variable",
    "text": "4.5 Extending to the Binomial Random Variable\nNow let’s build on this foundation. Suppose we flip the coin \\(n\\) times and count the total number of heads. This total is a binomial random variable \\(X\\), with parameters: - \\(n\\): number of trials, - \\(p\\): probability of heads.\n\n\n\n\n\n\nDefinition: Binomial Random Variable\n\n\n\nA binomial random variable is a discrete random variable with the \\(X\\) with probability distribution \\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0, 1, \\dots, n,\n\\] where \\(\\binom{n}{k}\\) is the number of ways to choose \\(k\\) successes in \\(n\\) trials.\n\n\nThe outcomes of a binomial random variable can be visualized as a binomial lattice. In the following figure Figure 4.1 we visualize the case where a coin is tossed twice.\n\n\n\n\n\n\n\n\nFigure 4.1: A binomial lattice\n\n\n\n\n\nNote that we the order of occurence of Heads and Tails does not matter. We only count how many times they occur. Therefore the sequence HT ends in the same node as TH.\nWe can compute its expected value and variance of a binomial random variable using what we’ve learned: \\[\n\\mathbb{E}[X] = n \\cdot p, \\quad \\text{Var}(X) = n \\cdot p \\cdot (1-p), \\quad \\text{SE}(X) = \\sqrt{n \\cdot p \\cdot (1-p)}.\n\\] Let us briefly derive these results from the definitions:\nTo derive the expected value and variance of a binomial random variable \\(X\\), we use the definitions and properties of expectation and variance step-by-step.\nA binomial random variable \\(X\\) represents the number of successes in \\(n\\)\nindependent trials, where each trial has:\n\nProbability of success: \\(p\\),\nProbability of failure: \\(1 - p\\).\n\nThe probability density function of \\(X\\) is given by: \\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0, 1, 2, \\dots, n.\n\\] The expected value is defined as: \\[\n\\mathbb{E}[X] = \\sum_{k=0}^n k \\cdot P(X = k).\n\\]\nSubstituting: \\[\n\\mathbb{E}[X] = \\sum_{k=0}^n k \\cdot \\binom{n}{k} p^k (1-p)^{n-k}.\n\\] The binomial random variable \\(X\\) can be viewed as the sum of \\(n\\) independent Bernoulli random variables \\(X_i\\), where each \\(X_i\\) is 1 for success and 0 for failure. That is: \\[\nX = \\sum_{i=1}^n X_i,\n\\] where \\(\\mathbb{E}[X_i] = p\\).\nBy the linearity of expectation: \\[\n\\mathbb{E}[X] = \\mathbb{E}\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n \\mathbb{E}[X_i] = n \\cdot p.\n\\] The variance of \\(X\\) is defined as: \\[\n\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2.\n\\] We know \\(X = \\sum_{i=1}^n X_i\\). Using the property of variance for independent random variables: \\[\n\\text{Var}(X) = \\text{Var}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\text{Var}(X_i).\n\\]\nFor a Bernoulli random variable \\(X_i\\): \\[\n\\text{Var}(X_i) = \\mathbb{E}[X_i^2] - (\\mathbb{E}[X_i])^2.\n\\]\nSince \\(X_i\\) takes values 0 and 1: \\[\nX_i^2 = X_i, \\quad \\text{so } \\mathbb{E}[X_i^2] = \\mathbb{E}[X_i] = p.\n\\]\nThus: \\[\n\\text{Var}(X_i) = p - p^2 = p(1-p).\n\\]\nNow summing over all \\(n\\) trials: \\[\n\\text{Var}(X) = \\sum_{i=1}^n \\text{Var}(X_i) = n \\cdot p(1-p).\n\\]\nFor \\(n = 10\\), \\(p = 0.5\\): \\[\n\\mathbb{E}[X] = 10 \\cdot 0.5 = 5, \\quad \\text{Var}(X) = 10 \\cdot 0.5 \\cdot 0.5 = 2.5, \\quad \\text{SE}(X) = \\sqrt{2.5} \\approx 1.58.\n\\]\nTo solidify these concepts, let’s compute and visualize the probability distribution of a binomial random variable for \\(n = 10\\), \\(p = 0.5\\). This shows the probabilities of obtaining 0, 1, 2, …, 10 heads.\n\n# R Code to Compute and Plot PMF\nn &lt;- 20\np &lt;- 0.5\nk &lt;- 0:n\npmf &lt;- dbinom(k, size = n, prob = p)\n\nplot(k, pmf, type = \"h\", lwd = 2, main = \"PMF of Binomial(20, 0.5)\",\n     xlab = \"Number of Heads (k)\", ylab = \"P(X = k)\", col = \"black\")\npoints(k, pmf, pch = 19, col = \"blue\")\n\n\n\n\n\n\n\n\nKey Observations: - The probability density function is symmetric around the expected value \\(\\mathbb{E}[X] = 10\\) because \\(p = 0.5\\). - The spread is captured by the variance and standard error, with most probabilities concentrated around the mean. The standard error is 2.2\nThe binomial distribution shows the power and versatility of the basic coin model we introduced right at the beginning of this course. It is the basic building block of this distribution and is very powerful in modeling simple experiments and deriving key properties: - Expected value provides the long-run average. - Variance and standard error measure variability. - The PMF describes the probabilities of all possible outcomes.\nThese concepts form the foundation for understanding random variables and their distributions, bridging theory and real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#covariance",
    "href": "04-lecture4.html#covariance",
    "title": "4  Random Variables",
    "section": "4.6 Covariance",
    "text": "4.6 Covariance\nWhen we have more than one random variable we can summarize how they vary together. The concept that does this for us is covariance. Here is the definition:\n\n\n\n\n\n\nDefinition: Covariance\n\n\n\nThe covaraince of two random variables \\(X\\) and \\(Y\\) is defined by \\[\n\\text{Cov}(X,Y) = \\mathbb{E}(X - \\mathbb{E}[X])(Y-\\mathbb{E}[Y])\n\\] Alternatively we can write \\[\n\\text{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\n\\] This definition is meaningful whenever \\(X\\) and \\(Y\\) have finite variance.\n\n\nNote the following key points about covariance:\n-If \\(\\text{Cov}(X,Y)&gt;0\\): \\(X\\) and \\(Y\\) tend to increase together (positive relationship).\n-If \\(\\text{Cov}(X,Y)&lt;0\\): \\(X\\) and \\(Y\\) tend to move in opposite directions (negative relationship).\n-If \\(\\text{Cov}(X,Y)=0\\): \\(X\\) and \\(Y\\) are linearly uncorrelated, though they may still have a nonlinear relationship.\nCovariance is often referred to as a linear measure of dependence. What does this mean? Covariance quantifies how the deviations of \\(X\\) and \\(Y\\) from their respective means are aligned. If \\(X\\) and \\(Y\\) tend to deviate in the same direction (both above or below their means), the product \\((X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\) will typically be positive, leading to a positive covariance. Conversely, if they deviate in opposite directions, this product will typically be negative, leading to a negative covariance.\nCovariance captures only linear relationships because it measures the degree to which \\(X\\) and \\(Y\\) co-vary in a straight-line manner. If \\(X\\) and \\(Y\\) are related nonlinearly (e.g., quadratic, exponential), their covariance might still be zero even though a relationship exists.\nHere’s an example to visualize covariance using two binomially distributed random variables:\n\n# Simulating two binomial random variables\nset.seed(123)\nn &lt;- 1000\nX &lt;- rbinom(n, size = 10, prob = 0.5)  # Binomial(n=10, p=0.5)\nY &lt;- X + rbinom(n, size = 10, prob = 0.3)  # Dependent variable\n\n# Covariance\ncov_xy &lt;- cov(X, Y)\n\n# Scatterplot with regression line\nplot(X, Y, main = \"Scatterplot of X and Y with Regression Line\",\n     xlab = \"X (Binomial Random Variable)\", ylab = \"Y (Dependent Variable)\",\n     pch = 16, col = rgb(0.2, 0.4, 0.6, 0.6))\nabline(lm(Y ~ X), col = \"red\", lwd = 2)  # Add regression line\nlegend(\"topright\", legend = paste(\"Covariance =\", round(cov_xy, 2)))\n\n\n\n\n\n\n\n\nHere is what this visualization shows:\n\nThe scatterplot shows the relationship between \\(X\\) (independent binomial variable) and \\(Y\\) (a dependent binomial variable).\nThe points are scattered around a straight line, which indicates a general linear trend. Covariance quantifies how well the variations in \\(X\\) align with those in \\(Y\\).\nThe red line is added to highlight the linear relationship between the two variables.\n\nIf the covariance is large (positive or negative), the points will cluster more tightly around this line, indicating a stronger linear alignment.\nIf covariance is close to zero, the scatterplot will show a diffuse cloud of points with no clear linear trend.\n\n\nCovariance describes only the linear component of the relationship. If \\(X\\) and \\(Y\\) have a nonlinear dependency, the scatterplot may show a pattern (e.g., a curve) that the regression line and covariance fail to capture.\nCovariance is not a good measure of dependence when the relationship between \\(X\\) and \\(Y\\) is nonlinear.\nFor example, if \\(Y = X^2\\), \\(X\\) and \\(Y\\) exhibit a strong quadratic relationship, but their covariance might be close to zero because the deviations \\((X - \\mathbb{E}[X])\\) and \\((Y - \\mathbb{E}[Y])\\) do not align linearly.\nHere is again a visualization:\n\n# Simulating a stronger nonlinear relationship with binomial random variables\nset.seed(123)\nn &lt;- 1000\nX &lt;- rbinom(n, size = 10, prob = 0.5)  # Binomial(n=10, p=0.5)\nY &lt;- (X - 5)^2  # Pronounced U-shaped relationship\n\n# Covariance\ncov_xy &lt;- cov(X, Y)\n\n# Scatterplot with LaTeX-style labels\nplot(X, Y, main = expression(\"Scatterplot of \" * Y == (X - 5)^2),\n     xlab = expression(X ~ \"(Binomial Random Variable)\"),\n     ylab = expression(Y ~ \"= (X - 5)^2\"),\n     pch = 16, col = rgb(0.2, 0.4, 0.6, 0.6))\nabline(lm(Y ~ X), col = \"red\", lwd = 2)  # Add regression line\nlegend(\"topright\", legend = paste(\"Covariance =\", round(cov_xy, 2)))\n\n\n\n\n\n\n\n\nTo capture nonlinear dependence, measures like the mutual information or a rank correlation coefficient (e.g., Spearman’s or Kendall’s) are more appropriate.\nBy understanding the limitations of covariance, we can use it effectively for its intended purpose while recognizing when other tools are needed to describe more complex relationships.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#variance-of-the-sum-of-random-variables",
    "href": "04-lecture4.html#variance-of-the-sum-of-random-variables",
    "title": "4  Random Variables",
    "section": "4.7 Variance of the Sum of Random Variables",
    "text": "4.7 Variance of the Sum of Random Variables\nThe variance of the sum of two random variables, \\(X\\) and \\(Y\\), is a natural place where covariance arises. Let’s explore why this happens.\nFor two random variables \\(X\\) and \\(Y\\), the variance of their sum is: \\[\n\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2 \\cdot \\text{Cov}(X, Y).\n\\]\nWhy does Covariance appear here? When adding \\(X\\) and \\(Y\\), the variance accounts not only for the individual variances of \\(X\\) and \\(Y\\), but also for how they interact.\nThe term \\(2 \\cdot \\text{Cov}(X, Y)\\) reflects this interaction: - If \\(X\\) and \\(Y\\) are positively correlated, the variability of their sum increases. - If \\(X\\) and \\(Y\\) are negatively correlated, it decreases.\nThis property reflects a basic principle of insurance: Diversification reduces risk. In an insurance pool, risks (e.g., claims or losses) that are negatively correlated—or at least uncorrelated—reduce the overall variability of total claims. When risks are positively correlated (e.g., claims rise simultaneously due to shared external factors like natural disasters), the total risk increases, making diversification less effective. By managing correlation, insurers aim to stabilize payouts and maintain predictability.\nConsider independent variables as a special case: If \\(X\\) and \\(Y\\) are independent, \\(\\text{Cov}(X, Y) = 0\\), and the formula simplifies: \\[\n\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y).\n\\]\nAs an example let’s go back to our previous case of tossing two coins:\nExample: Two Coin Tosses: Let \\(X\\) and \\(Y\\) represent the outcomes of two coin tosses (\\(X, Y = 1\\) for heads, \\(0\\) for tails).\n\nCase 1: Independent Tosses\n\nVariance of each toss: \\[\n\\text{Var}(X) = p(1-p), \\quad \\text{Var}(Y) = p(1-p).\n\\]\nCovariance: \\[\n\\text{Cov}(X, Y) = 0.\n\\]\nVariance of the sum: \\[\n\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) = 2p(1-p).\n\\]\n\nCase 2: Dependent Tosses\n\nSuppose \\(P(Y = 1 \\mid X = 1) = 0.7\\) and \\(P(Y = 1 \\mid X = 0) = 0.3\\).\nIn this case, \\(\\text{Cov}(X, Y) &gt; 0\\), and the variance of \\(X + Y\\) increases due to the positive relationship between \\(X\\) and \\(Y\\).\n\n\nThese insights can be extende to more than two random variables. For a sum of \\(n\\) random variables \\(X_1, X_2, \\dots, X_n\\), the variance becomes: \\[\n\\text{Var}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\text{Var}(X_i) + \\sum_{i \\neq j} \\text{Cov}(X_i, X_j).\n\\] This shows how the covariances between pairs of random variables collectively influence the overall variance.\nIn the special case of identically independently distributed variables we get: If \\(X_1, X_2, \\dots, X_n\\) are i.i.d.: \\[\n\\text{Var}\\left(\\sum_{i=1}^n X_i\\right) = n \\cdot \\text{Var}(X).\n\\] ## Example: The CAPM formula and the Role of Covariance in comparing stocks\nThe Capital asset pricing model (CAPM) is a simple market mode, that relates individual stock returns to the market average. The CAPM formula is given by:\n\\[\nr_{jt} = \\alpha_j + \\beta_j m_t + \\epsilon_{jt}\n\\]\n\nThe left-hand side, \\(r_{jt}\\), is the excess return of equity \\(j\\) at time \\(t\\). This measures the return of the asset minus a measure of the risk-free rate, which represents the return on holding low-risk debt, such as a U.S. Treasury bill. For simplicity, we will omit the risk-free rate in this explanation.\n\\(m_t\\) is the market return, which captures the aggregate return on the market at time \\(t\\). A common choice when looking at US data is the S&P 500, which weights companies according to their market capitalization (the total value of their stock).\n\\(\\epsilon_{jt}\\) is an error term, a random variable that captures the part of the excess return not explained by the market return. It has the following properties:\n\n\\(\\mathbb{E}[\\epsilon_{jt}] = 0\\), meaning it has no systematic bias.\n\\(\\mathbb{E}[m_t \\epsilon_{jt}] = 0\\), meaning it is uncorrelated with the market return.\n\n\nThe term \\(\\beta_j\\) in the CAPM formula reflects the sensitivity of the excess return of equity \\(j\\) to the market return \\(m_t\\). It is calculated using the covariance between the returns of the asset and the market. Specifically:\n\\[\n\\beta_j = \\frac{\\text{Cov}(r_j, m)}{\\text{Var}(m)}\n\\]\nWhere: - \\(\\text{Cov}(r_j, m)\\) is the covariance between the excess return of equity \\(j\\) and the market return. - \\(\\text{Var}(m)\\) is the variance of the market return.\nThis shows that \\(\\beta_j\\) represents the degree to which the return of equity \\(j\\) moves with the market. A higher \\(\\beta_j\\) implies that the equity’s return is more sensitive to market movements, indicating a higher systematic risk.\nThe CAPM formula is built on the assumption that the excess return of an asset can be linearly decomposed into:\n\nA systematic component (\\(\\beta_j m_t\\)), which is related to market movements.\nAn idiosyncratic component (\\(\\epsilon_{jt}\\)), which is specific to the asset and uncorrelated with the market.\n\nTo estimate \\(\\beta_j\\), consider the following relationship:\n\\[\n\\mathbb{E}[r_{jt}] = \\mathbb{E}[\\alpha_j] + \\beta_j \\mathbb{E}[m_t] + \\mathbb{E}[\\epsilon_{jt}]\n\\]\nSince \\(\\mathbb{E}[\\epsilon_{jt}] = 0\\), we have:\n\\[\n\\mathbb{E}[r_{jt}] = \\alpha_j + \\beta_j \\mathbb{E}[m_t]\n\\]\nThe covariance appears when analyzing the relationship between \\(r_{jt}\\) and \\(m_t\\) over time. Multiplying both sides of \\(r_{jt} = \\alpha_j + \\beta_j m_t + \\epsilon_{jt}\\) by \\(m_t\\) and taking expectations:\n\\[\n\\mathbb{E}[r_{jt} m_t] = \\mathbb{E}[\\alpha_j m_t] + \\mathbb{E}[\\beta_j m_t^2] + \\mathbb{E}[\\epsilon_{jt} m_t]\n\\]\n\nSince \\(\\alpha_j\\) is constant, \\(\\mathbb{E}[\\alpha_j m_t] = \\alpha_j \\mathbb{E}[m_t]\\).\nThe error term is uncorrelated with the market return, so \\(\\mathbb{E}[\\epsilon_{jt} m_t] = 0\\).\n\nThis simplifies to:\n\\[\n\\mathbb{E}[r_{jt} m_t] = \\alpha_j \\mathbb{E}[m_t] + \\beta_j \\mathbb{E}[m_t^2]\n\\]\nThe covariance formula \\(\\text{Cov}(r_j, m)\\) can be used to isolate \\(\\beta_j\\):\n\\[\n\\text{Cov}(r_j, m) = \\mathbb{E}[r_{jt} m_t] - \\mathbb{E}[r_{jt}]\\mathbb{E}[m_t]\n\\]\nBy substituting and rearranging, we recover the key relationship for \\(\\beta_j\\):\n\\[\n\\beta_j = \\frac{\\text{Cov}(r_j, m)}{\\text{Var}(m)}\n\\]\nThe covariance explicitly links the asset’s return with the market return and determines the value of \\(\\beta_j\\), capturing how much of the asset’s return can be attributed to systematic market risk. This makes covariance fundamental to the CAPM framework.\nThis simple model allows for a meaningful comparison between stocks based on their insurance properties, as well as whether the create value \\(\\alpha_j &gt;0\\), because the stock is providing value above what the model predicts or destroy value \\(\\alpha_j &lt; 0\\) because the stock provides less value than what the model predicts based on the performance of the market.\nLet’s compare 20 biggest stocks from the SP500. We read their data as of January 2025 using the tidyquant() package:\n\nlibrary(tidyquant)\n\n# Define the tickers for the 20 biggest stocks and the S&P 500 index\n# Ticker-Company Mapping:\n# NVDA - NVIDIA Corporation\n# AAPL - Apple Inc.\n# MSFT - Microsoft Corporation\n# AMZN - Amazon.com, Inc.\n# GOOGL - Alphabet Inc. (Class A)\n# META - Meta Platforms, Inc.\n# TSLA - Tesla, Inc.\n# UNH - UnitedHealth Group Incorporated\n# JNJ - Johnson & Johnson\n# V - Visa Inc.\n# XOM - Exxon Mobil Corporation\n# JPM - JPMorgan Chase & Co.\n# WMT - Walmart Inc.\n# MA - Mastercard Incorporated\n# PG - Procter & Gamble Company\n# HD - The Home Depot, Inc.\n# BAC - Bank of America Corporation\n# CVX - Chevron Corporation\n# LLY - Eli Lilly and Company\n# ^GSPC - S&P 500 Index\n\ntickers &lt;- c(\"NVDA\", \"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"TSLA\", \n             \"UNH\", \"JNJ\", \"V\", \"XOM\", \"JPM\", \"WMT\", \"MA\", \"PG\", \"HD\", \n             \"BAC\", \"CVX\", \"LLY\")\n\n# Add the S&P 500 index ticker (SPY is a common ETF that tracks the index)\ntickers &lt;- c(tickers, \"^GSPC\")\n\n# Get historical data starting from 2015\nprice_data &lt;- tq_get(\n  tickers, \n  get = \"stock.prices\", \n  from = \"2015-01-01\"\n)\n\nNow let’s do some definitions and data filtering first:\n\n# Define the market ticker for the S&P 500\n\nmarket_ticker &lt;- \"^GSPC\"\n\n# Filter the market and stock data from the previously downloaded dataset\n\nmarket_data &lt;- price_data[price_data$symbol == market_ticker, ]\nstocks_data &lt;- price_data[price_data$symbol != market_ticker, ]\n\nHere we first assign the string ^GSPC (the Yahoo Finance ticker symbol for the S&P 500 index) to the variable market_ticker. Then we filter rows from the price_data data frame where the symbol column matches the market_ticker (^GSPC), isolating data for the S&P 500 index. The result is stored in the variable market_data, which contains historical price data for the market.\nWe then filter rows from the price_data data frame where the symbol column does not match ^GSPC. The result is stored in the variable stocks_data, which contains historical price data for all individual stocks (excluding the S&P 500 index).\nNext we dos ome data preparation work:\n\n# Ensure the date column is properly formatted\n\nprice_data$date &lt;- as.Date(price_data$date)\n\n# Calculate daily returns for market and stocks\n\nreturns_data &lt;- do.call(rbind, lapply(split(price_data, price_data$symbol), \n                                      function(df) {\n  # Convert to xts object for dailyReturn\n  xts_data &lt;- xts::xts(df$adjusted, order.by = df$date)\n  df$daily_return &lt;- quantmod::dailyReturn(xts_data, type = \"log\")\n  return(df)\n}))\n\n# Separate market and stock returns\n\nmarket_returns &lt;- returns_data[returns_data$symbol == market_ticker, \n                               c(\"date\", \"daily_return\")]\nstock_returns &lt;- returns_data[returns_data$symbol != market_ticker, ]\n\n# Merge market returns with each stock's returns\n\nmerged_data &lt;- merge(stock_returns, market_returns, by = \"date\", \n                     suffixes = c(\"\", \"_market\"))\n\nLet’s unpack this a bit:\nFirst, we ensure the date column is properly formatted. We convert the date column in price_data to the Date format. The xts package (used for time-series analysis) requires its order.by argument to be a Date, POSIXct, or numeric value for proper time indexing.\nNext we calculate daily returns for market and stocks. We\n\nSplit price_data by stock symbol. The split()function divides price_data into a list of data frames, one for each stock (or market) identified by the symbol column.\nFor each stock, we create an xts object using the adjusted prices (as values) and date (as the time index). The xts format is necessary for dailyReturn().\n\nNow we are ready toc ompute the daily returns. We use the dailyReturn() function from the quantmod package to calculate log returns. Logarithmic returns are defined as: \\[\n       r_t = \\ln\\left(\\frac{P_t}{P_{t-1}}\\right)\n       \\] where \\(P_t\\) is the adjusted closing price at time \\(t\\). The dailyReturn() function outputs a time series of returns, which is added as a new column (daily_return) to the stock’s data frame.\nWe then recombine the list of modified data frames (each containing daily_return) into a single data frame, returns_data using the do.call()function.\nWe now split returns_data into two subsets:\n\nmarket_returns: Contains only the market index returns (e.g., S&P 500).\nstock_returns: Contains the returns for all individual stocks.\n\nTo Aligns the return of each stock with the corresponding market returns by their date. we use:\n\nmerge():\n\nCombines stock_returns and market_returns based on their shared date column.\nEnsures that stock and market returns are matched by the same trading day.\n\nSuffixes:\n\nsuffixes = c(\"\", \"_market\"): Adds _market to columns from market_returns in case of naming conflicts.\nFor example: daily_return from market_returns becomes daily_return_market.\n\n\nAs a final output we get merged_data which is a data frame containing:\n\nsymbol: The stock ticker.\ndate: The trading date.\ndaily_return: The stock’s daily return.\ndaily_return_market: The corresponding market’s daily return.\n\nThis structure is ready for regression analysis, such as fitting the CAPM.\nNow we fit a CAPM model to each stock:\n\n# Fit CAPM for each stock\ncapm_results &lt;- do.call(rbind, \n                lapply(split(merged_data, merged_data$symbol), function(df) {\n  model &lt;- lm(daily_return ~ daily_return_market, data = df)\n  data.frame(\n    symbol = unique(df$symbol),\n    alpha = coef(model)[\"(Intercept)\"],\n    beta = coef(model)[\"daily_return_market\"]\n  )\n}))\n\nThis code fits the Capital Asset Pricing Model (CAPM) for each stock in the dataset and extracts the alpha (\\(\\alpha\\)) and beta (\\(\\beta\\)) coefficients for each stock.\nWe start by splitting the data by stock symbol, which results in a list of data frames, where each data frame contains the data for one specific stock.\nThen we apply CAPM regression to each stock using lapply():\n\nlm(daily_return ~ daily_return_market, data = df): Fits a linear regression model where:\n\ndaily_return is the dependent variable (stock’s excess return).\ndaily_return_market is the independent variable (market’s excess return).\n\nThis corresponds to the CAPM formula: \\[\nr_{jt} = \\alpha_j + \\beta_j m_t + \\epsilon_{jt}\n\\]\nThe regression estimates:\n\n\\(\\alpha_j\\) (the intercept): The abnormal return of the stock independent of the market.\n\\(\\beta_j\\) (the slope): The sensitivity of the stock’s return to market movements.\n\nFor each stock:\n\nThe regression results are used to extract \\(\\alpha\\) and \\(\\beta\\).\n\n\nFinally we create a data frame for each stock where\n\nunique(df$symbol): Retrieves the stock’s ticker symbol (e.g., AAPL, GOOGL).\ncoef(model)[\"(Intercept)\"]: Extracts \\(\\alpha_j\\) (the intercept) from the regression model.\ncoef(model)[\"daily_return_market\"]: Extracts \\(\\beta_j\\) (the slope) from the regression model.\ndata.frame(...): Creates a data frame with three columns:\n\nsymbol: The stock ticker.\nalpha: The stock’s alpha (\\(\\alpha_j\\)).\nbeta: The stock’s beta (\\(\\beta_j\\)).\n\n\nOur final output is then a data frame with the CAPM regression results for all stocks, where each row contains:\n\nsymbol: The stock ticker.\nalpha: The estimated (_j) (abnormal return).\nbeta: The estimated (_j) (market sensitivity).\n\nNow we plot these data in a two dimensional figure, which shows the stocks in a \\((\\beta, \\alpha)\\) space.\n\n# Plot results\nplot(\n  capm_results$beta, \n  capm_results$alpha,\n  xlab = \"Beta (Systematic Risk)\", \n  ylab = \"Alpha (Abnormal Return)\", \n  main = \"CAPM Alpha and Beta for S&P 500 Stocks\",\n  pch = 19,  # Circle symbols\n  cex = 1,   # Uniform size for all points\n  col = \"blue\"\n)\n\n# Add grid lines\ngrid(col = \"lightgray\", lty = \"dotted\", lwd = 0.8)\n\n# Add stock symbols as text labels\ntext(\n  capm_results$beta, \n  capm_results$alpha, \n  labels = capm_results$symbol, \n  pos = 4, \n  cex = 0.7\n)\n\n# Add dashed lines for beta = 0 and alpha = 0\nabline(h = 0, lty = 2, col = \"red\", lwd = 1)  # Horizontal line at alpha = 0\nabline(v = 1, lty = 2, col = \"red\", lwd = 1) # Vertical line at beta = 1\n\n\n\n\n\n\n\n\nThis picture immediately allows you to asess market sensitivity and arbitrage opportunities. For example over the period 2015 - 2025 bothe Amazon (AMZ) and Google (GOOGL) have similar market sensitivity (\\(beta\\)) but Amazon has generated much more money independent of the overall market.\nEli Lilly and Company (LLY) an American pharmaceutical company1 exhibits remarkably low market sensitivity while generating high \\(\\alpha\\).\n1 Eli Lilly and Company, doing business as Lilly, is an American multinational pharmaceutical company headquartered in Indianapolis, Indiana, with offices in 18 countries. Its products are sold in approximately 125 countries. The company was founded in 1876 by Eli Lilly, a pharmaceutical chemist and Union Army veteran of the American Civil War for whom the company was later named.Some companies like Exxon Mobil Corporation (XOM) or Chevron Corporation (CVX) appear to have destoryed value over this period, because their \\(alpha\\) is negative.\nInformation like this can, for example, be used to to build portfolios that maximize mean returns and minimize variance in the face of uncertain future market conditions. It can also used in strategies like pairs trading, where you find two stocks with similar \\(\\beta\\) and buy higher \\(\\ælpha\\) while shortening the other.\nThis is an example where a simple property of random variables, covariance, can be used to get an intepretable model that translates raw data into information directly relevant for dcision making.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#working-with-the-binomial-distribution-in-r",
    "href": "04-lecture4.html#working-with-the-binomial-distribution-in-r",
    "title": "4  Random Variables",
    "section": "4.8 Working with the Binomial Distribution in R",
    "text": "4.8 Working with the Binomial Distribution in R\nR provides built-in functions for working with the binomial distribution. These functions follow a generic syntax that is consistent across many other distributions, making them powerful tools for statistical analysis. The key functions are:\nd is for the probability density function The probability distribution gives the probability of observing a specific number of successes in a binomial experiment: dbinom(x, size, prob) with\n\nx: Number of successes.\nsize: Number of trials (\\(n\\)).\nprob: Probability of success (\\(p\\)).\n\np is for the cumulative distribution function\nThe CDF gives the probability of observing up to a certain number of successes: pbinom(x, size, prob) with\n\nx: Upper limit of the number of successes.\nsize: Number of trials (\\(n\\)).\nprob: Probability of success (\\(p\\)).\n\nq is for the quantile function\nThe quantile function computes the smallest number of successes for which the cumulative probability exceeds a specified value: qbinom(p, size, prob) with\n\np: Cumulative probability.\nsize: Number of trials (\\(n\\)).\nprob: Probability of success (\\(p\\)).\n\nr is for random number generation\nThis function generates random samples from the binomial distribution: rbinom(n, size, prob) with\n\nn: Number of random values to generate.\nsize: Number of trials (\\(n\\)).\nprob: Probability of success (\\(p\\)).\n\nR uses a generic accross different distributions: The d, p, q, and r prefixes used in these functions are part of a generic framework in R:\n\nd for density/PMF.\np for cumulative probability (CDF).\nq for quantiles.\nr for random number generation.\n\nThis structure is consistent across other distributions in R, such as the normal (dnorm, pnorm, qnorm, rnorm) or Poisson (dpois, ppois, qpois, rpois) distributions. Understanding this generic syntax simplifies working with various probability distributions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#programming-in-r-the-binomial-lattic-model",
    "href": "04-lecture4.html#programming-in-r-the-binomial-lattic-model",
    "title": "4  Random Variables",
    "section": "4.9 Programming in R: The binomial lattic model",
    "text": "4.9 Programming in R: The binomial lattic model\nThe binomial lattice model is a simple yet powerful framework for simulating asset price dynamics under uncertainty. It is widely used in financial modeling, particularly in pricing derivative securities like options, as it captures the probabilistic nature of price movements in discrete time. At each time step, an asset can either move up or down by a specified factor, creating a lattice of possible price paths. It is an excellent application case for discrete random variables but also for an introduction to writing more complex programs in R.\nIn this lecture, we will apply the binomial lattice model to simulate the dynamics of a stock index which we have now used several times in this lecture: the S&P 500. This example provides a practical use case to reinforce programming concepts while exploring a key tool in financial mathematics.\nYou will learn how to:\n\nWrite more complex R programs, including functions for creating and exploring the lattice.\nUse control structures like loops (for, while, repeat) and conditional statements (if, else).\nUnderstand and leverage R’s powerful list structure for organizing and manipulating data.\nApply modular programming principles to write reusable, maintainable code.\n\nBy the end of this session, you’ll not only have a deeper understanding of the binomial lattice model but also the computational tools to extend this approach to more complex scenarios in financial modeling.\n\n4.9.1 The binomial lattice model\nTo define the binomia lattice model, we need to establish a basic period length, say 1 week or one day. The model describes a situation where we know the price at the beginning of the period. The price at the beginning of the next period is one of only two possible values. Usually these possibilities are defined as multiples of the price in the previous period, a multiple \\(u\\) (for up) and \\(d\\) (for down). Both \\(u\\) and \\(d\\) are positive with \\(u &gt; 1\\) and \\(d &lt; 1\\). Hence when the price at the beginning of the period is \\(S\\), it will be \\(uS\\) or \\(dS\\) at the beginning of the next period. The probabilities of these outcomes are given by \\(p\\) and \\(1-p\\). This model continues for several periods.\nBy now you will recognize that the outcomes of this model will be well described by a binomial random variable. The model is equivalent to tossing a coin multiple times.\nThis binomial lattice model is fundamentally linked to the binomial distribution. Consider the price after \\(n\\) periods. Each period represents a trial where the price either moves up (\\(u\\)) or down (\\(d\\)), with probabilities \\(p\\) and \\(1−p\\), respectively. The total price after \\(n\\) periods depends on the number of upward movements, denoted by \\(k\\), and is given by:\n\\[\nS_n = S_0 \\times  u^k \\times d^{n−k}\n\\]\nwhere \\(k\\) is the number of “up” movements and \\(n−k\\) is the number of “down” movements.\nThe distribution of \\(k\\) follows a binomial distribution: \\[\nP(k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0, 1, 2, \\dots, n\n\\] which describes the likelihood of observing \\(k\\) upward movements out of \\(n\\) trials.\nThis model may seem too simplistic for describing the price dynamics of a stock index, because there are only two possible outcomes per period.\nBut when you make the connection to the binomial distribution you will realize how this restriction becomes less severe when \\(\\Delta t\\) the time steps are small: the random variable \\(k\\), representing the count of up movements, directly determines the possible price levels of the asset. Over many periods \\(n\\), this simple two-step structure generates a range of prices consistent with the binomial distribution. This relationship also explains why the model becomes increasingly flexible and realistic as the number of periods grows, enabling it to approximate more complex price dynamics.\nBecause the model is multiplicative, the price will never become negative. We can thus work with the logarithm of the price as our fundamental variable.\nAccordinglly, we define \\(\\nu\\) as the expected annual growth rate \\[\n\\nu = \\mathbb[\\ln(S_T/S_0)]\n\\] where \\(S_0\\) is the initial stock price and \\(S_T\\) is the price at the end of the year. Likewise we define the variance as \\[\n\\sigma^2=\\text{var}[\\ln(S_T/S_0)]\n\\]\nIf the period length \\(\\Delta t\\) is chosen small compared to 1, the parameters of the binomial lattice can be selected as: \\[\\begin{eqnarray}\np &=& \\frac{1}{2} + \\frac{1}{2}\\left(\\frac{\\nu}{\\sigma}\\right) \\sqrt{\\Delta t} \\\\\nu &=& \\exp(\\sigma \\sqrt{\\Delta t}) \\\\\nd &=& \\exp(- \\sigma \\sqrt{\\Delta t})\n\\end{eqnarray}\\]\nThis means - for instance - that if \\(\\Delta t\\) is one week the time scaling factor would be \\(\\sqrt{1/52}\\) if it is a day it would be \\(\\sqrt{1/252}\\) assuming 252 trading days in a year.\n\n\n4.9.2 Modelling the Dynamics of the SP500\n\n4.9.2.1 Calibrating the model paraemters from data\nAs a use case we make a model which we calibrate to the SP500, a broad stock market index that we have used already before in our lectures. Let#s get the S&P data using the tidyquantpackage:\n\n# Load necessary libraries\nlibrary(tidyquant)\n\nWe start by retrieving the data first. I use the SP500 ticker symbol ^GSPC here. You could also use SPY which is an ETF tracking the SP500.\n\n# Retrieve S&P 500 data (adjust ticker as needed)\nsp500_data &lt;- tq_get(\"SPY\", from = \"2000-01-01\", to = \"2025-01-01\", get = \"stock.prices\")\n\n# Extract adjusted prices and dates\ndates &lt;- sp500_data$date\nadjusted_prices &lt;- sp500_data$adjusted\n\n# Find the first trading day of each year\nyears &lt;- unique(format(dates, \"%Y\"))  # Extract unique years\nannual_indices &lt;- sapply(years, function(year) {\n  which(format(dates, \"%Y\") == year)[1]  # Get the first trading day of the year\n})\n\n# Extract prices and dates for the first trading day of each year\nannual_prices &lt;- adjusted_prices[annual_indices]\nannual_dates &lt;- dates[annual_indices]\n\nLet me unpack this code-chunk a bit, especially focusing on how we use R to extract the first trading day in each year.\nThis snippet of R code retrieves stock price data for the S&P 500 (or a similar asset) and identifies the first trading day of each year in the data.\nBelow is a step-by-step breakdown with a focus on the sapply function, which plays a key role in identifying these days.\nWe first retrieve S&P 500 Data using the tq_get()function from the tidyquant package. We have done such and similar retrievals before, so I need not further comment on the details here. The result, sp500_data, is a data frame containing columns like date, adjusted (adjusted closing price), and others.\nIn a next step we extract the relevant columns Here: - dates is a vector of all the trading dates in the dataset. - adjusted_prices is a vector of the adjusted closing prices for those dates.\nNow comes our key step, extracting unique years from the dataframe which stores daily stock prices.\n\nyears &lt;- unique(format(dates, \"%Y\"))\n\nThe format(dates, \"%Y\") function converts each date into a 4-digit year (e.g., “2000”, “2001”). The unique() function ensures that only one instance of each year remains. The result is a vector of distinct years present in the dataset.\nNow we want to identify the first trading day of each year\n\nannual_indices &lt;- sapply(years, function(year) {\n  which(format(dates, \"%Y\") == year)[1]\n})\n\nTo understand this code understanding sapply is key. The sapply function applies a user-defined function over each element of a vector (years in this case) and simplifies the result into a vector. Think of it as a way to loop through each year and calculate something for it.\nThe function inside sapply does the following for each year in years:\n\nformat(dates, \"%Y\") == year creates a logical vector that checks which dates in dates belong to the current year.\nwhich(...)[1] finds the indices of TRUE values in the logical vector and selects the first one. This corresponds to the first trading day of the year.\n\nThe annual_indices vector contains the indices in dates that correspond to the first trading day of each year.\nNow we can extract first trading day prices and dates\n\nannual_prices &lt;- adjusted_prices[annual_indices]\nannual_dates &lt;- dates[annual_indices]\n\nHere:\n\nannual_prices holds the adjusted prices for the first trading day of each year.\nannual_dates holds the corresponding dates.\n\nHere’s why sapply is powerful:\n\nIterates Over Elements: It goes through each year in the years vector without requiring an explicit for loop.\nApplies a Function: It applies the provided anonymous function to each year. In this case, the function finds the first trading day’s index for the current year.\nSimplifies the Output: Unlike lapply (which returns a list), sapply returns a vector, making it easier to work with when you need a simple result like indices.\n\nWhy Use sapply Instead of a Loop? While a for loop could achieve the same result, sapply is:\n\nMore concise: You don’t need to explicitly initialize an output vector or update it manually.\nReadable: It clearly expresses the idea of applying a function to each element of years.\n\nIn terms of R programming this part of our code holds some key messages fro you:\n\nsapply Simplifies Iterations: It’s a useful tool when you want to apply a function to a vector or list and return a simplified result.\nLogical Indexing with which: The combination of which and logical conditions (format(dates, \"%Y\") == year) is a common way to find specific indices in R.\nChaining Steps Together: This code demonstrates how small functions like format, which, and sapply can be combined to perform complex tasks efficiently.\n\nNow let us go on in our task of modelling the SP500 using the binomial lattice model. We comute the log returns in a next step and do the calibration. Not much exciting new things are going on here in terms of R. This code-part should be straightformward.\n\n# Compute annual log returns\nlog_returns &lt;- diff(log(annual_prices))  # Compute log(S_T / S_0)\n\n# Combine into a data frame\nannual_log_returns &lt;- data.frame(\n  year = format(annual_dates[-1], \"%Y\"),  # Corresponding years for log returns\n  log_return = log_returns\n)\n\n# Parameters for weekly time step (1/52 years)\n\n# Weekly time step\ndelta_t &lt;- 1 / 52\n\n#Mean log return\nnu &lt;- mean(annual_log_returns$log_return, na.rm = TRUE)\n\n# Variance\nsigma_squared &lt;- var(annual_log_returns$log_return, na.rm = TRUE)  \n\n# Standard deviation\nsigma &lt;- sqrt(sigma_squared)  \n\n# Calibrate binomial model parameters\n\np &lt;- 0.5 + 0.5 * (nu / sigma) * sqrt(delta_t)\nu &lt;- exp(sigma * sqrt(delta_t))\nd &lt;- exp(-sigma * sqrt(delta_t))\n\n# View calibrated parameters\n\nparameters &lt;- list(\n  nu = nu,\n  sigma = sigma,\n  p = p,\n  u = u,\n  d = d\n)\n\nparameters\n\n$nu\n[1] 0.06734656\n\n$sigma\n[1] 0.176833\n\n$p\n[1] 0.5264071\n\n$u\n[1] 1.024825\n\n$d\n[1] 0.9757759\n\n\nWe retrieve a series of daily prices, extract a series of annual price changes from there and then compute the parameters and pack them into a list. The precise mathematical reason why we can calibrate the model like this and why this works, we will see in lecture 5.\n\n\n4.9.2.2 Building the lattice: For loops and lookup tables.\nWe are now going to build the lattice. At this stage let us just take a tiny number of time steps, so we can inspect the output of what we are doing with ease. You can later play for yourself with this code and just increase the value of \\(n\\).\n\n# Set the step number\nn &lt;- 3\n\n# Set the starting price S0 (choose most recent price)\n\nS0 &lt;- adjusted_prices[length(adjusted_prices)]\n\n# Initialize the lattice as a list\nlattice &lt;- vector(\"list\", n + 1)\n\n# Build the lattice\nfor (step in 0:n) {\n  prices &lt;- numeric(step + 1)\n  for (i in 0:step) {\n    prices[i + 1] &lt;- S0 * u^i * d^(step - i)\n  }\n  lattice[[step + 1]] &lt;- prices\n}\n\n# View the lattice\nlattice\n\n[[1]]\n[1] 586.08\n\n[[2]]\n[1] 571.8828 600.6297\n\n[[3]]\n[1] 558.0294 586.0800 615.5406\n\n[[4]]\n[1] 544.5117 571.8828 600.6297 630.8217\n\n\nThis code snippet uses for loops and a lookup table to build a binomial lattice. Both are important concepts when writing R programs-\nLet’s break it down step by step, focusing on the programming concepts involved.\nA for loop in R iterates over a sequence of values, executing a block of code for each value. Its syntax is:\n\nfor (variable in sequence) {\n  # Code to execute in each iteration\n}\n\nIn our code we have two loops: An outer loop for (step in 0:n) {. This loop iterates through each step of the lattice, from 0 to n (the total number of time steps in the model). The variable step represents the current level of the lattice.\nThe inner loop for (i in 0:step) {: For each level of the lattice (step), this loop iterates over the possible nodes (prices) at that level. The variable i represents the number of upward moves at this level.\nA lookup table is a data structure that stores values for quick access, allowing computations to refer to pre-calculated data instead of recomputing it repeatedly.\nIn this example, the lattice itself is the lookup table. It stores the prices at each step and node of the lattice in a list structure:\n\n  lattice[[step + 1]] &lt;- prices\n\nEach element of lattice corresponds to a time step. lattice[[step + 1]] stores a numeric vector containing all possible prices at the step level of the lattice.\nNow here’s how the code uses for loops and a lookup table to build the binomial lattice:\nWe first initialize the lattice by lattice &lt;- vector(\"list\", n + 1). A list of size n + 1 is created to store the prices for each step of the lattice (from step 0 to step n). Lists are ideal here because they allow each element to store a vector of prices, with the number of prices varying at each step.\nPre-allocating space for the output is good practice when writing for loops in R (or any other programming language) because it significantly improves performance and readability. Here’s why: It avoids repeated memory allocation: If you grow an object inside a loop (e.g., using c() to append values), R reallocates memory every time the object changes size. This is computationally expensive, especially for large loops.\nBy pre-allocating the required space, memory is allocated only once, and the values are directly written into the reserved space, saving computation time.\nPre-allocation makes the structure of the output clear before the loop begins and improves code readability. Readers can immediately understand the size and type of the expected result.\nFinally it avoids unnecessary complexity: Growing objects dynamically can make the code harder to follow and debug.\nPre-allocation ensures better performance and cleaner, more professional code!\nLet’s look a bit closer into what our loops do:\nThe Outer Loop iterates over each step of the lattice. At each step, a new vector prices is created to store the prices for that level.\nThe Inner Loop computes the price for each node at the current step. For node i, the price is calculated using: \\[\nS_i = S_0 \\cdot u^i \\cdot d^{\\text{step} - i}\n\\] The result is stored in the prices vector, with indices adjusted for R’s 1-based indexing (i + 1).\nAfter computing all prices for the current step, the prices vector is added to the lattice lookup table at index step + 1.\nThe final lattice contains all possible prices at each step, structured as a list: - lattice[[1]]: Prices at step 0 (initial price, S0). - lattice[[2]]: Prices at step 1 (two possible prices: up and down). - And so on, up to lattice[[n + 1]].\nLet us visualize the lattice. This, of course, only makes sense for small nubers of time steps because the lattice grows rapidly as the time steps get bigger. For our visualization here I use the ggplot2 library, a great library for visualization of all kinds with a consistent “grammar of graphics”.\nLet us prepare the data for our plot first:\n\n# Prepare data for plotting\nplot_data &lt;- data.frame(\n  Step = rep(0:n, times = seq_len(n + 1)),  # x-coordinates\n  Price = unlist(lattice),                 # y-coordinates\n  Label = round(unlist(lattice), 2)        # Labels with rounded prices\n)\n\n# Create edges for connections\nedges &lt;- do.call(rbind, lapply(1:n, function(step) {\n  data.frame(\n    x = rep(step - 1, times = step),           # Starting x-coordinates\n    xend = rep(step, times = step),           # Ending x-coordinates\n    y = lattice[[step]],                      # Starting y-coordinates\n    yend = c(lattice[[step + 1]][-1], lattice[[step + 1]][-length(lattice[[step + 1]])]) # Ending y-coordinates\n  )\n}))\n\nThis part of the code needs some explanation:\nThe code prepares two main components: data for plotting the nodes (prices at each step) and data for connecting those nodes with edges (showing transitions between prices).\nTo prepare the nodes, we create a data.frame for the lattice’s nodes, which represent the prices at each step in the lattice.\n\nStep (x-coordinates):\n\nrep(0:n, times = seq_len(n + 1)) creates a vector repeating each step number (0:n) a specific number of times.\nseq_len(n + 1) generates the sequence of repetitions: 1, 2, 3, ..., n + 1. This ensures that:\n\nAt step 0, there’s 1 node.\nAt step 1, there are 2 nodes.\nAt step 2, there are 3 nodes, and so on.\n\nExample for n = 2: Step = c(0, 1, 1, 2, 2, 2).\n\nPrice (y-coordinates):\n\nunlist(lattice) flattens the list lattice into a single vector.\nlattice is assumed to be a list where each element corresponds to the prices (y-coordinates) at a specific step in the lattice.\nFor instance, if lattice = list(c(100), c(110, 90), c(121, 100, 81)), then unlist(lattice) produces c(100, 110, 90, 121, 100, 81).\n\nLabel:\n\nround(unlist(lattice), 2) creates rounded labels for the prices, suitable for display in a plot.\n\n\nThe resulting data.frame contains three columns:\n\nStep: The x-coordinate (time step).\nPrice: The y-coordinate (price value).\nLabel: Rounded price values for display.\n\nTo create the edges we need to prepare the connections between nodes, which represent the transitions between prices at consecutive steps. Here is what is going on\n\nOuter Loop with lapply:\n\nLoops over each step (1:n) in the lattice. For each step:\n\nConnections are created between the nodes at the current step (step - 1) and the next step (step).\n\n\nStarting Coordinates (x, y):\n\nx: rep(step - 1, times = step) sets the starting x-coordinate for all connections at step step.\n\nE.g., for step 1, x = rep(0, 1); for step 2, x = rep(1, 2).\n\ny: lattice[[step]] provides the y-coordinates (prices) of the starting nodes.\n\nEnding Coordinates (xend, yend):\n\nxend: rep(step, times = step) sets the ending x-coordinate for all connections at the next step.\nyend: The tricky part! It selects the y-coordinates of the ending nodes:\n\nlattice[[step + 1]][-1]: Removes the first price at the next step.\nlattice[[step + 1]][-length(lattice[[step + 1]])]: Removes the last price at the next step.\nCombined using c(...) to connect prices at step step to both upward and downward transitions at step step + 1.\n\n\ndo.call(rbind, ...):\n\nCombines all the individual data.frame objects (one per step) into a single data.frame with the connection data.\n\n\nThe resulting edges data.frame has four columns:\n\nx: Starting x-coordinates.\nxend: Ending x-coordinates.\ny: Starting y-coordinates.\nyend: Ending y-coordinates.\n\nThis is a good opprotubity for understanding lapply and do.call:\n\nlapply applies a function to each element of a sequence (steps here) and returns a list of results.\ndo.call(rbind, ...) merges the list of data.frames into a single data.frame.\n\nNow we can do our visualization:\n\n# load ggplot2 library\n\nlibrary(ggplot2)\n\n# Plot\nggplot() +\n  # Add edges\n  geom_segment(data = edges, aes(x = x, y = y, xend = xend, yend = yend),\n               color = \"gray\", linetype = \"dashed\") +\n  # Add nodes\n  geom_point(data = plot_data, aes(x = Step, y = Price), color = \"blue\", size = 2) +\n  # Add price labels\n  geom_text(data = plot_data, aes(x = Step, y = Price, label = Label),\n            color = \"black\", vjust = -0.5, size = 3) +\n  labs(title = \"Enriched Binomial Lattice with Prices for S&P 500\",\n       x = \"Step\", y = \"Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBefore we go through the syntax of this code snippet let me first briefly introduce you to the logic of ggplot2\nThe ggplot2 package is one of the most popular tools in R for creating visualizations. It uses a layered grammar of graphics, which means you build a plot step-by-step by adding layers. Each layer corresponds to a specific aspect of the visualization, such as:\n\nThe Data: Define the dataset you want to visualize.\nAesthetic Mappings (aes): Specify how variables in your data map to visual properties (e.g., x/y positions, colors, sizes).\nGeometries (geom_): Add geometric objects like points, lines, or bars to represent the data visually.\nThemes and Labels: Customize the look of the plot (theme) and annotate it with titles, axis labels, etc.\n\nWith this understanding, let’s break down the provided code.\n\n\n\n\n4.9.3 The Code: Building a Binomial Lattice Plot\nWe first create a plot base through ggplot() +:\n\nThis initializes an empty plot. The ggplot() function can take a dataset as its argument, but in this case, it’s left empty because the plot will use multiple datasets (edges and plot_data) added in later layers.\n\nIn the next step we add edges by\n\ngeom_segment(data = edges, aes(x = x, y = y, xend = xend, yend = yend),\n             color = \"gray\", linetype = \"dashed\") +\n\n\ngeom_segment: This geometry adds line segments (edges) to the plot. Each segment connects a starting point (x, y) to an ending point (xend, yend).\ndata = edges: Specifies that this layer uses the edges dataset, which contains the start and end coordinates of each edge.\naes(x = x, y = y, xend = xend, yend = yend): Maps the start (x, y) and end (xend, yend) points for the edges from the edges dataset.\nStyling:\n\ncolor = \"gray\" makes the edges gray.\nlinetype = \"dashed\" creates dashed lines to distinguish the edges visually.\n\n\nNow we add nodes by evoking:\n\ngeom_point(data = plot_data, aes(x = Step, y = Price), color = \"blue\", size = 2) +\n\n\ngeom_point: This geometry adds points (nodes) to the plot.\ndata = plot_data: Specifies the dataset containing node information.\naes(x = Step, y = Price): Maps the Step (x-axis) and Price (y-axis) variables from the plot_data dataset.\nStyling:\n\ncolor = \"blue\" sets the points’ color.\nsize = 2 adjusts the size of the points to make them visible but not overpowering.\n\n\nNex we add price labels by\n\ngeom_text(data = plot_data, aes(x = Step, y = Price, label = Label),\n          color = \"black\", vjust = -0.5, size = 3) +\n\n\ngeom_text: This geometry adds text labels to the plot.\ndata = plot_data: Uses the same dataset as the nodes, since labels correspond to the nodes.\naes(x = Step, y = Price, label = Label):\n\nx = Step and y = Price position the labels at the nodes.\nlabel = Label specifies the text to display, which is the rounded price values from the Label column.\n\nStyling:\n\ncolor = \"black\" sets the text color.\nvjust = -0.5 adjusts the vertical alignment of the labels so they appear slightly above the nodes.\nsize = 3 adjusts the font size.\n\n\nFinally we define the plot lables and add the minimaltheme from the library of themes provided by ggplot2.\nThe resulting plot will look like:\n\nA binomial lattice with:\n\nDashed gray edges connecting nodes, representing the upward and downward transitions.\nBlue points for each node in the lattice, representing prices at different steps.\nBlack text labels above the nodes, displaying the rounded price values.\nA clean and minimal design with a title and labeled axes.\n\n\nThis code demonstrates how to layer multiple datasets (nodes and edges) in a single visualization using ggplot2. By carefully controlling each layer and its aesthetic mappings, you can create complex and informative visualizations.\n\n4.9.3.1 Exploring the Lattice: Flow Control\nWe can now use our lattice to demonstrate some other R programing tools. Let us explore the lattice by using whileloops to traverse the lattice and compute specific properties (e.g., maximum or minimum price).\n\n# Traverse lattice to find maximum price\nmax_price &lt;- -Inf\nstep &lt;- 1\n\nwhile (step &lt;= length(lattice)) {\n  max_price &lt;- max(max_price, max(lattice[[step]]))\n  step &lt;- step + 1\n}\n\nmax_price\n\n[1] 630.8217\n\n\nThis code traverses through a binomial lattice (stored as a list of price vectors) to find the maximum price across all steps. Along the way, it introduces the concept of an R while loop, which is a fundamental control structure used for iterative computations.\nWe first initialize the variables max_price and step-\n\nmax_price: This variable keeps track of the maximum price found so far.\n\nIt is initialized to -Inf (negative infinity) because any price in the lattice will be greater than this value, ensuring the first price comparison updates it.\n\nstep: This variable represents the current step (or level) of the lattice being processed. It starts at 1 to access the first step in the lattice.\n\nIn R, Inf (infinity) and -Inf (negative infinity) are special numeric constants that represent values beyond the largest or smallest numbers R can handle. They are used in mathematical operations and comparisons where results exceed the limits of finite numbers.\nInf is larger than any finite number, and -Inf is smaller than any finite number. Inf and -Inf are commonly used as initial values in iterative algorithms. For example:\n\nmax_price &lt;- -Inf ensures any number compared to it will update the value.\nmin_price &lt;- Inf works similarly for finding the smallest value.\n\nAfter this short excursion into Inf and -Inf in R we now come to the step where we transverse the lattice using a whileloop.\n\nwhile (step &lt;= length(lattice)) {\n  max_price &lt;- max(max_price, max(lattice[[step]]))\n  step &lt;- step + 1\n}\n\nHere are the key Concepts of a while Loop:\n\nA while loop repeats a block of code as long as the condition remains TRUE.\nIt’s useful when the number of iterations isn’t predetermined and depends on a condition being met.\n\nHere’s how it works in this snippet:\n\nCondition (step &lt;= length(lattice)):\n\nThe loop continues as long as step is less than or equal to the number of steps in the lattice (length(lattice)).\nThis ensures every level of the lattice is checked.\n\nUpdate Maximum Price:\n\n\n   max_price &lt;- max(max_price, max(lattice[[step]]))\n\n\nlattice[[step]]: Accesses the vector of prices at the current step.\nmax(lattice[[step]]): Finds the maximum price at the current step.\nmax(max_price, ...): Updates max_price with the larger value between the current max_price and the maximum price at the current step.\n\n\nThe expression step &lt;- step + 1 is the so called **increment step: After processing the current step, the step variable is increased by 1 to move to the next step in the lattice.\n\nAfter the loop completes, max_price contains the maximum price found across all steps of the lattice.\nThe General Structure of a while Loop: is:\n\n   while (condition) {\n       # Code block to execute\n   }\n\n\nThe loop runs as long as the condition evaluates to TRUE.\nYou must ensure the condition eventually becomes FALSE; otherwise, the loop will run indefinitely (infinite loop).\n\nThe key Features in our example:\n\nInitialization: Variables (max_price, step) are set up before the loop.\nCondition Check: The loop continues as long as step &lt;= length(lattice).\nUpdate Mechanism: Both max_price and step are updated within the loop, ensuring progress toward termination.\n\nThe common use case for while loops are:\n\nWhen the number of iterations isn’t known beforehand (e.g., processing until a condition is met).\nTraversing a list or dataset, as in this example.\n\nIt is best practice to:\n\nAlways ensure the condition can eventually become FALSE.\nUse print or cat statements inside the loop to debug and see intermediate results.\n\n\n\n4.9.3.2 Does the price exceed a threshold?: Conditional statements to annotate whether a price exceeds a certain threshold.\nLet’s consider next this codes snippet. It helps us finding out whether a price in the lattice exceeds a particular threshold. This is a question you will ask yourself often when analyzing options, for example.\n\n# Annotate prices exceeding 4100\nthreshold &lt;- 4100\n\nfor (step in 0:n) {\n  for (i in seq_along(lattice[[step + 1]])) {\n    if (lattice[[step + 1]][i] &gt; threshold) {\n      print(paste(\"Price exceeds threshold at step\", step, \":\", lattice[[step + 1]][i]))\n    }\n  }\n}\n\nThis code snippet traverses the binomial lattice (stored as a list of price vectors) to identify and print prices exceeding a given threshold (4100). It uses nested for loops to navigate through the lattice and an if conditional statement to check whether each price exceeds the threshold.\nIn the beginning we define our threshold by threshold &lt;- 4100. Any price greater than this value will trigger a message.\nWe have an outer loop\n\nfor (step in 0:n) {\n\n\nThe for loop iterates over the steps of the lattice, from 0 (the initial step) to n (the final step).\nEach step represents a time period or level in the lattice.\n\nWe also have an inner loop to traverse prices within each step, which is in the code segment\n\nfor (i in seq_along(lattice[[step + 1]])) {\n\n\nlattice[[step + 1]]:\n\nAccesses the list element for the current step (step + 1 because R uses 1-based indexing).\nThis element is a vector of prices at the given step.\n\nseq_along(...):\n\nCreates a sequence from 1 to the length of lattice[[step + 1]].\nThis ensures that every price in the current step is checked.\n\nThe i variable is the index of a price in the vector lattice[[step + 1]].\n\nNow comes a new concept: A conditional statement to check the threshold:\n\nif (lattice[[step + 1]][i] &gt; threshold) {\n  print(paste(\"Price exceeds threshold at step\", step, \":\", lattice[[step + 1]][i]))\n}\n\nLet me explain the syntax of the if statment in R:\n\nCondition:lattice[[step + 1]][i] &gt; threshold\n\nChecks whether the price at index i in the current step is greater than the threshold.\n\nIf the condition evaluates to TRUE:\n\nExecutes the code inside the curly braces {...}.\nIn this case, it prints a message containing:\n\nThe step number.\nThe price (lattice[[step + 1]][i]) that exceeds the threshold.\n\n\n\nWhen Condition is FALSE: If the condition evaluates to FALSE (price does not exceed the threshold), the code inside the braces is skipped.\nThe general structure of an if statement in R is\n\nif (condition) {\n  # Code to execute if condition is TRUE\n}\n\n\ncondition:\n\nA logical expression that evaluates to TRUE or FALSE.\nCan involve comparisons (&gt;, &lt;, ==, !=, etc.), logical operators (&, |, !), or built-in R functions.\n\nIf the condition is TRUE, the code block inside {} is executed.\nIf the condition is FALSE, the code block is skipped.\n\nYou always have the option to extend a conditional statement by an else block:\n\nif (condition) {\n  # Code to execute if condition is TRUE\n} else {\n  # Code to execute if condition is FALSE\n}\n\n\n\n\n4.9.4 Writing Modular Code: Reusable Functions for the Binomial Lattice\nIn programming, modularity refers to the practice of dividing your code into smaller, reusable, and independent pieces—typically functions—that can be composed to solve complex problems. Modularity enhances readability, maintainability, and reusability, making your code more professional and easier to debug or extend.\nLet’s refactor the code we’ve written so far into a set of modular functions, demonstrating how this principle can be applied to our binomial lattice model.\nWhy should we write our programs in a modular way? Here are four strong reasons:\n\nClarity: Breaking code into smaller, well-named functions makes the logic easier to follow.\nReusability: Functions can be reused across different projects or scenarios without rewriting the code.\nDebugging: Errors can be isolated to specific functions, simplifying the debugging process.\nTesting: Individual functions can be tested independently, ensuring correctness of each part.\n\nLet us demonstrate this approach by refactoring code we have written so far.\nWe will write the following functions:\n\ncalibrate_parameters: Calibrate the binomial lattice parameters from data.\nbuild_lattice: Construct the binomial lattice using calibrated parameters.\nplot_lattice: Visualize the lattice using ggplot2.\nexplore_lattice: Analyze properties of the lattice, such as maximum price or threshold crossings.\n\n\n4.9.4.1 Calibrating Parameters\nWe begin with a function to compute the lattice parameters from stock price data.\n\ncalibrate_parameters &lt;- function(annual_prices, delta_t) {\n  log_returns &lt;- diff(log(annual_prices))\n  \n  # Calculate mean and variance\n  nu &lt;- mean(log_returns, na.rm = TRUE)\n  sigma_squared &lt;- var(log_returns, na.rm = TRUE)\n  sigma &lt;- sqrt(sigma_squared)\n  \n  # Calibrate parameters\n  p &lt;- 0.5 + 0.5 * (nu / sigma) * sqrt(delta_t)\n  u &lt;- exp(sigma * sqrt(delta_t))\n  d &lt;- exp(-sigma * sqrt(delta_t))\n  \n  list(nu = nu, sigma = sigma, p = p, u = u, d = d)\n}\n\nThis function takes annual prices and a time step (delta_t) as inputs and returns a list of calibrated parameters. It encapsulates the parameter calibration logic, making it reusable for different assets or time frames.\n\n\n4.9.4.2 Building the Lattice\nNext, we encapsulate the lattice construction process into a reusable function:\n\nbuild_lattice &lt;- function(S0, n, u, d) {\n  lattice &lt;- vector(\"list\", n + 1)\n  \n  for (step in 0:n) {\n    prices &lt;- numeric(step + 1)\n    for (i in 0:step) {\n      prices[i + 1] &lt;- S0 * u^i * d^(step - i)\n    }\n    lattice[[step + 1]] &lt;- prices\n  }\n  \n  lattice\n}\n\nThis function constructs the lattice for a given starting price (S0), number of steps (n), and parameters u and d. It returns the lattice as a list of price vectors.\n\n\n4.9.4.3 Visualizing the lattice\nVisualization can also be modularized into a dedicated function:\n\nplot_lattice &lt;- function(lattice) {\n  n &lt;- length(lattice) - 1\n  \n  # Prepare data for plotting\n  plot_data &lt;- data.frame(\n    Step = rep(0:n, times = seq_len(n + 1)),\n    Price = unlist(lattice),\n    Label = round(unlist(lattice), 2)\n  )\n  \n  edges &lt;- do.call(rbind, lapply(1:n, function(step) {\n    data.frame(\n      x = rep(step - 1, times = step),\n      xend = rep(step, times = step),\n      y = lattice[[step]],\n      yend = c(lattice[[step + 1]][-1], lattice[[step + 1]][-length(lattice[[step + 1]])])\n    )\n  }))\n  \n  ggplot2::ggplot() +\n    ggplot2::geom_segment(data = edges, ggplot2::aes(x = x, y = y, xend = xend, yend = yend),\n                          color = \"gray\", linetype = \"dashed\") +\n    ggplot2::geom_point(data = plot_data, ggplot2::aes(x = Step, y = Price), color = \"blue\", size = 2) +\n    ggplot2::geom_text(data = plot_data, ggplot2::aes(x = Step, y = Price, label = Label),\n                       color = \"black\", vjust = -0.5, size = 3) +\n    ggplot2::labs(title = \"Binomial Lattice\", x = \"Step\", y = \"Price\") +\n    ggplot2::theme_minimal()\n}\n\nThis function accepts the lattice and produces a visualization using ggplot2. Note that in this function I use explicit name spaces for the ggplot2 package rather than loading the package by the library()command. The syntax in R is package_name::function_from_package. This allows you to call a function from a package you have already installed without the necessity to load the package via library.\nThis approach is particularly valuable for:\n\nWriting standalone functions (like in this case).\nSharing code in packages or scripts where dependencies may not be preloaded.\nAvoiding confusion when using multiple packages with overlapping function names (e.g., dplyr::filter vs. stats::filter).\n\n\n\n4.9.4.4 Exploring the Lattice\nWe can write modular functions for common exploratory tasks, such as finding the maximum price or checking for threshold crossings:\n\nfind_max_price &lt;- function(lattice) {\n  max_price &lt;- -Inf\n  for (step in seq_along(lattice)) {\n    max_price &lt;- max(max_price, max(lattice[[step]]))\n  }\n  max_price\n}\n\ncheck_threshold &lt;- function(lattice, threshold) {\n  for (step in seq_along(lattice)) {\n    for (price in lattice[[step]]) {\n      if (price &gt; threshold) {\n        print(paste(\"Price exceeds threshold at step\", step - 1, \":\", price))\n      }\n    }\n  }\n}\n\nThese functions encapsulate specific analytical tasks, making them easy to reuse or adapt.\n\n\n4.9.4.5 Putting It All Together\nHere’s how you could use these modular functions in practice:\n\n# Calibrate parameters\nparameters &lt;- calibrate_parameters(annual_prices, delta_t = 1 / 52)\n\n# Build the lattice\nlattice &lt;- build_lattice(S0 = adjusted_prices[length(adjusted_prices)], \n                         n = 5, \n                         u = parameters$u, \n                         d = parameters$d)\n\n# Plot the lattice\nplot_lattice(lattice)\n\n\n\n\n\n\n\n# Explore the lattice\nmax_price &lt;- find_max_price(lattice)\ncat(\"Maximum price in the lattice:\", max_price, \"\\n\")\n\nMaximum price in the lattice: 662.5314 \n\ncheck_threshold(lattice, threshold = 4100)\n\nBy structuring the program into modular functions, the overall workflow becomes more intuitive and maintainable. The binomial lattice model provides a rich framework to demonstrate not just R programming concepts but also best practices for writing clean, reusable code.\n\n\n\n4.9.5 Further Step: Creating a Package for the Binomial Lattice Model\nAs you progress in your programming journey, you’ll often find yourself writing multiple functions for a specific task or model, such as the binomial lattice. A valuable next step would be to collect these functions into an R package, making them reusable and shareable with others.\nAn R package provides a structured way to organize your functions, documentation, and even datasets. By packaging your functions for the binomial lattice model, you can streamline your workflow, enhance collaboration, and ensure consistency in your work.\nFor example, your package could include:\n\nFunctions for parameter calibration (calibrate_parameters),\nFunctions for lattice construction (build_lattice),\nFunctions for visualization (plot_lattice), and\nAnalytical tools (e.g., find_max_price, check_threshold).\n\nCreating a package may seem like a big step, but it builds on the concepts you’ve already learned. It’s an excellent way to deepen your understanding of R programming and software development practices.\nTo learn more about writing R packages, I highly recommend the book Wickham and Bryan (2023). This book provides a comprehensive, step-by-step guide to creating and managing R packages, covering topics such as:\n\nSetting up the package structure,\nWriting documentation with roxygen2,\nAdding tests to ensure your functions work correctly,\nPublishing your package on platforms like GitHub or CRAN.\n\nAlthough creating a package is beyond the scope of this lecture, it’s an exciting area to explore as you gain confidence in programming. For now, focus on writing clean, modular functions, as these are the building blocks of any great R package!\n\n\n4.9.6 Using an LLM to Assist in Developing More Complex Programs\nLeveraging a large language model (LLM) like ChatGPT can be a powerful tool when working on more complex programming tasks, such as developing the binomial lattice model or even creating an R package. Here are some important considerations and tips to help you make the most of this resource:\n\n4.9.6.1 Break Down Your Problem\nBefore asking an LLM for help, clearly define the specific task or problem you’re trying to solve. For example:\n\n“How do I implement a while loop to traverse a list in R?”\n“What is the best way to structure a function for parameter calibration?”\n“How can I generate a plot for the binomial lattice using ggplot2?”\n\nProviding clear, concise prompts allows the LLM to generate more relevant and accurate responses.\n\n\n4.9.6.2 Use LLMs for Code Suggestions\nLLMs are excellent at generating code snippets, but it’s important to review and test the code they provide. For instance:\n\nAsk for an example function to implement a specific part of your program, such as lattice visualization.\nRequest explanations of unfamiliar functions or libraries.\n\nFor example, you could ask: “How do I use sapply to iterate over a list and calculate a result for each element?”\n\n\n4.9.6.3 Seek Explanations and Debugging Help\nIf you encounter errors or unexpected behavior, LLMs can help you debug by:\n\nExplaining error messages.\nSuggesting ways to fix common issues.\nProviding step-by-step walkthroughs of the problematic part of your code.\n\nFor example: “Why am I getting an error when using geom_segment in ggplot2?”\n\n\n4.9.6.4 Ask for Best Practices\nLLMs can provide insights into programming best practices, such as:\n\nWriting modular and reusable code.\nImproving code readability.\nUsing proper function naming and documentation.\n\nYou might ask: “What are some best practices for writing a reusable R function?”\n\n\n4.9.6.5 Learn from Explanations\nWhen the LLM generates code, ask for an explanation of how the code works if something isn’t clear. For example: “Can you explain how the seq_along function is used in this loop?”\nThis turns the interaction into a learning experience and helps deepen your understanding of programming concepts.\n\n\n4.9.6.6 Iterate with Feedback\nUse the LLM interactively:\n\nStart with a basic question or request.\nReview the response, then refine your query based on what you need next.\nProvide feedback, such as clarifying your goals or specifying the context.\n\nFor example: “I like this function, but could you add error handling for invalid inputs?”\n\n\n4.9.6.7 Stay Mindful of Limitations\nWhile LLMs are powerful, they aren’t perfect. Keep these limitations in mind:\n\nThey might generate syntactically correct code that doesn’t solve your specific problem.\nThey may not fully understand the broader context of your program.\nThey can occasionally provide outdated or suboptimal solutions.\n\nAlways validate the code provided, and cross-check important suggestions with reliable documentation or resources.\n\n\n4.9.6.8 Use LLMs as a Collaborative Partner\nRather than relying on the LLM to provide all the answers, think of it as a collaborative partner:\n\nUse it to brainstorm ideas.\nGet inspiration for alternative approaches.\nConfirm your understanding of a concept.\n\n\n\n4.9.6.9 Example Use Case in This Lecture\nWhen developing the binomial lattice model, you might use an LLM in the following ways:\n\nAsk it to write a function for constructing the lattice based on specific parameters.\nRequest help in explaining how the binomial distribution relates to the model.\nSeek guidance on how to visualize the lattice effectively with ggplot2.\n\nBy combining your own problem-solving skills with the capabilities of an LLM, you can streamline the development process while enhancing your learning experience.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#project-portfolio-simulation-and-analysis-with-discrete-random-variables",
    "href": "04-lecture4.html#project-portfolio-simulation-and-analysis-with-discrete-random-variables",
    "title": "4  Random Variables",
    "section": "4.10 Project: Portfolio Simulation and Analysis with Discrete Random Variables",
    "text": "4.10 Project: Portfolio Simulation and Analysis with Discrete Random Variables\n\n4.10.1 Project Overview\nIn this project, you will simulate and analyze the performance of a portfolio composed of three assets under uncertain economic conditions. You will model the asset returns using discrete random variables, compute key portfolio metrics like expected return and variance, and understand the economic intuition behind portfolio diversification. This project will also reinforce key programming concepts such as modular functions, control structures, and R’s list structure.\n\n\n4.10.2 Project Objectives\nBy the end of this project, you should be able to:\n\nApply the concept of random variables to model asset returns under uncertainty.\nSimulate portfolio performance over multiple periods.\nCompute and interpret key portfolio metrics: expected return, variance, and covariance.\nUse modular programming, control structures, and lists in R to build efficient and reusable code.\nUnderstand the trade-offs between risk and return in portfolio theory.\n\n\n\n4.10.3 Define the Portfolio\n\nDefine three economic states: \"High\", \"Medium\", and \"Low\", and their probabilities: 30%, 50%, and 20%.\nAssign discrete returns for three assets under each state:\n\nAsset 1: c(0.08, 0.04, -0.02)\nAsset 2: c(0.12, 0.03, -0.05)\nAsset 3: c(0.05, 0.02, 0.01)\n\nSpecify portfolio weights: 50% for Asset 1, 30% for Asset 2, and 20% for Asset 3.\n\n\n\n4.10.4 Simulate Portfolio Performance\n\nWrite modular functions to:\n\nSimulate a random economic state based on the given probabilities.\nRetrieve asset returns for the simulated state using a list structure.\nCompute the portfolio return as the weighted sum of asset returns.\n\nSimulate portfolio performance over 1,000 periods and store the results in a list. Each element should include:\n\nThe simulated state.\nReturns for each asset.\nThe portfolio return.\n\n\n\n\n4.10.5 Analyze Portfolio Performance\n\nCompute summary statistics for the simulated portfolio returns:\n\nMean return.\nVariance of returns.\n\nCompare the simulated mean return with the theoretical expected portfolio return, calculated as: \\[\nE(R_{\\text{portfolio}}) = \\sum_{i=1}^n w^i \\cdot E(r^i)\n\\]\n(Optional) Compute the theoretical portfolio variance: \\[\n\\text{Var}(R_{\\text{portfolio}}) = \\sum_{i=1}^n (w^i)^2 \\cdot \\text{Var}(r^i) + \\sum_{i \\neq j} w^i \\cdot w^j \\cdot \\text{Cov}(r^i, r^j)\n\\]\n\n\n\n4.10.6 Visualize Results\n\nCreate a histogram of the simulated portfolio returns.\nCalculate and display the frequency of each economic state in the simulation to verify that the probabilities were correctly implemented.\n\n\n\n4.10.7 Reflection and Extensions\n\nReflect on the relationship between diversification and risk. How does combining assets with different return profiles reduce portfolio variance?\nAdd a fourth asset to the portfolio and observe its impact on portfolio metrics. Explore how changing portfolio weights affects expected return and variance.\n\n\n\n\n\nWickham, Hadley, and Jenifer Bryan. 2023. R Packages: Organize, Test, Document, and Share Your Code. O’Reilly. https://r-pkgs.org/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#project-overview",
    "href": "04-lecture4.html#project-overview",
    "title": "4  Random Variables",
    "section": "4.11 Project Overview",
    "text": "4.11 Project Overview\nIn this project, you will simulate and analyze the performance of a portfolio composed of three assets under uncertain economic conditions. You will model the asset returns using discrete random variables, compute key portfolio metrics like expected return and variance, and understand the economic intuition behind portfolio diversification. This project will also reinforce key programming concepts such as modular functions, control structures, and R’s list structure.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#project-objectives",
    "href": "04-lecture4.html#project-objectives",
    "title": "4  Random Variables",
    "section": "4.12 Project Objectives",
    "text": "4.12 Project Objectives\nBy the end of this project, you should be able to:\n\nApply the concept of random variables to model asset returns under uncertainty.\nSimulate portfolio performance over multiple periods.\nCompute and interpret key portfolio metrics: expected return, variance, and covariance.\nUse modular programming, control structures, and lists in R to build efficient and reusable code.\nUnderstand the trade-offs between risk and return in portfolio theory.\n\n\n4.12.1 Define the Portfolio\n\nDefine three economic states: \"High\", \"Medium\", and \"Low\", and their probabilities: 30%, 50%, and 20%.\nAssign discrete returns for three assets under each state:\n\nAsset 1: c(0.08, 0.04, -0.02)\nAsset 2: c(0.12, 0.03, -0.05)\nAsset 3: c(0.05, 0.02, 0.01)\n\nSpecify portfolio weights: 50% for Asset 1, 30% for Asset 2, and 20% for Asset 3.\n\n\n\n4.12.2 Simulate Portfolio Performance\n\nWrite modular functions to:\n\nSimulate a random economic state based on the given probabilities.\nRetrieve asset returns for the simulated state using a list structure.\nCompute the portfolio return as the weighted sum of asset returns.\n\nSimulate portfolio performance over 1,000 periods and store the results in a list. Each element should include:\n\nThe simulated state.\nReturns for each asset.\nThe portfolio return.\n\n\n\n\n4.12.3 Analyze Portfolio Performance\n\nCompute summary statistics for the simulated portfolio returns:\n\nMean return.\nVariance of returns.\n\nCompare the simulated mean return with the theoretical expected portfolio return, calculated as: \\[\nE(R_{\\text{portfolio}}) = \\sum_{i=1}^n w^i \\cdot E(r^i)\n\\]\n(Optional) Compute the theoretical portfolio variance: \\[\n\\text{Var}(R_{\\text{portfolio}}) = \\sum_{i=1}^n (w^i)^2 \\cdot \\text{Var}(r^i) + \\sum_{i \\neq j} w^i \\cdot w^j \\cdot \\text{Cov}(r^i, r^j)\n\\]\n\n\n\n4.12.4 Visualize Results\n\nCreate a histogram of the simulated portfolio returns.\nCalculate and display the frequency of each economic state in the simulation to verify that the probabilities were correctly implemented.\n\n\n\n4.12.5 Reflection and Extensions\n\nReflect on the relationship between diversification and risk. How does combining assets with different return profiles reduce portfolio variance?\nAdd a fourth asset to the portfolio and observe its impact on portfolio metrics. Explore how changing portfolio weights affects expected return and variance.\n\n\n\n\n\nWickham, Hadley, and Jenifer Bryan. 2023. R Packages: Organize, Test, Document, and Share Your Code. O’Reilly. https://r-pkgs.org/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html",
    "href": "01-lecture1.html",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1 Tossing a fair coin: First Probability ideas.\nLets start with a classic and basic example of a probability model: Tossing a fair coin. When probability theorists talk of a fair coin they mean that the coin has equal chances of coming up heads as coming up tails. When the coin is tossed it is uncertain whether it will ultimately come up Heads or Tails.\nFigure 1.1: Figure 1: Tossing a coin\nFigure 1.1 visualizes such a process.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#tossing-a-coin",
    "href": "01-lecture1.html#tossing-a-coin",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1.1 Random experiments, sample space, events\nIn the theory of probability a process leading to an uncertain outcome is called a random experiment. The example of tossing a coin helps us to give a precise meaning to the notion of an uncertain outcome. While we can imagine in principle that the coin lands on its rim and this is one outcome, we agree on the outset that we are going to consider the (practically relevant cases) that it will land on one of the faces.\nThe collection of all possible outcomes of a practical or conceptual random experiment is called in probability theory a sample space. While the sample space of tossing a coin is an idealization it is exactly this idealization which simplifies the theory without affecting its applicability. The basic outcomes in the random experiment of tossing a coin are that the die lands such that the upward showing face shows Heads or Tails. Let us encode this outcome by 1 for “Heads” and 0 for “Tails”. In the theory the collection of basic outcomes is denoted as a set. Thus the sample space of tossing a coin is given as the set \\({\\cal S} = \\{1,0\\}\\).\nThe sample space \\({\\cal S}\\) is the set of all basic outcomes, which are in our case only two: 1 or 0.\nIn more complex random experiments there will usually be more than two outcomes. Subsets of the sample space containing more than one basic outcome are called events in the theory of probability.\n\n\n\n\n\n\nNow you try\n\n\n\nConsider another random experiment from games of chance: Throwing a six sided die. What are the basic outcomes in this random experiment? What is the sample space in this case? How would you describe the event that the outcome of the die shows an odd number in terms of a subset of the sample space? Consider yet a new random experiment where you throw two coins, instead of one. What are the basic outcomes in this random experiment? What is the sample space in this case? How do these concepts look like when you throw two six sided dice instead of two coins?\n\n\n\n\n1.1.2 The role and nature of idealizations in applications\nIdealizations of the kind we discussed for the toss of a coin are standard in probability and we will encounter them again and again. For example the movement of a stock price is often though of as a conceptual random experiment. When we try to agree on what is the appropriate sample space of this experiment, we can say that the price can not fall below 0 but it is hard to agree on what will be the highest possible price. In the probabilistic treatment of stock prices, which we will discuss later in our course, it is for instance common to assume that the sample space of this random experiment is the entire interval of non-negative real numbers \\({\\cal S} = [0, \\infty)\\).\nMany of us would hesitate to claim that the price might rise without bound. Yet many models in applied Finance are based on such an assumption. The models allow arbitrary price hikes but with arbitrary small probability as the price gets higher and higher. Practically it does not make sense to believe that a security price can become arbitrarily high. The use of arbitrarily small probabilities in a financial model might seem absurd but it does no practical harm and makes the model simple and convenient to use. Moreover, if we seriously introduced an upper bound on a security price at \\(x\\) it would be also awkward to assume that it is impossible that it could be just a cent higher, an assumption equally unappealing than assuming it can get in principle arbitrarily high.\n\n\n1.1.3 Classical Probability: Measuring uncertainty\nProbability is a measure of how likely an event of an experiment is. But how could we measure chance? Here is the first big idea of probability theory and how the originators thought about it.1 How do you measure anything? If you think of length, for example, you take an arbitrary standard you agree on and then count. The originators of probability theory pursued the same idea with chance: To measure probability you choose equally probable cases and then count.\n1 In this discussion I use chapter 1 in Diaconis and Skyrms (2019)The probability of an event \\(A\\) according to the originators of the theory of probability is then \\[\\begin{equation}\nP(A) = \\frac{\\text{Number of cases where $A$ occurs}}{\\text{Total number of cases}}\n\\end{equation}\\]\nThus, if we want to know the probability of the coin ending up Heads, we have to compute according to this notion \\(1/2\\), since the total number of cases is 2 (Heads or Tails).\n\n\n\n\n\n\nNow you try\n\n\n\nConsider another random experiment of throwing a six sided die: According to the probability notion explained above what would be the chance that the die lands on on a face such that an even number is shown?. What would be the chance that in the random experiment of throwing two dice the sum of points would be 7? What is the chance that when rolling two six sided dice the sum is 2?\n\n\nNote that this classical notion of probability has a few interesting consequences, which we will discuss in more detail later but which already now naturally flow from this basic idea of measuring chance.\n\nProbability is never negative.\nIf an event \\(A\\) occurs in all cases \\(P(A)=1\\).\nIf \\(A\\) and \\(B\\) never occur in the same case, then \\(P(A \\,\\text{or}\\, B) = P(A) + P(B)\\).\n\nWhile rule 1. and 2. are intuitively fairly obvious (try to think of examples to convince yourself!), rule 3 might need some concrete explanation. The case described by rule 3 is often referred to as mutually exclusive events, i.e. they can not occur together.\n\n\n\n\n\n\nExample for rule 3:\n\n\n\nSuppose you have a standard deck of 52 playing cards, and you are interested in the probability of drawing a King \\(A\\) or a Queen \\(B\\).\n\nThe probability of drawing a King is \\(P(A) = \\frac{4}{52}\\), since there are 4 Kings in the deck.\nThe probability of drawing a Queen is \\(P(B) = \\frac{4}{52}\\), since there are also 4 Queens.\n\nSince a card cannot be both a King and a Queen, \\(A\\) and \\(B\\) are mutually exclusive events.\nThus, the probability of drawing a King or a Queen is: \\(P(A \\,\\text{or}\\, B) = P(A) + P(B) = \\frac{4}{52} + \\frac{4}{52} = \\frac{8}{52} = \\frac{2}{13}\\)\nThis illustrates how probabilities of mutually exclusive events add up.\n\n\nIn particular then the probability that an event does not occur is 1 less the probability that it occurs: \\(P(\\text{not}\\, A) = 1 - P(A)\\).\nNow let me show you how you can make use of a large language model, using ChatGPT as a use case, to support learning new concepts such as we have just introduced them here.\nWe have discussed new two classical and famous random experiments you will probably find in every textbook on probability. Assume you would like to deepen your understanding of these concepts and also consider more complex examples in particular from Finance. This is something the LLM is very good at. You could - for instance - ask ChatGPT at the prompt something like shown in Figure 1.2 :\n\n\n\n\n\n\n\n\nFigure 1.2: Figure 2: The ChatGPT prompt\n\n\n\n\n\nHere is an answer I would get for this prompt:\n\n\n\n\n\n\nAnswer by ChatGPT40\n\n\n\nHere are three detailed examples from finance to illustrate the concepts of a random experiment, sample space, basic outcomes, and events:\n\nExample 1: Daily Price Movement of a Stock\nRandom Experiment: Observe the daily price movement of a particular stock (e.g., Apple Inc.) at the close of the trading day.\n\nSample Space \\({\\cal S}\\): The set of all possible outcomes of the price movement. For simplicity, we can represent the sample space as: \\({\\cal S} = \\{ \\text{price increase, price decrease, no change} \\}\\)\nBasic Outcomes:\n\n“Price increases” (e.g., the stock closes higher than it opened).\n“Price decreases” (e.g., the stock closes lower than it opened).\n“No change” (e.g., the stock closes at the same price it opened).\n\nEvents:\n\nEvent \\(A\\): “The stock price increases.” \\(\\{ \\text{price increase} \\}\\)\nEvent \\(B\\): “The stock price either increases or remains unchanged.” \\(\\{ \\text{price increase, no change} \\}\\)\n\n\n\nExample 2: Credit Card Default Risk\nRandom Experiment: Assess whether a randomly selected credit card holder defaults on their monthly payment.\n\nSample Space \\({\\cal S}\\):\n\\(S = \\{\\text{default, no default} \\}\\)\nBasic Outcomes:\n\n“Default” (e.g., the cardholder fails to make the minimum payment).\n“No default” (e.g., the cardholder pays at least the minimum required).\n\nEvents:\n\nEvent \\(A\\): “The cardholder defaults.” ({default})\nEvent \\(B\\): “The cardholder does not default.” ({no default})\nEvent \\(C\\): “Default occurs for a customer from a specific demographic (e.g., under 30 years old).” This is a subset of the sample space but is conditioned on additional information.\n\n\n\nExample 3: Bond Ratings Transition\nRandom Experiment: Observe the change in the credit rating of a corporate bond over a year.\n\nSample Space \\({\\cal S}\\): The set of possible credit rating transitions. For example, using a simple rating scale: \\({\\cal S} = \\{\\text{AAA}, \\text{AA}, \\text{A}, \\text{BBB}, \\text{BB}, \\text{B}, \\text{CCC}, \\text{default} \\}\\)\nOver a year, the bond can transition from its initial rating to any of these, or stay the same.\nBasic Outcomes:\n\n“AAA to AAA” (no change in the highest credit quality).\n“BBB to BB” (downgrade).\n“A to default” (significant deterioration).\n\nEvents:\n\nEvent \\(A\\): “The bond is downgraded by one or more notches.” (Any outcome where the final rating is lower than the initial rating, e.g., {AAA to AA, A to BB}).\nEvent \\(B\\): “The bond defaults.” ({default}).\nEvent \\(C\\): “The bond remains investment-grade.” (All transitions where the final rating is AAA, AA, A, or BBB).\n\n\n\nThese examples show how the concepts apply to finance, helping to bridge the theoretical framework of probability with practical applications in the field. Would you like to dive deeper into any of these examples or extend them to computations like probabilities of events?\n\n\nLet us interrupt here our discussion of probability for a moment and ask how we can make good on our promise to make these ideas tangible, so we can play with them.\nFor this we will need the computer. Since we will talk to the computer in R, it is now the right time to look at the coin tossing example from the perspective of R and dive into some of its basic concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#tossing-coin-on-computer",
    "href": "01-lecture1.html#tossing-coin-on-computer",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.2 Tossing a coin on the computer: First steps in R",
    "text": "1.2 Tossing a coin on the computer: First steps in R\n\n1.2.1 The R User Interface\nBefore we can ask our tool to do anything for us, we need to know how to talk to it. In our case RStudio allows us to talk to our computer. It works like any other application. When you launch RStudio on your computer, in figure Figure 1.3 you see a screen looking like this:\n\n\n\n\n\n\n\n\nFigure 1.3: The RStudio startup screen\n\n\n\n\n\nIn this picture you see a screenshot of my RStudio screen. Interacting with the app is easy. You type commands via your keyboard at the prompt, which is the &gt; symbol. You find this symbol in the RStudio pane called Console. You can see it in the left pane in the screenshot. You send the command to the computer by pressing enter. After you have pressed enter, RStudio sends the command to R and displays the result of your command with a new prompt to enter new commands, like this:\n\n1+1\n\n[1] 2\n\n\nLet me pause to explain what you see here in this code example. First you see a light-gray box containing the command 1+1. This is an instance of a so called code chunk. Moving the cursor to the right upper corner of the chunk, will display a copy icon and you can click this icon to copy the code to the computer’s clipboard and then paste it at the prompt of your console (or for that matter anywhere else) and run it in R studio, if you wish. In the code-chunk the R-prompt &gt; is not displayed. This symbol of the prompt is only shown in the Console itself.\nThe [1] means that the line begins with the first value of your result. For example, if you enter the command 20:60 at the prompt of your console which means in the R language, “list all the integers from 20 to 60” and press enter you get:\n\n20:60\n\n [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[26] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nmeaning that 20 is the first value displayed in your result. Then there is a line break because not all values can be displayed on the same line and R tells you that 45 is the 26-th value of the result.\nThe colon operator : is a very useful function in R which we will need often. It allows us to create sequences of every integer between two given integers.\nR needs a complete command to be able to execute it, when the return key is pressed. Lets see what happens, if a command is incomplete, like for instance &gt; 5*.\nIn this case R will show the expression followed by a + instead of showing a new prompt. This means that the expression is incomplete. It expects more input. If we complete the expression, say like\n&gt; 5*\n+ 4\nthe expression can be evaluated and a new prompt is shown in the console.\nIf you type a command that R does not understand, you will be returned an error message. Don’t worry if you see an error message. It just is a way the computer tells you that he does not understand what you want him to do.\nFor instance, if you type 5%3 you will get an error message like this\n&gt; 5%3\nError: unexpected input in \"5%3\"\n&gt;\nSometimes it is obvious why a mistake occurred. In this case, that R just does not know what to do with the symbol %. It has no meaning in this context. Sometimes it is not so obvious what the error message actually means and what you might do about it.\nA useful strategy in this case is to type the error message into a search engine and see what you can find. The chance is very high that others encountered the same problem before you and got helpful advice how to fix it from other users on the internet. One site, we find particularly helpful for all kinds of questions related to R and R programming is https://stackoverflow.com/. Try it at the next opportunity. You can also show the error to the LLM prompt and ask how to fix it. In most cases chatGPT will give you a useful answer.\nNow with this basic knowledge, we can already make the first step to create a coin on the computer using R. If you think of a physical coin, the essential thing that matters is that it can show Heads or Tails. Lets encode Heads by 1 and Tails by 0. If you toss the coin it will usually land on one of these faces and the upward showing side of the coin shows the outcome of the toss. The colon operator : gives us a way to create a group of numbers from 0 to 1. R gives us the result as a one dimensional set of numbers.\n\n0:1\n\n[1] 0 1\n\n\nLets use these first steps in R to recap the probability concepts we have learned using this example of the six sided die: A basic outcome of tossing the coin is for example 1 if the upper side after tossing the coin happens to be Heads. The sample space of the experiment of tossing a coin is the set \\({\\cal S} = \\{0,1\\}\\). In probability theory we often use the symbol \\({\\cal S}\\) or \\(S\\) for sample space. In many probability texts the sample space is also often denoted by the symbol \\(\\Omega\\) the Greek letter for (big) Omega. A random experiment in this example is the tossing of the coin. The outcome is uncertain but once the coin is tossed the outcome can be determined precisely. The event that the outcome is a display of 2 is the empty set \\(A = \\emptyset\\). The symbol \\(\\emptyset\\) comes from set theory and means the set containing no elements. This event can contain no elements because we can not get any other number than 0 or 1 by tossing the coin.\n\n\n1.2.2 Objects\nYou can save data in R by storing them in objects. An object is a name, you can choose yourself to store data. For example, if you choose to store the value 1 in an object called Heads, you would type:\n\nHeads &lt;- 1\n\nat the prompt. R will the store the value 1 in the object called Heads, which you can use to refer to the value. If you type the name of your object at the prompt, R will display the value you have assigned. A useful key combination for typing the assignment operator &lt;- is to use the key combination ALT _. At the R prompt R will automatically print an assignment operator.\nNow if you type the name of the object and press enter, R will display the value or values stored in it, like this:\n\nHeads\n\n[1] 1\n\n\nNow you can use the name of the object to refer to its value. For instance, you could divide Heads by 2 and get a meaningful result\n\nHeads/2\n\n[1] 0.5\n\n\nNow to make our coin more tangible and useful, let us store it in an R object by typing the following command at the prompt. This command creates an object with name coin and assigns the vector 0,1 to it.\n\ncoin &lt;- 0:1\n\n\n\n\n\n\n\n\n\nFigure 1.4: The RStudio Environment pane keeps track of the objects you have created\n\n\n\n\n\nYou can now see in the right upper Environment pane in figure Figure 1.4 that R shows you that there is an object with the name coin that it consists of integers 0,1. As you create more objects they will be stored in the Environment pane and are ready for your reference, unless you delete them. You can remove or delete an object by typing rm(object) or by assigning the value coin &lt;- NULL which would also remove the object from your environment or workspace.\nYou can name your objects almost anything with a few exceptions. An object name must not start with a number. There are some special symbols which can also not be used in object names, like ^, !, $, @, +, -, /, *. Note that R is case sensitive and distinguishes small and big letters. If you assign a new value for an object you have already created, R will overwrite the object without warning.\nYou can see which objects are currently created and available for you in the Environment pane of your session of by typing ls(). The UNIX users among you will recognize this command from the unix shell, where it displays the files in a directory.\nBefore we learn how we can actually roll our die and perform a random experiment with it, let us briefly use the opportunity to explain a few things about how R does computations. We have already explained that we can use the object name to refer to the value. So for instance if we type\n\ncoin*coin\n\n[1] 0 1\n\n\nThis might irritate some of you because we have called the object a vector. In linear algebra multiplication of vectors is only allowed if there is an inner product. What happens here, if we use * the multiplication operator is that R does an element-wise multiplication of the six numbers of our die. Of course R allows to take an inner product as well, but this needs a different operator. To compute an inner product, we would type\n\ncoin %*% coin\n\n     [,1]\n[1,]    1\n\n\nNow R displays the result as a vectors with one row and one column, which is denoted in the output by [ , 1] for the column and [1, ] for the row. We will learn later more about the use and the meaning of this notation in R.\nThe element wise execution R usually uses also means that when you, for example type\n\ncoin + 1\n\n[1] 1 2\n\n\nR would subtract 1 from every component in the vector coin.\nAnother specific behavior of R, you need to know about is called recycling. If you give R two vectors of different length in an operation, R will repeat the shorter vector as long as it is of equal length with the longer one. For example, if you have:\n\ncoin + 1\n\n[1] 1 2\n\n\nyou see that R adds 1 to 0 and then starts over again by adding 1 to 1.\nIf the longer vectors is not a multiple of the shorter one, R recycles but the cuts off.2\n2 As of R version 4.2.0 (released in April 2022), the behavior regarding the recycling rule has changed. In older versions of R, a warning was issued when the length of one vector was not a multiple of the length of the other vector during arithmetic operations. The reasoning behind this change was to reduce unnecessary noise in the output. Many users found the warnings redundant in cases where the recycling rule was intentional and understood. However, this change means you need to be more cautious, as unexpected behavior might go unnoticed without the warning.\ncoin + 1:4\n\n[1] 1 3 3 5\n\n\nWhile this might seem awkward to some of you, we will see that for data manipulation element-wise execution is often extremely useful. It allows to manipulate groups of values in a systematic yet easy way.\n\n\n1.2.3 Functions\nR contains many functions which we can use to manipulate data and compute things. The syntax for using a function is very simple: You type the function name and put the value of the function argument in parentheses. Here we use for illustrations the function of the square root sqrt():\n\nsqrt(4)\n\n[1] 2\n\n\nor rounding a number:\n\nround(3.1415)\n\n[1] 3\n\n\nThe data you write in the parentheses are called the function arguments. Arguments can be all sorts of things: raw data, R objects, results from other functions.\nIf functions are nested, R evaluates the innermost function first and then goes on to the outer functions. To see examples of all these instances you can take\n\nnumbers &lt;- 1:7\nmean(numbers)\n\n[1] 4\n\nround(mean(numbers))\n\n[1] 4\n\n\nfor example.\nFor simulating random experiments, R has the very useful function sample(). With this function we can - for example - toss a coin on the computer and conduct actual random experiments.\nThe function takes as arguments a vector names x and a number named size. sample will return size elements randomly chosen from the vector x. Lets say:\n\nsample(x = 0:1, size = 1)\n\n[1] 0\n\n\nIn this case sample has chosen either 0 or 1 from the vector x = (0,1) at random.\nIf we want to toss the coin in our computer we can thus pass the coin as an argument to sample and use the number 1 for the size argument. Lets do a few tosses with our die\n\nsample(x = coin, size = 1)\n\n[1] 0\n\nsample(x = coin, size = 1)\n\n[1] 1\n\nsample(x = coin, size = 1)\n\n[1] 1\n\nsample(x = coin, size = 1)\n\n[1] 0\n\n\nThese are the random outcomes of our consecutive tosses. It is as if we had thrown an actual coin but in this case we have done the same thing on the computer. Isn’t it cool that this is possible at all? The sample() function will remain our good friend throughout this course.\nR functions can have many arguments, but they need to be separated by a comma.\nEvery argument in every function has a name. We specify which data are assigned to the arguments by setting a mathematical equal sign = between the name and the data. Names help us to avoid passing the wrong data and thereby mixing up things or committing errors. But using names is not necessary. If we just wrote\n\nsample(coin,1)\n\n[1] 1\n\n\nR would also know what to do. It is not always clear which names to use for a function. If you are not sure, you can use the function args() to look it up. Here we take the function roundas one example.\n\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n\nNote that the digits argument in the round function is already set to 0. Frequently R functions come with optional arguments. These arguments are optional because the come with a default value, which is 0 in case of the round function.\nWe recommend that you write out argument names as a rule. It gives clearer code and avoids errors. If you don’t write argument names, R matches your values to the arguments of the function by order.\n\n\n1.2.4 Writing your own functions\nNow we are ready to write our own function to toss the coin in our computer. Each function in R has the same elements: A name, a function body of code and a set of arguments. To write your own function, you have to write up all of these parts and save them in an R object. The syntax is:\nmy_function &lt;- function() {}\n\nThe name here is my_function, next comes the expression function() which needs to be assigned. The names of the function arguments have to be written between the parentheses. Then we have to write the actual code within the braces {}.\nTo do this for the die, lets write a function named toss_coin.\n\ntoss_coin &lt;- function(){coin &lt;- 0:1 \n                         sample(coin, size = 1)}\n\nNow we can toss our coin for a few times to show how the function works\n\ntoss_coin()\n\n[1] 0\n\ntoss_coin()\n\n[1] 1\n\ntoss_coin()\n\n[1] 0\n\ntoss_coin()\n\n[1] 1\n\ntoss_coin()\n\n[1] 1\n\n\nNote that in our function toss_coin() has no arguments, just the function body. This is perfectly legitimate in R. It is important that when we call the function we have to call it with the parenthesis like toss_coin(). If we only call the name toss_coin, R will display the code in the function body.\nA final remark in the sample function is in place here. If we look at the arguments of sample using the args function we see\n\nargs(sample)\n\nfunction (x, size, replace = FALSE, prob = NULL) \nNULL\n\n\nLets do not discuss all the details of this output but concentrate for a moment on the replace argument. What does this mean?\nAs we saw previously we can use the sample function to model the rolling of our die. If we set the size argument to 1 we get the toss of one coin. If we set the size argument to n, we would simulate the tossing of n coins. But now the replace argument becomes crucial. As we can see in the output of the args function replace has a default value FALSE. This is a logical argument. It tells R, for example, that if we set size = 2, meaning that two coins are tossed, if the first coin shows, say Heads - a value of 1- the coin die cannot show Heads as well.\nThis is clearly not what we have in mind when we model the tossing of two coins. It should be possible that both coins show the same value. To enable this behavior of the sample function, we have to change the default value of the replace argument to TRUE. Then R chooses a random draw from all of the six possible values for all dice rolled.\nCongratulations ! You have written your first R function for conducting a simple random experiment. Let me remind you once again: Think of the parentheses as a trigger that tells R to run the function. If you omit the trigger R just prints the body of the function. When you run a function, all the code in the function body is executed and R returns the result of the last line of code. If the last line of code does not return a value neither will R.\n\n\n\n\n\n\nNow you try\n\n\n\nConsider another random experiment of throwing a six sided die. Use the concepts learned in this section to write a function which simulates the rolling of a six sided die. Write a function to simulate the throwing of two dice. Think about how to set the replace argument in the sample function in this case. Use R’s help function to learn about the role of the replace argument. Alternatively ask the LLM to explain the use of replace in R’s sample function. You can also ask for examples or demonstrations what goes wrong when this argument is parametrized in the wrong way.\n\n\n\n\n1.2.5 Arguments\nImagine we remove the first line of code in our function body and changed the name die in the sample function to “ball”.\n\ntoss_coin2 &lt;- function(){sample(ball, size = 1)}\n\nIf we call the function now, we will get an error. The function call roll_die2() will result in the error message Error in sample(ball, size = 1) : object 'ball' not found (try it!)\nWe could supply ball when we call toss_coin2 if we make ball an argument of the function. Lets do this:\n\ntoss_coin2 &lt;- function(ball){sample(ball, size = 1)}\n\nNow the function will work as long as we supply ball when we call the function.\n\ntoss_coin2(ball = 0:1)\n\n[1] 1\n\n\nNote that we still get an error, if we forget to supply ball argument. This could be avoided if we give the function a default argument\n\ntoss_coin2 &lt;- function(ball= 0:1){sample(ball, size = 1)}\n\nNow if we type:\n\ntoss_coin2()\n\n[1] 1\n\n\neverything works, just as intended.\n\n\n1.2.6 Scripts\nSo far we have worked by interacting with the console. But what if you want to edit your functions? It would be much easier, if you could use a draft of your code and work form there. This can be done by using a script.\nYou create a script by going to File &gt; New File &gt; R script in the menu bar of RStudio. Using scripts is the standard way to write code in R. It not only helps you to keep track of your code, save it and edit it later. It also makes your work reproducible. You can edit and proofread your code and share it with others. To save your script go to File &gt; Save As in the menu bar.\n\n\n\n\n\nThe RStudio Script\n\n\n\n\nRStudio has many useful features to help you work with scripts. You can for instance automatically execute a line in a code by using the run button. You can also execute sections of code or the entire script. The entire script is executed by running the Source button. For all these commands there are key short cuts which you will learn as you work more with RStudio and R.\nFrom now on you should write all your R-code for this course in scripts. Make a new folder for the scripts you write and store your R-code there from now on.\n\n\n1.2.7 Using Quarto Documents\nQuarto documents provide an elegant way to combine explanatory text, R code, and the resulting output in one cohesive file. This makes them especially useful for documenting your learning process, conducting reproducible research, or preparing assignments.\nLet’s walk through how to use a Quarto document to write down and experiment with your coin-tossing function.\n\nCreating a New Quarto Document: To begin, create a new Quarto document in RStudio by navigating to File &gt; New File &gt; Quarto Document.... You will see a popup window allowing you to type a title for the document and your name. It also allows you to select a format for your document. Choose the HTML format for now, as it allows easy preview in your browser, and click the Create button The popup window will look something like this:\n\n\n\n\n\n\nThe quarto pop up window\n\n\n\n\n\nUnderstanding the Quarto Layout: The new Quarto document will have a YAML header at the top, which might look like this:\n\n\n\n\n\n\nThe quarto yaml header\n\n\n\n\nBelow this header, you will see some example text and R code chunks. Replace the example content with your own text and code.\n\nAdding the Coin Tossing Function: In your Quarto document, you can explain your approach to creating the coin-tossing function, and include the code in a code chunk.\nHere’s an example:\n\n\n\n\n\n\nCombining text and code in quarto documents\n\n\n\n\n\nRunning and Compiling the Document: Once you have written your text and code, save your document. You can execute the R code directly within the Quarto document. To run a code chunk, click the green play button in the upper-right corner of the chunk or use the shortcut Ctrl + Shift + Enter (Windows) or Cmd + Shift + Enter (Mac).\nAfter executing your code, you can compile the document into a readable output format by clicking the Render button or using the shortcut Ctrl + Shift + K (Windows) or Cmd + Shift + K (Mac).\n\nThere is of course a lot you can do to enhance your quarto documents. For example the lecture notes you are reading now are all written and composed in quarto as are the lecture slides.\nBy using Quarto documents, you can create well-documented, interactive code files that are both functional and easy to share with others. You can use quarto indpendently from R and combine it with other languages like phyton or julia. Ceck out: https://quarto.org/\n\n\n1.2.8 Using packages and finding Help\nWe have now a function which we can use to simulate the tossing of a coin, coin_toss(). If the coin is fair it should be the case that if we toss the coin often, Heads and Tails should occur about equally often. The coin should not be weighted in favor of a particular value.\nOne way to learn whether our coin is fair are repetition and visualization. These are tools we will need all the time, when working with data and when doing probability. While R has many useful functions, one of the great powers is that R is constantly extended by a huge community of users by providing packages.\nPackages are add on functions, which will not be available when you install R. They need to be installed and loaded before you can use them. Since packages are such a powerful tool in R we need to introduce what they are and how to use them in the beginning.\n\n\n1.2.9 Packages\nThere are many visualization tools in R that come with the basic installation. Since the point we want to make here is about packages, we will use a visualization function which is part of the add on package ggplot2, a very popular package for making all kinds of graphs. ggplot2 is not part of the base-R distribution. It has to be therefore loaded before we can use it.\nThere are many additional functions provided with this package. Indeed you could run a whole course on just that 3\n3 A very good starting point is for instance Healy (2019), in case you are interested. Also a good source is the first chapter in Wickham and Grolemund (2017).Since we want to use functios now which are not in the base R distribution. we need to load the package which provides the functions we want. In this example we use the package ggplot2.\nWhen we use a package for the first time it needs to be installed. Once installed it does not have to be reinstalled when we use it at later coding sessions again.\nTo install a package you need to be connected to the internet. If you have internet connection go to the command line and run at the command line: install.packages(\"ggplot2\").\nR displays what is happening while executing the command. Don’t worry if you don not know what all of these messages exactly mean and don’t panic that they are displayed in red. All packages can be installed like this. You have just to enter the correct name in the function install.packages(). The lower right pane in the RStudio software alternatively provides a tab called Packages. Using this tab, you can also install R packages interactively by clicking the install button on the upper left corner of the Packages tab.\nAfter installation the package is on our hard-drive but it can not yet be used. To use the package it has to be loaded. This is done with the command library. To load the ggplot2 package we type\n\nlibrary(\"ggplot2\")\n\nand hit the return key. Many things could be said about the R package system and you will learn it in more detail as we go along in our course. For the moment the most important thing to remember is that a package needs to be newly loaded whenever you want to use it in a new R session.\nTo check whether our coin is fair, we need to toss it many times. R provides a function, that does this for us. This function is called replicate() and provides an easy way to repeat a command many times. The number of times we want to repeat something is given as an argument to the replicate function.\nNow lets roll our die 100 times and save the result in an object we call tosses:\n\nset.seed(123)\ntosses &lt;- replicate(100, toss_coin())\n\nThe function set.seed() I used here with the argument 123 is just a device to make this experiment reproduceable. If this function is not used, each new run of the 100 replications would show a different outcome.\nI now show you a code for visualizing this outcome. The code contains some concepts that we did not yet discuss. Don’t worry. We will learn them soon. The point here is to shwo you how we use the functions of the package to produce a visualization of the outcome of our 100 coin tosses.\n\n# Create a data frame for plotting\ntosses_df &lt;- data.frame(\n  Outcome = factor(tosses, levels = c(0, 1), labels = c(\"Tails\", \"Heads\"))\n)\n\n# Plot the results using ggplot2\nggplot(tosses_df, aes(x = Outcome)) +\n  geom_bar(fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Occurrences of Heads and Tails\",\n    x = \"Outcome\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt looks like every value occurred roughly 50 times but there is still quite some variation. Tail occured fewer times than Heads.\nMaybe we have to give it another trial with more replications. With the computer we can do this with a fingertip. Let us roll our die 1000 times and plot the result.\nHere is how we would code this by recycling the code used before.\n\nset.seed(123)\ntosses &lt;- replicate(1000, toss_coin())\n\n# Create a data frame for plotting\ntosses_df &lt;- data.frame(\n  Outcome = factor(tosses, levels = c(0, 1), labels = c(\"Tails\", \"Heads\"))\n)\n\n# Plot the results using ggplot2\nggplot(tosses_df, aes(x = Outcome)) +\n  geom_bar(fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Occurrences of Heads and Tails\",\n    x = \"Outcome\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNow it looks much better: Each face comes up approximately 500 times. We will learn in our course why the picture get better as we increase the number of rolls of the die. For the moment, let us just state that at least for this case it seems that there is no significant visual evidence that our virtual die is loaded.\nBut wait a minute. We have introduced the idea of classical probability by constructing equally probable cases and then count. Here we have repeated a random experiment over and over again to draw some conclusion on the underlying probabilities by visualizing the frequency polts of outcomes? These ideas are connected. We have just informally introduced a new idea in probability, the relative frequency notion of probability. How the two concepts are related and why this notion makes sense we will learn more formally in the next lecture. For the moment let’s just stick with the idea that practitioners have used at all times to assess probabilities by looking at frequencies of particular events occurring. When and why this kind of reasoning about probability works we learn in the next lecture.\n\n\n\n\n\n\nTip for R novices: How to leverage the LLM to enhance understanding\n\n\n\nLet me give you a hint here how you could use the LLM to enhance your learning experience in another way. The students usually taking this bootcamp class have very different backgrounds. Some have much experience with programming but perhaps not necessarily in R. Many of you will - for example - know Python, because most of the MQF is taught and run with Python. In such a case you could use the LLM to translate the R code into Python and then compare.\nOr assume you are impatient and want to jump ahead by having somebody explaining you the ggplot2 code we used for the plotting function. Ask the LLM to explain you the R code and what it does, and I assure you you will get a pretty good and well explained answer.\nTry it!\n\n\n\n\n1.2.10 Getting Help\nWe have now learned a tiny number of R functions and we have written one function ourselves. We have learned how to make use of functions provided by packages.\nIt would be overwhelming to memorize and learn them all. In R, fortunately, every function comes with a detailed documentation and with its own help page. You need to learn how to use this source right from the beginning.\nTo access the help page of a function you type the function name preceded by a question mark at the prompt, like this\n\n?sample\n\nThen, after you have pressed the return key, a help page is opened in the right lower pane under the help tab as shown in Figure 1.5\n\n\n\n\n\n\n\n\nFigure 1.5: The Help-pane in RStudio\n\n\n\n\n\nThis help page has a particular structure that you will find for every other R function no matter whether it is provided by the base installation or by a package. In the upper left corner you see the name of the function (sample) and in curly brackets next to it the term base, which means that this is a function in the R base installation. Then you see a headline about what the function does.\nFrom the top of the page, you then first see the field Description. This is a short description what the function does. Here it says\nDescription\nsample takes a sample of the specified size from the elements of x using either with or without replacement.\nThe next field is Usage. It gives you the function description with the arguments. Here for example\nUsage\nsample(x, size, replace = FALSE, prob = NULL)\n\nsample.int(n, size = n, replace = FALSE, prob = NULL,\n           useHash = (!replace && is.null(prob) && size &lt;= n/2 && n &gt; 1e7))\n           \nThe first line in Usage should by now be familiar. Don’t worry about the second line. The function can obviously do more than we know so far.\nNext comes a list of arguments the function takes and what type of information R expects you to provide, as well as what R will do with this information. Here it says for example\nArguments\nx   \neither a vector of one or more elements from which to choose, or a positive integer. See ‘Details.’\n\nn   \na positive number, the number of items to choose from. See ‘Details.’\n\nsize    \na non-negative integer giving the number of items to choose.\n\nreplace \nshould sampling be with replacement?\n\nprob    \na vector of probability weights for obtaining the elements of the vector being sampled.\n\n...\n\nWe omit some information here.\nThen comes a field called Details which gives a more in-depth description of the function. The next field is called Value. It describes what the function returns when you run it. Then we have a reference to related R functions under the field See. Finally there is a field called Examples. This field contains example code that is guaranteed to work. It shows a couple of different cases how you can use the function in practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#coincidences",
    "href": "01-lecture1.html#coincidences",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.3 Application: Coincidences and the Blockchain",
    "text": "1.3 Application: Coincidences and the Blockchain\n\n1.3.1 The birthday problem\nIn this section, we will apply our foundational knowledge of probability - which is at the moment just the informal and intuitive notion of counting equally probably cases - to a fascinating real-world problem known as the birthday problem:\nThe starting question in the birthday problem is: What is the probability that at least two people in a room share the same birthday, when we neglect things like leap years, and when we assume that birthdays on any day of the year are equiprobable and the birthdays of the people in the room are independent. We have no twins for example.\nPerhaps some of you have already seen this problem before. If not, the result may surprise you. Even if you saw the birthday problem before, perhaps not many of you are aware that the significance of this problem reaches far beyond recreational math puzzles. This is because the birthday story is of course accidential. The mathematical formulation of the problem will reveal through the process of abstraction that this problem is at the heart a problem about the probability of coincidences. As such it has real world connections to cryptography and the blockchain security.4\n4 We follow here the discussion in Diaconis and Skyrms (2019)Through this problem, we will:\n\nExplore how to frame probability questions mathematically.\nUse basic R functions to compute probabilities.\nLearn how to simulate experiments to approximate probabilities. This problem is not only an entertaining mathematical puzzle but also a stepping stone to understanding its applications in cryptography and blockchain security.\n\nFirst, observe that we took our assumptions such that we are in the frame of classical probability like the originators of the field thought about it.\nNow for the sake of familiarizing ourselves with the new concepts, let us try to map the birthday problem into the probability notions we learned so far.\nThe sample space is the set of possible outcomes of the experiment. The sample space for our problem includes all possible combinations of birthdays for the group. For example, if there are 3 people, each with a birthday on any of 365 days, the sample space contains all possible sequences of three birthdays:\n\\((1,1,1),(1,1,2),…,(365,365,365)\\)\nIn general, for \\(n\\) people, there are \\(365^n\\) possible combinations, as each person’s birthday is independent of the others.5\n5 Mathematically you can describe the sample space for the birthday problem as a set with \\(365^n\\) ordered \\(n\\)-tuples or sequences of birthdays: \\({\\cal S} = \\{x | x \\in 365^n \\}\\) This is the sample space of this experiment, written as in set theoretic notation. It means the sample space is the set of all ordered \\(n-tuples\\) from the Cartesian product \\(365^n\\). Remember that in mathematics, a Cartesian product is a mathematical operation that returns a set (or product set or simply product) from multiple sets (in this case the sample space, \\({\\cal S}\\). That is, for sets \\(A\\) and \\(B\\), the Cartesian product \\(A \\times B\\) is the set of all ordered pairs \\((a,b)\\) where \\(a \\in A\\) and \\(b \\in B\\).Now given this sample space we can assign a probability to the event that two people chosen at random will have the same birthday. The denominator will this be \\(365^n\\). The nominator for this probability will be much more straightforward to figure out, if we compute the complement. We discussed before that the probability of the complement of an event is 1 minus the probability of the event: \\(P(\\text{not} A) = 1 - P(A)\\).\nLet \\(A\\) denote the event that no two people share the same brithday. We can then use the complement rule to compute the probability of at least two people sharing a birthday: Compute the probability that no two people share a birthday. Subtract this probability from 1 to get the probability of at least one match.\nHere’s how we compute the probability of no matches:\n\nThe first person can have any birthday \\((365/365)\\).\nThe second person must avoid the first person’s birthday \\((364/365)\\).\nThe third person must avoid the first two birthdays \\((363/365)\\).\n\nNow continue this reasoning for \\(n\\) people.\nThe final probability is:\n\\[\\begin{equation*}\nP(\\text{at least two people share birthday}) =\n1 - \\frac{365 \\times 364 \\times 363 \\cdots \\times 365 - n +1}{365^n}\n\\end{equation*}\\]\nNow there is an additional thing we did not yet introduce and which we will learn about in the next lecture. This refers to our assumption that the individual birthdays are independent. For the moment you need to take the following fact on faith: The probability of independent events is the product of the probability these events.\nNow comes the surprise for those of you who did not already see the birthday problem. Assume the group size of people is 23. Let us compute the birthday coincidence probability. For this computation we make use of the R function prod() which takes a vector of numbers as input and computes their product. For \\(n=23\\) we need the list of numbers from 365 to 343. Now we can use the : operator and take advantage from one of its cool properties. If we formulate the command as 365:343 then the colon operator will give us a descending list of integers starting at 365 and ending at 343. Then the probability can be computed as\n\n1 - prod(365:343)/365^23\n\n[1] 0.5072972\n\n\nWe have used the operator ^ which is needed to compute powers. Now we see from our computation that with 23 people the probability is already larger than 50 %. Quite stunning. You can now verify yourself that with 50 people this probability is already at 97 %.\nLet’s visualize how the probability of at least one shared birthday grows with the size of the group. This time I will use base R’s plotting functionality and not use the ggplot2 package.6\n6 I used ggplot2 before to explain the concept of a package and how to load it to enhance R’s functionality. Base R containts a lot of powerful plotting functions which work independently of gglot2.The plot below shows that the probability exceeds 50% with just 23 people and quickly approaches 100% as the group size increases.\n\n# Define the birthday_collisions function\n\nbirthday_collisions &lt;- function(n) {\n  1 - prod(365:(365 - n + 1)) / 365^n\n}\n\n# Generate data for group sizes\n\ngroup_sizes &lt;- 1:50\ncollision_probs &lt;- sapply(group_sizes, birthday_collisions)\n\n# Plot the probability of at least one shared birthday\n\nplot(group_sizes, collision_probs, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Group Size\", ylab = \"Collision Probability\",\n     main = \"Probability of At Least One Shared Birthday\")\nabline(h = 0.5, col = \"red\", lty = 2)  # Highlight the 50% threshold\n\n\n\n\n\n\n\n\nI will not go in detail into the explanation of the code at this stage. For those of you who crave for an explanation, please look at the marginal note here or use the LLM for a step by step explanation.7\n7 birthday_collisions Function: This function calculates the probability of at least one shared birthday for a group of \\(n\\) people using the formula derived earlier. group_sizes and collision_probs: group_sizes defines a range of group sizes (1 to 50). sapply applies the birthday_collisions function to each group size, computing the collision probabilities. Visualization: A line plot (type = \"l\") is used to visualize how the collision probability increases with group size. A horizontal red dashed line (abline) highlights the 50% threshold for better visual context.\n\n1.3.2 Determining birthday collisions by simulation\nInstead of deriving the probability analytically, we can simulate the birthday experiment if we use the relative frequency notion of probability which we informally had used in our visualization of coin tosses. Here’s how:\n\nGenerate random birthdays for nn people using sample(1:365, n, replace = TRUE).\nCount how often at least two people share a birthday using the duplicated() functionin combination with the logical function any().\nRepeat the simulation many times (e.g., 10,000) using replicate() to estimate the probability.\n\n\nsim_birthday &lt;- function(n, trials = 10000) {\n  results &lt;- replicate(trials, {\n    birthdays &lt;- sample(1:365, n, replace = TRUE)\n    any(duplicated(birthdays))\n  })\n  mean(results)\n}\n\nLet me give a step-by-step breakdown of this piece of code.\n\nDefining the Function:\n\nThe function sim_birthday has two arguments\n\nn: The number of people in the room (group size).\ntrials: The number of simulation repetitions (default is 10,000). These parameters allow us to customize the simulation for different group sizes and levels of precision.\n\n\nUsing replicate to Run Simulations:\nThe replicate() function repeats an experiment a specified number of times (trials). Each experiment simulates one instance of \\(n\\) people in a room.\nSimulating Birthdays:\nThe sample() function generates \\(n\\) random birthdays (integers between 1 and 365) with replacement, representing the assumption that birthdays are independent and uniformly distributed. This step directly reflects the assumptions made in the analytical solution:\n\nThe sample space consists of 365 possible birthdays.\nSampling is done with replacement because multiple people can share the same birthday.\n\nChecking for Duplicates:\nThe duplicated() function identifies repeated birthdays in the sample. The any() function checks if there is at least one duplicate. If duplicates exist, this corresponds to a “birthday collision.” This step operationalizes the concept of finding the complement (at least one collision) rather than calculating the probability of all unique birthdays.\nCalculating the Probability:\nThe results vector contains TRUE (collision occurred) or FALSE (no collision) for each trial. Taking the mean() of this logical vector calculates the proportion of trials with a collision, which corresponds to the simulated probability. This step ties back to the definition of relative frequency probability introduced earlier: the probability of an event is the ratio of favorable outcomes to the total number of trials.\n\nThis step requires some explanation of how R uses data types. We will learn about this more systematically in the next lecture. Here I give a quick and superficial explanation. R can force logical data types automatically to a numerical type when it applies arithmetic functions to a logical vector, forcing TRUE to 1 and FALSE to 0. Now, when you take the average (mean()) of a vector containing only 1 and 0 values you will get the proportion of 1’s or TRUE values. We will use such tricks often during the course of the lecture. For the moment I ask you to bear with this quick and superficial explanation of why this way of proceeding does what we want.\nNote that this code uses a function within a function, which makes the code very efficient.8\n8 As always there are many different ways to write this code a bit differently. Those of you who are more experienced in programming you can challenge yourself by using for instance the following approach: Create a single matrix where each row represents one trial, and each column corresponds to a birthday for one person in that trial.Use the apply()function for Row-Wise Operations checking for duplicates. The result should be a logical vector indicating whether each trial has a duplicate and then take the mean of this vector.Let’s compute the birthday collision probability for 23 (as in our analytical solution) and 50 people:\n\nsim_birthday(23)\n\n[1] 0.5111\n\nsim_birthday(50)\n\n[1] 0.9718\n\n\n\nFor \\(n = 23\\), the output should be close to the analytically derived probability \\(\\approx 0.507\\).\nFor \\(n = 50\\), the output should approximate \\(\\approx 0.970\\).\n\nWhat are the advantages of simulations. Here are three main advantages:\n\nFlexibility: This method works even when the assumptions (e.g., uniformity or independence) are adjusted.\nVerification: We can confirm analytical results with experimental data.\nReal-World Application: Simulation methods are widely used in practice when exact formulas are unavailable or complex.\n\nBy understanding and applying this simulation, you now have a practical tool to explore probability problems beyond purely mathematical derivations.\nThe birthday problem has practical implications beyond recreational math. In cryptography, the same underlying principles are used to analyze the likelihood of hash collisions in blockchain systems and digital signatures. Let’s explore this connection further.\n\n\n1.3.3 Hash-functions and the blockchain\nThe birthday problem provides the foundation for studying coincidences in various contexts. A critical application is in cryptography, where the concept of hash functions relies on similar probabilistic principles to ensure data security.\nA hash function is a mathematical tool that converts an input (e.g., a text, file, or number) of any size into a fixed-length output, often called a “hash value” or “digest.” For example, the widely used hash function SHA-256 maps its input to a 256-bit string. You can think of this hash value as a digital fingerprint: any change to the input, no matter how small, results in a completely different hash.\nHash functions have two critical properties that make them essential in cryptography:\n\nOne-wayness: It is computationally unfeasible to determine the original input from the hash value. A hash function is easy to evaluate or compute but it is practically impossible to learn from the value the initial argument by computing the inverse.\nCollision resistance: It is highly unlikely for two different inputs to produce the same hash value.\n\nIf the range of the hash-function is \\(M\\) and the hash-function maps into a 256 bit string then there are \\(2^{256}\\) basic outcomes. Since the hash-function maps a large string onto a smaller string it is possible that there are two different strings \\(x \\neq y\\) mapping to the same value \\(\\text{hash}(x)=\\text{hash}(y)\\). This would be a problem for message authentication because it would give the same “fingerprint” for two different strings.\nFor a cryptographically secure hash function it is therefore required that the probability of such a collision should be small enough to exclude a collision in all practically relevant circumstances.\nThe collision problem for hash functions is analogous to the birthday problem. Instead of asking how many people are needed in a room for a shared birthday, we ask: how many inputs (e.g., messages or files) need to be hashed before two of them produce the same hash value? This probability depends on the size of the hash space, which is determined by the number of bits9 in the hash function’s output.\n9 A bit, short for binary digit, is defined as the most basic unit of data in telecommunications and computing. Each bit is represented by either a 1 or a 0Let \\(M = 2^{256}\\) the number of possible hash values. The probability of no collisions after \\(n\\) hashes is\n\\[\\begin{equation*}\nP_{nc}(n) = 1 \\times \\left(1 - \\frac{1}{M}\\right)\\times\n\\left(1 - \\frac{2}{M} \\right) \\times \\cdots \\times \\left(1 - \\frac{(n-1)}{M} \\right)\n\\end{equation*}\\]\nFor \\(n\\) large this probability can be approximated by a factorial expansion as \\[\\begin{equation*}\nP_{nc}(n) \\approx \\prod_{k=0}^{n-1} \\left( 1 - \\frac{k}{M} \\right)\n\\end{equation*}\\]\nNow convert the product into a sum using the natural logarithm: \\[\\begin{equation}\n\\ln P_{nc}(n) \\approx \\sum_{k=0}^{n -1} \\ln \\left( 1 - \\frac{k}{M} \\right)\n\\end{equation}\\]\nNow to do the estimation of the threshold we use an approximation, which works with a simple property of the logarithm: \\(\\log(1-x)\\approx -x\\) when \\(x\\) is small. Thus for \\(n\\) much smaller than \\(M\\), or \\(n \\ll M\\), we can write the no-collision probability as \\[\\begin{equation*}\n\\ln P_{nc}(n) \\approx - \\sum_{k=0}^{n-1} \\frac{k}{M}\n\\end{equation*}\\] Simplify the summation by usin the fact that \\(\\sum_{k=0}^{n-1} = \\frac{n(n-1)}{2}\\) we get \\[\\begin{equation*}\n\\ln P_{nc}(n) \\approx -\\frac{n(n-1)}{2\\, M}\n\\end{equation*}\\] By taking exponentials on both sides this gives us \\[\\begin{equation*}\nP_{nc}(n) \\approx \\exp \\left( - \\frac{n(n-1)}{2 \\, M} \\right)\n\\end{equation*}\\]\nThus the probability of a hash-collision as a function of \\(n\\) is then approximately\n\\[\\begin{equation*}\nP_{c}(n) = 1- P_{nc}(n) \\approx \\frac{n^2}{M}\n\\end{equation*}\\]\nLet’s try a visualization to show how \\(P_{c}(n)\\) increases with \\(n\\):\n\nn &lt;- 1:100\nM &lt;- 2^256\n\ncollision_prob &lt;- (n^2) / (2 * M)\n\nplot(n, collision_prob, type = \"l\", log = \"y\", col = \"blue\",\n     xlab = \"Number of Inputs (n)\", ylab = \"Collision Probability (log scale)\",\n     main = \"Hash Collision Probability (SHA-256)\")\nabline(h = 0.5, col = \"red\", lty = 2)  # 50% threshold\n\n\n\n\n\n\n\n\nThese results show that for SHA-256:\n\nTo have a \\(50\\)% chance of a collision, an attacker would need to compute approximately \\(2^{128}\\) hashes.\nAt \\(10000\\) hashes per second, this would take longer than the age of the universe.\n\nThis enormous computational difficulty ensures the security of modern cryptographic systems. Even with the combined computing power of all devices on Earth, finding a collision is practically impossible.\nThe birthday problem demonstrates how fundamental probability concepts can inform critical applications like cryptographic hash functions. By understanding the collision probabilities, we see why hash functions like SHA-256 are effective for ensuring data integrity and security.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#summary",
    "href": "01-lecture1.html#summary",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.4 Summary",
    "text": "1.4 Summary\nIn this lecture we have taken a first step towards some very basic probability notions and some basic steps in R. Isn’t it amazing how much territory we could cover with so few concepts? You have learned about how to think probabilistically about collision probabilities and how to solve for them analytically, by simulation and for large numbers by approximation.\nWe have convinced ourselves using this knowledge only and taking on faith that the probability of independent events is the product of their individual probabilities, that the cryptographic Hash-function SHA-256, while it can produce collisions in theory, practically the number of hashes to make such a collision occur would be so large that we can be confident that hashing bit strings with SHA-256 gives us a unique fingerprint practically with certainty.\nThese are the basic probability concepts we have covered in this lecture:\n\nA random experiment: A process leading to an uncertain outcome.\nSample space: The collection of all possible outcomes of a random experiment.\nBasic outcome: A possible outcome of a random experiment.\nEvent: An event is a subset of basic outcomes. Any event which contains a single outcome is called a simple event.\nClassical probability find or make equally probably cases and the count them. The probability of an event \\(A\\) is the number of cases when \\(A\\) occurs divided by the total number of cases.\nRelative frequency probability defined as the number of times an event \\(A\\) occurs in a repeated number of trials divided by the total number of trials in a random experiment.\n\nThese are the R concepts we have covered in this lecture:\n\nobjects arbitrary names that can store different values and data types.\nfunctions ab R object that can accept other R objects as arguments, operate on them and return a new object.\nscripts files that store sequences of R commands and can be saved, repopened and allow the execution of commands.\nusing packages\nfinding help\nthe functions sample and replicate\n\nThese are the Applications we have covered in the lecture:\n\nConstructing a coin on the computer and toss it an arbitrary number of times\nThe birthday problem and how to solve it analytically as well as through simulation\nExtrapolating the birthday problem to analyze cryptographic collision resistance of hash-functions by using ideas from probability",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#project-designing-transaction-identifyers-for-digital-payment-systems",
    "href": "01-lecture1.html#project-designing-transaction-identifyers-for-digital-payment-systems",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.5 Project: Designing transaction identifyers for digital payment systems",
    "text": "1.5 Project: Designing transaction identifyers for digital payment systems\n\n1.5.1 Introduction: Real-World Importance of Unique Identifiers in Finance\nIn modern financial systems, every transaction is assigned a unique identifier, allowing stakeholders to track, verify, and manage transaction details efficiently. These identifiers are critical for maintaining transparency, accountability, and security across high-volume transaction systems. However, as the volume of transactions grows, the risk of identifier collisions—where two different transactions are assigned the same identifier—increases.\nThis issue is analogous to the birthday problem: as the number of transactions increases, the likelihood of collisions rises due to the combinatorial nature of the problem. A collision could result in processing errors or even security vulnerabilities, such as fraudulent transactions or misattribution of funds.\nTo mitigate such risks, many financial systems employ hash functions, such as the widely used SHA-256, to map data to a large space of possible outputs. These hash functions, with their vast output range (e.g., \\(2^{256}\\)), make collisions extremely unlikely, even for billions of transactions. By understanding the principles behind collision probabilities, financial institutions can design systems that maintain transaction integrity even under high workloads.\nIn this project, we will explore this critical aspect of financial systems by simulating transaction identifiers and analyzing collision probabilities. This exercise demonstrates how theoretical probability concepts translate directly to practical challenges in computational finance.10\n10 For more information and details you might want to check this source: https://www.hostmerchantservices.com/articles/what-are-transaction-ids/?utm_source=chatgpt.comIn this project, you will:\n\nSimulate a system assigning transaction identifiers.\nInvestigate the probability of identifier collisions under different scenarios.\nDiscuss implications for system design and security.\n\n\n\n1.5.2 Simulate transaction identifiers\n\nWrite a function to simulate transaction identifiers, where each identifier is randomly chosen from a pool of size \\(M\\) (e.g. \\(M = 10^6\\) or \\(M = 10^9\\))\nUsing your function, simulate \\(n\\) transactions and check for duplicates using R’s duplicated() function.\n\n\n\n1.5.3 Compute collision probabilities\n\nSimulate \\(n\\) transactions multiple times and estimate the collision probability as the fraction of simulations with at least one collision.\nUse this function to compute the collision probability for \\(n = 10^3, n = 10^6, n = 10^9\\) and \\(M = 10^6, M = 10^9\\)\n\n\n\n1.5.4 Visualize the results\nPlot the collision probability as a function of \\(n\\) for different values of \\(M\\). We did not cover Rs plotting functions in the lecture so far. We only showed some examples which we did not explain. But we explained how you could research R’s help system and how you could ask an LLM for help. With any of these resources you should be able to come up with some reasonable visualization.\n\n\n1.5.5 Investigate the system design\n\nReflect on how the size of the pool \\(M\\) influences the collision risk.\nWhat happens when the transaction volume \\(n\\) increases significantly?\n\n\n\n1.5.6 Financial implications\n\nImagine a financial institution processing transactions with \\(M=10^6\\) identifiers.\nIf the daily transaction volume grows to \\(n=10^5\\), what is the risk of a collision?\nDiscuss potential consequences of a collision (e.g., failed transactions, fraud risks) and propose ways to mitigate these risks (e.g., increasing \\(M\\)).\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html",
    "href": "02-lecture2.html",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "",
    "text": "2.1 Terminology\nProbability is a mathematical theory that provides a rigorous framework for understanding uncertainty. It is particularly useful when analyzing real-world phenomena, such as random sampling in surveys or fluctuations in stock prices.\nLet us now formalize the foundational concepts of this theory:\nTo clarify this concept, we define uncertain outcomes in advance. We pin down the possible outcomes by agreeing on the outset what we want to consider as the possible outcomes. Take the simple example of considering whether the price of a stock is going to rise or fall at the next day. In a practical situations the outcome of a move in the stock price can be that it rises or falls but it could in principle also stay the same. Still when we think about the experiment of observing the stock price tomorrow in many applications in Finance we usually agree that rise and fall are the only possible outcomes of this experiment. When we want to consider the random experiment that includes the case that the price stays the same we need to agree upfront the the possible outcomes are rise, fall and unchanged.\nThis collection of all possible outcomes in probability theory is called the\nIn the example before when we look at rises and falls in the stock price we would have \\({\\cal S} = \\{rise, fall \\}\\)\nExample: For \\({\\cal S} = \\{\\text{rise},\\text{fall} \\}\\), the event \\(\\{\\text{rise}\\}\\) is a simple event, while the event \\(\\{\\text{rise, fall}\\}\\) includes all outcomes and corresponds to the entire sample space.\nIn the first lecture we learned about two approaches to measure probability. But the theory of probability actually does not depend on how we measure it precisely. In the theory of probability this measure is an abstract concept.\nThus when we talk about probability in a precise and meaningful sense we can only do so in relation to a given sample space or to a certain conceptual experiment.\nI have introduced here probabilities with relation to a discrete sample space. The sample spaces discussed in Lecture 1 include finite sets, which can be small or large, such as the extensive output space of the SHA-256 hash function.\nThere are also more complicated discrete sample spaces: Think of the random experiment of tossing a coin as often as necessary to see Heads for the first time. We can begin writing down the basic outcomes as: \\(E_1=H, E_2=TH, E_3 = TTH, E_4 = TTTH, ...\\). An event where Heads never appear, denoted \\(E_0\\), may also be considered. In this case, when the basic events can be arranged into a simple sequence. A sample space is called discrete if it contains only finitely many points, or infinitely many points which can be arranged into a simple sequence.\nNot all sample spaces are discrete. Except for the technical tools required there is no essential difference between the two cases. In our discussion of probability in this lecture we consider mostly discrete sample spaces, however we will also discuss some basic non-discrete sample spaces later in the lectures.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#terminology",
    "href": "02-lecture2.html#terminology",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "",
    "text": "Definition: Random Experiment:\n\n\n\nA process with a set of possible outcomes, where the specific outcome cannot be predicted with certainty beforehand.\n\n\n\n\n\n\n\n\n\n\nDefinition: Sample space\n\n\n\nThe collection of all possible outcomes of an experiment is called the sample space and is denoted as the set \\({\\cal S}\\)\n\n\n\n\n\n\n\n\n\nDefinition: Basic outcome, event, simple event\n\n\n\nA basic outcome is a single possible result of a random experiment. An event is a subset of the sample space, representing one or more outcomes. A simple event is an event containing exactly one basic outcome.\n\n\n\n\n\n\n\n\n\n\nDefinition: Probability\n\n\n\nFor a given (discrete) sample space \\({\\cal S}\\), Probability is a function that assigns a value to each event, representing its likelihood. The function has to fulfill three properties:\n\n\\(P({\\cal S}) = 1\\).\nFor any event \\(A \\in {\\cal S}\\), \\(0 \\leq P(A) \\leq 1\\). The probability of an event can never be negative or larger than 1.\nFor mutually exclusive events \\(A\\) and \\(B\\), \\(P(A \\cup B) \\leq P(A) + P(B)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#probability-theory-and-applications",
    "href": "02-lecture2.html#probability-theory-and-applications",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.2 Probability in theory and applications of probability",
    "text": "2.2 Probability in theory and applications of probability\nProbabilities are expressed as numbers between 0 and 1. As mentioned by Feller (1968) in his famous probability textbook, these numbers are of the same nature as distances in geometry. In the theory we assume they are given to us.\nFrom the viewpoint of probability theory, we need not assume anything about how they are measured. In this sense probabilities in the theory of probability are an abstract measure of uncertainty.\nIn practical applications, determining probabilities or applying theory often requires sophisticated statistical methods. So, while the mathematical as well as the intuitive meaning of probability are clear only as we proceed with the theory we will get a better ability to see how we can apply this concept.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#probability-and-the-language-of-sets",
    "href": "02-lecture2.html#probability-and-the-language-of-sets",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.3 Probability and the language of sets",
    "text": "2.3 Probability and the language of sets\nProbability theory relies on the language of sets to describe relationships between events. Understanding key set operations is essential for working with probabilities effectively.\nLet’s go through them and illustrate the concepts in the context of the examples we have already developed in lecture 1.\n\n\n\n\n\n\nDefinition: Set Union\n\n\n\nThe union of two events \\(A\\) and \\(B\\) represents all outcomes that belong to \\(A\\), \\(B\\) or in both. It is written \\(A \\cup B\\).\n\n\nLet us use the example of the experiment of rolling a die. The sample space \\({\\cal S}\\) is the set of all possible outcomes of rolling the dice \\({\\cal S}=\\{1,2,3,4,5,6\\}\\). Assume one event is that the outcome is 1, 2 or 3. In set notation, we would write \\(A = \\{1,2,3\\}\\). Let us also assume that the second event is that the outcome is some even number, i.e. 2, 4, 6. Again using set notation we would write \\(B = \\{2,4,6\\}\\). The event \\(A \\cup B\\) is then the set of all outcomes such that the outcome is in \\(A\\) or in \\(B\\) or in both or these sets.\nYou can implement set operations in R because R provides functions for computing set operations. Let us use the occasion to show you briefly how to use these functions in the context of this example: We define the sets \\(A\\) and \\(B\\) first using the assignment operator:\n\nA &lt;- c(1,2,3)\nB &lt;- c(2,4,6)\n\nWe compute the union by using the function union()\n\nunion(A,B)\n\n[1] 1 2 3 4 6\n\n\nwhich gives us the union of both sets.\nTo understand this operation better we can visualize the set union in this example by looking at Figure 2.1\n\n\n\n\n\n\n\n\nFigure 2.1: The meaning of set union\n\n\n\n\n\nThe sample space \\({\\cal S}\\) is the gray set containing all possible outcomes of our random experiment. Graphically the union of \\(A\\) and \\(B\\), \\(A \\cup B\\) is a subset of the sample space, the entire colored area.\n\n\n\n\n\n\nIntersection\n\n\n\nThe intersection of two events are all outcomes that are both in \\(A\\) and in \\(B\\). It is written as \\(A \\cap B\\).\n\n\nIn R we would implement this operation by using the function intersect() and apply it to our sets \\(A\\) and \\(B\\) we have defined before.\n\nintersect(A,B)\n\n[1] 2\n\n\nFigure 2.2 visualizes this operation graphically\n\n\n\n\n\n\n\n\nFigure 2.2: The meaning of set intersection\n\n\n\n\n\nThe intersection of \\(A\\) and \\(B\\), \\(A \\cap B\\) is the orange area containing the dice face with two points. Indeed two is both in \\(A\\) and in \\(B\\), which is exactly the meaning of set intersection.\n\n\n\n\n\n\nComplement\n\n\n\nThe complement of an event \\(A\\) within the sample space \\({\\cal S}\\) is the set of all outcomes that are in \\({\\cal S}\\) but not in \\(A\\). It is written as \\({\\cal S} \\setminus A\\)\n\n\nLets say we want to get the complement, or the set difference of \\(A \\cup B\\) with respect to the sample space \\({\\cal S}\\).\nThe R implementation of the set difference operation is the function setdiff(). This is how we would tell R to compute the set difference of the union of \\(A\\) and \\(B\\) and the sample space \\({\\cal S}\\) in our example of the die:\n\nS &lt;- c(1,2,3,4,5,6)\n\nsetdiff(S, union(A,B))\n\n[1] 5\n\n\nThis can again be visualized in Figure 2.3\n\n\n\n\n\n\n\n\nFigure 2.3: The meaning of complement\n\n\n\n\n\nThis complement is the dice shown in the light redish area, i.e. all the elements of \\({\\cal S}\\) which are not in \\(A \\cup B\\).\n\n\n\n\n\n\nMutually Exclusive\n\n\n\nTwo events \\(A\\) and \\(B\\) are mutually exclusive if they can not occur simultaneously. This means \\(A \\cap B = \\emptyset\\), their intersection is empty.\n\n\nAn example in our context is the set of even outcomes \\(B=\\{2,4,6\\}\\) and the set of odd outcomes, let us call it \\(C=\\{1,3,5\\}\\). If we intersect these sets\n\nB &lt;- c(2,4,6)\nC &lt;- c(1,3,5)\n\nintersect(B,C)\n\nnumeric(0)\n\n\nwe get the empty set, which is expressed by R by giving the data type, in this case numeric, because we are intersecting sets of numeric values, followed by (0). This means, there is no numeric value in the intersection of \\(B\\) and \\(C\\).\nLet us discuss set operations a bit further by thinking about the probability of the union of two events \\(A\\) and \\(B\\) within the context of our visual examples. Remember that we had \\(A = \\{1,2,3 \\}\\) and \\(B = \\{ 2,4,6\\}\\). Look at Figure 2.4\n\n\n\n\n\n\n\n\nFigure 2.4: The meaning of set union\n\n\n\n\n\nWe would like to know what is the probability of \\(P(A \\cup B)\\)? Now if we add up \\(P(A)\\) and \\(P(B)\\) we would assign a probability to the outcome \\(2\\) twice. Such double counting must be avoided and thus we have to subtract \\(P(A \\cap B)\\) the intersection, so that: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\). Now you see what the qualification mutually exclusive does in our probability rule 3. When \\(A\\) and \\(B\\) are mutually exclusive \\(A \\cap B = \\emptyset\\) and in this case \\(P(A \\cup B) = P(A) + P(B)\\). We can add up the probabilities because when events are mutually exclusive we can not double count events by adding up individual probabilities.\n\n\n\n\n\n\nNow you try\n\n\n\nIf you drop the qualification that \\(A\\) and \\(B\\) must be mutually exclusive in point 3 in the definition of probability. How would rule 3 have to be changed?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#using-an-llm-to-deepen-your-understanding-of-set-theory-in-probability",
    "href": "02-lecture2.html#using-an-llm-to-deepen-your-understanding-of-set-theory-in-probability",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.4 Using an LLM to Deepen Your Understanding of Set Theory in Probability",
    "text": "2.4 Using an LLM to Deepen Your Understanding of Set Theory in Probability\nAn LLM like ChatGPT can be a helpful tool to explore concepts and solidify your understanding. Here are some example of what you could do:\n\n2.4.1 Ask for Clarifications\nIf a definition or concept isn’t clear, ask the LLM to explain it in simpler terms or using different examples. For instance:\n\n\n\n\n\n\nPrompt\n\n\n\n“What is the difference between the union and intersection of sets in probability? Can you give examples?”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“Can you compare this to a real-life scenario, like rolling a die or flipping a coin?”\n\n\n\n\n2.4.2 Generate Additional Examples\nUse the LLM to create new examples similar to the ones in the lecture. This will give you more practice applying the concepts.\n\n\n\n\n\n\nPrompt\n\n\n\n“Give me an example of mutually exclusive events involving sports outcomes.”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Can you show a sample space and events for tossing two coins?”\n\n\n\n\n2.4.3 Simulate Visualizations and Code Interpretation\nWhile the LLM doesn’t directly produce visuals, you can ask it to describe how a diagram or R output would look. This helps connect theoretical concepts to their graphical representations. For example:\n\n\n\n\n\n\nPrompt\n\n\n\n“Describe what a Venn diagram looks like for \\(A \\cup B\\), \\(A \\cap B\\), and \\(A \\setminus B\\).”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“What does the R function union(A, B) compute? How is it related to \\(A \\cup B\\)?”\n\n\n\n\n2.4.4 Practice Applying Definitions\nUse the LLM to test your understanding by quizzing yourself.\n\n\n\n\n\n\nPrompt\n\n\n\n“Ask me questions about the definitions of sample spaces, union, intersection, and complement.”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Give me a scenario and ask which set operation applies.”\n\n\n\n\n2.4.5 Explore Real-World Applications\nUse the LLM to explore how these concepts apply in real-world contexts beyond the lecture.\n\n\n\n\n\n\nPrompt\n\n\n\n“How is the concept of set intersection used in data science or finance?”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Explain how mutually exclusive events are important in designing experiments.”\n\n\n\n\n2.4.6 Learn R Through Step-by-Step Guidance\nIf you’re new to R, ask the LLM to guide you through using functions like union(), intersect(), and setdiff() step by step.\n\n\n\n\n\n\nPrompt\n\n\n\n“Explain how to use setdiff() in R with an example involving dice rolls.”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“How does this output relate to the complement of a set?”\n\n\n\nDive Deeper into Probability Rule 3:\n\nUse the LLM to generate explanations and examples that clarify how probabilities relate to set operations.\n\n\n\n\n\n\nFollow up\n\n\n\n“Explain why \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“Can you provide a numerical example to illustrate this rule?”\n\n\n\n\n2.4.7 Simulate Discussions\nAsk the LLM to take the role of a peer or instructor to simulate a conversation about the material.\n\n\n\n\n\n\nPrompt\n\n\n\n“Pretend you are my study partner. Let’s discuss the complement of events and its significance in probability.”\n\n\nBy actively engaging with the LLM through these kinds of prompts, you can practice, explore, and deepen your understanding of the material beyond the lecture. Try it alone or with your group.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#frequency",
    "href": "02-lecture2.html#frequency",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.5 Probability and Frequency",
    "text": "2.5 Probability and Frequency\nNow it is time to explain a connection we made so far informally when we looked at visualizations and simulations, where we used connection between relative frequencies of probabilities in repetions of a random experiment.\nThe frequency interpretation of probability is a practical approach to understanding uncertainty. It defines the probability of an event \\(A\\) as:\n\\[\\begin{equation*}\nP(A) = \\frac{\\text{Number of times $A$ occurs in repeated identical trials}}{\\text{Total number of trials}}\n\\end{equation*}\\]\nThis interpretation is intuitive and often used in fields like engineering, finance, and natural sciences. However, it raises important questions about the connection between observed frequencies and theoretical probabilities.\nThe origins of this discussion reach back to the seventeenth century. The philosophers Gottfried Wilhelm Leibnitz (1646 - 1716) and Jacob Bernoulli (1655 - 1705) had great hopes for the new field of probability to find applications in fields like medicine, law, commerce and finance. This interest in exploring new fields of potential applications drove them to study frequency evidence of events. They felt that relying on intuitively equally probable cases might not be enough for these ambitious application attempts.\nJacob Bernoulli gave an answer which is among the great ideas in probability theory (see Diaconis and Skyrms (2019)), the weak law of large numbers. It establishes one of the most important connections between frequency and probability.\n\n2.5.1 The Weak Law of Large Numbers (WLLN)\nThe Weak Law of Large Numbers provides a rigorous mathematical foundation for the frequency interpretation of probability. It states:\n\n\n\n\n\n\nWeak Law of Large Numbers\n\n\n\nAs the number of independent and identically distributed (i.i.d.) trials increases, the relative frequency of an event converges to its true probability with high probability.\n\n\nNow let us pause here and restate what the weak law says: The law says that\n\nOver many trials, the observed frequencies of outcomes will get closer to their theoretical probabilities.\nThis convergence occurs with high likelihood as the number of trials increases.\n\nObserve also what the weak law does not say:\n\nFrequencies are not probabilities. Instead frequencies approximate probabilities as trials increase.\nThe weak law does not guarantee exact convergence in finite samples - it describes long run outcomes of repeating identical experiments many times.\n\nLet’s go back to the exmple of the fair coin we played with in lecture 1: There we constructed a fair coin toss where \\(P(\\text{Heads})=0.5\\). When we have just a few tosses, say 10 times, you might observe 6 Heads (60%), which is close to but not exactly 50%. As you increase the tosses, say to 100, the frequency might be 52 Heads (52%), closer to 50%. if you go to even more tosses, say 10,000 tosses, the frequency approaches 50%.\nThis demonstrates that, as the number of trials increases, the relative frequency converges to the theoretical probability.\nIt is crucial to understand that Bernoulli’s Law of Large Numbers does not assert that frequencies are probabilities. Rather, it describes how, under certain conditions, frequencies fall within specific bounds relative to probabilities. Specifically, given a theoretical probability, a desired margin of error, and a confidence level, Bernoulli’s result provides an upper bound on the number of trials required for the relative frequency of an event to approximate its true probability.\nThis distinction highlights an important limitation: the Weak Law of Large Numbers addresses the problem of inference from probabilities to frequencies, not the reverse. That is, it shows how probabilities can predict the behavior of frequencies in repeated trials, but it does not justify using observed frequencies as definitive probabilities. Instead, the theorem ensures that as the number of trials grows, the relative frequency of an event will converge to its probability, within specified bounds.\nAlthough we will frequently use frequency-based measures of probability in practical applications, it is essential to recognize the conceptual gap between the idealized mathematical theory and the empirical realities it seeks to describe. Probability theory relies on the concept of limiting relative frequencies, which exist only in an idealized framework of infinite trials. Real-world applications, however, involve finite data and inherently require interpretation, judgment, and domain expertise.\nThis distinction between theory and practice is more than a technical nuance—it addresses a fundamental challenge for any theory with practical ambitions: How does the idealized framework relate to the messy complexities of reality? In probability, as in other fields, there is no direct or naive application of theory to practice. Bridging this gap demands a sound understanding of the theory, along with careful consideration of real-world conditions and context.1\n1 For a formal statement of the weak law of large numbers, we need more concepts which we have not yet introduced, in particular the notion of a random variable. Let me give the formal statement here for those of you who are interested and know the concept of a random variable already: Let \\(X_1, X_2, \\dots, X_n\\) be \\(n\\) independently, identically distributed random variables with expected value \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\bar{X}_n\\) denote the sample mean. Then the weak law of large numbers states: \\(P(|\\bar{X}_n - \\mu | \\geq \\epsilon) \\to 0 \\quad \\text{as } n \\to \\infty\\). This means that the probability of the sample mean deviating significantly from the expected value diminishes as the number of trials increases.\n\n2.5.2 Exploring the Weak Law of Large Numbers with R\nIn the following code chunk we first define a coin as we did in lecture 1 with the understanding that 1 represents Heads and 0 represents Tails. Then we use the replicate function and the sample function to toss the coin 1000 times.\nNow in the next step we use the logical condition results == 1 to create a logical vector indicating whether each toss resulted in Heads. cumsum()then calculates the cumulative count of Heads after each toss. Dividing this cumulative count by (1:n) (the toss number) gives the cumulative relative frequency of Heads at each step.\nThen we plot these frequencies againts the number of tosses. The \\(x\\)-axis represents the number of tosses, while the \\(y\\)-axis represents the relative frequency of heads. We draw a red horizontal line where the relative frequency is identical to the theoretical probability of \\(0.5\\).\n\n# Define the coin\ncoin &lt;- c(1, 0)\n\n# Toss the coin n times\nn &lt;- 1000\nresults &lt;- replicate(n, sample(coin, size = 1))\n\n# Calculate cumulative frequency of Heads\n\nheads_freq &lt;- cumsum(results == 1) / (1:n)\n\n# Plot the convergence\nplot(1:n, heads_freq, type = \"l\", ylim = c(0.4, 0.6),\n     xlab = \"Number of Tosses\", ylab = \"Frequency of Heads\",\n     main = \"Convergence of Relative Frequency to True Probability\")\nabline(h = 0.5, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nThis plot illustrates how the relative frequency of Heads approaches \\(P(\\text{Heads})=0.5\\) as the number of tosses increases.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#independence",
    "href": "02-lecture2.html#independence",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.6 Independence",
    "text": "2.6 Independence\nThe idealized thought experiment behind the Weak Law of Large Numbers assumes the idea of independence. Let us define this concept more precisely and explain its implications. While we have already used independence implicitly in earlier examples, it is important to formalize and understand it carefully.\n\n2.6.1 Intuition Behind Independence\nTwo events are said to be independent if the occurrence of one event does not influence the probability of the other event occurring. In other words, knowing that one event has occurred provides no information about the likelihood of the other event.\nFor example, consider rolling a fair six-sided die twice. The outcome of the first roll does not affect the outcome of the second roll because the rolls are independent. The probability of any number appearing on the second roll remains \\(1/6\\), regardless of what happened on the first roll.\n\n\n2.6.2 A Worked Example\nLet us calculate the probability of rolling a 5 on the first roll and a 6 on the second roll.\nThis is the probability of the event “5 on the first roll and 6 on the second roll,” which we write as \\(P(5 \\cap 6)\\).\nSince the rolls are independent: \\(P(5 \\cap 6) = P(5) \\times P(6)\\).\n\nThe probability of rolling a 5 on a fair six-sided die is \\(P(5) = 1/6\\).\nThe probability of rolling a 6 on a fair six-sided die is \\(P(6) = 1/6\\).\n\nThus: \\(P(5 \\cap 6) = \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{1}{36}\\).\nThis calculation uses the multiplication rule for independent events, which states that if two events \\(A\\) and \\(B\\) are independent, then the probability of both occurring is the product of their individual probabilities.\n\n\n2.6.3 Definition of Independence\nWe now formalize this concept:\n\n\n\n\n\n\nIndependence\n\n\n\nTwo events \\(A\\) and \\(B\\) are independent if and only if: \\(P(A \\cap B) = P(A) \\times P(B)\\).\n\n\nThis definition formalizes the idea that the occurrence of one event does not affect the likelihood of the other. Independence is an assumption we often make in theoretical models like the Weak Law of Large Numbers and many other probabilistic frameworks.\nWhile independence allows us to use the multiplication rule \\(P(A \\cap B) = P(A) \\times P(B)\\), it is critical to remember that the reverse is not true:\n- Just because you can multiply probabilities does not necessarily mean the events are independent.\n- For independence to hold, the probabilities of the events must truly be unaffected by one another.\nIndependence is a key assumption in the Weak Law of Large Numbers, where we consider a series of independent and identically distributed (i.i.d.) trials. Without independence, the law’s guarantees about the behavior of frequencies would not hold. As we move forward, we will see independence as a foundational concept in many probabilistic models and calculations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#moreR",
    "href": "02-lecture2.html#moreR",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.7 Some more concepts from R: Reading Data, R Objects, Subsetting and Modifying Values",
    "text": "2.7 Some more concepts from R: Reading Data, R Objects, Subsetting and Modifying Values\nLet us try out and apply some of the new ideas we just have learned and use the opportunity to learn some more important R concepts in the context of an example with stock price data.\n\n2.7.1 Reading data in R\nIn financial analysis, the first step is often acquiring and loading data. This could be historical stock prices, economic indicators, or portfolio metrics. In this section, we’ll learn how to load a prepared dataset of Apple stock prices into R for analysis.\nWhy learn to load data? Loading data is the foundation for everything else we’ll do in this course. Whether you’re calculating probabilities or visualizing stock trends, understanding how to access and manipulate data is essential.\n\n2.7.1.1 Reading csv files\nFor this example, I’ve prepared a dataset recording daily stock price information for Apple in a CSV file (aapl_prices.csv). This file contains the opening, high, low, and closing prices, trading volume, and adjusted closing prices2 for each day starting in January 1990.\n2 The adjusted closing price of a stock is a modified version of the closing price that reflects the effects of any corporate actions, such as stock splits, dividends, and rights offerings. It provides a more accurate representation of the stock’s value over time, especially when analyzing historical data or calculating investment returns.To load this file into R, we use the read.csv() function:\n\naapl_prices &lt;- read.csv(\"data/aapl_prices.csv\")\n\nHere, \"data/aapl_prices.csv\" specifies the path to the file relative to the working directory.3\n3 If you work with European-style decimals , instead of ., you might need read.csv2(). For now, we’ll stick to the default U.S. format.4 A great way to explore these options is to consult the R Data Import/Export Manual or experiment with sample datasets from various sources like Kaggle or the tidyverse’s guide.R is highly versatile when it comes to handling data formats. Beyond CSV files, R can read and write Excel files, JSON, XML, databases, and more. While many formats can be accessed using built-in functions with a syntax similar to read.csv(), others may require loading additional packages. For instance, the readxl package is excellent for Excel files, and the jsonlite package is ideal for JSON data.4\nTo check your current working directory, use the getwd() function:\n\ngetwd()\n\n[1] \"/home/martinsummer/Code/R/Probability_Introduction\"\n\n\nIf the file is not in the expected location, ensure the path is correct or adjust it as needed.\n\n\n2.7.1.2 Inspecting the data\nOnce the data is loaded, it’s important to inspect it. This helps you understand its structure and content. For example, we can use the head() function to view the first 10 rows:\n\nhead(aapl_prices, n = 10)\n\n   symbol       date     open     high      low    close    volume  adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2    AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3    AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4    AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6    AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n7    AAPL 1990-01-10 0.335938 0.335938 0.319196 0.321429 199718400 0.2533336\n8    AAPL 1990-01-11 0.323661 0.323661 0.308036 0.308036 211052800 0.2427781\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.2427781\n10   AAPL 1990-01-15 0.308036 0.319196 0.305804 0.305804 161739200 0.2410190\n\n\nThis reveals a table with columns such as the opening price, highest and lowest prices, and adjusted closing price for each day. While we won’t dive into the exact meaning of these financial terms just yet, this dataset will serve as the foundation for our analysis.\n\n\n2.7.1.3 Common pitfalls and tips\n\nIf you see an error like cannot open the connection, it likely means R can’t find the file. Use getwd() to confirm your working directory and ensure the file is in the specified location.\nExplore the dataset further! Try summary(aapl_prices) to get an overview of each variable or tail(aapl_prices, n = 10) to see the most recent rows.\n\nNow that we’ve successfully loaded and inspected the dataset, we’re ready to dive into analyzing it.\n\n\n\n2.7.2 R Objects and Stock Price Movements\nIn R, most data structures are built from atomic vectors, which are the simplest type of R objects. Atomic vectors store one-dimensional data, where all elements must be of the same type. Our stock price dataset contains examples of more complex structures made up of atomic vectors. Let’s explore atomic vectors using simplified stock price data to better understand how they work.\n\n2.7.2.1 Atomic Vectors in Stock Price Analysis\nTo introduce atomic vectors, consider a basic example of daily stock price movements. Suppose the price of a stock can change in one of three ways:\n\nDecrease by 1 (-1)\nStay the same (0)\nIncrease by 1 (+1)\n\nWe can represent these possible changes in price using an atomic vector:\n\nprice_changes &lt;- c(-1, 0, 1)\n\nAtomic vectors are one-dimensional and store data of a single type. In this case, price_changes is a numeric vector representing the possible daily price changes.\nYou can verify whether an object is an atomic vector using the is.vector() function:\n\nis.vector(price_changes)\n\n[1] TRUE\n\n\nThis will return TRUE since price_changes is indeed an atomic vector.\n\n\n2.7.2.2 Properties of Atomic Vectors\nEach atomic vector has several properties. For example:\n\nLength: The number of elements in the vector can be checked using length():\n\n\nlength(price_changes)\n\n[1] 3\n\n\nThis will return 3, as there are three possible price changes.\n\nData Type: Every atomic vector must have a single data type. You can determine the type using the typeof() function:\n\n\ntypeof(price_changes)\n\n[1] \"double\"\n\n\nIn this case, the type is double because the vector contains numeric data.\n\n\n2.7.2.3 Using Vectors in Simulations\nVectors are powerful tools for simulations. For example, we can simulate a week of stock price movements using the sample() function, which randomly selects elements from the vector:\n\nweek_movements &lt;- sample(price_changes, size = 7, replace = TRUE)\nweek_movements\n\n[1]  1  0  1  0 -1  1  1\n\n\nHere, size = 7 simulates 7 days of movements, and replace = TRUE allows values to be selected more than once.\n\n\n2.7.2.4 Connection to Probability\nThis simulation ties into the concept of probability. Each price change (-1, 0, 1) can be treated as a basic outcome in a sample space defined by \\({\\cal S} = \\{-1,0,1\\}\\). By specifying probabilities, we can simulate scenarios where some outcomes are more likely than others. For example:\n\nweek_movements_weighted &lt;- sample(price_changes, \n                                  size = 7, \n                                  replace = TRUE, \n                                  prob = c(0.3, 0.4, 0.3))\nweek_movements_weighted\n\n[1]  1  1 -1  1  1 -1  0\n\n\nHere, the probabilities 0.3, 0.4, and 0.3 represent the likelihood of a decrease, no change, or an increase in price, respectively.\n\n\n2.7.2.5 More About Atomic Vectors\nR supports six basic types of atomic vectors:\n\nDouble: Numeric data with decimal precision (e.g., stock prices).\nInteger: Whole numbers (e.g., the number of shares traded).\nCharacter: Text strings (e.g., stock symbols like \"AAPL\").\nLogical: Boolean values (TRUE or FALSE, often used in comparisons).\nComplex: Numbers with imaginary components (not used in this course).\nRaw: Binary data (not used in this course).\n\nWe will primarily work with doubles, integers, characters, and logicals.We will primarily work with doubles, integers, characters, and logicals.5\n5 Why should we care for distinguishing integers from doubles? This has to do with how a computer performs computations. Sometimes, differences in precision can lead to surprising effects. In your computer, 64 bits of memory are allocated for each double in an R program. While this allows for precise representation of numbers, not all numbers can be exactly represented with 64 bits. For example, \\(\\pi\\) has an infinite sequence of digits and must therefore be rounded by the computer. Usually, rounding errors introduced into computations go unnoticed, but sometimes they become apparent. For instance, the expression \\(\\sqrt{2^2} - 2\\) results in a small floating-point error because the square root of 2 cannot be expressed precisely. These are called floating-point errors. While integers avoid floating-point errors, they are often impractical for many applications. Luckily, floating-point arithmetic is sufficiently precise for most use cases.For example, the stock price dataset contains variables of different types: - Prices (e.g., opening and closing prices) are stored as doubles. - Ticker symbols are stored as characters. - Logical vectors can be created by applying conditions to the data. For instance, finding days when the closing price was higher than the opening price:\n\naapl_prices$up_day &lt;- aapl_prices$close &gt; aapl_prices$open\nhead(aapl_prices$up_day, n = 10)\n\n [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n\n2.7.2.6 Attributes of Atomic Vectors\nAtomic vectors can have attributes, such as:\n\nNames: Metadata to describe each element.\nDimensions: Turning a vector into a matrix or array.\n\nFor example, we can assign names to the elements of price_changes:\n\nnames(price_changes) &lt;- c(\"Decrease\", \"No Change\", \"Increase\")\nprice_changes\n\n Decrease No Change  Increase \n       -1         0         1 \n\n\nThe names don’t affect the values but provide context for analysis. Here you see that the names are entered into R as a character vector stores, strings of text, which have to be put between quotation marks \"\". Strings are the individual elements of a character vector. We could use character vectors for instance in building our virtual coin earlier in the lecture in a more human readable form by using coin &lt;- c(\"H\", \"T\") isntead of coin &lt;- c(1,0).\nNote that a string can be more than just letters. If you type, for instance the number 1 with quotation marks, like \"1\" R would interpret the value as a string not as a number. Sometimes one can get confused in R because both objects and characters appear as text in R code. Object names are without quotation marks strings always are between quotation marks.\nWe can also reshape the vector into a matrix, which introduces dimensions:\n\ndim(price_changes) &lt;- c(3, 1)\nprice_changes\n\n     [,1]\n[1,]   -1\n[2,]    0\n[3,]    1\n\n\nThis transforms the vector into a matrix with 3 rows and 1 column. With the attribute system R allows you to represent more data types. R uses, for example a special class to represent dates and times. The data variable in our stock data is - for example - represents as a type of this kind.\nTo illustrate this we take the R-function Sys.time(). This function returns the current time on your computer. It looks like a character string when you display it but it is actually a double with class POSIXct, POSIXt (it has two classes):\n\nnow &lt;- Sys.time()\nnow\n\n[1] \"2025-01-23 22:38:43 CET\"\n\ntypeof(now)\n\n[1] \"double\"\n\nclass(now)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nPOSIXct is a widely used framework for representing dates and times. But we will skip the details here.\n\n\n\n2.7.3 Extending R Objects: Factors, Data Frames, and Lists\nIn the previous section, we explored atomic vectors, the foundation of many R objects. However, real-world data often require more sophisticated structures. Let’s extend our understanding by examining factors, data frames, and lists, using examples from stock price analysis.\n\n2.7.3.1 Factors in Stock Price Analysis\nFactors are used to store categorical data, which can have a fixed set of possible values (called levels). In stock market data, factors are useful for categorizing data, such as trading days or stock types.\nFor example, suppose we want to represent the days of the working week in our dataset:\n\ndays &lt;- factor(c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\ndays\n\n[1] Monday    Tuesday   Wednesday Thursday  Friday   \nLevels: Friday Monday Thursday Tuesday Wednesday\n\n\nThis factor has unique levels (\"Monday\", \"Tuesday\", etc.) that categorize the data. Factors can also be ordered, which is useful when the order matters:\n\nordered_days &lt;- factor(days, levels = c(\"Monday\", \"Tuesday\", \n                                        \"Wednesday\", \"Thursday\", \n                                        \"Friday\"), ordered = TRUE)\n\nordered_days\n\n[1] Monday    Tuesday   Wednesday Thursday  Friday   \nLevels: Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; Friday\n\n\nFactors are internally stored as integers but behave like characters when displayed. You can inspect their attributes to see how they’re stored:\n\nattributes(ordered_days)\n\n$levels\n[1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n\n$class\n[1] \"ordered\" \"factor\" \n\n\nFactors are powerful not only for categorizing data but also for performing analyses based on these categories. Let’s analyze how often Apple’s stock price decreased on Mondays in the given dataset.\nSuppose we take our original data about the Apple stock price and remove the ùp_day`column, we had creatde before:\n\n# Preview of the dataset\naapl_prices$up_day &lt;- NULL\nhead(aapl_prices)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6   AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n\n\nTo identify days when the price decreased, we can compute the daily price change and create a factor to categorize the changes (\"up\", \"down\", \"unchanged\"). Then, we can use factors to tabulate occurrences for each day of the week.\nWe start by adding week days and price change factors:\n\n# Convert date to Date type and extract weekdays\naapl_prices$date &lt;- as.Date(aapl_prices$date)\naapl_prices$weekday &lt;- factor(weekdays(aapl_prices$date), \n                             levels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\n# Compute daily price change and categorize as up, down, or unchanged\n\n# Create a lookup table as a named character vector\nlookup_table &lt;- c(\n  \"up\" = \"up\",\n  \"down\" = \"down\",\n  \"unchanged\" = \"unchanged\"\n)\n\n# Calculate price differences\nprice_diff &lt;- aapl_prices$close - aapl_prices$open\n\n# Create a character vector based on conditions\nprice_change &lt;- rep(NA, length(price_diff))\nprice_change[price_diff &gt; 0] &lt;- lookup_table[\"up\"]\nprice_change[price_diff &lt; 0] &lt;- lookup_table[\"down\"]\nprice_change[price_diff == 0] &lt;- lookup_table[\"unchanged\"]\n\n# Convert the result to a factor\naapl_prices$price_change &lt;- factor(price_change, levels = names(lookup_table))\n\n# View the updated data\nhead(aapl_prices)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6   AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n    weekday price_change\n1   Tuesday           up\n2 Wednesday         down\n3  Thursday         down\n4    Friday    unchanged\n5    Monday           up\n6   Tuesday         down\n\n\nThe factor structure allows us quick tabulations, like this:\n\n# Tabulate price changes by weekday\ntabulated_data &lt;- table(aapl_prices$weekday, aapl_prices$price_change)\ntabulated_data\n\n           \n             up down unchanged\n  Monday    895  726        39\n  Tuesday   887  876        45\n  Wednesday 917  855        34\n  Thursday  846  891        39\n  Friday    835  889        41\n\n\nThen we can use our table to see how many down moves we count on Mondays in our data set.\n\n# Extract the number of down moves on Mondays\ndown_on_mondays &lt;- tabulated_data[\"Monday\", \"down\"]\ndown_on_mondays\n\n[1] 726\n\n\nThis analysis reveals how many times Apple’s stock price decreased on Mondays in the dataset. Using factors, we can categorize data and extract meaningful insights, making them an invaluable tool for financial analysis and beyond. They are a powerful tool in exploratory data analysis involving categorical data.\n\n\n2.7.3.2 Data Frames: Organizing Stock Price Data\nA data frame is a two-dimensional structure that organizes data into rows and columns, where each column can store a different type of data. The stock price dataset we loaded earlier is a perfect example of a data frame.\nLet’s revisit the aapl_prices dataset:\n\nhead(aapl_prices)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6   AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n    weekday price_change\n1   Tuesday           up\n2 Wednesday         down\n3  Thursday         down\n4    Friday    unchanged\n5    Monday           up\n6   Tuesday         down\n\ntypeof(aapl_prices)\n\n[1] \"list\"\n\nclass(aapl_prices)\n\n[1] \"data.frame\"\n\n\nEach column in a data frame is an atomic vector, meaning it contains values of a single type:\n\nDoubles: Prices (e.g., opening, closing).\nCharacters: Ticker symbols (\"AAPL\").\nLogicals: New variables created based on conditions.\n\nFor example, we can create a logical column to indicate whether the stock closed higher than it opened:\n\naapl_prices$up_day &lt;- aapl_prices$close &gt; aapl_prices$open\nhead(aapl_prices[c(\"date\", \"close\", \"up_day\")], n = 5)\n\n        date    close up_day\n1 1990-01-02 0.332589   TRUE\n2 1990-01-03 0.334821  FALSE\n3 1990-01-04 0.335938  FALSE\n4 1990-01-05 0.337054  FALSE\n5 1990-01-08 0.339286   TRUE\n\n\nData frames are extremely flexible. You can subset them, filter rows, or add new columns:\n\n# Subset rows where the stock closed higher than it opened\n\nhigher_close &lt;- aapl_prices[aapl_prices$up_day == TRUE, ]\nhead(higher_close)\n\n   symbol       date     open     high      low    close    volume  adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.2427781\n11   AAPL 1990-01-16 0.299107 0.312500 0.292411 0.311384 214244800 0.2454167\n14   AAPL 1990-01-19 0.301339 0.308036 0.299107 0.305804 265137600 0.2410190\n17   AAPL 1990-01-24 0.290179 0.305804 0.287946 0.303571 169792000 0.2392588\n     weekday price_change up_day\n1    Tuesday           up   TRUE\n5     Monday           up   TRUE\n9     Friday           up   TRUE\n11   Tuesday           up   TRUE\n14    Friday           up   TRUE\n17 Wednesday           up   TRUE\n\n\n\n\n2.7.3.3 Lists: Combining Multiple Data Types\nWhile data frames organize tabular data, lists allow us to group together objects of different types and structures. This makes lists a powerful tool for storing heterogeneous data.\nFor example, let’s create a list to summarize key information about Apple’s stock prices:\n\nstock_summary &lt;- list(\n  ticker = \"AAPL\",\n  price_summary = summary(aapl_prices$close),\n  highest_price = max(aapl_prices$high, na.rm = TRUE),\n  date_range = range(aapl_prices$date, na.rm = TRUE)\n)\nstock_summary\n\n$ticker\n[1] \"AAPL\"\n\n$price_summary\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  0.1155   0.3834   3.3300  30.5265  28.8950 259.0200 \n\n$highest_price\n[1] 260.1\n\n$date_range\n[1] \"1990-01-02\" \"2024-12-27\"\n\n\nYou can access elements in a list using double brackets ([[):\n\nstock_summary[[\"highest_price\"]]\n\n[1] 260.1\n\n\nLists can also contain nested lists or data frames, enabling complex data structures. For example:\n\nnested_list &lt;- list(\n  summary = stock_summary,\n  recent_data = head(aapl_prices, n = 5)\n)\nnested_list\n\n$summary\n$summary$ticker\n[1] \"AAPL\"\n\n$summary$price_summary\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  0.1155   0.3834   3.3300  30.5265  28.8950 259.0200 \n\n$summary$highest_price\n[1] 260.1\n\n$summary$date_range\n[1] \"1990-01-02\" \"2024-12-27\"\n\n\n$recent_data\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n    weekday price_change up_day\n1   Tuesday           up   TRUE\n2 Wednesday         down  FALSE\n3  Thursday         down  FALSE\n4    Friday    unchanged  FALSE\n5    Monday           up   TRUE\n\n\n\n\n2.7.3.4 Factors, Data Frames, and Lists in Practice\nTo see how these structures work together, consider this small example. We’ll summarize Apple’s stock price movements over a week and organize the results in a list:\nHere’s the adjusted example using your actual dataset aapl_prices. This example simulates weekly price movements while working within the context of your existing data structure:\nWe will create a weekly summary based on simulated daily price movements and analyze the results using a summary list. This demonstrates how to summarize factor data combined with numerical summaries.\n\n# Extract the first five rows of the dataset to simulate one week of data\nweekly_data &lt;- head(aapl_prices, 5)\n\n# Add a factor for days of the week\nweekly_data$day &lt;- factor(\n  c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"),\n  levels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"),\n  ordered = TRUE\n)\n\n# Simulate price changes (-1 = down, 0 = unchanged, 1 = up)\nset.seed(42)  # For reproducibility\nweekly_data$price_change &lt;- sample(\n  c(-1, 0, 1), \n  size = nrow(weekly_data), \n  replace = TRUE, \n  prob = c(0.3, 0.4, 0.3)\n)\n\n# Summarize the data\nsummary_list &lt;- list(\n  week_data = weekly_data,\n  positive_days = sum(weekly_data$price_change &gt; 0),\n  total_change = sum(weekly_data$price_change)\n)\n\n# Display the summary list\nsummary_list\n\n$week_data\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n    weekday price_change up_day       day\n1   Tuesday           -1   TRUE    Monday\n2 Wednesday           -1  FALSE   Tuesday\n3  Thursday            0  FALSE Wednesday\n4    Friday           -1  FALSE  Thursday\n5    Monday            1   TRUE    Friday\n\n$positive_days\n[1] 1\n\n$total_change\n[1] -2\n\n\nHere is an explanation of the individual steps:\n\nExtract Weekly Data: The head function selects the first five rows from aapl_prices to simulate a single trading week.\nAdd Days as Factors: A new column day assigns ordered factors representing the days of the week.\nSimulate Price Movements: Random price movements (-1, 0, 1) are generated using sample, with probabilities for down, unchanged, and up movements.\nCreate a Summary List:\n\nweek_data holds the weekly data frame.\npositive_days calculates the number of days with an upward price movement.\ntotal_change sums the net price movements across the week.\n\n\nThis example combines:\n\nA factor (days) to represent the days of the week.\nA data frame (weekly_data) to organize daily price changes.\nA list (summary_list) to store the data frame and summary statistics.\n\nFactors, data frames, and lists are essential for organizing and analyzing real-world data. In financial analysis, they allow us to:\n\nCategorize data efficiently (factors).\nAnalyze structured datasets (data frames).\nIntegrate heterogeneous data (lists).\n\nArmed with these tools, you’re ready to perform deeper analyses and connect R concepts to practical financial problems. Next, we’ll use this knowledge to explore probabilistic questions with our stock price data.\n\n\n\n2.7.4 Example: Will the Stock Price of Apple Move Up or Down?\nLet us revisit the first lines of our dataset:\n\nhead(aapl_prices, n = 10)\n\n   symbol       date     open     high      low    close    volume  adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2    AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3    AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4    AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6    AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n7    AAPL 1990-01-10 0.335938 0.335938 0.319196 0.321429 199718400 0.2533336\n8    AAPL 1990-01-11 0.323661 0.323661 0.308036 0.308036 211052800 0.2427781\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.2427781\n10   AAPL 1990-01-15 0.308036 0.319196 0.305804 0.305804 161739200 0.2410190\n     weekday price_change up_day\n1    Tuesday           up   TRUE\n2  Wednesday         down  FALSE\n3   Thursday         down  FALSE\n4     Friday    unchanged  FALSE\n5     Monday           up   TRUE\n6    Tuesday         down  FALSE\n7  Wednesday         down  FALSE\n8   Thursday         down  FALSE\n9     Friday           up   TRUE\n10    Monday         down  FALSE\n\n\n\n2.7.4.1 Understanding the Structure of the Data\nTo begin, let’s confirm the type and structure of aapl_prices:\n\ntypeof(aapl_prices)\n\n[1] \"list\"\n\nclass(aapl_prices)\n\n[1] \"data.frame\"\n\n\nAs expected, the object is a list with the class data.frame. This means the dataset organizes data in rows and columns. We can use the dim() function to see how many trading days are recorded:\n\ndim(aapl_prices)\n\n[1] 8815   11\n\n\nThe dataset contains price information for 8815 trading days.\n\n\n\n2.7.4.2 Subsetting Data: Accessing Specific Elements\nTo analyze the dataset, we need to extract specific values or subsets of data. R provides a powerful and flexible notation system for subsetting:\naapl_prices[row_indices, column_indices]\nHere are the six main subsetting methods in R:\n\nPositive Integers:\n\nExample: Select the closing price on the first trading day:\n\n\n\naapl_prices[1, \"close\"]\n\n[1] 0.332589\n\n\n\nSelect the first 5 closing prices:\n\n\naapl_prices[1:5, \"close\"]\n\n[1] 0.332589 0.334821 0.335938 0.337054 0.339286\n\n\n\nNegative Integers:\n\nExclude the first observation (show first 3 entries after this operation):\n\n\n\n     head(aapl_prices[-1, \"close\"], 3)\n\n[1] 0.334821 0.335938 0.337054\n\n\n\nZero:\n\nCreates an empty object:\n\n\n\n     aapl_prices[0, 0]\n\ndata frame with 0 columns and 0 rows\n\n\n\nBlank Spaces:\n\nSelect all values in a dimension:\n\n\n\n     sel &lt;- aapl_prices[, \"close\"]\n     length(sel)\n\n[1] 8815\n\n\n\nLogical Values:\n\nExample: Use a logical vector to select the first closing price:\n\n\n\n     aapl_prices[1, c(FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE)]\n\n      high    close\n1 0.334821 0.332589\n\n\n\nNames:\n\nSelect using column names:\n\n\n\naapl_prices[1, \"close\"]\n\n[1] 0.332589\n\n\n\n\n2.7.4.3 Calculating Daily Price Differences\nLet us compute the day-to-day differences in closing prices. Using indexing, we can calculate these differences manually:\n\naux_1 &lt;- aapl_prices[2:8044, \"close\"]\naux_2 &lt;- aapl_prices[1:8043, \"close\"]\ndiff_close &lt;- aux_1 - aux_2\nhead(diff_close, n = 10)\n\n [1]  0.002231985  0.001117021  0.001116008  0.002231985 -0.003347993\n [6] -0.014508992 -0.013393015  0.000000000 -0.002231985  0.005579978\n\n\nAlternatively, the built-in diff() function simplifies this task:\n\naapl_prices$diff &lt;- c(NA, diff(aapl_prices$close))\nhead(aapl_prices, n = 5)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n    weekday price_change up_day        diff\n1   Tuesday           up   TRUE          NA\n2 Wednesday         down  FALSE 0.002231985\n3  Thursday         down  FALSE 0.001117021\n4    Friday    unchanged  FALSE 0.001116008\n5    Monday           up   TRUE 0.002231985\n\n\nThe first observation is NA because there is no prior day to calculate a difference.\n\n\n2.7.4.4 Frequency-Based Probability of Upward Moves\nTo determine the probability of the stock moving up on any given day, we create a logical column that indicates whether the price difference is positive:\n\naapl_prices$diff_pos &lt;- aapl_prices$diff &gt; 0\nhead(aapl_prices, n = 5)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n    weekday price_change up_day        diff diff_pos\n1   Tuesday           up   TRUE          NA       NA\n2 Wednesday         down  FALSE 0.002231985     TRUE\n3  Thursday         down  FALSE 0.001117021     TRUE\n4    Friday    unchanged  FALSE 0.001116008     TRUE\n5    Monday           up   TRUE 0.002231985     TRUE\n\n\nUsing the relative frequency approach, we calculate the probability of an upward move as:\n\nmean(aapl_prices$diff_pos, na.rm = TRUE)\n\n[1] 0.5073746\n\n\nor about 51 %.\nThis one line of code does several important things that are worth unpacking.\nFirst, R has an internal behavior known as type coercion, where logical values (TRUE and FALSE) are automatically converted into numerical values when used in numerical operations. Specifically:\n\nTRUE is coerced to 1\nFALSE is coerced to 0\n\nBy applying the mean() function to a logical vector, R computes the average of its numerical representation, effectively calculating the proportion of TRUE values. In this case, the mean gives the relative frequency of upward price movements in the dataset.\nAdditionally, this approach relies on the fact that the mean() function automatically divides the sum of the values by their count. Since TRUE values are coerced to 1, the sum of the vector corresponds to the total count of upward movements. 6\n6 In fact, using sum(aapl_prices$diff_pos, na.rm = TRUE) would provide the total number of days where the stock price increased, as sum() adds up the 1s in the binary vector.This step also highlights the flexibility and power of R’s vectorized operations, where simple functions like mean() and sum() can directly compute meaningful results for logical or binary data without additional loops or transformations.\n\n\n2.7.4.5 Applying Probability Concepts\nAssume that price movements are independent of one another. This means that the direction of the stock price movement today does not influence tomorrow’s movement. With this assumption, we can calculate compound probabilities over multiple days.\n\nProbability of Consecutive Increases: What is the probability that the stock price increases every day over a week (5 trading days)?\n\\(P(U \\cap U \\cap U \\cap U \\cap U) = P(U)^5 = 0.51^5 = 0.035\\)\nProbability of One Decrease and Four Increases: Consider the probability of a decrease on any one day, and increases on the other four:\n\\(P(D \\cap U \\cap U \\cap U \\cap U) = 0.49 \\cdot 0.51^4 = 0.033\\)\nSince there are 5 such (mutually exclusive) scenarios (one for each trading day), the total probability is:\n\\(5 \\cdot 0.033 = 0.132\\)\n\n\n\n2.7.4.6 Reflecting on Assumptions\nIs this analysis any good? How could we possibly judge this? Interestingly the relative frequencies of up and down moves look similar to a random experiment of a few thousand tosses of a fair coin. But can we learn anything from this? Are the up and down moves independent? Independence - of course - does not follow from the result we just got.\nThe idea that stock prices may fluctuate randomly was first discussed systematically by Louis Bachelier (1870 - 1946), a French mathematician who studied stock price movements mathematically. In 1965 the economist Samuelson (1965) published an article with the title “Proof that stock prices fluctuate randomly”. He argues in this paper that randomness comes about through the active participation of traders seeking to maximize their wealth. A huge army of investors would aggressively use the smallest informational advantage and in doing so, they incorporate the information into market prices, which quickly eliminates this profit opportunity. This lead to a cornerstone of modern Finance theory called the random walk hypothesis of stock price fluctuations.\nIf this theory was true, it would give an argument, why we might look at the up and down movements in the stock price of apple as if it was the outcome of tossing a fair coin. In this case the probability of an up or a down movement should be 1/2 and with the number of trials approaching infinity the frequency of ups and downs should approach this probability.\nThe literature on stock price fluctuations which came later, however, presented evidence that stock prices are predictable to a certain degree and do not fluctuate randomly. A good reference summarizing this evidence is Lo and MacKinlay (2019) In this case our approach would perhaps produce a misleading answer.\nWe cannot give a clear cut answer here. The point of this brief discussion is that you just cannot apply a theoretical machinery mechanically without giving it further thought and without maintaining a healthy amount of skepticism. It is fascinating that there are situations where abstract theories, like the theory of probability, show a robust relation to real world phenomena. But the nature, the precise meaning and the robustness of this relation has to be investigated for each single case.\nAs Louis Bachelier and Paul Samuelson noted, randomness in financial markets may emerge from the interplay of rational traders, yet this randomness is not absolute. A nuanced understanding of market dynamics is crucial for drawing meaningful conclusions.\n\n\n\n2.7.5 Example: Benford’s Law and Trading Volumes\nLet us conclude this lecture with an example that ties together our understanding of empirical probabilities and relative frequencies while also showcasing a surprising pattern in real-world data. This example will further strengthen your R skills and prepare you for the assigned project.\n\n2.7.5.1 The Curious Case of Leading Digits\nFirst, let us define leading digits: the leading digit of a number is simply its first non-zero digit. For instance: - The leading digit of 7829 is 7. - The leading digit of 0.00453 is 4. - The leading digit of 10892 is 1.\nYou might expect that in a random dataset, all leading digits from 1 to 9 occur with roughly equal frequencies (approximately 11% each). However, real-world datasets often show a very different pattern: smaller digits like 1 appear more frequently than larger digits like 9. This pattern follows a logarithmic distribution known as Benford’s Law, which states: \\(P(d) = \\log_{10}\\left(1 + \\frac{1}{d}\\right)\\)\nwhere \\(d \\in \\{1, 2, \\dots, 9\\}\\).\n\n\n2.7.5.2 Benford’s Law in Trading Volumes\nLet’s see if trading volumes follow Benford’s Law. First, we extract the trading volumes from the aapl_prices dataset:\n\nvolumes &lt;- aapl_prices$volume\n\nNext, we filter out invalid values (e.g., zeros or missing values) and extract the leading digits:\n\nvalid_volumes &lt;- volumes[volumes &gt; 0 & !is.na(volumes)]\n\nleading_digits &lt;- as.numeric(substr(as.character(valid_volumes), 1, 1))\n\nLet’s break down this code:\nHere’s an explanation of the two code lines tailored for students:\n1. Filter Valid Trading Volumes\n\nvalid_volumes &lt;- volumes[volumes &gt; 0 & !is.na(volumes)]\n\nThis line ensures that only valid trading volumes are included in the analysis. Let’s break it down:\n\nvolumes &gt; 0:\n\nThis condition filters out any non-positive values (e.g., 0 or negative numbers).\nThese values are not meaningful for analyzing leading digits because they do not have valid non-zero digits.\n\n!is.na(volumes):\n\nThe is.na() function checks if a value is missing (NA).\nThe ! operator negates this, keeping only non-missing values.\n\nCombining Conditions:\n\nThe & operator combines the two conditions. Only entries that satisfy both (i.e., are greater than 0 and not missing) are retained.\n\nSubsetting:\n\nvolumes[...] applies these conditions to the volumes vector. The result is a new vector, valid_volumes, containing only the valid trading volumes.\n\n\n2. Extract Leading Digits\n\nleading_digits &lt;- as.numeric(substr(as.character(valid_volumes), 1, 1))\n\nThis line extracts the leading digit of each valid trading volume. Here’s how it works:\n\nConvert to Character Strings:\n\n\nhead(as.character(valid_volumes),5)\n\n[1] \"183198400\" \"207995200\" \"221513600\" \"123312000\" \"101572800\"\n\n\n\nSince trading volumes are numeric, we need to treat them as text to extract specific characters.\nas.character() converts each number into a string representation.\n\n\nExtract the First Character:\n\n\nhead(substr(as.character(valid_volumes), 1, 1),5)\n\n[1] \"1\" \"2\" \"2\" \"1\" \"1\"\n\n\n\nThe substr() function extracts substrings from each string.\n1, 1 specifies that we extract the substring starting at position 1 and ending at position 1 (i.e., the first character of the string). This gives the leading digit as a character.\n\n\nConvert Back to Numeric:\nas.numeric(...): The extracted leading digit is initially a character. as.numeric() converts it back into a numeric value, allowing us to perform calculations like counting frequencies.\n\nNow we compute the empirical frequencies of the leading digits and compare them to the theoretical probabilities:\n\n# Tabulate empirical frequencies\nemp_freq &lt;- table(leading_digits) / length(leading_digits)\n\n# Create a data frame with empirical and Benford probabilities\nbenford &lt;- data.frame(\n  Digit = 1:9,\n  Empirical_Freq = as.numeric(emp_freq[1:9]), # Match digits 1 to 9\n  Benford_Prob = log10(1 + 1 / (1:9))\n)\n\nHere’s an explanation of the code chunk, step by step:\n1. Tabulate Empirical Frequencies\n\nemp_freq &lt;- table(leading_digits) / length(leading_digits)\n\n\ntable(leading_digits):\n\nThis function counts how many times each unique value appears in leading_digits.\nIn our case, leading_digits contains numbers between 1 and 9, so the output will have counts for each of these digits.\n\nlength(leading_digits):\n\nThis calculates the total number of entries in leading_digits, i.e., the total number of leading digits observed.\n\ntable(...) / length(...):\n\nDividing the counts from table(leading_digits) by the total count gives the relative frequencies of each digit.\nThese are the empirical probabilities of each digit appearing as the leading digit.\n\n\n\n\n2.7.5.3 2. Create a Data Frame for Comparison\n\nDigit = 1:9:\n\nThis creates a column in the data frame with the digits 1 through 9. These are the leading digits we’re analyzing.\n\nEmpirical_Freq = as.numeric(emp_freq[1:9]):\n\nSubsetting emp_freq[1:9]:\n\nThe table() function includes entries for all unique values in leading_digits. If there are missing digits (e.g., if leading_digits doesn’t include a certain number), emp_freq may not have all 9 entries.\nBy explicitly subsetting with [1:9], we ensure the frequencies for digits 1 through 9 are aligned with the Digit column, ignoring any gaps or extra entries (e.g., 0).\n\nas.numeric():\n\nThe table() output is a special type of object, not a standard numeric vector. Converting it ensures that the values can be used for computations and stored in the data frame.\n\n\nBenford_Prob = log10(1 + 1 / (1:9)):\n\nThis calculates the theoretical probabilities for digits 1 through 9 according to Benford’s Law: \\(P(d) = \\log_{10}\\left(1 + \\frac{1}{d}\\right)\\)\n1:9 provides the range of digits for which we compute probabilities.\n\nCombining into a Data Frame:\n\nThe data.frame() function organizes the information into a table with three columns:\n\nDigit: The digits 1 through 9.\nEmpirical_Freq: The observed relative frequencies of these digits from emp_freq.\nBenford_Prob: The theoretical probabilities predicted by Benford’s Law.\n\n\n\nWe can now display the results\n\n# Display the comparison\nknitr::kable(benford)\n\n\n\n\nDigit\nEmpirical_Freq\nBenford_Prob\n\n\n\n\n1\n0.3072036\n0.3010300\n\n\n2\n0.1674419\n0.1760913\n\n\n3\n0.1185479\n0.1249387\n\n\n4\n0.0976744\n0.0969100\n\n\n5\n0.0713556\n0.0791812\n\n\n6\n0.0670448\n0.0669468\n\n\n7\n0.0620533\n0.0579919\n\n\n8\n0.0584231\n0.0511525\n\n\n9\n0.0502552\n0.0457575\n\n\n\n\n\n\n\n2.7.5.4 Interpreting the Results\nThe table above compares the observed frequencies of leading digits in Apple’s trading volumes with the predictions of Benford’s Law. The match is stunningly close. The example illustrates how this surprising regularity applies even in financial datasets like trading volumes.\n\n\n2.7.5.5 Broader Applications\nBenford’s Law has been observed across diverse datasets, such as: - Market capitalization of companies, - GDP of countries or regions, - Population data of cities or counties.\nIts applicability hinges on datasets spanning multiple orders of magnitude and not being artificially bounded. Deviations from Benford’s Law have even been used to detect fraud in tax filings and financial records.\nFor your project, you will explore whether Benford’s Law holds for another financial dataset, allowing you to test this “law of leading digits” in a real-world context.\n\n\n\n2.7.6 Enhancing Your Learning with an LLM: Debugging and Exploring Data Analysis Pitfalls\nOne of the most powerful ways to use a Large Language Model (LLM), like ChatGPT, is as a debugging companion and a tool for uncovering potential pitfalls in your data analysis. This can be particularly valuable as you learn new concepts, such as the ones covered in this lecture on probability and data analysis with R.\nUse Case: Debugging R Code and Avoiding Common Mistakes\nWhen working with R, especially as a beginner, you might encounter errors, unexpected outputs, or difficulties understanding how certain functions behave. Here’s how you can use an LLM to enhance your learning:\n1. Explaining Error Messages\nWhen you run into an error message in R, an LLM can help you interpret it and suggest solutions. For example:\nError Message:\nError in table(leading_digits): all arguments must have the same length\nHow to Use an LLM: - Copy the error message and your code into the LLM, and ask:\n\n\n\n\n\n\nPrompt\n\n\n\nI encountered this error in R. Here is my code: [paste your code]. Can you explain what went wrong and suggest how to fix it?”\n\n\n2. Exploring Alternatives and Best Practices\nLLMs can also suggest alternative ways to achieve the same task or point out best practices you might not be aware of.\nExample: You might ask:\n\n\n\n\n\n\nPrompt\n\n\n\nI used substr(as.character(valid_volumes), 1, 1) to extract leading digits. Are there other, possibly better ways to do this in R?”\n\n\n3. Checking Your Understanding with “What-If” Questions\nLLMs are great for exploring “what-if” scenarios that test your understanding of concepts.\nExample Questions:\n\n\n\n\n\n\nPrompt\n\n\n\nWhat happens if I include zero or negative values in the dataset when applying Benford’s Law?“*\n\n\nor you might ask\n\n\n\n\n\n\nPrompt\n\n\n\nIf my dataset has fewer than 100 observations, how reliable are the empirical probabilities in approximating theoretical probabilities?”\n\n\nThis type of interaction helps you connect the theoretical content (e.g., the weak law of large numbers) with practical considerations.\n4. Summarizing and Synthesizing Knowledge\nYou can use an LLM to review and consolidate your learning by asking it to summarize key concepts or connect them in new ways.\n\n\n\n\n\n\nPrompt\n\n\n\nCan you summarize the main differences between empirical probabilities and theoretical probabilities, and explain how they relate to Benford’s Law?\n\n\nThis exercise reinforces your understanding while revealing gaps or misconceptions.\nDebugging and exploring potential pitfalls are essential skills for any data scientist or analyst. By leveraging an LLM, you not only solve immediate problems but also gain deeper insights into your learning process. This approach fosters independence and critical thinking, preparing you to tackle more complex challenges in the future.\nAs you work on the project for this lecture, try using an LLM to explain error messages, explore alternative solutions, and answer your “what-if” questions. You might be surprised at how much you can learn from this collaborative process!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#summary",
    "href": "02-lecture2.html#summary",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.8 Summary",
    "text": "2.8 Summary\nIn this lecture, we explored foundational concepts in probability theory and developed practical skills for working with R. Here’s a summary of what we covered:\n\n2.8.0.1 Probability Concepts\n\nPrecise definitions of:\n\nSample space: The set of all possible outcomes of an experiment.\nBasic outcomes: Individual elements of the sample space.\nEvents: Subsets of the sample space.\nProbability: A numerical measure of the likelihood of an event, satisfying the key properties of non-negativity, normalization, and additivity.\n\nDiscrete probability:\n\nHow probabilities are assigned to events in a finite or countably infinite sample space.\nThe construction of events using set operations like union, intersection, and complement.\nThe concept of mutually exclusive events and its implications for probabilities.\n\nEmpirical probability (or frequency probability):\n\nThe relative frequency of an event in a series of repeated experiments.\nHow empirical probabilities approximate theoretical probabilities as the number of trials increases, as demonstrated by the weak law of large numbers.\n\nIndependence of events:\n\nDefinition: Two events (A) and (B) are independent if (P(A B) = P(A) P(B)).\nImplications for computing probabilities in repeated or combined experiments.\n\n\n\n\n2.8.0.2 R Concepts\n\nR Objects:\n\nOverview of atomic vectors and their six types: double, integer, character, logical, complex, and raw.\nAttributes of atomic vectors, such as names, dimensions, and classes.\nIntroduction to data frames, the primary data structure for organizing and analyzing tabular data in R.\n\nSubsetting in R:\n\nMethods for subsetting R objects using positive integers, negative integers, zero, blank spaces, logical vectors, and names.\nPractical examples to manipulate data frames and vectors efficiently.\n\nString manipulation in R:\n\nExtracting and processing information from character data.\n\n\n\n\n\n2.8.0.3 Applications\n\nSimulating coin tosses:\n\nUsing R to simulate repeated trials and compute relative frequencies.\n\nReading data:\n\nLoading datasets from stored CSV files and directly from online sources.\n\nStock price analysis:\n\nUsing subsetting techniques to analyze up and down movements in stock prices.\nComputing empirical probabilities of price movements and using independence assumptions to estimate probabilities of sequences of moves.\n\nBenford’s Law:\n\nExploring the surprising regularity of leading digits in real-world datasets.\nApplying Benford’s Law to trading volumes and comparing empirical distributions to theoretical predictions.\n\n\nThrough this lecture, we reinforced the connection between probability theory and data analysis, demonstrating how abstract concepts like empirical probabilities and independence can be applied in real-world scenarios using R. The integration of theoretical ideas with computational tools prepares you for more advanced analysis and real-world problem-solving.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#project-2-financial-data-forensics-investigating-financial-reports-using-benfords-law",
    "href": "02-lecture2.html#project-2-financial-data-forensics-investigating-financial-reports-using-benfords-law",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.9 Project 2: Financial Data Forensics – Investigating Financial Reports Using Benford’s Law**",
    "text": "2.9 Project 2: Financial Data Forensics – Investigating Financial Reports Using Benford’s Law**\n\n2.9.1 Overview\nThis project challenges you to apply Benford’s Law to detect potential anomalies in financial data. By analyzing the leading digits of revenue and expenditure data for a set of companies, you will explore whether these datasets follow the natural logarithmic distribution predicted by Benford’s Law. Through this project, you will reinforce your understanding of empirical probabilities and their applications, while also practicing critical data analysis skills in R.\n\n\n2.9.2 Objectives\n\nAnalyze the distribution of leading digits in revenue and expenditure data.\nCompare empirical distributions with the theoretical predictions of Benford’s Law.\nIdentify and interpret deviations from Benford’s Law.\nReflect on the implications of your findings in the context of financial forensics.\n\n\n\n2.9.3 Steps\nStep 1: Understand the Research Question Your main tasks are: 1. To determine if the leading digits of revenues and expenditures conform to Benford’s Law. 2. To interpret deviations, particularly in expenditure data, which may suggest anomalies such as fraud or manipulation.\nStep 2: Obtain and Inspect the Dataset\n\nDownload the Dataset:\n\nMock Dataset: You will receive a CSV file named company_financials.csv, containing simulated data for revenues and expenditures of 200 companies. This dataset includes some subtle anomalies in the expenditures.\n\nInspect the Data:\n\nLoad the dataset in R and examine its structure using functions like head(), summary(), and str().\nEnsure you understand the data columns:\n\nCompanyID: A unique identifier for each company.\nRevenue: The revenue of the company (in dollars).\nExpenditure: The expenditure of the company (in dollars).\n\n\n\nStep 3: Prepare the Data\n\nFilter Valid Data:\n\nExclude invalid entries:\n\nNon-positive values (e.g., 0 or negative numbers).\nMissing values (NA).\n\n\nExtract Leading Digits:\n\nUse string manipulation to extract the first digit from each valid value:\n\n\nStep 4: Analyze the Data\n\nCompute Empirical Frequencies:\n\nTabulate the frequencies of the leading digits for revenues and expenditures:\n\nCompare with Benford’s Law:\n\nCreate data frames for comparison:\n\nVisualize the Results:\n\nPlot bar charts comparing empirical and theoretical distributions for revenues and expenditures.\n\n\nStep 5: Interpret the Results\n\nEvaluate Conformity:\n\nDoes the revenue data closely match Benford’s predictions?\nDo expenditures show significant deviations?\n\nHypothesize Causes:\n\nWhat might explain deviations in expenditures? Consider:\n\nRounded or artificial values.\nAnomalies such as fraud.\n\n\nProbability Context:\n\nRelate empirical frequencies to probabilities and discuss the implications of large sample sizes.\n\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nLo, Andrew, and Craig MacKinlay. 2019. A Non-Random Walk down Wallstreet. Princeton University Press.\n\n\nSamuelson, Paul. 1965. “Proof That Properly Anticipated Prices Fluctuate Randomly.” Industrial Management Review 6.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html",
    "href": "03-lecture3.html",
    "title": "3  Conditional Probability",
    "section": "",
    "text": "3.1 Why neglecting conditional probability may be expensive: A case study.\nWith this motivation in mind let us turn to a historical example that demonstrates the importance of understanding conditional probability: the financial crisis of 2007-2008. This crisis revealed how wrong assumptions about independence and neglect of dependence in events and conditional probabilities can lead to systemic failures in structured finance.1\nTo understand the real world aspects of this example it is necessarry to understand some basic ideas of structured finance and the engineering of specific risk profiles from a portfolio of risky securities in the first place. I will explain the finance context with a simple and stylized example and then discuss how understanding conditional probability may help us to make better financial decisions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#why-neglecting-conditional-probability-may-be-expensive-a-case-study.",
    "href": "03-lecture3.html#why-neglecting-conditional-probability-may-be-expensive-a-case-study.",
    "title": "3  Conditional Probability",
    "section": "",
    "text": "1  See Tooze (2018): For an engaging and comprehensive exploration of the great financial crisis of 2007 - 2008 and its causes and aftermath. It is highly recommended for deeper study.\n\n3.1.1 Bonds and Credit Risk\nA bond is a financial instrument where the issuer agrees to pay the holder a specific amount, the face value or par value, at maturity. Bonds are widely used as fixed-income securities but carry the risk of default if the issuer faces financial difficulties.\nTo quantify this risk, bonds are rated by agencies such as Moody’s and Standard & Poor’s. Investment-grade bonds are considered low-risk, while speculative or “junk” bonds are riskier and more likely to default. Here is a summary of their rating schemes and what the ratings mean in words:\n\n\n\nRating Category\nMoody’s\nStandard & Poor’s\n\n\n\n\nHigh grade\nAaa\nAAA\n\n\n\nAa\nAA\n\n\nMedium grade\nA\nA\n\n\n\nBaa\nBBB\n\n\nSpeculative grade\nBa\nBB\n\n\n\nB\nB\n\n\nDefault danger\nCaa\nCCC\n\n\n\nCa\nCC\n\n\n\nC\nC\n\n\n\n\nD\n\n\n\n\n\n3.1.2 Pooling and Tranching: The Innovation\nStructured finance emerged in the early 2000s as a way to manage risk through pooling and tranching. By pooling risky assets and dividing cash flows into “tranches” with distinct risk profiles, financial engineers created new bonds, including investment-grade securities, from portfolios of bonds which individually would be rated as speculative grade or junk bonds. A major product of this innovation was the mortgage-backed security (MBS). Many other products were then invented using similar financial engineering ideas.\nLet us develop an intuitive understanding of structured finance and its relation to probability through a simplified example, which I learned from Karl Schmedder’s course.2\n2 See https://www.coursera.org/learn/introductiontoprobability\n\n3.1.3 A Simple Event Tree for One Bond\nConsider a single bond you can own today that pays €1 at maturity at some point in the future. Time is often abbreviated as \\(t\\) and the points in time are symbolized by letting \\(t\\) take different values like \\(t=0\\) for today and \\(t=1\\) for a future point in time, say a year from today.\nThis bond has a 10% chance of default, meaning there is a 90% chance it will not default. With a default probability of 10%, this bond would likely receive a speculative grade rating, such as ‘B’ or ‘B-’ in the rating tables presented earlier. This poor rating reflects the significant risk of non-payment associated with such a bond, which could deter risk-averse investors and highlight its ‘junk’ bond status. The payoff is structured as follows:\n\nIf the bond does not default (\\(N\\)), the payoff is €1.\nIf the bond defaults (\\(D\\)), the payoff is €0.\n\nThis situation can be graphically represented as a simple probability tree of Figure 3.1\n\n\n\n\n\n\n\n\nFigure 3.1: Event tree for one bond\n\n\n\n\n\nThe graph above visualizes the outcomes of a single bond. Each node represents a possible state of the bond at different times:\n\n\\(t=0\\) is the starting point.\n\\(t=1\\) No Default \\(N\\) occurs with a probability of \\(P(N) = 0.9\\).\n\\(t=1\\) Default \\(D\\) occurs with a probability of \\(P(D) = 0.1\\).\n\nYou could see this in analogy to the toss of a coin with the difference that both sides of the coin show with different probability. With this analogy - using the concepts of the last two lectures - you can understand the bond in probabilistic terms as a random experiment with a sample space consisting of two basic outcomes, \\(N\\) and \\(D\\) with given probabilities \\(P(N)\\) and \\(P(D)\\).\n\n\n3.1.4 Combining Two Bonds with Independence Assumption\nWe now combine two such bonds into a portfolio. The assumption of independence implies that the defaults of these bonds occur independently of each other. This means that the default of one bond has no influence on the probability of the other bond defaulting. Under the assumption of independence you would treat the default probability of one bond as unconnected with the default probability of the other.\nWhile this assumption simplifies calculations, it was historically used by financial engineers to justify creating tranches from risky portfolios. The reasoning was that diversification reduces the likelihood of joint defaults, making some tranches appear safer.\nAt the time, financial engineers relied on historical data and market conditions to argue for this independence. Defaults were often uncorrelated under normal economic conditions, and diversification was seen as a proven strategy for mitigating risk. For example, if bond defaults were driven by isolated events (such as company-specific issues), the assumption of independence seemed reasonable. Moreover, the packaging of diverse assets from different industries into portfolios strengthened the appearance of safety, as individual economic shocks were less likely to affect the entire portfolio simultaneously.\nHowever, this reasoning neglected systemic risks. During economic downturns or financial crises, defaults often become highly correlated due to shared macroeconomic pressures, such as declining housing markets or credit tightening. For instance, in the lead-up to the 2008 financial crisis, rising mortgage defaults were driven by broader economic factors that impacted many bonds simultaneously. With this in mind it would be not plausible to assume that bonds can be stacked together in a portfolio without the default risks of one being not pushed up by the default risk of others.\nEven without the formal use of probability theory, financial engineers could have questioned whether diversification truly guaranteed independence in the context of systemic risks.\nThe idea that junk plus junk could be transformed into investment-grade bonds through pooling should have raised skepticism. Careful critical thinking—considering broader economic dependencies—would have revealed that this transformation was too good to be true. By ignoring these dependencies, financial engineers failed to see how small cracks in the system could cascade into systemic failures.3\n3  A famous voice at the time warning about the flawed reasoning was Raghuram Rajan, former chief economist of the International Monetary Fund. He warned that rather than reducing risk through diversification, CDOs and other derivatives spread risk and uncertainty about the value of the underlying assets more widely. But let us not jump ahead and try to see how the tree for two bonds looks like when we assume independence in Figure 3.2.\n\n\n\n\n\n\n\n\nFigure 3.2: Event tree for two bonds with independent default risk\n\n\n\n\n\n\nThe outcome of combining two bonds can be represented as a double event tree, showing all possible combinations of defaults and no defaults at \\(t=1\\). Let me unpack this more complex tree for you.\nSince we have a portfolio of two bonds, instead of one as before, we have now two event trees combined. Since we have to take into account all of the possible combinations of basic outcomes this means that we have to attach a new bond event tree to each outcome of the initial tree.\nNote the time index. In the example the realizations of basic outcomes for the portfolio happen simultaneously. So the walk from the origin to the end of the tree along a path is taking place in one time step and only the outcomes at \\(t=1\\) are observed. There is one subtlety in this representation which I would like to explicitly point out. The tree suggests a sequence - the first bond realizes its state and then the second. This is true but it does not mean at the same time that the bond holders observe this sequence. From their point of you they only see outcomes at the indicated time steps. (at the dashed lines, if you will). This information structure suggests that we should think of these state realizations of the bonds as occurring simulateously. Think of it as if yo u would toss tow coins at once.\nAt the edges we write the probability of the outcomes. For example \\(P(N)\\) is the probability of the first bond (represented by the upper tree) is not defaulting whereas \\(P(D)\\) denotes the probability of the bond not defaulting. The assumption of independence is hidden in this tree by modelling the probabilities of \\(N\\) and \\(D\\) for the second bond in exactly the same way no matter whether the first bond defaults or not. It is modelled in anaology to the toss of two fair coins. The probability of the second coin showing Heads is \\(1/2\\) no matter whether the first coin shows Heads or Tails.\nAt the end of the tree we have written the outcome of each path in the notation \\[\\begin{equation*}\nB_{\\text{state of bond 1 at} \\, t = 1 \\, \\text{state of bond 2 at} \\, t=1} = \\binom{\\text{payoff of bond 1 at } \\, t= 1}{\\text{payoff of bond 2 at } \\, t = 1}\n\\end{equation*}\\] So, for example, \\(B_{NN} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) means that bond 1 does not default and bond two does not default (\\(B_{NN}\\)). Bond 1 has in this case a payoff of 1 and bond 2 also has a payoff of 1.\n\n\n\n3.1.5 Pooling and Tranching\nWhen you look at this portfolio under the independence assumption there seems to be room for re-engineering the risk profile of this portfolio. In all outcomes but in the one where both bonds default you can guarantee a payoff of 1. There remains an adverse outcome, where both bonds default in which case you can’t pay out anything. But under the assumption of independence this risk is small. The probability of this event - remember our treatment of independent events in the first lecture - would be: \\(P(D) \\times P(D) = 0.1 \\times 0.1 = 0.01\\). Pretty low, actually. For example assume that the probability of default refer to the probability of the bond defaulting over a year, the usual time frame taken in ratings, this would be a one in a hundred years event. In \\(99%\\) we would get a sure payoff of 1. So under this restructuring the first restructured bond would qualify as an investment grade bond.\nSo this is the idea. We pool the payoffs of both securities and define two new securities by changing their payoff profile. The first one pays always 1 except when both bonds default in which case this bond pays 0. The other one will always pay 0 except in the case where both bonds do not default. This is under independence an event with probability \\(P(N) \\times P(N) = 0.81\\). Rember the complementarity rule? This says that the second restructured bond will thus have a default probability of \\(19\\) % instead of \\(10%\\) it would be speculative grade or close to toxic junk now.\nHere is picture how you can visualize this piece fo financial magic. This picture can be read in exactly the same way as the previous picture. There is only one additional element. We have written the payoff of the original bonds by \\(B\\). Underneath these original bonds we draw a black horizontal like, think of it as the financial engineering lab that does the restructuring and below we get new bonds, with different payout promises, which we denote by \\(R\\) (for restructured).\n\n\n\n\n\n\n\n\nFigure 3.3: Event tree for two bonds with independent default risk with pooling and tranching\n\n\n\n\n\nWhat is done here is that the aggregate payoffs of both bonds are collected in a pool and new securities - called R - in this picture are issued against the pool. One, the upper one is now an investment grade asset paying 1 in every state except one and the other is a toxic junk bond paying always 0 except in one state. Note that the investment grade status could be engineered under the assumption that the risks are independent.\n\n\n3.1.6 Pooling and Tranching without Independent Risks\nNow, let us consider a hypothetical question: How would the event tree change if the independence assumption does not hold? Dependence would alter the probabilities in a way that reflects the increased likelihood of joint defaults during systemic events.\nSuppose we now assume that the probability of Bond 2 defaulting changes rather than staying unchanged under the condition that Bond 1 has defaulted:\n\nIf Bond 1 does not default, Bond 2 defaults with a lower probability than before, here 0.044 instead of 0.1.\nIf Bond 1 does default, Bond 2 defaults with a higher probability of 0.6 due to systemic dependence.\n\nTo express this formally we need a piece of new notation. The convention in probability theory is that the notation is - for example - \\(P(N | D)\\). This would read as bond 2 does not default given bond 1 has defaulted. The event we are interested in is written first separated by a \\(|\\) from the conditioning event, which is in our case the outcome that bond 1 defaults.\nThis dependence reflects a scenario where defaults are more likely to occur together, such as during an economic downturn. The resulting event tree can be visualized as follows:\n\n\n\n\n\n\n\n\nFigure 3.4: Event tree for two bonds with independent default risk with pooling and tranching with dependent default risk\n\n\n\n\n\nOn the first sight this looks almost identical to the picture before. Only the numbers on the edges of the second tree have changed. These changed numbers reflect the idea that the event that one bond has defaulted changes the probability of the second bond defaulting as well. How could such a dependence occur?\nHere’s a real-world example to illustrate how dependence can occur: In the context of bonds, dependence in default probabilities can arise from shared exposure to systemic risks or interconnected factors. For instance, consider two companies that issue bonds and operate in the same industry, such as the energy sector. If oil prices plummet due to an economic downturn or geopolitical instability, both companies might experience financial stress, making it more likely that one default is followed by another.\nAnother example is during a financial crisis, such as the 2008 global financial meltdown. A bank’s default on its obligations can lead to cascading defaults in other institutions due to counterparty risks or a general loss of confidence in the financial system. In such cases, the probability of a second default is no longer independent of the first because the events are tied to the same underlying macroeconomic factors.\nThese examples highlight that the assumption of independence between bond defaults might hold under normal market conditions but breaks down during systemic crises. Such dependencies must be carefully modeled to avoid underestimating risk, as was the case in structured finance products leading up to the 2008 crisis.\nA prudent risk manager must keep such a scenario in mind when he analyzes a portfolio. Think about it in the context of the toy example. In the first case the default risk of the first asset created by pooling and tranching was \\(P(D) \\times P(D) = 0.1 \\times 0.1 = 0.01\\). Under a scenario with dependent risks this changes to \\(P(D) \\times P(D | D) = 0.1 \\times 0.6 = 0.06\\), a risk larger by a factor of 6! While in the first case the first restructured bond would be rated as investment grade, in the second case the same restructured bond would be rated as speculative grade and the magic from pooling and tranching suddenly disappears. Junk plus junk remains junk after all.\nFor pooling and tranching to reduce overall risk and create safe tranches we need:\n\nDiversification: Assets must come from independent sectors with minimal systemic risk.\nStable Macroeconomic Conditions: Systemic risks must be low to maintain independence assumptions.\nTransparent Modeling: Dependence structures must be explicitly modeled and accounted for in risk assessments.\n\nThe neglect of these conditions led to a flawed sense of security in structured finance, which contributed to the 2008 financial crisis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#conditional-probability",
    "href": "03-lecture3.html#conditional-probability",
    "title": "3  Conditional Probability",
    "section": "3.2 Conditional Probability",
    "text": "3.2 Conditional Probability\nConditional probability provides us with a concept to formalize how the probability of one event changes when another event is known to occur, providing a framework for understanding dependencies quantitatively.\nHere is the mathematical definition:\n\n\n\n\n\n\nDefinition: Conditional probability\n\n\n\nLet \\(B\\) be an event with positive probability. For an arbitrary event \\(A\\) we define the conditional probability of \\(A\\) given \\(B\\) as \\[\\begin{equation*}\nP(A\\,|\\,B) = \\frac{P(A \\cap B)}{P(B)}\\,\\,\\, \\text{provided}\\,\\,\\, P(B) \\neq 0\n\\end{equation*}\\]\n\n\nNote that conditional probabilities remain undefined when the conditioning event \\(B\\) has probability 0.4\n4 As pointed out in Feller (1968) p 115, this has no consequences in the case of discrete sample spaces but is important in the general theory.Let us clarify a few things about this concept. As in the example of the financial crisis, which we discussed before we really did not much more than introducing one piece of new notation to indicate that the probabilities now have changed.\nLet’s revisit our example.\n\n3.2.1 The Probability Tree and Conditional Probabilities\nThe tree is labeled with edge probabilities, which correspond to the marginal and conditional probabilities of events at each level:\n\nAt \\(t=0\\): \\(P(B_1 = N) = 0.9\\) and \\(P(B_1 = D) = 0.1\\).\nAt \\(t=1\\): The probabilities for \\(B_2\\) defaulting given \\(B_1\\)’s outcome are:\n\n\\(P(B_2 = N \\,|\\, B_1 = N) = 0.956\\), \\(P(B_2 = D \\,|\\, B_1 = N) = 0.044\\).\n\\(P(B_2 = N \\,|\\, B_1 = D) = 0.4\\), \\(P(B_2 = D \\,|\\, B_1 = D) = 0.6\\).\n\n\n\n\n\n3.2.2 Defining Probabilities in R\nWe start by defining the probabilities from the probability tree:\n\n# Define the probabilities\n\n# Marginal probabilities for B_1\nP_N &lt;- 0.9  # Probability that B_1 does not default\nP_D &lt;- 0.1  # Probability that B_1 defaults\n\n# Conditional probabilities for B_2 given B_1\nP_N_given_N &lt;- 0.8604/0.9  # Probability that B_2 does not default given B_1 does not default\nP_D_given_N &lt;- 0.0396/0.9  # Probability that B_2 defaults given B_1 does not default\nP_N_given_D &lt;- 0.4    # Probability that B_2 does not default given B_1 defaults\nP_D_given_D &lt;- 0.6    # Probability that B_2 defaults given B_1 defaults\n\n\n\n\n3.2.3 Computing Joint Probabilities\nThe joint probabilities, \\(P(A \\cap B)\\), are calculated using the multiplication rule: \\[\nP(A \\cap B) = P(A \\,|\\, B) \\cdot P(B).\n\\] These correspond to probabilities computed along the edges of the tree.\n\n# Calculate joint probabilities\n\nP_NN &lt;- P_N * P_N_given_N  # Both bonds do not default\nP_ND &lt;- P_N * P_D_given_N  # B_1 does not default, B_2 defaults\nP_DN &lt;- P_D * P_N_given_D  # B_1 defaults, B_2 does not default\nP_DD &lt;- P_D * P_D_given_D  # Both bonds default\n\n\n\n\n3.2.4 Constructing the Contingency Table\nWe can summarize the joint probabilities in a contingency table, which also includes marginal probabilities:\n\n# Load necessary library\nlibrary(kableExtra)\n\n# Define marginal probabilities\nP_N_total &lt;- P_N\nP_D_total &lt;- P_D\nP_N_given_total &lt;- round(P_NN + P_DN,2)\nP_D_given_total &lt;- round(P_ND + P_DD,2)\n\n# Create the contingency table\ncontingency_table &lt;- matrix(\n  c(\n    P_DD, P_DN, P_D_total,    # Row 1: Bond 1 Default\n    P_ND, P_NN, P_N_total,    # Row 2: Bond 1 No Default\n    P_D_given_total, P_N_given_total, 1  # Row 3: Column totals\n  ),\n  nrow = 3,\n  byrow = TRUE,\n  dimnames = list(\n    c(\"Bond 1: Default (D)\", \"Bond 1: No Default (N)\", \"Total\"),\n    c(\"Bond 2: Default (D)\", \"Bond 2: No Default (N)\", \"Total\")\n  )\n)\n\n# Create a styled table\nkable_output &lt;- knitr::kable(\n  contingency_table,\n  caption = \"Contingency Table of Joint and Marginal Probabilities\",\n  format = \"html\",\n  escape = FALSE,\n  col.names = c(\"\", \"Default (D)\", \"No Default (N)\", \"Total\")\n) %&gt;%\n  kableExtra::column_spec(1, bold = TRUE)\n\n# Render the table\nkable_output\n\n\nContingency Table of Joint and Marginal Probabilities\n\n\n\nDefault (D)\nNo Default (N)\nTotal\n\n\n\n\nBond 1: Default (D)\n0.0600\n0.0400\n0.1\n\n\nBond 1: No Default (N)\n0.0396\n0.8604\n0.9\n\n\nTotal\n0.1000\n0.9000\n1.0\n\n\n\n\n\n\n\nNote that conditional probabilities are in general not symmetric.\n\n# Recalculate marginal probabilities\nP_B2_D &lt;- P_N * P_D_given_N + P_D * P_D_given_D\n\n# Recompute conditional probabilities\nP_B2_given_B1 &lt;- P_DD / P_D_total  # P(B_2 = D | B_1 = D)\nP_B1_given_B2 &lt;- P_DD / P_B2_D     # P(B_1 = D | B_2 = D)\n\n# Display results\ncat(\"P(B_2 = D | B_1 = D):\", round(P_B2_given_B1, 4), \"\\n\")\n\nP(B_2 = D | B_1 = D): 0.6 \n\ncat(\"P(B_1 = D | B_2 = D):\", round(P_B1_given_B2, 4), \"\\n\")\n\nP(B_1 = D | B_2 = D): 0.6024 \n\n\nThe example above demonstrates that conditional probabilities are, in general, not symmetric. Using the constructed contingency table and the probability tree, we calculated two conditional probabilities:\n\n\\(P(B_2 = D \\,|\\, B_1 = D)\\): the probability that bond \\(B_2\\) defaults given that bond \\(B_1\\) defaults.\n\\(P(B_1 = D \\,|\\, B_2 = D)\\): the probability that bond \\(B_1\\) defaults given that bond \\(B_2\\) defaults.\n\nThese probabilities are computed as follows: \\[\nP(B_2 = D \\,|\\, B_1 = D) = \\frac{P(B_1 = D \\cap B_2 = D)}{P(B_1 = D)}\n\\] and \\[\nP(B_1 = D \\,|\\, B_2 = D) = \\frac{P(B_1 = D \\cap B_2 = D)}{P(B_2 = D)}.\n\\]\nDespite using the same joint probability, \\(P(B_1 = D \\cap B_2 = D)\\), the denominators differ: one uses \\(P(B_1 = D)\\), while the other uses \\(P(B_2 = D)\\). Because these marginal probabilities are generally not equal, the resulting conditional probabilities are also unequal. This asymmetry reflects the fundamental property of conditional probability and underscores the importance of clearly defining the conditioning event in probabilistic reasoning.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#simulating-a-bond-portfolio",
    "href": "03-lecture3.html#simulating-a-bond-portfolio",
    "title": "3  Conditional Probability",
    "section": "3.3 Simulating a bond portfolio",
    "text": "3.3 Simulating a bond portfolio\nWe can extend this theoretical framework to simulate a bond portfolio with defaults and compute conditional probabilities empirically:\nWe first set up the parameters as in our tree or as in the contingency table.\n\n# Set up simulation parameters\nN &lt;- 5000  # Total number of bonds\nP_DD &lt;- 0.06  # P(X = D, Y = D)\nP_DN &lt;- 0.04  # P(X = D, Y = N)\nP_ND &lt;- 0.04  # P(X = N, Y = D)\nP_NN &lt;- 0.86  # P(X = N, Y = N)\n\n# Verify probabilities sum to 1\nstopifnot(abs(P_DD + P_DN + P_ND + P_NN - 1) &lt; 1e-6)\n\nNow we simulate joint outcomes based on our contingency table:\n\n# Simulate joint outcomes based on the contingency table\nsimulate_defaults &lt;- function(N, probs) {\n  sample(\n    c(\"DD\", \"DN\", \"ND\", \"NN\"), \n    size = N, \n    replace = TRUE, \n    prob = probs\n  )\n}\n\nThis sets the ground for generating the portfolio data in a special dataframe:\n\n# Generate portfolio data\n\nportfolio &lt;- data.frame(\n  BondID = 1:N,\n  BondType = sample(c(\"B1\", \"B2\"), N, replace = TRUE, prob = c(0.5, 0.5))\n)\n\nThe portfolio dataframe serves as the dataset where conditional probabilities are derived by filtering rows based on specific conditions, such as whether a default has occurred.\nLet us now assign the joint defaults:\n\n# Assign joint default outcomes\nportfolio$JointOutcome &lt;- simulate_defaults(N, c(P_DD, P_DN, P_ND, P_NN))\nportfolio$X_Defaulted &lt;- portfolio$JointOutcome %in% c(\"DD\", \"DN\")\nportfolio$Y_Defaulted &lt;- portfolio$JointOutcome %in% c(\"DD\", \"ND\")\n\nNow we can compute the unconditional and the conditional probabilties from our simulated data:\n\n# Compute unconditional probabilities\nP_X_D &lt;- mean(portfolio$X_Defaulted)  # P(X = D)\nP_Y_D &lt;- mean(portfolio$Y_Defaulted)  # P(Y = D)\n\n# Compute conditional probabilities\nP_X_given_Y_D &lt;- mean(portfolio$X_Defaulted[portfolio$Y_Defaulted])  # P(X = D | Y = D)\nP_Y_given_X_D &lt;- mean(portfolio$Y_Defaulted[portfolio$X_Defaulted])  # P(Y = D | X = D)\n\n# Display results\ncat(\"Unconditional Probabilities:\\n\")\n\nUnconditional Probabilities:\n\ncat(\"P(X = D):\", round(P_X_D, 4), \"\\n\")\n\nP(X = D): 0.1076 \n\ncat(\"P(Y = D):\", round(P_Y_D, 4), \"\\n\\n\")\n\nP(Y = D): 0.1138 \n\ncat(\"Conditional Probabilities:\\n\")\n\nConditional Probabilities:\n\ncat(\"P(X = D | Y = D):\", round(P_X_given_Y_D, 4), \"\\n\")\n\nP(X = D | Y = D): 0.5905 \n\ncat(\"P(Y = D | X = D):\", round(P_Y_given_X_D, 4), \"\\n\")\n\nP(Y = D | X = D): 0.6245 \n\n# Verify calibration matches input probabilities\ncalibration_check &lt;- table(portfolio$JointOutcome) / N\nexpected_probs &lt;- c(P_DD, P_DN, P_ND, P_NN)\n\ncalibration_result &lt;- data.frame(\n  JointOutcome = names(calibration_check),\n  Frequency = as.numeric(calibration_check),\n  Expected = expected_probs\n)\n\nprint(calibration_result)\n\n  JointOutcome Frequency Expected\n1           DD    0.0672     0.06\n2           DN    0.0404     0.04\n3           ND    0.0466     0.04\n4           NN    0.8458     0.86\n\n\nNote that computing conditional probabilities involves focusing on a subset of the data where a specific condition holds. For example, to compute P(X = D | Y = D), we calculate the relative frequency of X_Defaulted being TRUE within the subset of rows where Y_Defaulted is TRUE.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#advanced-r-concepts-environments-scoping-rules-and-closures",
    "href": "03-lecture3.html#advanced-r-concepts-environments-scoping-rules-and-closures",
    "title": "3  Conditional Probability",
    "section": "3.4 Advanced R Concepts: Environments, Scoping Rules, and Closures",
    "text": "3.4 Advanced R Concepts: Environments, Scoping Rules, and Closures\nIn this section, we will explore some advanced R programming concepts that are essential for understanding how R evaluates and stores variables, as well as how you can create reusable and dynamic functions. We will demonstrate these concepts through examples related to conditional probability and financial modeling.\n\n3.4.1 Introduction to Environments\nAn environment in R is where objects (variables, functions, etc.) are stored and looked up. R uses environments to determine where a variable exists and what its value is. The most common environment is the global environment, where user-created variables and functions are stored.\nExample: Setting Global and Local Variables\nSuppose we are modeling interest rates in a financial portfolio. Globally, we set the baseline interest rate. Locally, we may override this rate for specific calculations.\n\n# Global interest rate\ninterest_rate &lt;- 0.05  # 5%\n\n# Function to calculate interest payments\ncalculate_interest &lt;- function(principal, rate = interest_rate) {\n  interest &lt;- principal * rate  # Uses the rate passed to the function\n  return(interest)\n}\n\n# Global calculation\nglobal_interest &lt;- calculate_interest(1000)  # Uses global interest_rate\ncat(\"Global Interest:\", global_interest, \"\\n\")\n\nGlobal Interest: 50 \n\n# Local override\nlocal_interest &lt;- calculate_interest(1000, rate = 0.07)  # Overrides global interest_rate\ncat(\"Local Interest:\", local_interest, \"\\n\")\n\nLocal Interest: 70 \n\n\nGlobal variables are available everywhere, but local variables (like rate) take precedence within a function. Understanding this behavior is crucial for writing clear and predictable code.\n\n\n3.4.2 Scoping Rules\nR follows specific scoping rules to determine where and how to find variables. These rules become important when working with nested functions.\nExample: Variable Lookup in Nested Functions\nLet’s calculate conditional probabilities using nested functions. We simulate a financial scenario where we compute probabilities of default for different credit ratings.\n\n# Define global default rates for credit ratings\n\ndefault_rates &lt;- c(\n  AAA = 0.01,  # Global default rate for AAA bonds\n  BBB = 0.02,  # Global default rate for BBB bonds\n  Junk = 0.05  # Global default rate for Junk bonds\n)\n\n# Function to calculate conditional default probability\nconditional_default &lt;- function(rating) {\n  \n  # Lookup table for default rates\n  local_default_rates &lt;- c(\n    AAA = unname(default_rates[\"AAA\"]),  # Local default for AAA\n    BBB = unname(default_rates[\"BBB\"]),  # Local default for BBB\n    Junk = unname(default_rates[\"Junk\"]) # Local default for Junk\n  )\n  \n  # Return the default rate using vectorized subsetting\n  \n  return(local_default_rates[rating])\n}\n\n# Test the function\ncat(\"Default rate for Junk bonds:\", conditional_default(\"Junk\"), \"\\n\")\n\nDefault rate for Junk bonds: 0.05 \n\ncat(\"Default rate for BBB bonds:\", conditional_default(\"BBB\"), \"\\n\")\n\nDefault rate for BBB bonds: 0.02 \n\ncat(\"Default rate for AAA bonds:\", conditional_default(\"AAA\"), \"\\n\")\n\nDefault rate for AAA bonds: 0.01 \n\n\nHere you see how R is using Lexical scoping. This ensures that R looks for variables in the closest environment first, then moves outward (from local to global). Nested functions can use both local and global variables.\nThis example uses a concept you might find useful in many other contexts: The lookup table. The concept of a lookup table is a simple yet powerful way to map input values to corresponding outputs. In R, we can create a lookup table using a named vector, where each element has a name (the input) and a value (the corresponding output). This allows us to retrieve the correct value by directly referencing the name.\nIn the example, we used a named vector local_default_rates to store the default probabilities for different credit ratings: “AAA”, “BBB”, and “Junk”. Each credit rating serves as a key, and the corresponding default probability serves as the value. When we pass the rating (e.g., “Junk”) to the function, R uses it to subset the vector and directly return the associated probability. This approach is efficient and avoids the need for verbose or complex conditional statements.\nBy using a lookup table, we also demonstrate an important principle of programming: separation of data and logic. The mapping of ratings to probabilities is encapsulated in a single data structure (local_default_rates), making the function simpler and easier to modify. For instance, if the default probabilities change, you only need to update the values in the vector—no changes to the function logic are required. This approach is especially useful in financial modeling, where mappings like these are common and can evolve over time.\n\n\n3.4.3 Closures\nA closure is a function that remembers the environment in which it was created. Closures are powerful for creating dynamic, reusable functions, such as calculators for different conditional probabilities.\nExample: Probability Calculator Factory\nThis code demonstrates a powerful concept in R: closures. A closure is a function that “remembers” the environment in which it was created, allowing you to dynamically generate new functions with specific behaviors. Let’s create a function factory that generates specific probability calculators based on a given event.\n\n# Function factory for conditional probability calculators\n\nprobability_calculator_factory &lt;- function(event_probability) {\n  function(conditional_probability) {\n    joint_probability &lt;- event_probability * conditional_probability\n    return(joint_probability)\n  }\n}\n\n# Create calculators for different events\n\njunk_calculator &lt;- probability_calculator_factory(0.05)  # Junk bonds\nbbb_calculator &lt;- probability_calculator_factory(0.02)   # BBB bonds\n\n# Calculate joint probabilities\n\njunk_joint &lt;- junk_calculator(0.1)  # P(Default | Junk) * P(Junk)\nbbb_joint &lt;- bbb_calculator(0.2)    # P(Default | BBB) * P(BBB)\n\ncat(\"Joint probability for Junk bonds:\", junk_joint, \"\\n\")\n\nJoint probability for Junk bonds: 0.005 \n\ncat(\"Joint probability for BBB bonds:\", bbb_joint, \"\\n\")\n\nJoint probability for BBB bonds: 0.004 \n\n\nLet’s unpack the code step by step:\nThe probability_calculator_factory is a function factory. It takes one argument, event_probability, and returns a new function that calculates the joint probability for a given conditional probability:\n\nInput:\n\nevent_probability: The probability of the event (e.g., the probability of a bond being “Junk”).\nThe returned function takes conditional_probability as its argument (e.g., the probability of default given the bond is “Junk”).\n\nOutput:\n\nThe joint probability, \\(P(A \\cap B) = P(A | B) \\times P(B)\\).\n\n\nThis structure encapsulates the logic for joint probability into a reusable framework.\nThe junk_calculator and bbb_calculator are functions created by the factory. Each calculator “remembers” the event_probability it was initialized with: - junk_calculator: \\(P(Junk) = 0.05\\). - bbb_calculator: \\(P(BBB) = 0.02\\).\nThese calculators are then used to compute joint probabilities by providing the corresponding conditional probabilities:\n\njunk_joint &lt;- junk_calculator(0.1):\n\n\\(P(\\text{Default} \\cap \\text{Junk}) = P(\\text{Default | Junk}) \\times P(\\text{Junk})\\).\n\\(0.1 \\times 0.05 = 0.005\\) (0.5%).\n\nbbb_joint &lt;- bbb_calculator(0.2):\n\n\\(P(\\text{Default} \\cap \\text{BBB}) = P(\\text{Default | BBB}) \\times P(\\text{BBB})\\).\n\\(0.2 \\times 0.02 = 0.004\\) (0.4%).\n\n\nThe cat() function displays the results:\nA closure allows you to “lock in” parameters (like event_probability) when the function is created, while still allowing flexibility for additional inputs.\n\n\n\n\n\n\nNow you try\n\n\n\n\nModify the calculate_interest function to add a penalty rate for overdue payments using local variables.\nExtend the conditional_default function to include an additional credit rating (e.g., “CC”).\nUse the probability_calculator_factory to compute joint probabilities for a new event, such as “Real Estate Sector Default.”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#bayes-rule-one-of-the-great-ideas-in-probability",
    "href": "03-lecture3.html#bayes-rule-one-of-the-great-ideas-in-probability",
    "title": "3  Conditional Probability",
    "section": "3.5 Bayes’ Rule: One of the Great Ideas in Probability",
    "text": "3.5 Bayes’ Rule: One of the Great Ideas in Probability\nBayes’ Rule stands among the ten great ideas in probability. Its power lies in solving a fundamental problem: how to infer underlying chances from observed frequencies. This insight filled a critical gap left by Bernoulli’s weak law of large numbers, which explained how observed frequencies converge to probabilities but left unanswered the question of how to reason from those frequencies back to the chances that generated them.\nAt its core, Bayes’ Rule provides a framework for updating our beliefs in light of new information. But what does it mean to assign a probability to a belief? This idea rests on another deep insight in probability theory: judgments about uncertainty can be measured, and when those judgments are coherent, they follow the rules of probability. 5\n5 This idea goes back to the work of Frank Ramsey (1903–1930). He was a British mathematician, philosopher, and economist whose profound contributions spanned multiple fields despite his tragically short life. In probability, he established the foundation for subjective probability theory, showing that coherent judgments about uncertainty adhere to the axioms of probability. Ramsey also made groundbreaking advances in decision theory, logic, and economics, including the famous Ramsey pricing in public economics and his foundational work in mathematical logic. For more, on Ramsey’s ideas of connecting judegment to probability see chapter 2 in Diaconis and Skyrms (2019)The key breakthrough of Bayes’ Rule is that it ties these coherent judgments—our initial beliefs, or priors—to evidence, using conditional probability. This process transforms subjective judgments into a systematic method for reasoning under uncertainty, with profound applications across science, finance, and everyday decision-making.\n\n3.5.1 A simple example: Revising beliefs about market valuation\nWe discussed the big financial crisis of 2007-2008 earlier in this lecture. Imagine now you are an investor in the year 2011, just a bit later. The market is gripped by fear of another financial crisis, this time triggered by tensions in the Eurozone.\nLet’s imagine how you could have asessed the market situation by looking at financial data. This is also an excellent opportunity to introdcue you to one of many great opportunities to load real world data directly into R using add on packages. The package we are going to use here is called tidyquant. To be able to use it you must first install it using the install.packages()function of base R or the package installer pane in RStudio. Let us do that - assuming that this package has been installed. You might remember how to load a package from before.\n\nlibrary(tidyquant)\n\nNow tidyquant has many functions allowing you to retrieve and transform real world financial data. We do not go into any detail here. This is something you can do yourself using the excellent documentation of this package, the many examples on the web or by interrogating your LLM.6. One of the core workhorse functions in tidyquant is the function tq_get() which allows you to retrieve data.\n6 Check out https://cran.r-project.org/web/packages/tidyquant/index.htmlOne thing an anlyst might be interested in is how the stock market as measured by a broad index does at the moment compared to its historical values. Let’s say you are looking back 11 years from 2011 into the past and see how the S&P500 does now compared to this history. This is how you would use tidyquant to do this:\n\n# Fetch S&P 500 data from 2000 to 2011\n\nsp500_data &lt;- tq_get(\"SPY\", from = \"2000-01-01\", to = \"2011-12-31\")\n\nIn order to do load data you need the string of the name of the series you are interested in, which you can learn from the documentation. In our case this is “SPY” for the SP500. You can also specify a time range or just the beginning of the series. In the latter case it will give you all the data from the beginning up to the current or most recent trading day. We have written the data into an R object called sp500_data and now you might inspect it.\n\nhead(sp500_data)\n\n# A tibble: 6 × 8\n  symbol date        open  high   low close   volume adjusted\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 SPY    2000-01-03  148.  148.  144.  145.  8164300     92.7\n2 SPY    2000-01-04  144.  144.  140.  140.  8089800     89.1\n3 SPY    2000-01-05  140.  142.  137.  140  12177900     89.2\n4 SPY    2000-01-06  140.  142.  138.  138.  6227200     87.8\n5 SPY    2000-01-07  140.  146.  140.  146.  8066500     92.9\n6 SPY    2000-01-10  146.  147.  145.  146.  5741700     93.2\n\n\nThe output says that the data object is a tibble. Don’t worry about this detail at the moment and think of a tibbleas something equivalent to a dataframe.\nNow let us visualize the data using base R’s plot() function. Use the help facilities or your LLM to find out abut the syntax details.\n\n# Base R plot\nplot(\n  sp500_data$date, sp500_data$adjusted,\n  type = \"l\",                # Line plot\n  col = \"blue\",              # Line color\n  lwd = 2,                   # Line width\n  xlab = \"Date\",             # X-axis label\n  ylab = \"Adjusted Price\",   # Y-axis label\n  main = \"S&P 500 Price Trends (2000–2011)\" # Title\n)\n\n# Add grid lines for better readability\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nWe see that the index is about the same level as it was 10 years ago. The index alone does not tell you very much. How about relating this to some relevant market fundamentals?\nOne powerful feature of tidyquant() is that it can fetch data from various data sources. Here I get, for the sake of this example, data on corporate profits from the FRED database of the Fed St. Louis7\n7 See https://fred.stlouisfed.org/\n# Get corporate profits data from FRED\ncorporate_profits &lt;- tq_get(\"CP\", from = \"2000-01-01\", \n                            to = \"2011-12-31\", get = \"economic.data\")\n\n# Visualize corporate profits\nplot(\n  corporate_profits$date, corporate_profits$price,\n  type = \"l\",\n  col = \"blue\",\n  lwd = 2,\n  xlab = \"Year\",\n  ylab = \"Corporate Profits (Index)\",\n  main = \"Corporate Profits (2000–2011)\"\n)\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nProfits have roughly tripled over the same period. The market does seem to be undervalued. Let’s put this into perspective by looking at Price-Earning data or P/E ratios.\nNow we run into a problem which you will often encounter when working with data. It is rare that a tool covers all cases. P/E rations seem to be difficult to retrieve with tidyquant. These data do however exist on the web, for example at the website Macrotrends.8\n8 The P/E ratio for the SP500 can be found for instance here: https://www.macrotrends.net/2577/sp-500-pe-ratio-price-to-earnings-chartTo retrieve data from this site, I had to download the data locally first before I could get them into R. Let’s look at them a bit closer now using again R’s visualization tools.\n\n# Set the path to the downloaded CSV file\nfile_path &lt;- \"data/sp-500-pe-ratio-price-to-earnings-chart.csv\"\n\n# Read the CSV file into R\npe_data &lt;- read.csv(file_path, stringsAsFactors = FALSE, skip = 16,\n                    col.names = c(\"Date\", \"PE_ratio\"))\n\n# Convert the date column to Date class\npe_data$Date &lt;- as.Date(pe_data$Date, format = \"%Y-%m-%d\")\n\n# Filter data for the desired date range\npe_data_filtered &lt;- subset(pe_data, Date &gt;= as.Date(\"2000-01-01\") & \n                             Date &lt;= as.Date(\"2011-12-31\"))\n\n# Calculate the average P/E ratio over the specified period\naverage_pe &lt;- mean(pe_data_filtered$PE_ratio, na.rm = TRUE)\n\n# Plot the P/E ratio for the y-axis\nplot(\n  pe_data_filtered$Date, pe_data_filtered$PE_ratio,\n  type = \"l\",                   # Line plot\n  col = \"blue\",                 # Line color\n  lwd = 2,                      # Line width\n  xlab = \"Year\",                # X-axis label\n  ylab = \"P/E Ratio\", # Y-axis label\n  main = \"S&P 500 P/E Ratio (2000–2011)\", # Plot title\n)\n\n# Add a horizontal line for the average P/E ratio\nabline(h = average_pe, col = \"red\", lwd = 2, lty = 2)\n\n# Add a legend to the plot\nlegend(\n  \"topright\",                   # Legend position\n  legend = c(\"P/E Ratio\", \"Average P/E Ratio\"), # Labels\n  col = c(\"blue\", \"red\"),       # Line colors\n  lty = c(1, 2),                # Line types\n  lwd = c(2, 2)                 # Line widths\n)\n\n\n\n\n\n\n\n\nAt first glance, the market seems undervalued, with a trailing price-to-earnings (P/E) ratio of 14, well below the historical average of 29.\nAs a cautious investor, you form the hypothesis that these low valuations are deceptive. You believe that corporate profits, which are at record highs of 1672 compared to the historical average of 1119, will revert to the mean. When this happens, earnings will drop, pushing the P/E ratio from a seemingly cheap to expensive.\nWith this belief in mind, you decide to hold off on investing, waiting for valuations to normalize. However, as the years pass—2012, 2013, 2014, and beyond— corporate profits remain elevated, and the market continues to rally. Each year, your cautious stance leaves you with underwhelming returns.\n\n# Get corporate profits data from FRED\ncorporate_profits &lt;- tq_get(\"CP\", from = \"2000-01-01\", \n                            to = \"2019-12-31\", get = \"economic.data\")\n\n# Visualize corporate profits with a vertical line at 2011\nplot(\n  corporate_profits$date, corporate_profits$price,\n  type = \"l\",\n  col = \"blue\",\n  lwd = 2,\n  xlab = \"Year\",\n  ylab = \"Corporate Profits (Index)\",\n  main = \"Corporate Profits (2000–2019)\"\n)\n\n# Add a vertical line at 2011\nabline(v = as.Date(\"2011-01-01\"), col = \"red\", lwd = 2, lty = 2)\n\n# Add grid lines for readability\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nBy 2019, you’re forced to confront the possibility that your belief about corporate profit mean-reversion might be wrong.\n\n3.5.1.1 The Need for Updating Probability Asessments\nThis scenario highlights the importance of updating beliefs in the face of new evidence. Initially, your hypothesis about profit margins reverting to the mean was reasonable, based on historical data. But as year after year passed without mean-reversion, the accumulating evidence should have prompted you to revise your prior beliefs.\nBayes’ Rule offers a principled way to do this. It allows you to combine your initial belief (the prior probability) with new evidence (e.g., sustained elevated profit margins) to calculate an updated belief (the posterior probability). This process ensures that your decisions adapt as reality unfolds, helping you avoid the dangers of clinging to outdated assumptions.\n\n\n\n3.5.2 Bayes’ Rule: Intuitive Understanding with a Speck of Sand\nBayes’ rule, one of the cornerstone ideas of probability, provides a systematic method for updating probabilities based on new evidence. It is formalized as: \\[\\begin{equation*}\nP(B|A) = \\frac{P(A|B) P(B)}{P(A)}\n\\end{equation*}\\]\nHere:\n\n\\(A\\): Represents the new evidence or data that has been observed.\n\\(B\\): Represents the hypothesis or prior belief about an event.\n\nThis formula arises naturally from the multiplication rule and the symmetry of \\((A \\cap B)\\). Here’s a quick derivation:\n\nBy the multiplication rule: \\(P(B|A)P(A) = P(A \\cap B)\\)\nSimilarly: \\(P(A|B)P(B) = P(A \\cap B)\\)\nEquating the two expressions for \\(P(A \\cap B)\\) and dividing by \\(P(A)\\): \\(P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\)\n\nBayes’ rule is often challenging to grasp intuitively. To build understanding, let’s explore a simple and vivid example involving a speck of sand.\n\n3.5.2.1 The Speck of Sand: An Intuitive Illustration\nImagine a square of area 1, representing our entire sample space. Within this square, there is a circle \\(B\\), with area equal to \\(P(B)\\). You have a tiny speck of sand on your finger, which accidentally falls somewhere within the square. The location of the speck is entirely random.\n\n\n\n\n\nWhere is the speck of sand?\n\n\n\n\nThe probability that the speck lands in \\(B\\) is simply the area of \\(B\\), \\(P(B)\\), since the speck could have landed anywhere within the square with equal likelihood.\n\n\n3.5.2.2 Updating Beliefs with New Information\nNow, suppose you are told that the speck landed within another circle \\(A\\) that also lies inside the square:\n\n\n\n\n\nIn fact we learn that the speck is inside circle \\(A\\)\n\n\n\n\nHow does this new information affect the probability that the speck is in \\(B\\) ? Mathematically, we now want to compute \\(P(B|A)\\), the probability that the speck is in \\(B\\), given that it is inside \\(A\\).\n\n\n3.5.2.3 Overlap Between \\(A\\) and \\(B\\)\nIntuitively, the updated probability \\(P(B|A)\\) depends on the overlap of \\(B\\) and \\(A\\). Specifically, it is the fraction of \\(A\\) that lies within \\(B\\), expressed as: \\[\\begin{equation*}\nP(B|A) = \\frac{\\text{Area of } A \\cap B}{\\text{Area of } A} = \\frac{P(A \\cap B)}{P(A)}\n\\end{equation*}\\]\n\nIf \\(A\\) and \\(B\\) overlap only slightly, \\(P(B|A)\\) will be small:\n\n\n\n\n\n\nSmall overlap between \\(A\\) and \\(B\\)\n\n\n\n\n\nIf the overlap is large, \\(P(B|A)\\) will be large:\n\n\n\n\n\n\nLarge overlap between \\(A\\) and \\(B\\)\n\n\n\n\n\n\n3.5.2.4 Bayesian Interpretation\nIn Bayesian reasoning:\n\n\\(P(B)\\) is the prior probability, representing our belief in the hypothesis \\(B\\) before observing the data.\n\\(P(A|B)\\) is the likelihood, describing how consistent the observed data \\(A\\) is with the hypothesis \\(B\\).\n\\(P(A)\\) normalizes the result, ensuring all probabilities sum to 1.\n\nThe speck-of-sand example illustrates how Bayesian updating works:\n\nStart with a prior \\(P(B)\\).\nReceive new evidence \\(P(A|B)\\).\nUpdate the probability of the hypothesis given the evidence \\(P(B|A)\\).\n\nBayes’ theorem quantifies this intuitive process of revising beliefs based on data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#using-an-llm-to-explore-conditional-probability",
    "href": "03-lecture3.html#using-an-llm-to-explore-conditional-probability",
    "title": "3  Conditional Probability",
    "section": "3.6 Using an LLM to Explore Conditional Probability",
    "text": "3.6 Using an LLM to Explore Conditional Probability\nIn each lecture I try to involve a use case showing you how you could leverage the power of LLMs to deepen your learning experience. Conditional probability is an excellent topic to explore with the LLM. Understanding conditional probability can be challenging because our intuitions about likelihoods often conflict with the precise rules of probability theory. Unlike geometric intuition which is hardwired in the human mind through the needs of our visual system, we humans lack an innate intuitive understanding of probability. This is something that has to be acquired through training and practice. An LLM like ChatGPT can serve as a dialogue partner to help you explore this gap and refine your understanding.\nHere is a suggested use case: You can use an LLM to simulate and analyze scenarios where our intuition might mislead us. Here’s how to structure a dialogue with the model:\n\nPose a Real-World Scenario:\n\nExample: “Suppose you test positive for a rare disease with a prevalence of 1%. The test is 95% accurate, meaning it correctly identifies positives 95% of the time and negatives 95% of the time. What is the probability that you actually have the disease?”\n\nEngage in a Dialogue:\n\nAsk the model to explain how to approach the problem using conditional probability.\nExplore intermediate steps such as calculating the prior probability, likelihood, and posterior probability using Bayes’ Rule.\n\nChallenge Your Intuition:\n\nShare your intuitive guess about the probability with the model.\nLet the model explain where intuition might diverge from formal reasoning and why.\n\nExperiment with Variations:\n\nModify the problem (e.g., change the disease prevalence or test accuracy) and ask the model to recompute probabilities. Compare how small changes in assumptions affect outcomes.\n\n\n\n3.6.1 Example Dialogue\nHere’s an example interaction you could try:\nYou: “I think the probability of actually having the disease should be very high since the test is 95% accurate. Is this correct?”\nModel: “Let’s compute it formally using Bayes’ Rule: \\[\\begin{equation*}\nP(D|T) = \\frac{P(T|D) \\cdot P(D)}{P(T|D) \\cdot P(D) + P(T|ND) \\cdot P(ND)}\n\\end{equation*}\\] Where: - \\(P(D) = 0.01\\) (prevalence), - \\(P(ND) = 1 - P(D) = 0.99\\), - \\(P(T|D) = 0.95\\) (true positive rate), - \\(P(T|ND) = 0.05\\) (false positive rate).\nSubstituting the values: \\(P(D|T) = \\frac{0.95 \\cdot 0.01}{0.95 \\cdot 0.01 + 0.05 \\cdot 0.99} \\approx 0.16\\)\nThis means the probability you actually have the disease is about 16%, much lower than intuition might suggest because the disease is so rare.”\nWhy can exercises like this be valuable?\n\nRefines Understanding: Engaging in such dialogues clarifies the application of Bayes’ Rule and highlights the importance of considering base rates.\nReveals Pitfalls of Intuition: You’ll learn to appreciate why our gut feelings can sometimes mislead us in probabilistic reasoning.\nEncourages Exploration: By modifying scenarios and asking “what if” questions, you deepen your grasp of conditional probabilities in diverse contexts.\n\nLeverage this approach to build a stronger connection between the theory and its real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#summary",
    "href": "03-lecture3.html#summary",
    "title": "3  Conditional Probability",
    "section": "3.7 Summary",
    "text": "3.7 Summary\n\n3.7.1 Probability Concepts\nIn this lecture, we covered the following key probability concepts: - Conditional Probability: Understanding the probability of an event occurring given that another event has occurred, and its relevance in real-world scenarios, especially in finance. - Dependence and Independence: How events can be dependent or independent and the role of conditional probabilities in analyzing their relationships. - Bayes’ Rule: Using conditional probabilities to update beliefs based on new evidence.\nThese concepts were illustrated with practical examples, including a discussion of the financial crisis of 2007-2008 to highlight the risks of neglecting dependencies in probability modeling.\n\n\n3.7.2 R Concepts\nDuring the lecture, we used R to: - Simulate Conditional Probabilities: Generate random data and compute conditional probabilities to illustrate theoretical concepts. - Visualize Dependencies: Create intuitive visualizations of event overlaps and relationships between probabilities. - Practical Applications: Implement real-world examples to explore conditional probabilities and Bayes’ Rule, showcasing how to use R for data analysis in finance.\nThis lecture bridged theoretical probability concepts with practical computational tools in R, enabling a deeper understanding of the material and its applications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#project-evaluating-credit-risk-using-conditional-probabilities",
    "href": "03-lecture3.html#project-evaluating-credit-risk-using-conditional-probabilities",
    "title": "3  Conditional Probability",
    "section": "3.8 Project: Evaluating Credit Risk Using Conditional Probabilities",
    "text": "3.8 Project: Evaluating Credit Risk Using Conditional Probabilities\n\n3.8.1 Problem Description\nA bank is evaluating a loan application using historical data to estimate the likelihood of default. The borrower has a low credit score, and the bank has the following data:\n\nDefault Rates:\n\nProbability of default for all customers: \\(P(D) = 0.04\\).\nProbability of non-default for all customers: \\(P(ND) = 0.96\\).\n\nEvidence:\n\nProbability of a low credit score given default: \\(P(L|D) = 0.7\\).\nProbability of a low credit score given non-default: \\(P(L|ND) = 0.1\\).\n\n\nThe goal is to determine the posterior probability of default given the borrower’s low credit score, \\(P(D|L)\\), using Bayes’ Rule. Additionally, you will verify this theoretical result by simulating customer data and analyzing outcomes.\n\n\n\n3.8.2 Questions\n\nCompute \\(P(D|L)\\) Theoretically:\n\nUse Bayes’ Rule to calculate the posterior probability of default given a low credit score.\n\nSimulate the Scenario in R:\n\nSimulate a dataset of 10,000 customers where each customer is randomly assigned a default status based on \\(P(D)\\).\nBased on the assigned default status, simulate whether each customer has a low credit score using \\(P(L|D)\\) and \\(P(L|ND)\\).\n\nCompute \\(P(D|L)\\) from Simulated Data:\n\nUse the simulated data to compute \\(P(D|L)\\) and compare it to the theoretical result.\n\nVisualize Results:\n\nCreate a simple bar plot comparing the simulated and theoretical probabilities. If you are unfamiliar with visualization tools, check out the barplot() function in base R.\n\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nTooze, Adam. 2018. Crashed. How a Decade of Financial Crisis Changed the World. Viking.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "06-references.html",
    "href": "06-references.html",
    "title": "5  References",
    "section": "",
    "text": "Diaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about\nChance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its\nApplications. 3rd ed. Vol. 1. Wiley.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical\nIntroduction. Princeton University Press.\n\n\nLo, Andrew, and Craig MacKinlay. 2019. A Non-Random Walk down\nWallstreet. Princeton University Press.\n\n\nSamuelson, Paul. 1965. “Proof That Properly Anticipated Prices\nFluctuate Randomly.” Industrial Management Review 6.\n\n\nTooze, Adam. 2018. Crashed. How a Decade of Financial Crisis Changed\nthe World. Viking.\n\n\nWickham, Hadley, and Jenifer Bryan. 2023. R Packages: Organize,\nTest, Document, and Share Your Code. O’Reilly. https://r-pkgs.org/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data\nScience. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>References</span>"
    ]
  }
]