[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Probability",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee (knuth84?) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html#what-are-these-lecture-notes-about",
    "href": "index.html#what-are-these-lecture-notes-about",
    "title": "An Introduction to Probability",
    "section": "What are these lecture notes about",
    "text": "What are these lecture notes about\nThese lecture notes support the course An Introduction to Probability - with Applications to Computational Finance using R. The course introduces essential probability concepts that every finance practitioner must understand. As probability is crucial for solving financial problems computationally, we will use computational finance as the context to develop these ideas. The focus will be hands-on learning, enabling you to understand and internalize concepts by applying them yourself.\nTo make abstract concepts like randomness, chance, and probability more accessible, we will leverage computational tools, in our course this will be R, and visualizations built by using R. By actively engaging with these tools, you’ll not only deepen your understanding of probability but also enhance your ability to apply it to real-world finance problems.\n\n\n\n\n\n\nKey learning objectives\n\n\n\n\nUnderstand foundational probability concepts.\nApply probability principles to solve computational finance problems.\nGain hands-on experience using R for probability and finance applications.\nDevelop proficiency in interpreting and visualizing probabilistic data.\n\n\n\nBut how can we build abstract concepts such as probability, random phenomena and chance by our own hands? While probability is a mathematical theory, it gains practical value and an intuitive meaning in connection with real or conceptual experiments such as, the future change in a stock price index, the future value of a portfolio of securities, the chance that a creditor is not going to be able to pay back your loan.\nMany of these experiments we can nowadays simulate on the computer. We can can construct, build and simulate a huge variety of random phenomena. We can - for instance - implement models of random fluctuations of asset prices. We can model financial risks and contemplate possible future scenarios through simulation.\nThe arrival of the computer has been indeed a sea change for the mathematics of probability because it allows both reflecting about probability through theorems and proofs as well as by making use of computer simulation. We can approach our understanding of abstract concepts by building them with our own hands on the computer. This is the approach to teaching you probability in this course.\nLet me give you one example right away. Don’t worry if you do not understand precisely what the following snippet of code is doing. We will learn the syntax of the R language as we go along. This is just to demonstrate you what I explained in words before by a simple example highlighting simulation of random processes and visualization.\n\n\n\n\n\n\nAn R example for illustration\n\n\n\n\n# Simulate daily returns for a stock\n\nset.seed(123)  # Ensure reproducibility\nn &lt;- 100       # Number of days\ndaily_returns &lt;- rnorm(n, mean = 0.001, sd = 0.02)  # Normal distribution\n\n# Compute cumulative returns\nprice &lt;- cumprod(1 + daily_returns) * 100  # Starting price = 100\n\n# Visualize the stock price\nplot(price, type = \"l\", col = \"blue\", lwd = 2,\n     main = \"Simulated Stock Price\",\n     xlab = \"Days\", ylab = \"Price\")\n\n\n\n\n\n\n\n\n\n\nThis example highlights:\n\nHow randomness influences stock price movements\nR’s usefulness in simulations and visualization.\n\nIt is, however important to always keep in mind that the real world is different from the theory world as well as from the simulated world. The real world is both richer and more complex than both theory and simulation. It may contain features which need to be be taken into account for the explanation of phenomena which are absent in theory or in the simulation. We will try our best to develop your feeling for the difference, between theoretical concepts, simulations and applications as well as for the interdependence of these three areas.\nThere are many ways to build virtual objects and to run simulations to manipulate them. For this we will need a programming language. The language we choose for this course is R and the integrated development environment RStudio. This is one of the main languages used in data analysis, statistics and data science and is widely used in industry and academia. It will be our tool to do probability in this course.\nSince this course was being taught the first time new AI tools such as large language models (LLMs) like chatGPT, Claude or Gemini have been introduced. These new powerful tools create new opportunities for learning and reinforcing knowledge such as the concepts learned in this course. In this year I will show you throughout the course, how you can leverage AI tools like ChatGPT for enhancing your learning experience. I encourage you to use these tools to generate examples, explain concepts, and debug R code or perhaps translate R code into a language you might be more familiar with on the fly to help you understand how things are expressed in R.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#downloading-and-installing-r",
    "href": "index.html#downloading-and-installing-r",
    "title": "An Introduction to Probability",
    "section": "Downloading and installing R",
    "text": "Downloading and installing R\nSo lets start by downloading and installing R first. R is an open source project maintained by an international team of developers. The software is made available through a website called the comprehensive R archive network (http://cran.r-project.org).\nAt the top of this website, in a box named “Download and install R” you will find three links for downloading R. Choose the link that describes your operating system, Windows, Mac or Linux. These links will lead you to the necessary information you need to install a current version of R. The easiest install option is to install R from precompiled binaries. There is also the option to built R from source on all operating systems if you have the tools and the expertise to do so. R also comes both in a 32-bit and a 64-bit version. It does not make a substantial difference which version you use. 64-bit versions can handle larger files and data sets with fewer memory management problems.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#downloading-and-installing-rstudio",
    "href": "index.html#downloading-and-installing-rstudio",
    "title": "An Introduction to Probability",
    "section": "Downloading and installing RStudio",
    "text": "Downloading and installing RStudio\nRStudio is an application that helps you write and develop R code. It makes using R much easier for you than using it in isolation. The interface of RStudio looks the same across all operating systems.\nYou can download RStudio for free from https://www.rstudio.com/products/rstudio/. Select the box RStudio Desktop and follow the download instructions. RStudio Desktop is free. Note that you need to have a version of R installed to use RStudio.\nIf you have successfully installed R and RStudio, we are ready to start.\nThroughout this course I will use R with RStudio and when I do code demonstrations I will use this environment. R can be used in many other ways of course. Some of you might be accustomed to work with Jupyter Notebooks. I will just mention here that with some minor tweaks R can be used with Jupyter notebooks as well. I will not go into the details here. Those of you who are interested in such a setup are encouraged to ask chatGPT:\n\n“Please give me a step by step instruction how to set up the system to use R through Jupyter notebooks instead of RStudio.”",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "An Introduction to Probability",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course is built as an intuitive and elementary introduction to the field. No knowledge of probability or R is assumed and all concepts are built from scratch. You should be able to study for this course by relying on the slides and these lecture notes only without having to consult a textbook or further literature.\nMany of you will have heard or studied probability at some stage in college or at university. I hope the course will appeal to the newcomers as well as to the experienced. While it should allow the first group to quickly learn some of the most important concepts and raise your curiosity and excitement about a field of knowledge which is both beautiful and immensely practical, I hope it will open a new perspective for the more experienced among you and thus help deepening your understanding of concepts you might already have heard elsewhere.\nThe lectures develop probability concepts and R concepts in parallel. This follows the idea underlying this course to develop your knowledge of probability concepts by building them on the computer yourself and play with them. In this way we hope to build your R skills more thoroughly because they appear in a natural context and that this context also reinforces your understanding of probability.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-notes",
    "href": "index.html#structure-of-the-notes",
    "title": "An Introduction to Probability",
    "section": "Structure of the Notes",
    "text": "Structure of the Notes\nThe notes are structured into five main chapters. The idea is to cover each main chapter topic in one of the five double units allocated to this course.\nEach of the main chapters or lectures are followed by a so called project. The projects are meant to be worked on by you in the assigned groups between the lectures and should help you to practice and to deepen what you have learned. We will discuss the project questions at the end of each lecture. We will also distribute a worked problem after you have worked on it yourself.\nWe begin the first lecture, lecture 1 by discussing some of the main basic probability fundamentals by putting them in their historical context where the first ideas appear of how you could measure something such elusive as chance at all. Surprisingly these first simple ideas are already very rich and lead us quite far into the field and its problems.\nIn this lecture we will also give a first panoramic run down of what you can do with R. In this lecture the first application will be a simple game of chance, the tossing of a coin. Already this very elementary example has direct relevance for computational financial modelling of security prices as we will learn throughout the course.\nEven the very basic ideas of probability and R enable us, already at this early stage, to take on a serious real world problem as well. In fact, with only a few intuitive concepts we can study and understand problems of the chance of coincidences. This class of problems is at the heart of the science of constructing cryptographically safe finger prints, so called hash-functions, which are at the heart of crypto-currencies such as Bitcoin.\nThe first project will deal with engineering secure transaction indentifiers for a digital payment system and study how the construction of such identifiers has to take into account key system paramaters such as transaction volume and number of participants in the payment system.\nLecture 2 will introduce the first basic intuitive ideas a bit more formally and also generalize them. In particular it explains the basic ideas of how probability and frequencies of chance events are related. We will discuss the power as well as the limits of these ideas for applied work in probability.\nIn this lecture we will explain in detail how you can use R to read, store and manipulate data. We also start working with stock market data and randomly fluctuating asset prices and discuss how we can deepen our understanding of them using probability and R.\nProject 2 will help you to train and practice your understanding of data manipulation in R by analyzing a real world supervisory data set from the European Banking Authority EBA. We will use a really cool empirical probability pattern on the empirical frequency of leading digits - Benford’s law - for checking whether the data could be trusted or do look fishy on purely probabilistic grounds.\nIn Lecture 3 we will learn about the key concept of conditional probability. This lecture contains lots of concepts but it is also of key importance for understanding many problems in Finance and the management of financial risks. We will explain how the misunderstanding or perhaps willful ignorance of the concept of dependence is at the root of the big financial crisis of 2007-2008. We will also learn, how updating your probability assessments in the light of new data is important for investment decisions.\nOn top of this directly practical considerations, we will also learn how this particular problem of updating probability assessments in the light of new information helps us close a conceptual gap in the pure frequency interpretations of probability.\nIn terms of R this lecture gives you a bit of a break in the sense that it applies what we have learned before and thereby reinforces what we have already learned. It will introduce some useful techniques and functions along the way.\nIn project 3 you will deepen your understanding of how probability updating works and how its sound understanding can make a real difference for the success of investment decisions.\nLecture 4 will introduce you more precisely to the key probabilistic model of random returns and other random phenomena in Finance: The concept of a random variable and the concepts of expected value, variance, standard deviation, covariance and correlation. We will discuss at length the modelling of random returns and learn about the basic workhorse model of asset price dynamics, the binomial lattice.\nThis discussion gives us also the appropriate context to discuss how to write programs in R and some of the principles and constructions that are most useful for this task.\nIn the project for lecture 4 you will learn about how you can apply the ideas of random variables and the binomial distribution to deepen your understanding of credit risk.\nThe final Lecture 5 discusses the concept of continuous random variables, which is very important for a large body of applied work. You will be introduced to the normal distribution and its power and limits for modelling random returns and financial risks.\nIn terms of R this lecture will provide a good context for discussing what matters when you want to make your R code fast and efficient.\nThe last project will then allow us to connect all the concepts of the five lectures with the project following lecture 4 to get a somewhat realistic and practically useful model for analyzing and managing credit risk.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "An Introduction to Probability",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to thank my friend and colleague Branko Urošević that he entrusted me to develop this course for his new and innovative program in computational finance.\nI am very grateful for the opportunity to make a contribution to this effort and for all the encouragement, support and advice I got from him for this project. Of course he bears no responsibility whatsoever for any shortcomings of these lecture notes.\nI would also like to sincerely thank the former students of the bootcamp probability course who gave me feedback and studied from the lecture. They were a fantastic and enthusiastic audience from who I learned a lot.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "An Introduction to Probability",
    "section": "References",
    "text": "References\nWhen you teach such a fundamental field as probability theory and its applications in Finance you teach from a core knowledge that has been built by many pioneers, experts and other teachers in this area. In this sense nothing contained in this course is original work. The only contribution a teacher can make is the way how the material is presented and combined. In this sense these lecture notes are nothing but a collection of well known and time tested material as well as an experiment in yet one more, hopefully innovative exposition.\nBut even in this field I have copied and recycled a lot from the teaching materials and collective wisdom of other colleagues whose examples and problems I found particularly helpful. Without any claim to completeness let me briefly go through the sources I have relied on most strongly when developing this course.\nWith respect to probability theory two sources have influenced me most and I have followed their example extensively in this course: William Fellers “An introduction to probability theory and its applications volume 1.” which first appeared in 1957. (Feller (1968)). While really old by now it is still in my view one of the most outstanding reference with respect to exposition and clarity of explanation as well as an inspiring source of enthusiasm for the subject. I just would like to mention as an aside that Feller (born Vilibald Srećko Feller) was a Croatian emigrant to first Germany and then the US in the 1930ies. You might claim him as a citizen of Croatia, former Yugoslavia or even Austria, since when he was born in 1906, what is now Croatia was still a part of the Austrian-Hungarian empire.\nMy second most important source for this course in terms of probability was Karl Schmedder’s course “An intuitive introduction to probabiliy”, which Karl developed for the online platform Coursera. I consider this course an outstanding didactical achievement and masterpiece. If you find this lectures fun, I can only encourage you to also do the course on Coursera, when you find time. It is for a reason that Karl’s course is the mostly viewed Coursera course from Switzerand.1\n1 See https://www.coursera.org/learn/introductiontoprobabilityIn terms of R and R programming I have followed and copied a lot from Garett Grolemund’s excellent book “Hands on programming with R”.(Grolemund (2014)) I encourage you to study this book. From it I have not only learned about one of the most excellent explanations of R concepts but also the insight that these concepts are best explained within a very concrete and non trivial application context rather than in a patchy series of toy examples.\nWith respect to Finance and investments I have relied on the excellent book by David Luenberger, “Investment Science”. (Luenberger (2009)) If you can get a hold of this fantastic work, I can only encourage you to get one and have it on your shelf. It is a role model of clarity of exposition and has much more to offer than we can cover in this book.\nFinally with respect to the history and the interpretations of probability, I have learned the most form the excellent book “10 great ideas about chance”, by Persi Diaconis and Brian Skyrms. (Diaconis and Skyrms (2019)) If you find probability interesting or even fascinating I encourage you to read this book at some stage, if you find time. It is full of highly interesting philosophical, mathematical, historical and fun facts and ideas about this very rich subject.\nBut now, lets get straight down to the matter at hand.\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nGrolemund, Garrett. 2014. Hands on Programming with R. O’Reilly. https://rstudio-education.github.io/hopr/.\n\n\nLuenberger, David. 2009. Investment Science. Oxford University Press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#leveraging-chatgpt-free-version",
    "href": "index.html#leveraging-chatgpt-free-version",
    "title": "An Introduction to Probability",
    "section": "Leveraging ChatGPT (Free Version)",
    "text": "Leveraging ChatGPT (Free Version)\n\nUsing an LLM to Enhance Your Learning\nThroughout this course, you’ll learn how to use ChatGPT to support your studies. As a free tool, it offers capabilities that can make learning probability more interactive and engaging. Here’s how you can leverage it:\n\nClarify Concepts:\nAsk ChatGPT to explain concepts in simple terms or elaborate on topics you find difficult:\n\n“Explain the concept of a probability distribution with an example.”\n\nGenerate Examples:\nUse it to create additional examples for practice:\n\n“Can you give me an example of a random variable and how it applies in finance?”\n\nDebug R Code:\nIf you encounter issues with your R code, paste the code and ask:\n\n“Why does this R code not run, and how can I fix it?”\n\nPractice Exercises:\nRequest custom exercises:\n\n“Create three exercises to practice calculating probabilities for dice rolls.”\n\nSimulate Interactive Discussions:\nSimulate discussions with ChatGPT to test your understanding:\n\n“I think the variance of a constant is zero. Am I correct? Explain why or why not.”\n\nLearn Best Practices in R:\nAsk for tips to improve your coding practices:\n\n“What are the best practices for writing clean and efficient R code?”\n\n\nImportant Note:\nWhile ChatGPT is a powerful learning tool, remember to verify the outputs, especially for complex calculations, and always critically evaluate its suggestions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#signing-up-for-chatgpt-free-version",
    "href": "index.html#signing-up-for-chatgpt-free-version",
    "title": "An Introduction to Probability",
    "section": "Signing up for ChatGPT (Free Version)",
    "text": "Signing up for ChatGPT (Free Version)\nTo access the free version of ChatGPT, follow these steps:\n\nGo to https://chat.openai.com in your web browser.\nClick on the “Creat a free account” button to create an account. You can sign up using an email address or your existing Google or Microsoft account.\nFollow the prompts to verify your email address and complete the registration process.\nOnce registered, log in to access ChatGPT. You can start using it immediately for your learning needs.\n\nUsing ChatGPT is free, but note that certain features or advanced versions (e.g., GPT-40) may require a paid subscription. The free version (GPT-3.5) is sufficient for getting a feeling of what the LLM can do. They payed version is more powerful.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#using-jupyter-notebooks-with-r",
    "href": "index.html#using-jupyter-notebooks-with-r",
    "title": "An Introduction to Probability",
    "section": "Using Jupyter Notebooks with R",
    "text": "Using Jupyter Notebooks with R\nIf you prefer working in Jupyter notebooks instead of RStudio, follow these steps to set up your system:\n\nInstall Jupyter Notebook:\nInstall Jupyter using Python’s pip package manager:\npip install notebook\nInstall R and the IRkernel:\nInstall R on your system, then install the IRkernel within R:\ninstall.packages(\"IRkernel\")\nIRkernel::installspec()\nStart Jupyter Notebook:\nLaunch Jupyter from your terminal:\njupyter notebook\nCreate a New R Notebook:\nIn Jupyter, click “New” and select “R” to start an R notebook.\nInstall R Packages in Jupyter:\nUse the usual R syntax to install packages:\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nJupyter provides a flexible, interactive environment for running R code, making it an excellent alternative to RStudio.\n\nWith these resources and tools, you are ready to dive into the fascinating world of probability and computational finance!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#leveraging-the-possibilities-of-chatgpt",
    "href": "index.html#leveraging-the-possibilities-of-chatgpt",
    "title": "An Introduction to Probability",
    "section": "Leveraging the possibilities of ChatGPT",
    "text": "Leveraging the possibilities of ChatGPT\nIn these lectures I try to help you building experience how to integrate large language models or LLMs into your learning and study process as well as to support you in coding and programming. There are several such models currently on the market. For this course I will use openAI’s chatGPT as a tool. chatGPT is available in a free version and in an enhanced pay-version. For the purposes of this course the free version is sufficient.\n\nUsing an LLM to Enhance Your Learning\nThroughout this course, you’ll learn how to use ChatGPT to support your studies. As a free tool, it offers capabilities that can make learning probability more interactive and engaging. Here’s how you can leverage it:\n\nClarify Concepts:\nAsk ChatGPT to explain concepts in simple terms or elaborate on topics you find difficult:\n\n“Explain the concept of a probability distribution with an example.”\n\nGenerate Examples:\nUse it to create additional examples for practice:\n\n“Can you give me an example of a random variable and how it applies in finance?”\n\nDebug R Code:\nIf you encounter issues with your R code, paste the code and ask:\n\n“Why does this R code not run, and how can I fix it?”\n\nPractice Exercises:\nRequest custom exercises:\n\n“Create three exercises to practice calculating probabilities for dice rolls.”\n\nSimulate Interactive Discussions:\nSimulate discussions with ChatGPT to test your understanding:\n\n“I think the variance of a constant is zero. Am I correct? Explain why or why not.”\n\nLearn Best Practices in R:\nAsk for tips to improve your coding practices:\n\n“What are the best practices for writing clean and efficient R code?”\n\n\nImportant Note:\nWhile ChatGPT is a powerful learning tool, remember to verify the outputs, especially for complex calculations, and always critically evaluate its suggestions. One such tool you might consider for cross checking is Wolfram | Alpha, which you can find here: https://www.wolframalpha.com/ in a free version-",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#using-an-llm-to-enhance-your-learning",
    "href": "index.html#using-an-llm-to-enhance-your-learning",
    "title": "An Introduction to Probability",
    "section": "Using an LLM to Enhance Your Learning",
    "text": "Using an LLM to Enhance Your Learning\nIn these lectures I try to help you building experience how to integrate large language models or LLMs into your learning and study process as well as to support you in coding and programming. There are several such models currently on the market. For this course I will use openAI’s chatGPT as a tool. ChatGPT is available in a free version and in an enhanced pay-version. For the purposes of this course the free version is sufficient.\nThroughout this course, you’ll learn how to use ChatGPT to support your studies. As a free tool, it offers capabilities that can make learning probability more interactive and engaging. Here’s how you can leverage it:\n\nClarify Concepts:\nAsk ChatGPT to explain concepts in simple terms or elaborate on topics you find difficult:\n\n“Explain the concept of a probability distribution with an example.”\n\nGenerate Examples:\nUse it to create additional examples for practice:\n\n“Can you give me an example of a random variable and how it applies in finance?”\n\nDebug R Code:\nIf you encounter issues with your R code, paste the code and ask:\n\n“Why does this R code not run, and how can I fix it?”\n\nPractice Exercises:\nRequest custom exercises:\n\n“Create three exercises to practice calculating probabilities for dice rolls.”\n\nSimulate Interactive Discussions:\nSimulate discussions with ChatGPT to test your understanding:\n\n“I think the variance of a constant is zero. Am I correct? Explain why or why not.”\n\nLearn Best Practices in R:\nAsk for tips to improve your coding practices:\n\n“What are the best practices for writing clean and efficient R code?”\n\n\nImportant Note:\nWhile ChatGPT is a powerful learning tool, remember to verify the outputs, especially for complex calculations, and always critically evaluate its suggestions. One such tool you might consider for cross checking is Wolfram | Alpha, which you can find here: https://www.wolframalpha.com/ in a free version-",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-lecture1.html",
    "href": "01-lecture1.html",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1 Tossing a fair coin: First Probability ideas.\nLets start with a classic and basic example of a probability model: Tossing a fair coin. When probability theorists talk of a fair coin they mean that the coin has equal chances of coming up heads as coming up tails. When the coin is tossed it is uncertain whether it will ultimately come up Heads or Tails.\nFigure 1.1: Figure 1: Tossing a coin\nFigure 1.1 visualizes such a process.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#rolling-a-die",
    "href": "01-lecture1.html#rolling-a-die",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1.1 Random experiments, sample space, events\nIn the theory of probability a process leading to an uncertain outcome is called a random experiment. The example of throwing a die helps us to give a precise meaning to the notion of an uncertain outcome. While we can imagine in principle that the die lands on one of its corners and this is one outcome, we agree on the outset that we are going to consider the (practically relevant cases) that it will land on one of the faces.\nThe collection of all possible outcomes of a practical or conceptual random experiment is called in probability theory a sample space. While the sample space of throwing a die is an idealization it is exactly this idealization which simplifies the theory without affecting its applicability. The basic outcomes in the random experiment of throwing a die are that the die lands such that the upward showing face shows a 1 a 2, 3, 4, 5 or a 6. In the theory the collection of basic outcomes is denoted as a set. Thus the sample space of throwing a die is given as the set \\({\\cal S} = \\{1,2,3,4,5,6\\}\\).\nThe sample space \\({\\cal S}\\) is the set of all basic outcomes. The subsets of the sample space are often called events in probability theory. An event could be - for example - an outcome where the die shows an upward looking face with an even number \\(A = \\{2,4,6\\}\\).\n\n\n1.1.2 The role and nature of idealizations in applications\nIdealizations of the kind we discussed for the throw of a die are standard in probability and we will encounter them again and again. For example the movement of a stock price is often though of as a conceptual random experiment. When we try to agree on what is the appropriate sample space of this experiment, we can say that the price can not fall below 0 but it is hard to agree on what will be the highest possible price. In the probabilistic treatment of stock prices, which we will discuss later in our course, it is for instance common to assume that the sample space of this random experiment is the entire interval of non-negative real numbers \\({\\cal S} = [0, \\infty)\\).\nMany of us would hesitate to claim that the price might rise without bound. Yet many models in applied Finance are based on such an assumption. The models allow arbitrary price hikes but with arbitrary small probability as the price gets higher and higher. Practically it does not make sense to believe that a security price can become arbitrarily high. The use of arbitrarily small probabilities in a financial model might seem absurd but it does no practical harm and makes the model simple and convenient to use. Moreover, if we seriously introduced an upper bound on a security price at \\(x\\) it would be also awkward to assume that it is impossible that it could be just a cent higher, an assumption equally unappealing than assuming it can get in principle arbitrarily high.\n\n\n1.1.3 Classical Probability: Measuring uncertainty\nProbability is a measure of how likely an event of an experiment is. But how could we measure chance? Here is the first big idea of probability theory and how the originators thought about it.1 How do you measure anything? If you think of length, for example, you take an arbitrary standard you agree on and then count. The originators of probability theory pursued the same idea with chance: To measure probability you choose equally probable cases and then count.\n1 In this discussion I use chapter 1 in Diaconis and Skyrms (2019)The probability of an event \\(A\\) according to the originators of the theory of probability is then \\[\\begin{equation}\nP(A) = \\frac{\\text{Number of cases where $A$ occurs}}{\\text{Total number of cases}}\n\\end{equation}\\]\nThus, if we want to know the probability of the die ending up on a face such that an even number is shown (2,4,6) we have to compute according to this notion \\(3/6\\) (since the total number of cases are six basic outcomes) which is a chance of \\(50 \\%\\).\nNote that this classical notion of probability has a few interesting consequences, which we will discuss in more detail later but which already now naturally flow from this basic idea of measuring chance.\n\nProbability is never negative.\nIf an event \\(A\\) occurs in all cases \\(P(A)=1\\).\nIf \\(A\\) and \\(B\\) never occur in the same case, then \\(P(A \\,\\text{or}\\, B) = P(A) + P(B)\\).\n\nIn particular then the probability that an event does not occur is 1 less the probability that it occurs: \\(P(\\text{not}\\, A) = 1 - P(A)\\).\nLet us interrupt here our discussion of probability for a moment and ask how we can make good on our promise to make these ideas tangible, so we can play with them.\nFor this we will need the computer. Since we will talk to the computer in R, it is now the right time to look at the die rolling example from the perspective of R and dive into some of its basic concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#rolling-die-on-computer",
    "href": "01-lecture1.html#rolling-die-on-computer",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.2 Rolling a die on the computer: First steps in R",
    "text": "1.2 Rolling a die on the computer: First steps in R\n\n1.2.1 The R User Interface\nBefore we can ask our tool to do anything for us, we need to know how to talk to it. In our case RStudio allows us to talk to our computer. It works like any other application. When you launch RStudio on your computer, in figure @ref(fig:rstudio-start-screen) you see a screen looking like this:\n\n\n\n\n\nThe RStudio startup screen\n\n\n\n\nIn this picture you see a screenshot of my RStudio screen. Interacting with the app is easy. You type commands via your keyboard at the prompt, which is the &gt; symbol. You find this symbol in the RStudio pane called Console. You can see it in the left pane in the screenshot. You send the command to the computer by pressing enter. After you have pressed enter, RStudio sends the command to R and displays the result of your command with a new prompt to enter new commands, like this:\n\n1+1\n\n[1] 2\n\n\nLet me pause to explain what you see here in this code example. First you see a light-gray box containing the command 1+1. This is an instance of a so called code chunk. Moving the cursor to the right upper corner of the chunk, will display a copy icon and you can click this icon to copy the code to the computer’s clipboard and then paste it at the prompt of your console (or for that matter anywhere else) and run it in R studio, if you wish. In the code-chunk the R-prompt &gt; is not displayed. This symbol of the prompt is only shown in the Console itself.\nWhen the code is executed, you see the result in the second light-gray box, just below the first one. It starts with a double hash ##, indicating that it shows an output of running the above code chunk and then displays the output as it would appear in the command window [1] 2.\nThe [1] means that the line begins with the first value of your result. For example, if you enter the command 20:60 at the prompt of your console which means in the R language, “list all the integers from 20 to 60” and press enter you get:\n\n20:60\n\n [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[26] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nmeaning that 20 is the first value displayed in your result. Then there is a line break because not all values can be displayed on the same line and R tells you that 45 is the 26-th value of the result.\nThe colon operator : is a very useful function in R which we will need often. It allows us to create sequences of every integer between two given integers.\nR needs a complete command to be able to execute it, when the return key is pressed. Lets see what happens, if a command is incomplete, like for instance &gt; 5*.\nIn this case R will show the expression followed by a + instead of showing a new prompt. This means that the expression is incomplete. It expects more input. If we complete the expression, say like\n&gt; 5*\n+ 4\nthe expression can be evaluated and a new prompt is shown in the console.\nIf you type a command that R does not understand, you will be returned an error message. Don’t worry if you see an error message. It just is a way the computer tells you that he does not understand what you want him to do.\nFor instance, if you type 5%3 you will get an error message like this\n&gt; 5%3\nError: unexpected input in \"5%3\"\n&gt;\nSometimes it is obvious why a mistake occurred. In this case, that R just does not know what to do with the symbol %. It has no meaning in this context. Sometimes it is not so obvious what the error message actually means and what you might do about it.\nA useful strategy in this case is to type the error message into a search engine and see what you can find. The chance is very high that others encountered the same problem before you and got helpful advice how to fix it from other users on the internet. One site, we find particularly helpful for all kinds of questions related to R and R programming is https://stackoverflow.com/. Try it at the next opportunity.\nNow with this basic knowledge, we can already make the first step to create a die on the computer using R. If you think of a physical die, the essential thing that matters are the points on its six sides. If you throw the die it will usually land on one of these sides and the upward showing side of the die shows the number of points. The colon operator : gives us a way to create a group of numbers from 1 to 6. R gives us the result as a one dimensional set of numbers.\n\n1:6\n\n[1] 1 2 3 4 5 6\n\n\nLets use these first steps in R to recap the probability concepts we have learned using this example of the six sided die: A basic outcome of rolling a six-sided die is for example 6 if the upper side after rolling the die happens to be the side with 6 points. The sample space of the experiment of rolling a six-sided die is the set \\({\\cal S} = \\{1,2,3,4,5,6\\}\\). In probability theory we often use the symbol \\({\\cal S}\\) or \\(S\\) for sample space. In many probability texts the sample space is also often denoted by the symbol \\(\\Omega\\) the Greek letter for (big) Omega. A random experiment in this example is the rolling of the die. The outcome is uncertain but once the die is rolled the outcome can be determined precisely. The event that the outcome is a display of 10 points is the empty set \\(A = \\emptyset\\). The symbol \\(\\emptyset\\) comes from set theory and means the set containing no elements. This event can contain no elements because we can not get a score of 10 by rolling a six sided die.\n\n\n1.2.2 Objects\nYou can save data in R by storing them in objects. An object is a name, you can choose yourself to store data. For example, if you choose to store the value 6 in an object called point_six, you would type:\n\npoint_six &lt;- 6\n\nat the prompt. R will the store the value 6 in the object called point_six, which you can use to refer to the value. If you type the name of your object at the prompt, R will display the value you have assigned. A useful key combination for typing the assignment operator &lt;- is to use the key combination ALT _. At the R prompt R will automatically print an assignment operator.\nNow if you type the name of the object and press enter, R will display the value or values stored in it, like this:\n\npoint_six\n\n[1] 6\n\n\nNow you can use the name of the object to refer to its value. For instance, you could divide point_sixby 2and get a meaningful result\n\npoint_six/2\n\n[1] 3\n\n\nNow to make our die more tangible and useful, let us store it in an R object by typing the following command at the prompt. This command creates an object with name die and assigns the vector 1,2,3,4,5, 6 to it.\n\ndie &lt;- 1:6\n\n\n\n\n\n\nThe RStudio Environment pane keeps track of the objects you have created\n\n\n\n\nYou can now see in the right upper Environment pane in figure @ref(fig:rstudio-screen-with-command) that R shows you that there is an object with the name die that it consists of integers 1,2,3,4,5. As you create more objects they will be stored in the Environment pane and are ready for your reference, unless you delete them. You can remove or delete an object by typing rm(object) or by assigning the value die &lt;- NULL which would also remove the object from your environment or workspace.\nYou can name your objects almost anything with a few exceptions. An object name must not start with a number. There are some special symbols which can also not be used in object names, like ^, !, $, @, +, -, /, *. Note that R is case sensitive and distinguishes small and big letters. If you assign a new value for an object you have already created, R will overwrite the object without warning.\nYou can see which objects are currently created and available for you in the Environment pane of your session of by typing ls(). The UNIX users among you will recognize this command from the unix shell, where it displays the files in a directory.\nBefore we learn how we can actually roll our die and perform a random experiment with it, let us briefly use the opportunity to explain a few things about how R does computations. We have already explained that we can use the object name to refer to the value. So for instance if we type\n\ndie*die\n\n[1]  1  4  9 16 25 36\n\n\nThis might irritate some of you because we have called the object a vector. In linear algebra multiplication of vectors is only allowed if there is an inner product. What happens here, if we use * the multiplication operator is that R does an element-wise multiplication of the six numbers of our die. Of course R allows to take an inner product as well, but this needs a different operator. To compute an inner product, we would type\n\ndie %*% die\n\n     [,1]\n[1,]   91\n\n\nNow R displays the result as a vectors with one row and one column, which is denoted in the output by [ , 1] for the column and [1, ] for the row. We will learn later more about the use and the meaning of this notation in R.\nThe element wise execution R usually uses also means that when you, for example type\n\ndie - 1\n\n[1] 0 1 2 3 4 5\n\n\nR would subtract 1 from every component in the vector die.\nAnother specific behavior of R, you need to know about is called recycling. If you give R two vectors of different length in an operation, R will repeat the shorter vector as long as it is of equal length with the longer one. For example, if you have: ::: {.cell}\ndie + 1:2\n\n[1] 2 4 4 6 6 8\n\n:::\nyou see that R adds 1 to 1 and 2 to 2 and then starts over again by adding 1 to 3 and 2 to 4 and then starts over once again by adding 1 to 5 and 2 to 6. If the longer vectors is not a multiple of the shorter one, R recycles but the cuts off. It will give you a warning though. ::: {.cell}\ndie + 1:4\n\nWarning in die + 1:4: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 2 4 6 8 6 8\n\n:::\nWhile this might seem awkward to some of you, we will see that for data manipulation element-wise execution is often extremely useful. It allows to manipulate groups of values in a systematic yet easy way.\n\n\n1.2.3 Functions\nR contains many functions which we can use to manipulate data and compute things. The syntax for using a function is very simple: You type the function name and put the value of the function argument in parentheses. Here we use for illustrations the function of the square root sqrt(): ::: {.cell}\nsqrt(4)\n\n[1] 2\n\n::: or rounding a number: ::: {.cell}\nround(3.1415)\n\n[1] 3\n\n::: The data you write in the parentheses are called the function arguments. Arguments can be all sorts of things: raw data, R objects, results from other functions.\nIf functions are nested, R evaluates the innermost function first and then goes on to the outer functions. To see examples of all these instances you can take ::: {.cell}\nmean(1:6)\n\n[1] 3.5\n\nmean(die)\n\n[1] 3.5\n\nround(mean(die))\n\n[1] 4\n\n::: for example.\nFor simulating random experiments, R has the very useful function sample(). With this function we can roll our die on the computer and conduct actual random experiments.\nThe function takes as arguments a vector names x and a number named size. sample will return size elements randomly chosen from the vector x. Lets say: ::: {.cell}\nsample(x = 1:4, size = 2)\n\n[1] 2 1\n\n:::\nIn this case sample has chosen 4,1 from the vector x = (1,2,3,4) at random.\nIf we want to roll the die in our computer we can thus pass the die as an argument to sample and use the number 1 for the size argument. Lets do a few rolls with our die ::: {.cell}\nsample(x = die, size = 1)\n\n[1] 3\n\nsample(x = die, size = 1)\n\n[1] 2\n\nsample(x = die, size = 1)\n\n[1] 6\n\nsample(x = die, size = 1)\n\n[1] 5\n\n::: These are the random outcomes of our consecutive rolls. It is as if we had thrown an actual die but in this case we have done the same thing on the computer. Isn’t it cool that this is possible at all? The sample() function will remain our good friend throughout this course.\nR functions can have many arguments, but they need to be separated by a comma.\nEvery argument in every function has a name. We specify which data are assigned to the arguments by setting a mathematical equal sign = between the name and the data. Names help us to avoid passing the wrong data and thereby mixing up things or committing errors. But using names is not necessary. If we just wrote ::: {.cell}\nsample(die,1)\n\n[1] 3\n\n::: R would also know what to do. It is not always clear which names to use for a function. If you are not sure, you can use the function args() to look it up. Here we take the function roundas one example. ::: {.cell}\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n::: Note that the digits argument in the round function is already set to 0. Frequently R functions come with optional arguments. These arguments are optional because the come with a default value, which is 0 in case of the round function.\nWe recommend that you write out argument names as a rule. It gives clearer code and avoids errors. If you don’t write argument names, R matches your values to the arguments of the function by order.\n\n\n1.2.4 Writing your own functions\nNow we are ready to write our own function to roll the die in our computer. Each function in R has the same elements: A name, a function body of code and a set of arguments. To write your own function, you have to write up all of these parts and save them in an R object. The syntax is:\nmy_function &lt;- function() {}\n\nThe name here is my_function, next comes the expression function() which needs to be assigned. The names of the function arguments have to be written between the parentheses. Then we have to write the actual code within the braces {}.\nTo do this for the die, lets write a function named roll_die. ::: {.cell}\nroll_die &lt;- function(){die &lt;- 1:6 \n                         sample(die, size = 1)}\n:::\nNow we can roll our die for a few times to show how the function works ::: {.cell}\nroll_die()\n\n[1] 3\n\nroll_die()\n\n[1] 1\n\nroll_die()\n\n[1] 1\n\nroll_die()\n\n[1] 6\n\nroll_die()\n\n[1] 5\n\n::: Note that in our function roll_die() has no arguments, just the function body. This is perfectly legitimate in R. It is important that when we call the function we have to call it with the parenthesis like roll_die(). If we only call the name roll_die, R will display the code in the function body.\nA final remark in the sample function is in place here. If we look at the arguments of sample using the args function we see ::: {.cell}\nargs(sample)\n\nfunction (x, size, replace = FALSE, prob = NULL) \nNULL\n\n::: Lets do not discuss all the details of this output but concentrate for a moment on the replace argument. What does this mean?\nAs we saw previously we can use the sample function to model the rolling of our die. If we set the size argument to 1 we get the roll of one die. If we set the size argument to n, we would simulate the rolling of n dies. But now the replace argument becomes crucial. As we can see in the output of the args function replace has a default value FALSE. This is a logical argument. It tells R, for example, that if we set size = 2, meaning that two dice are rolled, if the first dice shows, say a value of 3, the second die cannot show 3 as well.\nThis is clearly not what we have in mind when we model the rolling of 2 dice. It should be possible that both dies show the same value. To enable this behavior of the sample function, we have to change the default value of the replace argument to TRUE. Then R chooses a random draw from all of the six possible values for all dice rolled.\nCongratulations ! You have written your first R function for conducting a simple random experiment. Let me remind you once again: Think of the parentheses as a trigger that tells R to run the function. If you omit the trigger R just prints the body of the function. When you run a function, all the code in the function body is executed and R returns the result of the last line of code. If the last line of code does not return a value neither will R.\n\n\n1.2.5 Arguments\nImagine we remove the first line of code in our function body and changed the name die in the sample function to “ball”. ::: {.cell}\nroll_die2 &lt;- function(){sample(ball, size = 1)}\n::: If we call the function now, we will get an error. The function call roll_die2() will result in the error message Error in sample(ball, size = 1) : object 'ball' not found (try it!)\nWe could supply ball when we call roll_die2 if we make ball an argument of the function. Lets do this: ::: {.cell}\nroll_die2 &lt;- function(ball){sample(ball, size = 1)}\n::: Now the function will work as long as we supply ball when we call the function. ::: {.cell}\nroll_die2(ball = 1:6)\n\n[1] 3\n\n::: Note that we still get an error, if we forget to supply ball argument. This could be avoided if we give the function a default argument ::: {.cell}\nroll_die2 &lt;- function(ball= 1:6){sample(ball, size = 1)}\n::: Now if we type: ::: {.cell}\nroll_die2()\n\n[1] 1\n\n::: everything works, just as intended.\n\n\n1.2.6 Scripts\nSo far we have worked by interacting with the console. But what if you want to edit your functions? It would be much easier, if you could use a draft of your code and work form there. This can be done by using a script.\nYou create a script by going to File &gt; New File &gt; R script in the menu bar of RStudio. Using scripts is the standard way to write code in R. It not only helps you to keep track of your code, save it and edit it later. It also makes your work reproducible. You can edit and proofread your code and share it with others. To save your script go to File &gt; Save As in the menu bar.\n\n\n\n\n\nThe RStudio Script\n\n\n\n\nRStudio has many useful features to help you work with scripts. You can for instance automatically execute a line in a code by using the run button. You can also execute sections of code or the entire script. The entire script is executed by running the Source button. For all these commands there are key short cuts which you will learn as you work more with RStudio and R.\nFrom now on you should write all your R-code for this course in scripts. Make a new folder for the scripts you write and store your R-code there from now on.\n\n\n1.2.7 Using packages and finding Help\nWe have now a function which we can use to simulate the rolling of a die, roll_die(). If the die is fair it should be the case that if we roll the die often, all numbers should occur about equally often. The die should not be weighted in favor of a particular value.\nOne way to learn whether our die is fair are repetition and visualization. These are tools we will need all the time, when working with data and when doing probability. While R has many useful functions, one of the great powers is that R is constantly extended by a huge community of users by providing packages.\nPackages are add on functions, which will not be available when you install R. They need to be installed and loaded before you can use them. Since packages are such a powerful tool in R we need to introduce what they are and how to use them in the beginning.\n\n\n1.2.8 Packages\nThere are many visualization tools in R that come with the basic installation. Since the point we want to make here is about packages, we will use a visualization function which is part of the add on package ggplot2, a very popular package for making all kinds of graphs. ggplot2 is not part of the base-R distribution. It has to be therefore loaded before we can use it.\nThere are many additional functions provided with this package. Indeed you could run a whole course on just that 2 For the point we want to make here we will use just one of them, called qplot(), a shorthand for for quick plot.\n2 A very good starting point is for instance Healy (2019), in case you are interested. Also a good source is the first chapter in Wickham and Grolemund (2017).Since qplot() is a function in the package ggplot2, we first need to install this package. To install a package you need to be connected to the internet. If you have internet connection go to the command line and run at the command line: install.packages(\"ggplot2\").\nR displays what is happening while executing the command. Don’t worry if you don not know what all of these messages exactly mean and don’t panic that they are displayed in red. All packages can be installed like this. You have just to enter the correct name in the function install.packages(). The lower right pane in the RStudio software alternatively provides a tab called Packages. Using this tab, you can also install R packages interactively by clicking the install button on the upper left corner of the Packages tab.\nAfter installation the package is on our hard-drive but it can not yet be used. To use the package it has to be loaded. This is done with the command library. To load the ggplot2 package we type ::: {.cell}\nlibrary(\"ggplot2\")\n::: and hit the return key. Many things could be said about the R package system and you will learn it in more detail as we go along in our course. For the moment the most important thing to remember is that a package needs to be newly loaded whenever you want to use it in a new R session.\nTo check whether our dies is fair, we need to roll it many times. R provides a function, that does this for us. This function is called replicate() and provides an easy way to repeat a command many times. The number of times we want to repeat something is given as an argument to the replicate function.\nNow lets roll our die 100 times and save the result in an object we call rolls: ::: {.cell}\nrolls &lt;- replicate(100, roll_die())\n::: We now use the qplot()function from the ggplot2 library to make a quick visualization, by typing the command qplot(rolls). I include the resulting picture in the following figure ::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: ::: If you try this and your picture looks a bit different from mine, don’t worry. This is as things should be. I have stored the picture after one particular run of the replicate function and saved it in my figures folder to these lecture notes. Each time you run the replicate function anew a new random outcome is stored which will look not exactly the same as any other or course. This is what randomness does to the outcome of rolling a die.\nIt looks like every value occurred roughly 16 times but there is still quite some variation. For instance, the value 3 seems to occur more than 20 times whereas the value 2 occurs less than 15 times.\nMaybe we have to give it another trial with more replications. With the computer we can do this with a fingertip. Let us roll our die 10000 times and plot the result.\n&gt; rolls2 &lt;- replicate(10000, roll_die())\n&gt; qplot(rolls2m bindwidth = 1)\nNow the picture looks better.\n\n\n\n\n\nFrequencies of 1, 2, 3, 4, 5, 6 after rolling our virtual die 10000 times\n\n\n\n\nWe will learn in our course why the picture get better as we increase the number of rolls of the die. For the moment, let us just state that at least for this case it seems that there is no significant visual evidence that our virtual die is loaded.\n\n\n1.2.9 Getting Help\nWe have now learned a tiny number of R functions and we have written one function ourselves. We have learned how to make use of functions provided by packages.\nIt would be overwhelming to memorize and learn them all. In R, fortunately, every function comes with a detailed documentation and with its own help page. You need to learn how to use this source right from the beginning.\nTo access the help page of a function you type the function name preceded by a question mark at the prompt, like this ::: {.cell}\n?sample\n:::\nThen, after you have pressed the return key, a help page is opened in the right lower pane under the help tab as shown in @ref(fig:helpinr) ::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\nThis help page has a particular structure that you will find for every other R function no matter whether it is provided by the base installation or by a package. In the upper left corner you see the name of the function (sample) and in curly brackets next to it the term base, which means that this is a function in the R base installation. Then you see a headline about what the function does.\nFrom the top of the page, you then first see the field Description. This is a short description what the function does. Here it says\nDescription\nsample takes a sample of the specified size from the elements of x using either with or without replacement.\nThe next field is Usage. It gives you the function description with the arguments. Here for example\nUsage\nsample(x, size, replace = FALSE, prob = NULL)\n\nsample.int(n, size = n, replace = FALSE, prob = NULL,\n           useHash = (!replace && is.null(prob) && size &lt;= n/2 && n &gt; 1e7))\n           \nThe first line in Usage should by now be familiar. Don’t worry about the second line. The function can obviously do more than we know so far.\nNext comes a list of arguments the function takes and what type of information R expects you to provide, as well as what R will do with this information. Here it says for example\nArguments\nx   \neither a vector of one or more elements from which to choose, or a positive integer. See ‘Details.’\n\nn   \na positive number, the number of items to choose from. See ‘Details.’\n\nsize    \na non-negative integer giving the number of items to choose.\n\nreplace \nshould sampling be with replacement?\n\nprob    \na vector of probability weights for obtaining the elements of the vector being sampled.\n\n...\n\nWe omit some information here.\nThen comes a field called Details which gives a more in-depth description of the function. The next field is called Value. It describes what the function returns when you run it. Then we have a reference to related R functions under the field See. Finally there is a field called Examples. This field contains example code that is guaranteed to work. It shows a couple of different cases how you can use the function in practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#coincidences",
    "href": "01-lecture1.html#coincidences",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.3 Application: Coincidences and the Blockchain",
    "text": "1.3 Application: Coincidences and the Blockchain\n\n1.3.1 The birthday problem\nIn this section, we will apply our foundational knowledge of probability - which is at the moment just the informal and intuitive notion of counting equally probably cases - to a fascinating real-world problem known as the birthday problem:\nThe starting question in the birthday problem is: What is the probability that at least two people in a room share the same birthday, when we neglect things like leap years, and when we assume that birthdays on any day of the year are equiprobable and the birthdays of the people in the room are independent. We have no twins for example.\nPerhaps some of you have already seen this problem before. If not, the result may surprise you. Even if you saw the birthday problem before, perhaps not many of you are aware that the significance of this problem reaches far beyond recreational math puzzles. This is because the birthday story is of course accidential. The mathematical formulation of the problem will reveal through the process of abstraction that this problem is at the heart a problem about the probability of coincidences. As such it has real world connections to cryptography and the blockchain security.4\n4 We follow here the discussion in Diaconis and Skyrms (2019)Through this problem, we will:\n\nExplore how to frame probability questions mathematically.\nUse basic R functions to compute probabilities.\nLearn how to simulate experiments to approximate probabilities. This problem is not only an entertaining mathematical puzzle but also a stepping stone to understanding its applications in cryptography and blockchain security.\n\nFirst, observe that we took our assumptions such that we are in the frame of classical probability like the originators of the field thought about it.\nNow for the sake of familiarizing ourselves with the new concepts, let us try to map the birthday problem into the probability notions we learned so far.\nThe sample space is the set of possible outcomes of the experiment. The sample space for our problem includes all possible combinations of birthdays for the group. For example, if there are 3 people, each with a birthday on any of 365 days, the sample space contains all possible sequences of three birthdays:\n\\((1,1,1),(1,1,2),…,(365,365,365)\\)\nIn general, for \\(n\\) people, there are \\(365^n\\) possible combinations, as each person’s birthday is independent of the others.5\n5 Mathematically you can describe the sample space for the birthday problem as a set with \\(365^n\\) ordered \\(n\\)-tuples or sequences of birthdays: \\({\\cal S} = \\{x | x \\in 365^n \\}\\) This is the sample space of this experiment, written as in set theoretic notation. It means the sample space is the set of all ordered \\(n-tuples\\) from the Cartesian product \\(365^n\\). Remember that in mathematics, a Cartesian product is a mathematical operation that returns a set (or product set or simply product) from multiple sets (in this case the sample space, \\({\\cal S}\\). That is, for sets \\(A\\) and \\(B\\), the Cartesian product \\(A \\times B\\) is the set of all ordered pairs \\((a,b)\\) where \\(a \\in A\\) and \\(b \\in B\\).Now given this sample space we can assign a probability to the event that two people chosen at random will have the same birthday. The denominator will this be \\(365^n\\). The nominator for this probability will be much more straightforward to figure out, if we compute the complement. We discussed before that the probability of the complement of an event is 1 minus the probability of the event: \\(P(\\text{not} A) = 1 - P(A)\\).\nLet \\(A\\) denote the event that no two people share the same brithday. We can then use the complement rule to compute the probability of at least two people sharing a birthday: Compute the probability that no two people share a birthday. Subtract this probability from 1 to get the probability of at least one match.\nHere’s how we compute the probability of no matches:\n\nThe first person can have any birthday \\((365/365)\\).\nThe second person must avoid the first person’s birthday \\((364/365)\\).\nThe third person must avoid the first two birthdays \\((363/365)\\).\n\nNow continue this reasoning for \\(n\\) people.\nThe final probability is:\n\\[\\begin{equation*}\nP(\\text{at least two people share birthday}) =\n1 - \\frac{365 \\times 364 \\times 363 \\cdots \\times 365 - n +1}{365^n}\n\\end{equation*}\\]\nNow there is an additional thing we did not yet introduce and which we will learn about in the next lecture. This refers to our assumption that the individual birthdays are independent. For the moment you need to take the following fact on faith: The probability of independent events is the product of the probability these events.\nNow comes the surprise for those of you who did not already see the birthday problem. Assume the group size of people is 23. Let us compute the birthday coincidence probability. For this computation we make use of the R function prod() which takes a vector of numbers as input and computes their product. For \\(n=23\\) we need the list of numbers from 365 to 343. Now we can use the : operator and take advantage from one of its cool properties. If we formulate the command as 365:343 then the colon operator will give us a descending list of integers starting at 365 and ending at 343. Then the probability can be computed as\n\n1 - prod(365:343)/365^23\n\n[1] 0.5072972\n\n\nWe have used the operator ^ which is needed to compute powers. Now we see from our computation that with 23 people the probability is already larger than 50 %. Quite stunning. You can now verify yourself that with 50 people this probability is already at 97 %.\nLet’s visualize how the probability of at least one shared birthday grows with the size of the group. This time I will use base R’s plotting functionality and not use the ggplot2 package.6\n6 I used ggplot2 before to explain the concept of a package and how to load it to enhance R’s functionality. Base R containts a lot of powerful plotting functions which work independently of gglot2.The plot below shows that the probability exceeds 50% with just 23 people and quickly approaches 100% as the group size increases.\n\n# Define the birthday_collisions function\n\nbirthday_collisions &lt;- function(n) {\n  1 - prod(365:(365 - n + 1)) / 365^n\n}\n\n# Generate data for group sizes\n\ngroup_sizes &lt;- 1:50\ncollision_probs &lt;- sapply(group_sizes, birthday_collisions)\n\n# Plot the probability of at least one shared birthday\n\nplot(group_sizes, collision_probs, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Group Size\", ylab = \"Collision Probability\",\n     main = \"Probability of At Least One Shared Birthday\")\nabline(h = 0.5, col = \"red\", lty = 2)  # Highlight the 50% threshold\n\n\n\n\n\n\n\n\nI will not go in detail into the explanation of the code at this stage. For those of you who crave for an explanation, please look at the marginal note here or use the LLM for a step by step explanation.7\n7 birthday_collisions Function: This function calculates the probability of at least one shared birthday for a group of nn people using the formula derived earlier. group_sizes and collision_probs: group_sizes defines a range of group sizes (1 to 50). sapply applies the birthday_collisions function to each group size, computing the collision probabilities. Visualization: A line plot (type = \"l\") is used to visualize how the collision probability increases with group size. A horizontal red dashed line (abline) highlights the 50% threshold for better visual context.\n\n1.3.2 Determining birthday collisions by simulation\nInstead of deriving the probability analytically, we can simulate the birthday experiment if we use the relative frequency notion of probability which we informally had used in our visualization of coin tosses. Here’s how:\n\nGenerate random birthdays for nn people using sample(1:365, n, replace = TRUE).\nCount how often at least two people share a birthday using the duplicated() functionin combination with the logical function any().\nRepeat the simulation many times (e.g., 10,000) using replicate() to estimate the probability.\n\n\nsim_birthday &lt;- function(n, trials = 10000) {\n  results &lt;- replicate(trials, {\n    birthdays &lt;- sample(1:365, n, replace = TRUE)\n    any(duplicated(birthdays))\n  })\n  mean(results)\n}\n\nLet me give a step-by-step breakdown of this piece of code.\n\nDefining the Function:\n\nThe function sim_birthday has two arguments\n\nn: The number of people in the room (group size).\ntrials: The number of simulation repetitions (default is 10,000). These parameters allow us to customize the simulation for different group sizes and levels of precision.\n\n\nUsing replicate to Run Simulations:\nThe replicate() function repeats an experiment a specified number of times (trials). Each experiment simulates one instance of \\(n\\) people in a room.\nSimulating Birthdays:\nThe sample() function generates \\(n\\) random birthdays (integers between 1 and 365) with replacement, representing the assumption that birthdays are independent and uniformly distributed. This step directly reflects the assumptions made in the analytical solution:\n\nThe sample space consists of 365 possible birthdays.\nSampling is done with replacement because multiple people can share the same birthday.\n\nChecking for Duplicates:\nThe duplicated() function identifies repeated birthdays in the sample. The any() function checks if there is at least one duplicate. If duplicates exist, this corresponds to a “birthday collision.” This step operationalizes the concept of finding the complement (at least one collision) rather than calculating the probability of all unique birthdays.\nCalculating the Probability:\nThe results vector contains TRUE (collision occurred) or FALSE (no collision) for each trial. Taking the mean() of this logical vector calculates the proportion of trials with a collision, which corresponds to the simulated probability. This step ties back to the definition of relative frequency probability introduced earlier: the probability of an event is the ratio of favorable outcomes to the total number of trials.\n\nThis step requires some explanation of how R uses data types. We will learn about this more systematically in the next lecture. Here I give a quick and superficial explanation. R can force logical data types automatically to a numerical type when it applies arithmetic functions to a logical vector, forcing TRUE to 1 and FALSE to 0. Now, when you take the average (mean()) of a vector containing only 1 and 0 values you will get the proportion of 1’s or TRUE values. We will use such tricks often during the course of the lecture. For the moment I ask you to bear with this quick and superficial explanation of why this way of proceeding does what we want.\nLet’s compute the birthday collision probability for 23 (as in our analytical solution) and 50 people:\n\nsim_birthday(23)\n\n[1] 0.5111\n\nsim_birthday(50)\n\n[1] 0.9718\n\n\n\nFor \\(n = 23\\), the output should be close to the analytically derived probability \\(\\approx 0.507\\).\nFor \\(n = 50\\), the output should approximate \\(\\approx 0.970\\).\n\nWhat are the advantages of simulations. Here are three main advantages:\n\nFlexibility: This method works even when the assumptions (e.g., uniformity or independence) are adjusted.\nVerification: We can confirm analytical results with experimental data.\nReal-World Application: Simulation methods are widely used in practice when exact formulas are unavailable or complex.\n\nBy understanding and applying this simulation, you now have a practical tool to explore probability problems beyond purely mathematical derivations.\nThe birthday problem has practical implications beyond recreational math. In cryptography, the same underlying principles are used to analyze the likelihood of hash collisions in blockchain systems and digital signatures. Let’s explore this connection further.\n\n\n1.3.3 Hash-functions and the blockchain\nThe birthday problem provides the foundation for studying coincidences in various contexts. A critical application is in cryptography, where the concept of hash functions relies on similar probabilistic principles to ensure data security.\nA hash function is a mathematical tool that converts an input (e.g., a text, file, or number) of any size into a fixed-length output, often called a “hash value” or “digest.” For example, the widely used hash function SHA-256 maps its input to a 256-bit string. You can think of this hash value as a digital fingerprint: any change to the input, no matter how small, results in a completely different hash.\nHash functions have two critical properties that make them essential in cryptography:\n\nOne-wayness: It is computationally unfeasible to determine the original input from the hash value. A hash function is easy to evaluate or compute but it is practically impossible to learn from the value the initial argument by computing the inverse.\nCollision resistance: It is highly unlikely for two different inputs to produce the same hash value.\n\nIf the range of the hash-function is \\(M\\) and the hash-function maps into a 256 bit string then there are \\(2^{256}\\) basic outcomes. Since the hash-function maps a large string onto a smaller string it is possible that there are two different strings \\(x \\neq y\\) mapping to the same value \\(\\text{hash}(x)=\\text{hash}(y)\\). This would be a problem for message authentication because it would give the same “fingerprint” for two different strings.\nFor a cryptographically secure hash function it is therefore required that the probability of such a collision should be small enough to exclude a collision in all practically relevant circumstances.\nThe collision problem for hash functions is analogous to the birthday problem. Instead of asking how many people are needed in a room for a shared birthday, we ask: how many inputs (e.g., messages or files) need to be hashed before two of them produce the same hash value? This probability depends on the size of the hash space, which is determined by the number of bits8 in the hash function’s output.\n8 A bit, short for binary digit, is defined as the most basic unit of data in telecommunications and computing. Each bit is represented by either a 1 or a 0Let \\(M = 2^{256}\\) the number of possible hash values. The probability of no collisions after \\(n\\) hashes is\n\\[\\begin{equation*}\nP_{nc}(n) = 1 \\times \\left(1 - \\frac{1}{M}\\right)\\times\n\\left(1 - \\frac{2}{M} \\right) \\times \\cdots \\times \\left(1 - \\frac{(n-1)}{M} \\right)\n\\end{equation*}\\]\nFor \\(n\\) large this probability can be approximated by a factorial expansion as \\[\\begin{equation*}\nP_{nc}(n) \\approx \\prod_{k=0}^{n-1} \\left( 1 - \\frac{k}{M} \\right)\n\\end{equation*}\\]\nNow convert the product into a sum using the natural logarithm: \\[\\begin{equation}\n\\ln P_{nc}(n) \\approx \\sum_{k=0}^{n -1} \\ln \\left( 1 - \\frac{k}{M} \\right)\n\\end{equation}\\]\nNow to do the estimation of the threshold we use an approximation, which works with a simple property of the logarithm: \\(\\log(1-x)\\approx -x\\) when \\(x\\) is small. Thus for \\(n\\) much smaller than \\(M\\), or \\(n \\ll M\\), we can write the no-collision probability as \\[\\begin{equation*}\n\\ln P_{nc}(n) \\approx - \\sum_{k=0}^{n-1} \\frac{k}{M}\n\\end{equation*}\\] Simplify the summation by usin the fact that \\(\\sum_{k=0}^{n-1} = \\frac{n(n-1)}{2}\\) we get \\[\\begin{equation*}\n\\ln P_{nc}(n) \\approx -\\frac{n(n-1)}{2\\, M}\n\\end{equation*}\\] By taking exponentials on both sides this gives us \\[\\begin{equation*}\nP_{nc}(n) \\approx \\exp \\left( - \\frac{n(n-1)}{2 \\, M} \\right)\n\\end{equation*}\\]\nThus the probability of a hash-collision as a function of \\(n\\) is then approximately\n\\[\\begin{equation*}\nP_{c}(n) = 1- P_{nc}(n) \\approx \\frac{n^2}{M}\n\\end{equation*}\\]\nLet’s try a visualization to show how \\(P_{c}(n)\\) increases with \\(n\\):\n\nn &lt;- 1:100\nM &lt;- 2^256\n\ncollision_prob &lt;- (n^2) / (2 * M)\n\nplot(n, collision_prob, type = \"l\", log = \"y\", col = \"blue\",\n     xlab = \"Number of Inputs (n)\", ylab = \"Collision Probability (log scale)\",\n     main = \"Hash Collision Probability (SHA-256)\")\nabline(h = 0.5, col = \"red\", lty = 2)  # 50% threshold\n\n\n\n\n\n\n\n\nThese results show that for SHA-256:\n\nTo have a \\(50\\)% chance of a collision, an attacker would need to compute approximately \\(2^{128}\\) hashes.\nAt \\(10000\\) hashes per second, this would take longer than the age of the universe.\n\nThis enormous computational difficulty ensures the security of modern cryptographic systems. Even with the combined computing power of all devices on Earth, finding a collision is practically impossible.\nThe birthday problem demonstrates how fundamental probability concepts can inform critical applications like cryptographic hash functions. By understanding the collision probabilities, we see why hash functions like SHA-256 are effective for ensuring data integrity and security.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#summary",
    "href": "01-lecture1.html#summary",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.4 Summary",
    "text": "1.4 Summary\nIn this lecture we have taken a first step towards some very basic probability notions and some basic steps in R. Isn’t it amazing how much territory we could cover with so few concepts? You have learned about how to think probabilistically about collision probabilities and how to solve for them analytically, by simulation and for large numbers by approximation.\nWe have convinced ourselves using this knowledge only and taking on faith that the probability of independent events is the product of their individual probabilities, that the cryptographic Hash-function SHA-256, while it can produce collisions in theory, practically the number of hashes to make such a collision occur would be so large that we can be confident that hashing bit strings with SHA-256 gives us a unique fingerprint practically with certainty.\nThese are the basic probability concepts we have covered in this lecture:\n\nA random experiment: A process leading to an uncertain outcome.\nSample space: The collection of all possible outcomes of a random experiment.\nBasic outcome: A possible outcome of a random experiment.\nEvent: An event is a subset of basic outcomes. Any event which contains a single outcome is called a simple event.\nClassical probability find or make equally probably cases and the count them. The probability of an event \\(A\\) is the number of cases when \\(A\\) occurs divided by the total number of cases.\nRelative frequency probability defined as the number of times an event \\(A\\) occurs in a repeated number of trials divided by the total number of trials in a random experiment.\n\nThese are the R concepts we have covered in this lecture:\n\nobjects arbitrary names that can store different values and data types.\nfunctions ab R object that can accept other R objects as arguments, operate on them and return a new object.\nscripts files that store sequences of R commands and can be saved, repopened and allow the execution of commands.\nusing packages\nfinding help\nthe functions sample and replicate\n\nThese are the Applications we have covered in the lecture:\n\nConstructing a coin on the computer and toss it an arbitrary number of times\nThe birthday problem and how to solve it analytically as well as through simulation\nExtrapolating the birthday problem to analyze cryptographic collision resistance of hash-functions by using ideas from probability",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#project-1-craps-a-casion-dice-game.",
    "href": "01-lecture1.html#project-1-craps-a-casion-dice-game.",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.5 Project 1: Craps: A casion dice game.",
    "text": "1.5 Project 1: Craps: A casion dice game.\nThe first project is about one of the very few dice games offered at a Casino: Craps.8\n8  This will be the only project in this course where we do not go into a direct financial application. Before we can do so, we need to build still a bit more knowledge of probability and skills in R.In Craps people throw a pair of dice and bet on outcomes of these throws. In this project we practice our new knowledge by implementing the dice throws of craps and the automatic summation of points on the computer. We try to find out the probability of the possible outcomes and check the fairness of the dice.\n\nYour first task in this project will be to write a function which allows you to virtually throw a pair of dice and sum up the points shown after the throw.\nSimulate and plot the result of your simulation using qplot() with the binwidth argument set to 1.\nAre these dice fair? Why?\nAssume the dice were unfair in the following sense: Numbers 1,2,3,4 and 5 have a probability of \\(1/8\\) while the 6 has a probability of \\(3/8\\). Study the help page of the samplefunction and find out how you could give these new probability weights to the function. If you redo your simulation analysis with the loaded dice, how does the picture change?\nWrite a script for this random experiment.\nNow look at the random experiment of throwing two dice with the concepts from probability theory: What is the sample space of this experiment. What are the probabilities of the basic outcomes? What is the probability of getting sum 7, what is the probability of getting sum 2 for the case of the fair dice, for the case of the unfair dice?\nImagine now that you are at the casino which uses fair dice and you can spend 150 Euro for chips. Since you have figured out that 7 is the most likely outcome you would like to buy bets on 7. The casino offers you a bet for 15 cent. When you win you get 1 Euro for the bet, when you loose you get nothing. Every bet costs 15 cents, so you can make 1000 bets in total on 7 from your budget of 150. Is this a good or a bad deal for you? Try to think about this problem in terms of a simulation.\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html",
    "href": "01-lecture1_updated.html",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1 Rolling a die: First Probability ideas.\nLets start with a classic and old example of a probability model, which occupied the attention of Pascal and Fermat as well as their gambling friend the Chevalier de Méré: Rolling a six sided die.\nWhen you role a die or several dice like shown in the following picture @ref(fig:dice), it is uncertain on which face it will finally end up after the throw.\nRolling dice: An old game of chance\nWhat seems clear though is, that it will turn out that it is either one of the six possible faces of the die.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html#rolling-a-die",
    "href": "01-lecture1_updated.html#rolling-a-die",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1.1 Random experiments, sample space, events\nIn the theory of probability a process leading to an uncertain outcome is called a random experiment. The example of throwing a die helps us to give a precise meaning to the notion of an uncertain outcome. While we can imagine in principle that the die lands on one of its corners and this is one outcome, we agree on the outset that we are going to consider the (practically relevant cases) that it will land on one of the faces.\nThe collection of all possible outcomes of a practical or conceptual random experiment is called in probability theory a sample space. While the sample space of throwing a die is an idealization it is exactly this idealization which simplifies the theory without affecting its applicability. The basic outcomes in the random experiment of throwing a die are that the die lands such that the upward showing face shows a 1 a 2, 3, 4, 5 or a 6. In the theory the collection of basic outcomes is denoted as a set. Thus the sample space of throwing a die is given as the set \\({\\cal S} = \\{1,2,3,4,5,6\\}\\).\nThe sample space \\({\\cal S}\\) is the set of all basic outcomes. The subsets of the sample space are often called events in probability theory. An event could be - for example - an outcome where the die shows an upward looking face with an even number \\(A = \\{2,4,6\\}\\).\n\n\n1.1.2 The role and nature of idealizations in applications\nIdealizations of the kind we discussed for the throw of a die are standard in probability and we will encounter them again and again. For example the movement of a stock price is often though of as a conceptual random experiment. When we try to agree on what is the appropriate sample space of this experiment, we can say that the price can not fall below 0 but it is hard to agree on what will be the highest possible price. In the probabilistic treatment of stock prices, which we will discuss later in our course, it is for instance common to assume that the sample space of this random experiment is the entire interval of non-negative real numbers \\({\\cal S} = [0, \\infty)\\).\nMany of us would hesitate to claim that the price might rise without bound. Yet many models in applied Finance are based on such an assumption. The models allow arbitrary price hikes but with arbitrary small probability as the price gets higher and higher. Practically it does not make sense to believe that a security price can become arbitrarily high. The use of arbitrarily small probabilities in a financial model might seem absurd but it does no practical harm and makes the model simple and convenient to use. Moreover, if we seriously introduced an upper bound on a security price at \\(x\\) it would be also awkward to assume that it is impossible that it could be just a cent higher, an assumption equally unappealing than assuming it can get in principle arbitrarily high.\n\n\n1.1.3 Classical Probability: Measuring uncertainty\nProbability is a measure of how likely an event of an experiment is. But how could we measure chance? Here is the first big idea of probability theory and how the originators thought about it.1 How do you measure anything? If you think of length, for example, you take an arbitrary standard you agree on and then count. The originators of probability theory pursued the same idea with chance: To measure probability you choose equally probable cases and then count.\n1 In this discussion I use chapter 1 in Diaconis and Skyrms (2019)The probability of an event \\(A\\) according to the originators of the theory of probability is then \\[\\begin{equation}\nP(A) = \\frac{\\text{Number of cases where $A$ occurs}}{\\text{Total number of cases}}\n\\end{equation}\\]\nThus, if we want to know the probability of the die ending up on a face such that an even number is shown (2,4,6) we have to compute according to this notion \\(3/6\\) (since the total number of cases are six basic outcomes) which is a chance of \\(50 \\%\\).\nNote that this classical notion of probability has a few interesting consequences, which we will discuss in more detail later but which already now naturally flow from this basic idea of measuring chance.\n\nProbability is never negative.\nIf an event \\(A\\) occurs in all cases \\(P(A)=1\\).\nIf \\(A\\) and \\(B\\) never occur in the same case, then \\(P(A \\,\\text{or}\\, B) = P(A) + P(B)\\).\n\nIn particular then the probability that an event does not occur is 1 less the probability that it occurs: \\(P(\\text{not}\\, A) = 1 - P(A)\\).\nLet us interrupt here our discussion of probability for a moment and ask how we can make good on our promise to make these ideas tangible, so we can play with them.\nFor this we will need the computer. Since we will talk to the computer in R, it is now the right time to look at the die rolling example from the perspective of R and dive into some of its basic concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html#rolling-die-on-computer",
    "href": "01-lecture1_updated.html#rolling-die-on-computer",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.2 Rolling a die on the computer: First steps in R",
    "text": "1.2 Rolling a die on the computer: First steps in R\n\n1.2.1 The R User Interface\nBefore we can ask our tool to do anything for us, we need to know how to talk to it. In our case RStudio allows us to talk to our computer. It works like any other application. When you launch RStudio on your computer, in figure @ref(fig:rstudio-start-screen) you see a screen looking like this:\n\n\n\n\n\nThe RStudio startup screen\n\n\n\n\nIn this picture you see a screenshot of my RStudio screen. Interacting with the app is easy. You type commands via your keyboard at the prompt, which is the &gt; symbol. You find this symbol in the RStudio pane called Console. You can see it in the left pane in the screenshot. You send the command to the computer by pressing enter. After you have pressed enter, RStudio sends the command to R and displays the result of your command with a new prompt to enter new commands, like this:\n\n1+1\n\n[1] 2\n\n\nLet me pause to explain what you see here in this code example. First you see a light-gray box containing the command 1+1. This is an instance of a so called code chunk. Moving the cursor to the right upper corner of the chunk, will display a copy icon and you can click this icon to copy the code to the computer’s clipboard and then paste it at the prompt of your console (or for that matter anywhere else) and run it in R studio, if you wish. In the code-chunk the R-prompt &gt; is not displayed. This symbol of the prompt is only shown in the Console itself.\nWhen the code is executed, you see the result in the second light-gray box, just below the first one. It starts with a double hash ##, indicating that it shows an output of running the above code chunk and then displays the output as it would appear in the command window [1] 2.\nThe [1] means that the line begins with the first value of your result. For example, if you enter the command 20:60 at the prompt of your console which means in the R language, “list all the integers from 20 to 60” and press enter you get:\n\n20:60\n\n [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[26] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nmeaning that 20 is the first value displayed in your result. Then there is a line break because not all values can be displayed on the same line and R tells you that 45 is the 26-th value of the result.\nThe colon operator : is a very useful function in R which we will need often. It allows us to create sequences of every integer between two given integers.\nR needs a complete command to be able to execute it, when the return key is pressed. Lets see what happens, if a command is incomplete, like for instance &gt; 5*.\nIn this case R will show the expression followed by a + instead of showing a new prompt. This means that the expression is incomplete. It expects more input. If we complete the expression, say like\n&gt; 5*\n+ 4\nthe expression can be evaluated and a new prompt is shown in the console.\nIf you type a command that R does not understand, you will be returned an error message. Don’t worry if you see an error message. It just is a way the computer tells you that he does not understand what you want him to do.\nFor instance, if you type 5%3 you will get an error message like this\n&gt; 5%3\nError: unexpected input in \"5%3\"\n&gt;\nSometimes it is obvious why a mistake occurred. In this case, that R just does not know what to do with the symbol %. It has no meaning in this context. Sometimes it is not so obvious what the error message actually means and what you might do about it.\nA useful strategy in this case is to type the error message into a search engine and see what you can find. The chance is very high that others encountered the same problem before you and got helpful advice how to fix it from other users on the internet. One site, we find particularly helpful for all kinds of questions related to R and R programming is https://stackoverflow.com/. Try it at the next opportunity.\nNow with this basic knowledge, we can already make the first step to create a die on the computer using R. If you think of a physical die, the essential thing that matters are the points on its six sides. If you throw the die it will usually land on one of these sides and the upward showing side of the die shows the number of points. The colon operator : gives us a way to create a group of numbers from 1 to 6. R gives us the result as a one dimensional set of numbers.\n\n1:6\n\n[1] 1 2 3 4 5 6\n\n\nLets use these first steps in R to recap the probability concepts we have learned using this example of the six sided die: A basic outcome of rolling a six-sided die is for example 6 if the upper side after rolling the die happens to be the side with 6 points. The sample space of the experiment of rolling a six-sided die is the set \\({\\cal S} = \\{1,2,3,4,5,6\\}\\). In probability theory we often use the symbol \\({\\cal S}\\) or \\(S\\) for sample space. In many probability texts the sample space is also often denoted by the symbol \\(\\Omega\\) the Greek letter for (big) Omega. A random experiment in this example is the rolling of the die. The outcome is uncertain but once the die is rolled the outcome can be determined precisely. The event that the outcome is a display of 10 points is the empty set \\(A = \\emptyset\\). The symbol \\(\\emptyset\\) comes from set theory and means the set containing no elements. This event can contain no elements because we can not get a score of 10 by rolling a six sided die.\n\n\n1.2.2 Objects\nYou can save data in R by storing them in objects. An object is a name, you can choose yourself to store data. For example, if you choose to store the value 6 in an object called point_six, you would type:\n\npoint_six &lt;- 6\n\nat the prompt. R will the store the value 6 in the object called point_six, which you can use to refer to the value. If you type the name of your object at the prompt, R will display the value you have assigned. A useful key combination for typing the assignment operator &lt;- is to use the key combination ALT _. At the R prompt R will automatically print an assignment operator.\nNow if you type the name of the object and press enter, R will display the value or values stored in it, like this:\n\npoint_six\n\n[1] 6\n\n\nNow you can use the name of the object to refer to its value. For instance, you could divide point_sixby 2and get a meaningful result\n\npoint_six/2\n\n[1] 3\n\n\nNow to make our die more tangible and useful, let us store it in an R object by typing the following command at the prompt. This command creates an object with name die and assigns the vector 1,2,3,4,5, 6 to it.\n\ndie &lt;- 1:6\n\n\n\n\n\n\nThe RStudio Environment pane keeps track of the objects you have created\n\n\n\n\nYou can now see in the right upper Environment pane in figure @ref(fig:rstudio-screen-with-command) that R shows you that there is an object with the name die that it consists of integers 1,2,3,4,5. As you create more objects they will be stored in the Environment pane and are ready for your reference, unless you delete them. You can remove or delete an object by typing rm(object) or by assigning the value die &lt;- NULL which would also remove the object from your environment or workspace.\nYou can name your objects almost anything with a few exceptions. An object name must not start with a number. There are some special symbols which can also not be used in object names, like ^, !, $, @, +, -, /, *. Note that R is case sensitive and distinguishes small and big letters. If you assign a new value for an object you have already created, R will overwrite the object without warning.\nYou can see which objects are currently created and available for you in the Environment pane of your session of by typing ls(). The UNIX users among you will recognize this command from the unix shell, where it displays the files in a directory.\nBefore we learn how we can actually roll our die and perform a random experiment with it, let us briefly use the opportunity to explain a few things about how R does computations. We have already explained that we can use the object name to refer to the value. So for instance if we type\n\ndie*die\n\n[1]  1  4  9 16 25 36\n\n\nThis might irritate some of you because we have called the object a vector. In linear algebra multiplication of vectors is only allowed if there is an inner product. What happens here, if we use * the multiplication operator is that R does an element-wise multiplication of the six numbers of our die. Of course R allows to take an inner product as well, but this needs a different operator. To compute an inner product, we would type\n\ndie %*% die\n\n     [,1]\n[1,]   91\n\n\nNow R displays the result as a vectors with one row and one column, which is denoted in the output by [ , 1] for the column and [1, ] for the row. We will learn later more about the use and the meaning of this notation in R.\nThe element wise execution R usually uses also means that when you, for example type\n\ndie - 1\n\n[1] 0 1 2 3 4 5\n\n\nR would subtract 1 from every component in the vector die.\nAnother specific behavior of R, you need to know about is called recycling. If you give R two vectors of different length in an operation, R will repeat the shorter vector as long as it is of equal length with the longer one. For example, if you have: ::: {.cell}\ndie + 1:2\n\n[1] 2 4 4 6 6 8\n\n:::\nyou see that R adds 1 to 1 and 2 to 2 and then starts over again by adding 1 to 3 and 2 to 4 and then starts over once again by adding 1 to 5 and 2 to 6. If the longer vectors is not a multiple of the shorter one, R recycles but the cuts off. It will give you a warning though. ::: {.cell}\ndie + 1:4\n\nWarning in die + 1:4: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 2 4 6 8 6 8\n\n:::\nWhile this might seem awkward to some of you, we will see that for data manipulation element-wise execution is often extremely useful. It allows to manipulate groups of values in a systematic yet easy way.\n\n\n1.2.3 Functions\nR contains many functions which we can use to manipulate data and compute things. The syntax for using a function is very simple: You type the function name and put the value of the function argument in parentheses. Here we use for illustrations the function of the square root sqrt(): ::: {.cell}\nsqrt(4)\n\n[1] 2\n\n::: or rounding a number: ::: {.cell}\nround(3.1415)\n\n[1] 3\n\n::: The data you write in the parentheses are called the function arguments. Arguments can be all sorts of things: raw data, R objects, results from other functions.\nIf functions are nested, R evaluates the innermost function first and then goes on to the outer functions. To see examples of all these instances you can take ::: {.cell}\nmean(1:6)\n\n[1] 3.5\n\nmean(die)\n\n[1] 3.5\n\nround(mean(die))\n\n[1] 4\n\n::: for example.\nFor simulating random experiments, R has the very useful function sample(). With this function we can roll our die on the computer and conduct actual random experiments.\nThe function takes as arguments a vector names x and a number named size. sample will return size elements randomly chosen from the vector x. Lets say: ::: {.cell}\nsample(x = 1:4, size = 2)\n\n[1] 3 1\n\n:::\nIn this case sample has chosen 4,1 from the vector x = (1,2,3,4) at random.\nIf we want to roll the die in our computer we can thus pass the die as an argument to sample and use the number 1 for the size argument. Lets do a few rolls with our die ::: {.cell}\nsample(x = die, size = 1)\n\n[1] 3\n\nsample(x = die, size = 1)\n\n[1] 1\n\nsample(x = die, size = 1)\n\n[1] 2\n\nsample(x = die, size = 1)\n\n[1] 1\n\n::: These are the random outcomes of our consecutive rolls. It is as if we had thrown an actual die but in this case we have done the same thing on the computer. Isn’t it cool that this is possible at all? The sample() function will remain our good friend throughout this course.\nR functions can have many arguments, but they need to be separated by a comma.\nEvery argument in every function has a name. We specify which data are assigned to the arguments by setting a mathematical equal sign = between the name and the data. Names help us to avoid passing the wrong data and thereby mixing up things or committing errors. But using names is not necessary. If we just wrote ::: {.cell}\nsample(die,1)\n\n[1] 4\n\n::: R would also know what to do. It is not always clear which names to use for a function. If you are not sure, you can use the function args() to look it up. Here we take the function roundas one example. ::: {.cell}\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n::: Note that the digits argument in the round function is already set to 0. Frequently R functions come with optional arguments. These arguments are optional because the come with a default value, which is 0 in case of the round function.\nWe recommend that you write out argument names as a rule. It gives clearer code and avoids errors. If you don’t write argument names, R matches your values to the arguments of the function by order.\n\n\n1.2.4 Writing your own functions\nNow we are ready to write our own function to roll the die in our computer. Each function in R has the same elements: A name, a function body of code and a set of arguments. To write your own function, you have to write up all of these parts and save them in an R object. The syntax is:\nmy_function &lt;- function() {}\n\nThe name here is my_function, next comes the expression function() which needs to be assigned. The names of the function arguments have to be written between the parentheses. Then we have to write the actual code within the braces {}.\nTo do this for the die, lets write a function named roll_die. ::: {.cell}\nroll_die &lt;- function(){die &lt;- 1:6 \n                         sample(die, size = 1)}\n:::\nNow we can roll our die for a few times to show how the function works ::: {.cell}\nroll_die()\n\n[1] 5\n\nroll_die()\n\n[1] 4\n\nroll_die()\n\n[1] 2\n\nroll_die()\n\n[1] 6\n\nroll_die()\n\n[1] 1\n\n::: Note that in our function roll_die() has no arguments, just the function body. This is perfectly legitimate in R. It is important that when we call the function we have to call it with the parenthesis like roll_die(). If we only call the name roll_die, R will display the code in the function body.\nA final remark in the sample function is in place here. If we look at the arguments of sample using the args function we see ::: {.cell}\nargs(sample)\n\nfunction (x, size, replace = FALSE, prob = NULL) \nNULL\n\n::: Lets do not discuss all the details of this output but concentrate for a moment on the replace argument. What does this mean?\nAs we saw previously we can use the sample function to model the rolling of our die. If we set the size argument to 1 we get the roll of one die. If we set the size argument to n, we would simulate the rolling of n dies. But now the replace argument becomes crucial. As we can see in the output of the args function replace has a default value FALSE. This is a logical argument. It tells R, for example, that if we set size = 2, meaning that two dice are rolled, if the first dice shows, say a value of 3, the second die cannot show 3 as well.\nThis is clearly not what we have in mind when we model the rolling of 2 dice. It should be possible that both dies show the same value. To enable this behavior of the sample function, we have to change the default value of the replace argument to TRUE. Then R chooses a random draw from all of the six possible values for all dice rolled.\nCongratulations ! You have written your first R function for conducting a simple random experiment. Let me remind you once again: Think of the parentheses as a trigger that tells R to run the function. If you omit the trigger R just prints the body of the function. When you run a function, all the code in the function body is executed and R returns the result of the last line of code. If the last line of code does not return a value neither will R.\n\n\n1.2.5 Arguments\nImagine we remove the first line of code in our function body and changed the name die in the sample function to “ball”. ::: {.cell}\nroll_die2 &lt;- function(){sample(ball, size = 1)}\n::: If we call the function now, we will get an error. The function call roll_die2() will result in the error message Error in sample(ball, size = 1) : object 'ball' not found (try it!)\nWe could supply ball when we call roll_die2 if we make ball an argument of the function. Lets do this: ::: {.cell}\nroll_die2 &lt;- function(ball){sample(ball, size = 1)}\n::: Now the function will work as long as we supply ball when we call the function. ::: {.cell}\nroll_die2(ball = 1:6)\n\n[1] 1\n\n::: Note that we still get an error, if we forget to supply ball argument. This could be avoided if we give the function a default argument ::: {.cell}\nroll_die2 &lt;- function(ball= 1:6){sample(ball, size = 1)}\n::: Now if we type: ::: {.cell}\nroll_die2()\n\n[1] 6\n\n::: everything works, just as intended.\n\n\n1.2.6 Scripts\nSo far we have worked by interacting with the console. But what if you want to edit your functions? It would be much easier, if you could use a draft of your code and work form there. This can be done by using a script.\nYou create a script by going to File &gt; New File &gt; R script in the menu bar of RStudio. Using scripts is the standard way to write code in R. It not only helps you to keep track of your code, save it and edit it later. It also makes your work reproducible. You can edit and proofread your code and share it with others. To save your script go to File &gt; Save As in the menu bar.\n\n\n\n\n\nThe RStudio Script\n\n\n\n\nRStudio has many useful features to help you work with scripts. You can for instance automatically execute a line in a code by using the run button. You can also execute sections of code or the entire script. The entire script is executed by running the Source button. For all these commands there are key short cuts which you will learn as you work more with RStudio and R.\nFrom now on you should write all your R-code for this course in scripts. Make a new folder for the scripts you write and store your R-code there from now on.\n\n\n1.2.7 Using packages and finding Help\nWe have now a function which we can use to simulate the rolling of a die, roll_die(). If the die is fair it should be the case that if we roll the die often, all numbers should occur about equally often. The die should not be weighted in favor of a particular value.\nOne way to learn whether our die is fair are repetition and visualization. These are tools we will need all the time, when working with data and when doing probability. While R has many useful functions, one of the great powers is that R is constantly extended by a huge community of users by providing packages.\nPackages are add on functions, which will not be available when you install R. They need to be installed and loaded before you can use them. Since packages are such a powerful tool in R we need to introduce what they are and how to use them in the beginning.\n\n\n1.2.8 Packages\nThere are many visualization tools in R that come with the basic installation. Since the point we want to make here is about packages, we will use a visualization function which is part of the add on package ggplot2, a very popular package for making all kinds of graphs. ggplot2 is not part of the base-R distribution. It has to be therefore loaded before we can use it.\nThere are many additional functions provided with this package. Indeed you could run a whole course on just that 2 For the point we want to make here we will use just one of them, called qplot(), a shorthand for for quick plot.\n2 A very good starting point is for instance Healy (2019), in case you are interested. Also a good source is the first chapter in Wickham and Grolemund (2017).Since qplot() is a function in the package ggplot2, we first need to install this package. To install a package you need to be connected to the internet. If you have internet connection go to the command line and run at the command line: install.packages(\"ggplot2\").\nR displays what is happening while executing the command. Don’t worry if you don not know what all of these messages exactly mean and don’t panic that they are displayed in red. All packages can be installed like this. You have just to enter the correct name in the function install.packages(). The lower right pane in the RStudio software alternatively provides a tab called Packages. Using this tab, you can also install R packages interactively by clicking the install button on the upper left corner of the Packages tab.\nAfter installation the package is on our hard-drive but it can not yet be used. To use the package it has to be loaded. This is done with the command library. To load the ggplot2 package we type ::: {.cell}\nlibrary(\"ggplot2\")\n::: and hit the return key. Many things could be said about the R package system and you will learn it in more detail as we go along in our course. For the moment the most important thing to remember is that a package needs to be newly loaded whenever you want to use it in a new R session.\nTo check whether our dies is fair, we need to roll it many times. R provides a function, that does this for us. This function is called replicate() and provides an easy way to repeat a command many times. The number of times we want to repeat something is given as an argument to the replicate function.\nNow lets roll our die 100 times and save the result in an object we call rolls: ::: {.cell}\nrolls &lt;- replicate(100, roll_die())\n::: We now use the qplot()function from the ggplot2 library to make a quick visualization, by typing the command qplot(rolls). I include the resulting picture in the following figure ::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: ::: If you try this and your picture looks a bit different from mine, don’t worry. This is as things should be. I have stored the picture after one particular run of the replicate function and saved it in my figures folder to these lecture notes. Each time you run the replicate function anew a new random outcome is stored which will look not exactly the same as any other or course. This is what randomness does to the outcome of rolling a die.\nIt looks like every value occurred roughly 16 times but there is still quite some variation. For instance, the value 3 seems to occur more than 20 times whereas the value 2 occurs less than 15 times.\nMaybe we have to give it another trial with more replications. With the computer we can do this with a fingertip. Let us roll our die 10000 times and plot the result.\n&gt; rolls2 &lt;- replicate(10000, roll_die())\n&gt; qplot(rolls2m bindwidth = 1)\nNow the picture looks better.\n\n\n\n\n\nFrequencies of 1, 2, 3, 4, 5, 6 after rolling our virtual die 10000 times\n\n\n\n\nWe will learn in our course why the picture get better as we increase the number of rolls of the die. For the moment, let us just state that at least for this case it seems that there is no significant visual evidence that our virtual die is loaded.\n\n\n1.2.9 Getting Help\nWe have now learned a tiny number of R functions and we have written one function ourselves. We have learned how to make use of functions provided by packages.\nIt would be overwhelming to memorize and learn them all. In R, fortunately, every function comes with a detailed documentation and with its own help page. You need to learn how to use this source right from the beginning.\nTo access the help page of a function you type the function name preceded by a question mark at the prompt, like this ::: {.cell}\n?sample\n:::\nThen, after you have pressed the return key, a help page is opened in the right lower pane under the help tab as shown in @ref(fig:helpinr) ::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\nThis help page has a particular structure that you will find for every other R function no matter whether it is provided by the base installation or by a package. In the upper left corner you see the name of the function (sample) and in curly brackets next to it the term base, which means that this is a function in the R base installation. Then you see a headline about what the function does.\nFrom the top of the page, you then first see the field Description. This is a short description what the function does. Here it says\nDescription\nsample takes a sample of the specified size from the elements of x using either with or without replacement.\nThe next field is Usage. It gives you the function description with the arguments. Here for example\nUsage\nsample(x, size, replace = FALSE, prob = NULL)\n\nsample.int(n, size = n, replace = FALSE, prob = NULL,\n           useHash = (!replace && is.null(prob) && size &lt;= n/2 && n &gt; 1e7))\n           \nThe first line in Usage should by now be familiar. Don’t worry about the second line. The function can obviously do more than we know so far.\nNext comes a list of arguments the function takes and what type of information R expects you to provide, as well as what R will do with this information. Here it says for example\nArguments\nx   \neither a vector of one or more elements from which to choose, or a positive integer. See ‘Details.’\n\nn   \na positive number, the number of items to choose from. See ‘Details.’\n\nsize    \na non-negative integer giving the number of items to choose.\n\nreplace \nshould sampling be with replacement?\n\nprob    \na vector of probability weights for obtaining the elements of the vector being sampled.\n\n...\n\nWe omit some information here.\nThen comes a field called Details which gives a more in-depth description of the function. The next field is called Value. It describes what the function returns when you run it. Then we have a reference to related R functions under the field See. Finally there is a field called Examples. This field contains example code that is guaranteed to work. It shows a couple of different cases how you can use the function in practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html#coincidences",
    "href": "01-lecture1_updated.html#coincidences",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.3 Application: Coincidences and the Blockchain",
    "text": "1.3 Application: Coincidences and the Blockchain\n\n1.3.1 The birthday problem\nWe learned now some basic notions of probability and of R with the example of the rolling of a fair die. It is really surprising what already can be done by just applying the simple ideas we have just learned. The application we want to show you now is known to probability theorists as the birthday problem. It originates in a piece of recreational maths and math-puzzles but it reaches out until cryptography, computer security and blockchain architecture. It also gives us an opportunity to apply some of our newly acquired R-knowledge.\nThe starting question in the birthday puzzle is: What is the probability that at least two people in a room share the same birthday, when we neglect things like leap years, and when we assume that birthdays on any day of the year are equiprobable and the birthdays of the people in the room are independent. We have no twins for example. Perhaps some of you have already seen this problem before. If not, the result may surprise you. Even if you saw the birthday problem before, perhaps not many of you have seen the connection to the cryptography and the blockchain.3\n3 We follow here the discussion in Diaconis and Skyrms (2019)First, observe that we took our assumptions such that we are in the frame of classical probability like the originators of the field thought about it.\nNow for the sake of familiarizing ourselves with the new concepts, let us try to map the birthday problem into the probability notions we learned so far.\nThe sample space is the set of possible outcomes of the experiment. Assume we have \\(n\\) people in the room. Since each person can have a birthday at any of the 365 days in the year (note that we assumed we exclude leap years) and we have \\(n\\) people in the room, the possible basic outcomes are \\(365 \\times 365 \\times 365 \\cdots 365\\) taken \\(n\\) times. This will produce a set with \\(365^{n}\\) ordered \\(n-tuples\\) or sequences of birthdays. This is the sample space of this experiment, written as \\({\\cal S} = \\{x | x \\in 365^n\\}\\) in set theoretic notation. It means the sample space is the set of all ordered \\(n-tuples\\) from the Cartesian product \\(365^n\\).\nRemember that in mathematics, a Cartesian product is a mathematical operation that returns a set (or product set or simply product) from multiple sets (in this case the sample space, \\({\\cal S}\\). That is, for sets \\(A\\) and \\(B\\), the Cartesian product \\(A \\times B\\) is the set of all ordered pairs \\((a, b)\\) where \\(a \\in A\\) and \\(b \\in B\\).\nNow given this sample space we can assign a probability to the event that two people chosen at random will have the same birthday. The denominator will this be \\(365^n\\). The nominator for this probability will be much more straightforward to figure out, if we compute the complement. We discussed before that the probability of the complement of an event is 1 minus the probability of the event. Translated to the birthday problem, this means we look for the 1 minus the probability that that all birthdays are different. The probability that the second person has a different birthday from the first is \\(\\frac{364}{365}\\). If they are different the probability that the third person has a different birthday from them is \\(\\frac{363}{365}\\) and so on for all \\(n\\) in the room. Thus the probability of a shared birthday in the room is: \\[\\begin{equation*}\nP(\\text{at least two people share birthday}) =\n1 - \\frac{365 \\times 364 \\times 363 \\cdots \\times 365 - n +1}{365^n}\n\\end{equation*}\\] Now there is an additional thing we did not yet introduce and which we will learn about in the next lecture. This refers to our assumption that the individual birthdays are independent. For the moment you need to take the following fact on faith: The probability of independent events is the product of the probability these events.\nNow comes the surprise for those of you who did not already see the birthday problem. Assume the group size of people is 23. Let us compute the birthday coincidence probability. For this computation we make use of the R function prod() which takes a vector of numbers as input and computes their product. For \\(n=23\\) we need the list of numbers from 365 to 343. Now we can use the : operator and take advantage from one of its cool properties. If we formulate the command as 365:343 then the colon operator will give us a descending list of integers starting at 365 and ending at 343. Then the probability can be computed as ::: {.cell}\n1 - prod(365:343)/365^23\n\n[1] 0.5072972\n\n::: We have used the operator ^ which is needed to compute powers. Now we see from our computation that with 23 people the probability is already larger than 50 %. Quite stunning. You can now verify yourself that with 50 people this probability is already at 97 %.\n\n\n1.3.2 An R function to compute birthday probabilities\nNow let us use our knowledge about how to write R functions to write a function to compute birthday probabilities. The function argument is the number \\(n\\) of people in class. The coincidence probability is given by the formula we have derived. Now here is how we could go about writing this function: ::: {.cell}\nbirthday_collisions &lt;- function(n){\n  1 - prod(365:(365-n+1))/365^n\n}\n::: Now lets verify what I have claimed before about the group size 50. ::: {.cell}\nbirthday_collisions(50)\n\n[1] 0.9703736\n\n::: Voila, indeed with 50 people in the group the collision probability is already at 97 %.\n\n\n1.3.3 Determining birthday collisions by simulation\nNow this was an argument derived analytically and then we just packed the derived result into a function to compute the exact birthday-collision probabilities. Here is how we could figure out these probabilities by simulation, even with the minimal tools we have learned so far.\nLet us simulate first birthdays for 23 people in a room using the sample function. We stick to the assumption that our year has 365 days (no leap years), and that all birthdays are equally likely, so we can sample from the list of days from 1 to 365 with the sample function which we have already encountered.\nWe need to take care to set the replace argument in the sample function to the value TRUE. This is because we always draw for every person in the room from the same list of values. If one birthday is drawn it is possible in principle that later in the process the same birthday is drawn again. Before, when we worked with only one die we did not need to care whether sample drew with replacement or not. ::: {.cell}\nb_23 &lt;- sample(1:365, 23, replace = TRUE)\n::: This little piece of code is to be read in the same way as before when we built the process of rolling a die on the computer. We sample 23 values from the vector \\(1,2,3,\\cdots,365\\) without replacement. Think about a situation where I use uniform random numbers to call 23 people in your group and write the birthdays I get in this way into the object b_23.\nHere is how I can find the number of matches in b_23. R has a built in function, which is called unique(). So if there were two people with the same birthday in b_23, applying unique() to b_23 would only keep one value. Now clearly if we subtract the length of the resulting object from 23 we get the number of collisions in b_23. Fortunately we can use the R function length() to achieve this task. length() computes the length of any object we give it as an argument. ::: {.cell}\nm_23 &lt;- 23 - length(unique(b_23))\n::: So for instance if sample() drew 23 different birthdays m_23 would be 0.\nLet’s pack this process into a function: ::: {.cell}\nsim_23 &lt;- function(){\n  b_23 &lt;- sample(1:365, 23, replace = TRUE)\n  (23 - length(unique(b_23)))\n}\n::: Now we can repeat this process many times - say 10000 times - using the replicate function. ::: {.cell}\nsimprobs_23 &lt;- replicate(10000, sim_23())\n:::\nNow how do we get from here to probabilities? Here we anticipate and introduce a new probability concept which we will discuss in more detail in the next lecture but which you perhaps already came across a few times.\nThis concept could be called the relative frequency probability . In this concept the probability of an event \\(A\\) is defined as \\[\\begin{equation*}\nP(A) = \\frac{\\text{Number of times $A$ occurs in repeated identical trials}}{\\text{Total number of trials in a random experiment}}\n\\end{equation*}\\] Applying this concept to our data generated by the replicate function and stored in b_23 would require two things. First we would have to count the number of times we saw a 1,2 or more matches, which in our case means 1,2 or more birthday collisions, add up the count and divide by the number of replications. Clearly it is also easier here to work the other way round. We count the number of no collisions and use the complement rule, by computing 1 minus the relative frequency of no collisions. Let us do that.\nNow this is clearly too tedious to do by hand. We will discuss systematically in the next lecture how to refer to individual values in an R object and select and operate on ranges of values etc.\nFor the moment, look at the next step just as a piece of magic and don’t worry if you do not exactly understand the syntax at the moment.\nIn the numerator I ask R to count the number of 0s (no birthday collisions) in the object simprobs_23 and then I divide it by 10000, just as in the definition of relative frequency probability. Then I compute 1 minus this relative frequency ::: {.cell}\n1 - length((simprobs_23[simprobs_23 == 0]))/10000\n\n[1] 0.5067\n\n::: Voila, this is pretty good, isn’t it? Remember that the analytical solution we just got a bit earlier was 0.5072972\nIsn’t this cool? Instead of using formal reasoning we built a random experiment on the computer and came up with a pretty good approximation of the collision probability.\nLet’s check the result with 50 for the sake of completeness and for the sake of practice, with an experiment with 10000 trials. I now stitch all steps together. ::: {.cell}\nsim_50 &lt;- function(){\n  b_50 &lt;- sample(1:365, 50, replace = TRUE)\n  (50 - length(unique(b_50)))}\n\nsimprobs_50 &lt;- replicate(100000, sim_50())\n1 - length((simprobs_50[simprobs_50 == 0]))/100000\n\n[1] 0.97027\n\n::: Pretty amazing! Before we got the exact value as 0.9703736.\nBut now let us stay still with the birthday problem but now get away from this playful application to something really serious: Cryptography.\n\n\n1.3.4 Hash-functions and the blockchain\nThe birthday problem is the canonical model for the probabilistic study of coincidences of all kinds. A particular important application of this idea is in cryptography, and its concept of so called hash-functions.\nA hash-function maps a string of arbitrary but finite length to a fixed length string of output.4 A very frequently used hash-function in practice is the function SHA-256, which maps its input to a string of 256 bits5. So, you could for instance give the text of these lecture notes as an input to SHA-256 and it would map this into a 256-bit string, which functions like a finger print of this text. This function is an instance of a special from of mappings called a one-way-function meaning that it is easy to evaluate or compute but it is practically impossible to learn from the value the initial argument by computing the inverse.\n4 We have taken this discussion from https://brilliant.org/wiki/birthday-paradox/5 A bit, short for binary digit, is defined as the most basic unit of data in telecommunications and computing. Each bit is represented by either a 1 or a 0Hash-Functions are key pillars of modern cryptography, where they play a major role in message authentication. This is because it is impossible to modify the input without significantly changing the output. So in our previous example, if you only deleted or added a comma to these notes and hash-them again they would hash into a completely different value than the previous version which still was without this minor change. Comparing the hash-values would reveal immediately that something has changed.\nThe collision problem for hash-functions is formally equivalent to the birthday problem. The event we are interested in is that at least two input strings hash-to the same value. Again it is easier to think about the complementary event that all inputs hash to a different value.\nIf the range of the hash-function is \\(M\\) and the hash-function maps into a 256 bit string then there are \\(2^{256}\\) basic outcomes. Since the hash-function maps a large string onto a smaller string it is possible that there are two different strings \\(x \\neq y\\) mapping to the same value \\(\\text{hash}(x)=\\text{hash}(y)\\). This would be a problem for message authentication because it would give the same “fingerprint” for two different strings.\nFor a cryptographically secure hash function it is therefore required that the probability of such a collision should be small enough to exclude a collision in all practically relevant circumstances.\nNote that this is a problem formally similar to the birthday problem. In the cryptography context we ask, how many values does a potential attacker have to hash through SHA-256 before the probability of a collision is larger than a certain threshold?\nThe probability of no-collision (\\(nc\\)) with \\(n\\) hashed-messages through a SHA-256 hash function is \\[\\begin{equation*}\nP_{nc}(n) = 1 \\times \\left(1 - \\frac{1}{2^{256}}\\right)\\times\n\\left(1 - \\frac{2}{2^{256}}\\right) \\times \\cdots \\times \\left(1 - \\frac{(n-1)}{2^{256}}\\right)\n\\end{equation*}\\]\nNow to do the estimation of the threshold we use an approximation, which works with a simple property of the logarithm: \\(\\log(1-x)\\approx -x\\) when \\(x\\) is small. Thus for \\(n\\) much smaller than \\(M\\), or \\(n \\ll M\\), we can write the no-collision probability approximately as \\[\\begin{align*}\nP_{nc}(n) &\\approx 1 \\times \\exp\\left(-\\frac{1}{2^{256}}\\right) \\times \\exp\\left(-\\frac{2}{2^{256}}\\right) \\times \\cdots\n\\times \\exp\\left(-\\frac{(n-1)}{2^{256}}\\right) \\\\\n&\\approx 1 \\times \\exp\\left(-\\frac{(1+2+3+...+(n-1))}{2^{256}} \\right) \\\\\n&\\approx \\exp\\left(-\\frac{n \\times (n-1)}{2 \\times 2^{256}}\\right)\n\\end{align*}\\] Now if \\(n \\ll 2^{256}\\) we can again use the approximation \\(\\exp(x) \\approx 1 + x\\) to get the probability of a hash-collision as \\[\\begin{align*}\nP_{nc}(n) &\\approx 1 - \\frac{n \\times (n-1)}{2\\times 2^{256}}\\\\\n       & \\approx 1 - \\frac{n^2}{2\\times 2^{256}}\n\\end{align*}\\] Thus the probability of a hash-collision as a function of \\(n\\) is then approximately \\[\\begin{equation*}\nP_{c}(n) = \\frac{n^2}{2 \\times 2^{256}}\n\\end{equation*}\\] Thus to have a collision probability with \\(99.8%\\) you need ::: {.cell}\nsqrt(2*2^256*0.998)\n\n[1] 4.807505e+38\n\n::: which is about \\(5*10^{38}\\) hashes. To have a collision probability of \\(50 \\%\\) you need ::: {.cell}\nsqrt(2^256)\n\n[1] 3.402824e+38\n\n::: which amounts to \\(2^{128}\\) hashes. This is, of course a really big number.\nLet’s put this into perspective: If a computer computes \\(10000\\) hashes per second, it would take more than \\(10^{27}\\) years to compute \\(2^{128}\\) hashes. Narayanan et al. (2016) formulate it like this: “…if every computer ever made by humanity had been computing since the beginning of the universe, the odds that they would have found a collision by now are still infinitesimally small. So small that it it’s far less than the odds that a meteor will land on your house.”.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html#summary",
    "href": "01-lecture1_updated.html#summary",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.4 Summary",
    "text": "1.4 Summary\nIn this lecture we have taken a first step towards some very basic probability notions and some basic steps in R. Isn’t it amazing how much territory we could cover with so few concepts? You have learned about how to think probabilistically about collision probabilities and how to solve for them analytically, by simulation and for large numbers by approximation.\nWe have convinced ourselves using this knowledge only and taking on faith that the probability of independent events is the product of their individual probabilities, that the cryptographic Hash-function SHA-256, while it can produce collisions in theory, practically the number of hashes to make such a collision occur would be so large that we can be confident that hashing bit strings with SHA-256 gives us a unique fingerprint practically with certainty.\nThese are the basic probability concepts we have covered in this lecture:\n\nA random experiment: A process leading to an uncertain outcome.\nSample space: The collection of all possible outcomes of a random experiment.\nBasic outcome: A possible outcome of a random experiment.\nEvent: An event is a subset of basic outcomes. Any event which contains a single outcome is called a simple event.\nClassical probability find or make equally probably cases and the count them. The probability of an event \\(A\\) is the number of cases when \\(A\\) occurs divided by the total number of cases.\nRelative frequency probability defined as the number of times an event \\(A\\) occurs in a repeated number of trials divided by the total number of trials in a random experiment.\n\nThese are the R concepts we have covered in this lecture:\n\nobjects arbitrary names that can store different values and data types.\nfunctions ab R object that can accept other R objects as arguments, operate on them and return a new object.\nscripts files that store sequences of R commands and can be saved, repopened and allow the execution of commands.\nusing packages\nfinding help\nthe functions sample and replicate\n\nThese are the Applications we have covered in the lecture:\n\nConstructing a die on the computer and roll it an arbitrary number of times\nThe birthday problem and how to solve it analytically as well as through simulation\nExtrapolating the birthday problem to analyze cryptographic collision resistance of hash-functions by using ideas from probability",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1_updated.html#project-1-craps-a-casion-dice-game.",
    "href": "01-lecture1_updated.html#project-1-craps-a-casion-dice-game.",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.5 Project 1: Craps: A casion dice game.",
    "text": "1.5 Project 1: Craps: A casion dice game.\nThe first project is about one of the very few dice games offered at a Casino: Craps.6\n6  This will be the only project in this course where we do not go into a direct financial application. Before we can do so, we need to build still a bit more knowledge of probability and skills in R.In Craps people throw a pair of dice and bet on outcomes of these throws. In this project we practice our new knowledge by implementing the dice throws of craps and the automatic summation of points on the computer. We try to find out the probability of the possible outcomes and check the fairness of the dice.\n\nYour first task in this project will be to write a function which allows you to virtually throw a pair of dice and sum up the points shown after the throw.\nSimulate and plot the result of your simulation using qplot() with the binwidth argument set to 1.\nAre these dice fair? Why?\nAssume the dice were unfair in the following sense: Numbers 1,2,3,4 and 5 have a probability of \\(1/8\\) while the 6 has a probability of \\(3/8\\). Study the help page of the samplefunction and find out how you could give these new probability weights to the function. If you redo your simulation analysis with the loaded dice, how does the picture change?\nWrite a script for this random experiment.\nNow look at the random experiment of throwing two dice with the concepts from probability theory: What is the sample space of this experiment. What are the probabilities of the basic outcomes? What is the probability of getting sum 7, what is the probability of getting sum 2 for the case of the fair dice, for the case of the unfair dice?\nImagine now that you are at the casino which uses fair dice and you can spend 150 Euro for chips. Since you have figured out that 7 is the most likely outcome you would like to buy bets on 7. The casino offers you a bet for 15 cent. When you win you get 1 Euro for the bet, when you loose you get nothing. Every bet costs 15 cents, so you can make 1000 bets in total on 7 from your budget of 150. Is this a good or a bad deal for you? Try to think about this problem in terms of a simulation.\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nNarayanan, Arvind, Joseph Bonneau, Edward Felten, Andrew Miller, and Steven Goldfeder. 2016. Bitcoin and Cryptocurrency Technologies: A Comprehensive Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#tossing-a-coin",
    "href": "01-lecture1.html#tossing-a-coin",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "1.1.1 Random experiments, sample space, events\nIn the theory of probability a process leading to an uncertain outcome is called a random experiment. The example of tossing a coin helps us to give a precise meaning to the notion of an uncertain outcome. While we can imagine in principle that the coin lands on its rim and this is one outcome, we agree on the outset that we are going to consider the (practically relevant cases) that it will land on one of the faces.\nThe collection of all possible outcomes of a practical or conceptual random experiment is called in probability theory a sample space. While the sample space of tossing a coin is an idealization it is exactly this idealization which simplifies the theory without affecting its applicability. The basic outcomes in the random experiment of tossing a coin are that the die lands such that the upward showing face shows Heads or Tails. Let us encode this outcome by 1 for “Heads” and 0 for “Tails”. In the theory the collection of basic outcomes is denoted as a set. Thus the sample space of tossing a coin is given as the set \\({\\cal S} = \\{1,0\\}\\).\nThe sample space \\({\\cal S}\\) is the set of all basic outcomes, which are in our case only two: 1 or 0.\nIn more complex random experiments there will usually be more than two outcomes. Subsets of the sample space containing more than one basic outcome are called events in the theory of probability.\n\n\n\n\n\n\nNow you try\n\n\n\nConsider another random experiment from games of chance: Throwing a six sided die. What are the basic outcomes in this random experiment? What is the sample space in this case? How would you describe the event that the outcome of the die shows an odd number in terms of a subset of the sample space? Consider yet a new random experiment where you throw two coins, instead of one. What are the basic outcomes in this random experiment? What is the sample space in this case? How do these concepts look like when you throw two six sided dice instead of two coins?\n\n\n\n\n1.1.2 The role and nature of idealizations in applications\nIdealizations of the kind we discussed for the toss of a coin are standard in probability and we will encounter them again and again. For example the movement of a stock price is often though of as a conceptual random experiment. When we try to agree on what is the appropriate sample space of this experiment, we can say that the price can not fall below 0 but it is hard to agree on what will be the highest possible price. In the probabilistic treatment of stock prices, which we will discuss later in our course, it is for instance common to assume that the sample space of this random experiment is the entire interval of non-negative real numbers \\({\\cal S} = [0, \\infty)\\).\nMany of us would hesitate to claim that the price might rise without bound. Yet many models in applied Finance are based on such an assumption. The models allow arbitrary price hikes but with arbitrary small probability as the price gets higher and higher. Practically it does not make sense to believe that a security price can become arbitrarily high. The use of arbitrarily small probabilities in a financial model might seem absurd but it does no practical harm and makes the model simple and convenient to use. Moreover, if we seriously introduced an upper bound on a security price at \\(x\\) it would be also awkward to assume that it is impossible that it could be just a cent higher, an assumption equally unappealing than assuming it can get in principle arbitrarily high.\n\n\n1.1.3 Classical Probability: Measuring uncertainty\nProbability is a measure of how likely an event of an experiment is. But how could we measure chance? Here is the first big idea of probability theory and how the originators thought about it.1 How do you measure anything? If you think of length, for example, you take an arbitrary standard you agree on and then count. The originators of probability theory pursued the same idea with chance: To measure probability you choose equally probable cases and then count.\n1 In this discussion I use chapter 1 in Diaconis and Skyrms (2019)The probability of an event \\(A\\) according to the originators of the theory of probability is then \\[\\begin{equation}\nP(A) = \\frac{\\text{Number of cases where $A$ occurs}}{\\text{Total number of cases}}\n\\end{equation}\\]\nThus, if we want to know the probability of the coin ending up Heads, we have to compute according to this notion \\(1/2\\), since the total number of cases is 2 (Heads or Tails).\n\n\n\n\n\n\nNow you try\n\n\n\nConsider another random experiment of throwing a six sided die: According to the probability notion explained above what would be the chance that the die lands on on a face such that an even number is shown?. What would be the chance that in the random experiment of throwing two dice the sum of points would be 7? What is the chance that when rolling two six sided dice the sum is 2?\n\n\nNote that this classical notion of probability has a few interesting consequences, which we will discuss in more detail later but which already now naturally flow from this basic idea of measuring chance.\n\nProbability is never negative.\nIf an event \\(A\\) occurs in all cases \\(P(A)=1\\).\nIf \\(A\\) and \\(B\\) never occur in the same case, then \\(P(A \\,\\text{or}\\, B) = P(A) + P(B)\\).\n\nWhile rule 1. and 2. are intuitively fairly obvious (try to think of examples to convince yourself!), rule 3 might need some concrete explanation. The case described by rule 3 is often referred to as mutually exclusive events, i.e. they can not occur together.\n\n\n\n\n\n\nExample for rule 3:\n\n\n\nSuppose you have a standard deck of 52 playing cards, and you are interested in the probability of drawing a King \\(A\\) or a Queen \\(B\\).\n\nThe probability of drawing a King is \\(P(A) = \\frac{4}{52}\\), since there are 4 Kings in the deck.\nThe probability of drawing a Queen is \\(P(B) = \\frac{4}{52}\\), since there are also 4 Queens.\n\nSince a card cannot be both a King and a Queen, \\(A\\) and \\(B\\) are mutually exclusive events.\nThus, the probability of drawing a King or a Queen is: \\(P(A \\,\\text{or}\\, B) = P(A) + P(B) = \\frac{4}{52} + \\frac{4}{52} = \\frac{8}{52} = \\frac{2}{13}\\)\nThis illustrates how probabilities of mutually exclusive events add up.\n\n\nIn particular then the probability that an event does not occur is 1 less the probability that it occurs: \\(P(\\text{not}\\, A) = 1 - P(A)\\).\nNow let me show you how you can make use of a large language model, using ChatGPT as a use case, to support learning new concepts such as we have just introduced them here.\nWe have discussed new two classical and famous random experiments you will probably find in every textbook on probability. Assume you would like to deepen your understanding of these concepts and also consider more complex examples in particular from Finance. This is something the LLM is very good at. You could - for instance - ask ChatGPT at the prompt something like shown in Figure 1.2 :\n\n\n\n\n\n\n\n\nFigure 1.2: Figure 2: The ChatGPT prompt\n\n\n\n\n\nHere is an answer I would get for this prompt:\n\n\n\n\n\n\nAnswer by ChatGPT40\n\n\n\nHere are three detailed examples from finance to illustrate the concepts of a random experiment, sample space, basic outcomes, and events:\n\nExample 1: Daily Price Movement of a Stock\nRandom Experiment: Observe the daily price movement of a particular stock (e.g., Apple Inc.) at the close of the trading day.\n\nSample Space \\({\\cal S}\\): The set of all possible outcomes of the price movement. For simplicity, we can represent the sample space as: \\({\\cal S} = \\{ \\text{price increase, price decrease, no change} \\}\\)\nBasic Outcomes:\n\n“Price increases” (e.g., the stock closes higher than it opened).\n“Price decreases” (e.g., the stock closes lower than it opened).\n“No change” (e.g., the stock closes at the same price it opened).\n\nEvents:\n\nEvent \\(A\\): “The stock price increases.” \\(\\{ \\text{price increase} \\}\\)\nEvent \\(B\\): “The stock price either increases or remains unchanged.” \\(\\{ \\text{price increase, no change} \\}\\)\n\n\n\nExample 2: Credit Card Default Risk\nRandom Experiment: Assess whether a randomly selected credit card holder defaults on their monthly payment.\n\nSample Space \\({\\cal S}\\):\n\\(S = \\{\\text{default, no default} \\}\\)\nBasic Outcomes:\n\n“Default” (e.g., the cardholder fails to make the minimum payment).\n“No default” (e.g., the cardholder pays at least the minimum required).\n\nEvents:\n\nEvent \\(A\\): “The cardholder defaults.” ({default})\nEvent \\(B\\): “The cardholder does not default.” ({no default})\nEvent \\(C\\): “Default occurs for a customer from a specific demographic (e.g., under 30 years old).” This is a subset of the sample space but is conditioned on additional information.\n\n\n\nExample 3: Bond Ratings Transition\nRandom Experiment: Observe the change in the credit rating of a corporate bond over a year.\n\nSample Space \\({\\cal S}\\): The set of possible credit rating transitions. For example, using a simple rating scale: \\({\\cal S} = \\{\\text{AAA}, \\text{AA}, \\text{A}, \\text{BBB}, \\text{BB}, \\text{B}, \\text{CCC}, \\text{default} \\}\\)\nOver a year, the bond can transition from its initial rating to any of these, or stay the same.\nBasic Outcomes:\n\n“AAA to AAA” (no change in the highest credit quality).\n“BBB to BB” (downgrade).\n“A to default” (significant deterioration).\n\nEvents:\n\nEvent \\(A\\): “The bond is downgraded by one or more notches.” (Any outcome where the final rating is lower than the initial rating, e.g., {AAA to AA, A to BB}).\nEvent \\(B\\): “The bond defaults.” ({default}).\nEvent \\(C\\): “The bond remains investment-grade.” (All transitions where the final rating is AAA, AA, A, or BBB).\n\n\n\nThese examples show how the concepts apply to finance, helping to bridge the theoretical framework of probability with practical applications in the field. Would you like to dive deeper into any of these examples or extend them to computations like probabilities of events?\n\n\nLet us interrupt here our discussion of probability for a moment and ask how we can make good on our promise to make these ideas tangible, so we can play with them.\nFor this we will need the computer. Since we will talk to the computer in R, it is now the right time to look at the die rolling example from the perspective of R and dive into some of its basic concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#tossing-coin-on-computer",
    "href": "01-lecture1.html#tossing-coin-on-computer",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.2 Tossing a coin on the computer: First steps in R",
    "text": "1.2 Tossing a coin on the computer: First steps in R\n\n1.2.1 The R User Interface\nBefore we can ask our tool to do anything for us, we need to know how to talk to it. In our case RStudio allows us to talk to our computer. It works like any other application. When you launch RStudio on your computer, in figure Figure 1.3 you see a screen looking like this:\n\n\n\n\n\n\n\n\nFigure 1.3: The RStudio startup screen\n\n\n\n\n\nIn this picture you see a screenshot of my RStudio screen. Interacting with the app is easy. You type commands via your keyboard at the prompt, which is the &gt; symbol. You find this symbol in the RStudio pane called Console. You can see it in the left pane in the screenshot. You send the command to the computer by pressing enter. After you have pressed enter, RStudio sends the command to R and displays the result of your command with a new prompt to enter new commands, like this:\n\n1+1\n\n[1] 2\n\n\nLet me pause to explain what you see here in this code example. First you see a light-gray box containing the command 1+1. This is an instance of a so called code chunk. Moving the cursor to the right upper corner of the chunk, will display a copy icon and you can click this icon to copy the code to the computer’s clipboard and then paste it at the prompt of your console (or for that matter anywhere else) and run it in R studio, if you wish. In the code-chunk the R-prompt &gt; is not displayed. This symbol of the prompt is only shown in the Console itself.\nThe [1] means that the line begins with the first value of your result. For example, if you enter the command 20:60 at the prompt of your console which means in the R language, “list all the integers from 20 to 60” and press enter you get:\n\n20:60\n\n [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[26] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nmeaning that 20 is the first value displayed in your result. Then there is a line break because not all values can be displayed on the same line and R tells you that 45 is the 26-th value of the result.\nThe colon operator : is a very useful function in R which we will need often. It allows us to create sequences of every integer between two given integers.\nR needs a complete command to be able to execute it, when the return key is pressed. Lets see what happens, if a command is incomplete, like for instance &gt; 5*.\nIn this case R will show the expression followed by a + instead of showing a new prompt. This means that the expression is incomplete. It expects more input. If we complete the expression, say like\n&gt; 5*\n+ 4\nthe expression can be evaluated and a new prompt is shown in the console.\nIf you type a command that R does not understand, you will be returned an error message. Don’t worry if you see an error message. It just is a way the computer tells you that he does not understand what you want him to do.\nFor instance, if you type 5%3 you will get an error message like this\n&gt; 5%3\nError: unexpected input in \"5%3\"\n&gt;\nSometimes it is obvious why a mistake occurred. In this case, that R just does not know what to do with the symbol %. It has no meaning in this context. Sometimes it is not so obvious what the error message actually means and what you might do about it.\nA useful strategy in this case is to type the error message into a search engine and see what you can find. The chance is very high that others encountered the same problem before you and got helpful advice how to fix it from other users on the internet. One site, we find particularly helpful for all kinds of questions related to R and R programming is https://stackoverflow.com/. Try it at the next opportunity. You can also show the error to the LLM prompt and ask how to fix it. In most cases chatGPT will give you a useful answer.\nNow with this basic knowledge, we can already make the first step to create a coin on the computer using R. If you think of a physical coin, the essential thing that matters is that it can show Heads or Tails. Lets encode Heads by 1 and Tails by 0. If you toss the coin it will usually land on one of these faces and the upward showing side of the coin shows the outcome of the toss. The colon operator : gives us a way to create a group of numbers from 0 to 1. R gives us the result as a one dimensional set of numbers.\n\n0:1\n\n[1] 0 1\n\n\nLets use these first steps in R to recap the probability concepts we have learned using this example of the six sided die: A basic outcome of tossing the coin is for example 1 if the upper side after tossing the coin happens to be Heads. The sample space of the experiment of tossing a coin is the set \\({\\cal S} = \\{0,1\\}\\). In probability theory we often use the symbol \\({\\cal S}\\) or \\(S\\) for sample space. In many probability texts the sample space is also often denoted by the symbol \\(\\Omega\\) the Greek letter for (big) Omega. A random experiment in this example is the tossing of the coin. The outcome is uncertain but once the coin is tossed the outcome can be determined precisely. The event that the outcome is a display of 2 is the empty set \\(A = \\emptyset\\). The symbol \\(\\emptyset\\) comes from set theory and means the set containing no elements. This event can contain no elements because we can not get any other number than 0 or 1 by tossing the coin.\n\n\n1.2.2 Objects\nYou can save data in R by storing them in objects. An object is a name, you can choose yourself to store data. For example, if you choose to store the value 1 in an object called Heads, you would type:\n\nHeads &lt;- 1\n\nat the prompt. R will the store the value 1 in the object called Heads, which you can use to refer to the value. If you type the name of your object at the prompt, R will display the value you have assigned. A useful key combination for typing the assignment operator &lt;- is to use the key combination ALT _. At the R prompt R will automatically print an assignment operator.\nNow if you type the name of the object and press enter, R will display the value or values stored in it, like this:\n\nHeads\n\n[1] 1\n\n\nNow you can use the name of the object to refer to its value. For instance, you could divide Heads by 2 and get a meaningful result\n\nHeads/2\n\n[1] 0.5\n\n\nNow to make our coin more tangible and useful, let us store it in an R object by typing the following command at the prompt. This command creates an object with name coin and assigns the vector 0,1 to it.\n\ncoin &lt;- 0:1\n\n\n\n\n\n\n\n\n\nFigure 1.4: The RStudio Environment pane keeps track of the objects you have created\n\n\n\n\n\nYou can now see in the right upper Environment pane in figure Figure 1.4 that R shows you that there is an object with the name coin that it consists of integers 0,1. As you create more objects they will be stored in the Environment pane and are ready for your reference, unless you delete them. You can remove or delete an object by typing rm(object) or by assigning the value coin &lt;- NULL which would also remove the object from your environment or workspace.\nYou can name your objects almost anything with a few exceptions. An object name must not start with a number. There are some special symbols which can also not be used in object names, like ^, !, $, @, +, -, /, *. Note that R is case sensitive and distinguishes small and big letters. If you assign a new value for an object you have already created, R will overwrite the object without warning.\nYou can see which objects are currently created and available for you in the Environment pane of your session of by typing ls(). The UNIX users among you will recognize this command from the unix shell, where it displays the files in a directory.\nBefore we learn how we can actually roll our die and perform a random experiment with it, let us briefly use the opportunity to explain a few things about how R does computations. We have already explained that we can use the object name to refer to the value. So for instance if we type\n\ncoin*coin\n\n[1] 0 1\n\n\nThis might irritate some of you because we have called the object a vector. In linear algebra multiplication of vectors is only allowed if there is an inner product. What happens here, if we use * the multiplication operator is that R does an element-wise multiplication of the six numbers of our die. Of course R allows to take an inner product as well, but this needs a different operator. To compute an inner product, we would type\n\ncoin %*% coin\n\n     [,1]\n[1,]    1\n\n\nNow R displays the result as a vectors with one row and one column, which is denoted in the output by [ , 1] for the column and [1, ] for the row. We will learn later more about the use and the meaning of this notation in R.\nThe element wise execution R usually uses also means that when you, for example type\n\ncoin + 1\n\n[1] 1 2\n\n\nR would subtract 1 from every component in the vector coin.\nAnother specific behavior of R, you need to know about is called recycling. If you give R two vectors of different length in an operation, R will repeat the shorter vector as long as it is of equal length with the longer one. For example, if you have:\n\ncoin + 1\n\n[1] 1 2\n\n\nyou see that R adds 1 to 0 and then starts over again by adding 1 to 1.\nIf the longer vectors is not a multiple of the shorter one, R recycles but the cuts off.2\n2 As of R version 4.2.0 (released in April 2022), the behavior regarding the recycling rule has changed. In older versions of R, a warning was issued when the length of one vector was not a multiple of the length of the other vector during arithmetic operations. The reasoning behind this change was to reduce unnecessary noise in the output. Many users found the warnings redundant in cases where the recycling rule was intentional and understood. However, this change means you need to be more cautious, as unexpected behavior might go unnoticed without the warning.\ncoin + 1:4\n\n[1] 1 3 3 5\n\n\nWhile this might seem awkward to some of you, we will see that for data manipulation element-wise execution is often extremely useful. It allows to manipulate groups of values in a systematic yet easy way.\n\n\n1.2.3 Functions\nR contains many functions which we can use to manipulate data and compute things. The syntax for using a function is very simple: You type the function name and put the value of the function argument in parentheses. Here we use for illustrations the function of the square root sqrt():\n\nsqrt(4)\n\n[1] 2\n\n\nor rounding a number:\n\nround(3.1415)\n\n[1] 3\n\n\nThe data you write in the parentheses are called the function arguments. Arguments can be all sorts of things: raw data, R objects, results from other functions.\nIf functions are nested, R evaluates the innermost function first and then goes on to the outer functions. To see examples of all these instances you can take\n\nnumbers &lt;- 1:7\nmean(numbers)\n\n[1] 4\n\nround(mean(numbers))\n\n[1] 4\n\n\nfor example.\nFor simulating random experiments, R has the very useful function sample(). With this function we can - for example - toss a coin on the computer and conduct actual random experiments.\nThe function takes as arguments a vector names x and a number named size. sample will return size elements randomly chosen from the vector x. Lets say:\n\nsample(x = 0:1, size = 1)\n\n[1] 1\n\n\nIn this case sample has chosen either 0 or 1 from the vector x = (0,1) at random.\nIf we want to toss the coin in our computer we can thus pass the coin as an argument to sample and use the number 1 for the size argument. Lets do a few tosses with our die\n\nsample(x = coin, size = 1)\n\n[1] 1\n\nsample(x = coin, size = 1)\n\n[1] 0\n\nsample(x = coin, size = 1)\n\n[1] 1\n\nsample(x = coin, size = 1)\n\n[1] 0\n\n\nThese are the random outcomes of our consecutive tosses. It is as if we had thrown an actual coin but in this case we have done the same thing on the computer. Isn’t it cool that this is possible at all? The sample() function will remain our good friend throughout this course.\nR functions can have many arguments, but they need to be separated by a comma.\nEvery argument in every function has a name. We specify which data are assigned to the arguments by setting a mathematical equal sign = between the name and the data. Names help us to avoid passing the wrong data and thereby mixing up things or committing errors. But using names is not necessary. If we just wrote\n\nsample(coin,1)\n\n[1] 0\n\n\nR would also know what to do. It is not always clear which names to use for a function. If you are not sure, you can use the function args() to look it up. Here we take the function roundas one example.\n\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n\nNote that the digits argument in the round function is already set to 0. Frequently R functions come with optional arguments. These arguments are optional because the come with a default value, which is 0 in case of the round function.\nWe recommend that you write out argument names as a rule. It gives clearer code and avoids errors. If you don’t write argument names, R matches your values to the arguments of the function by order.\n\n\n1.2.4 Writing your own functions\nNow we are ready to write our own function to toss the coin in our computer. Each function in R has the same elements: A name, a function body of code and a set of arguments. To write your own function, you have to write up all of these parts and save them in an R object. The syntax is:\nmy_function &lt;- function() {}\n\nThe name here is my_function, next comes the expression function() which needs to be assigned. The names of the function arguments have to be written between the parentheses. Then we have to write the actual code within the braces {}.\nTo do this for the die, lets write a function named toss_coin.\n\ntoss_coin &lt;- function(){coin &lt;- 0:1 \n                         sample(coin, size = 1)}\n\nNow we can toss our coin for a few times to show how the function works\n\ntoss_coin()\n\n[1] 0\n\ntoss_coin()\n\n[1] 0\n\ntoss_coin()\n\n[1] 0\n\ntoss_coin()\n\n[1] 0\n\ntoss_coin()\n\n[1] 0\n\n\nNote that in our function toss_coin() has no arguments, just the function body. This is perfectly legitimate in R. It is important that when we call the function we have to call it with the parenthesis like toss_coin(). If we only call the name toss_coin, R will display the code in the function body.\nA final remark in the sample function is in place here. If we look at the arguments of sample using the args function we see\n\nargs(sample)\n\nfunction (x, size, replace = FALSE, prob = NULL) \nNULL\n\n\nLets do not discuss all the details of this output but concentrate for a moment on the replace argument. What does this mean?\nAs we saw previously we can use the sample function to model the rolling of our die. If we set the size argument to 1 we get the toss of one coin. If we set the size argument to n, we would simulate the tossing of n coins. But now the replace argument becomes crucial. As we can see in the output of the args function replace has a default value FALSE. This is a logical argument. It tells R, for example, that if we set size = 2, meaning that two coins are tossed, if the first coin shows, say Heads - a value of 1- the coin die cannot show Heads as well.\nThis is clearly not what we have in mind when we model the tossing of two coins. It should be possible that both coins show the same value. To enable this behavior of the sample function, we have to change the default value of the replace argument to TRUE. Then R chooses a random draw from all of the six possible values for all dice rolled.\nCongratulations ! You have written your first R function for conducting a simple random experiment. Let me remind you once again: Think of the parentheses as a trigger that tells R to run the function. If you omit the trigger R just prints the body of the function. When you run a function, all the code in the function body is executed and R returns the result of the last line of code. If the last line of code does not return a value neither will R.\n\n\n\n\n\n\nNow you try\n\n\n\nConsider another random experiment of throwing a six sided die. Use the concepts learned in this section to write a function which simulates the rolling of a six sided die. Write a function to simulate the throwing of two dice. Think about how to set the replace argument in the sample function in this case. Use R’s help function to learn about the role of the replace argument. Alternatively ask the LLM to explain the use of replace in R’s sample function. You can also ask for examples or demonstrations what goes wrong when this argument is parametrized in the wrong way.\n\n\n\n\n1.2.5 Arguments\nImagine we remove the first line of code in our function body and changed the name die in the sample function to “ball”.\n\ntoss_coin2 &lt;- function(){sample(ball, size = 1)}\n\nIf we call the function now, we will get an error. The function call roll_die2() will result in the error message Error in sample(ball, size = 1) : object 'ball' not found (try it!)\nWe could supply ball when we call toss_coin2 if we make ball an argument of the function. Lets do this:\n\ntoss_coin2 &lt;- function(ball){sample(ball, size = 1)}\n\nNow the function will work as long as we supply ball when we call the function.\n\ntoss_coin2(ball = 0:1)\n\n[1] 0\n\n\nNote that we still get an error, if we forget to supply ball argument. This could be avoided if we give the function a default argument\n\ntoss_coin2 &lt;- function(ball= 0:1){sample(ball, size = 1)}\n\nNow if we type:\n\ntoss_coin2()\n\n[1] 0\n\n\neverything works, just as intended.\n\n\n1.2.6 Scripts\nSo far we have worked by interacting with the console. But what if you want to edit your functions? It would be much easier, if you could use a draft of your code and work form there. This can be done by using a script.\nYou create a script by going to File &gt; New File &gt; R script in the menu bar of RStudio. Using scripts is the standard way to write code in R. It not only helps you to keep track of your code, save it and edit it later. It also makes your work reproducible. You can edit and proofread your code and share it with others. To save your script go to File &gt; Save As in the menu bar.\n\n\n\n\n\nThe RStudio Script\n\n\n\n\nRStudio has many useful features to help you work with scripts. You can for instance automatically execute a line in a code by using the run button. You can also execute sections of code or the entire script. The entire script is executed by running the Source button. For all these commands there are key short cuts which you will learn as you work more with RStudio and R.\nFrom now on you should write all your R-code for this course in scripts. Make a new folder for the scripts you write and store your R-code there from now on.\n\n\n1.2.7 Using Quarto Documents\nQuarto documents provide an elegant way to combine explanatory text, R code, and the resulting output in one cohesive file. This makes them especially useful for documenting your learning process, conducting reproducible research, or preparing assignments.\nLet’s walk through how to use a Quarto document to write down and experiment with your coin-tossing function.\n\nCreating a New Quarto Document: To begin, create a new Quarto document in RStudio by navigating to File &gt; New File &gt; Quarto Document.... You will see a popup window allowing you to type a title for the document and your name. It also allows you to select a format for your document. Choose the HTML format for now, as it allows easy preview in your browser, and click the Create button The popup window will look something like this:\n\n\n\n\n\n\nThe quarto pop up window\n\n\n\n\n\nUnderstanding the Quarto Layout: The new Quarto document will have a YAML header at the top, which might look like this:\n\n\n\n\n\n\nThe quarto yaml header\n\n\n\n\nBelow this header, you will see some example text and R code chunks. Replace the example content with your own text and code.\n\nAdding the Coin Tossing Function: In your Quarto document, you can explain your approach to creating the coin-tossing function, and include the code in a code chunk.\nHere’s an example:\n\n\n\n\n\n\nCombining text and code in quarto documents\n\n\n\n\n\nRunning and Compiling the Document: Once you have written your text and code, save your document. You can execute the R code directly within the Quarto document. To run a code chunk, click the green play button in the upper-right corner of the chunk or use the shortcut Ctrl + Shift + Enter (Windows) or Cmd + Shift + Enter (Mac).\nAfter executing your code, you can compile the document into a readable output format by clicking the Render button or using the shortcut Ctrl + Shift + K (Windows) or Cmd + Shift + K (Mac).\n\nThere is of course a lot you can do to enhance your quarto documents. For example the lecture notes you are reading now are all written and composed in quarto as are the lecture slides.\nBy using Quarto documents, you can create well-documented, interactive code files that are both functional and easy to share with others. You can use quarto indpendently from R and combine it with other languages like phyton or julia. Ceck out: https://quarto.org/\n\n\n1.2.8 Using packages and finding Help\nWe have now a function which we can use to simulate the tossing of a coin, coin_toss(). If the coin is fair it should be the case that if we toss the coin often, Heads and Tails should occur about equally often. The coin should not be weighted in favor of a particular value.\nOne way to learn whether our coin is fair are repetition and visualization. These are tools we will need all the time, when working with data and when doing probability. While R has many useful functions, one of the great powers is that R is constantly extended by a huge community of users by providing packages.\nPackages are add on functions, which will not be available when you install R. They need to be installed and loaded before you can use them. Since packages are such a powerful tool in R we need to introduce what they are and how to use them in the beginning.\n\n\n1.2.9 Packages\nThere are many visualization tools in R that come with the basic installation. Since the point we want to make here is about packages, we will use a visualization function which is part of the add on package ggplot2, a very popular package for making all kinds of graphs. ggplot2 is not part of the base-R distribution. It has to be therefore loaded before we can use it.\nThere are many additional functions provided with this package. Indeed you could run a whole course on just that 3\n3 A very good starting point is for instance Healy (2019), in case you are interested. Also a good source is the first chapter in Wickham and Grolemund (2017).Since we want to use functios now which are not in the base R distribution. we need to load the package which provides the functions we want. In this example we use the package ggplot2.\nWhen we use a package for the first time it needs to be installed. Once installed it does not have to be reinstalled when we use it at later coding sessions again.\nTo install a package you need to be connected to the internet. If you have internet connection go to the command line and run at the command line: install.packages(\"ggplot2\").\nR displays what is happening while executing the command. Don’t worry if you don not know what all of these messages exactly mean and don’t panic that they are displayed in red. All packages can be installed like this. You have just to enter the correct name in the function install.packages(). The lower right pane in the RStudio software alternatively provides a tab called Packages. Using this tab, you can also install R packages interactively by clicking the install button on the upper left corner of the Packages tab.\nAfter installation the package is on our hard-drive but it can not yet be used. To use the package it has to be loaded. This is done with the command library. To load the ggplot2 package we type\n\nlibrary(\"ggplot2\")\n\nand hit the return key. Many things could be said about the R package system and you will learn it in more detail as we go along in our course. For the moment the most important thing to remember is that a package needs to be newly loaded whenever you want to use it in a new R session.\nTo check whether our coin is fair, we need to toss it many times. R provides a function, that does this for us. This function is called replicate() and provides an easy way to repeat a command many times. The number of times we want to repeat something is given as an argument to the replicate function.\nNow lets roll our die 100 times and save the result in an object we call tosses:\n\nset.seed(123)\ntosses &lt;- replicate(100, toss_coin())\n\nThe function set.seed() I used here with the argument 123 is just a device to make this experiment reproduceable. If this function is not used, each new run of the 100 replications would show a different outcome.\nI now show you a code for visualizing this outcome. The code contains some concepts that we did not yet discuss. Don’t worry. We will learn them soon. The point here is to shwo you how we use the functions of the package to produce a visualization of the outcome of our 100 coin tosses.\n\n# Create a data frame for plotting\ntosses_df &lt;- data.frame(\n  Outcome = factor(tosses, levels = c(0, 1), labels = c(\"Tails\", \"Heads\"))\n)\n\n# Plot the results using ggplot2\nggplot(tosses_df, aes(x = Outcome)) +\n  geom_bar(fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Occurrences of Heads and Tails\",\n    x = \"Outcome\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt looks like every value occurred roughly 50 times but there is still quite some variation. Tail occured fewer times than Heads.\nMaybe we have to give it another trial with more replications. With the computer we can do this with a fingertip. Let us roll our die 1000 times and plot the result.\nHere is how we would code this by recycling the code used before.\n\nset.seed(123)\ntosses &lt;- replicate(1000, toss_coin())\n\n# Create a data frame for plotting\ntosses_df &lt;- data.frame(\n  Outcome = factor(tosses, levels = c(0, 1), labels = c(\"Tails\", \"Heads\"))\n)\n\n# Plot the results using ggplot2\nggplot(tosses_df, aes(x = Outcome)) +\n  geom_bar(fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Occurrences of Heads and Tails\",\n    x = \"Outcome\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNow it looks much better: Each face comes up approximately 500 times. We will learn in our course why the picture get better as we increase the number of rolls of the die. For the moment, let us just state that at least for this case it seems that there is no significant visual evidence that our virtual die is loaded.\nBut wait a minute. We have introduced the idea of classical probability by constructing equally probable cases and then count. Here we have repeated a random experiment over and over again to draw some conclusion on the underlying probabilities by visualizing the frequency polts of outcomes? These ideas are connected. We have just informally introduced a new idea in probability, the relative frequency notion of probability. How the two concepts are related and why this notion makes sense we will learn more formally in the next lecture. For the moment let’s just stick with the idea that practitioners have used at all times to assess probabilities by looking at frequencies of particular events occurring. When and why this kind of reasoning about probability works we learn in the next lecture.\n\n\n\n\n\n\nTip for R novices: How to leverage the LLM to enhance understanding\n\n\n\nLet me give you a hint here how you could use the LLM to enhance your learning experience in another way. The students usually taking this bootcamp class have very different backgrounds. Some have much experience with programming but perhaps not necessarily in R. Many of you will - for example - know Python, because most of the MQF is taught and run with Python. In such a case you could use the LLM to translate the R code into Python and then compare.\nOr assume you are impatient and want to jump ahead by having somebody explaining you the ggplot2 code we used for the plotting function. Ask the LLM to explain you the R code and what it does, and I assure you you will get a pretty good and well explained answer.\nTry it!\n\n\n\n\n1.2.10 Getting Help\nWe have now learned a tiny number of R functions and we have written one function ourselves. We have learned how to make use of functions provided by packages.\nIt would be overwhelming to memorize and learn them all. In R, fortunately, every function comes with a detailed documentation and with its own help page. You need to learn how to use this source right from the beginning.\nTo access the help page of a function you type the function name preceded by a question mark at the prompt, like this\n\n?sample\n\nThen, after you have pressed the return key, a help page is opened in the right lower pane under the help tab as shown in Figure 1.5\n\n\n\n\n\n\n\n\nFigure 1.5: The Help-pane in RStudio\n\n\n\n\n\nThis help page has a particular structure that you will find for every other R function no matter whether it is provided by the base installation or by a package. In the upper left corner you see the name of the function (sample) and in curly brackets next to it the term base, which means that this is a function in the R base installation. Then you see a headline about what the function does.\nFrom the top of the page, you then first see the field Description. This is a short description what the function does. Here it says\nDescription\nsample takes a sample of the specified size from the elements of x using either with or without replacement.\nThe next field is Usage. It gives you the function description with the arguments. Here for example\nUsage\nsample(x, size, replace = FALSE, prob = NULL)\n\nsample.int(n, size = n, replace = FALSE, prob = NULL,\n           useHash = (!replace && is.null(prob) && size &lt;= n/2 && n &gt; 1e7))\n           \nThe first line in Usage should by now be familiar. Don’t worry about the second line. The function can obviously do more than we know so far.\nNext comes a list of arguments the function takes and what type of information R expects you to provide, as well as what R will do with this information. Here it says for example\nArguments\nx   \neither a vector of one or more elements from which to choose, or a positive integer. See ‘Details.’\n\nn   \na positive number, the number of items to choose from. See ‘Details.’\n\nsize    \na non-negative integer giving the number of items to choose.\n\nreplace \nshould sampling be with replacement?\n\nprob    \na vector of probability weights for obtaining the elements of the vector being sampled.\n\n...\n\nWe omit some information here.\nThen comes a field called Details which gives a more in-depth description of the function. The next field is called Value. It describes what the function returns when you run it. Then we have a reference to related R functions under the field See. Finally there is a field called Examples. This field contains example code that is guaranteed to work. It shows a couple of different cases how you can use the function in practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#introduction-real-world-importance-of-unique-identifiers-in-finance",
    "href": "01-lecture1.html#introduction-real-world-importance-of-unique-identifiers-in-finance",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.1 Introduction: Real-World Importance of Unique Identifiers in Finance",
    "text": "2.1 Introduction: Real-World Importance of Unique Identifiers in Finance\nIn modern financial systems, every transaction is assigned a unique identifier, allowing stakeholders to track, verify, and manage transaction details efficiently. These identifiers are critical for maintaining transparency, accountability, and security across high-volume transaction systems. However, as the volume of transactions grows, the risk of identifier collisions—where two different transactions are assigned the same identifier—increases.\nThis issue is analogous to the birthday problem: as the number of transactions increases, the likelihood of collisions rises due to the combinatorial nature of the problem. A collision could result in processing errors or even security vulnerabilities, such as fraudulent transactions or misattribution of funds.\nTo mitigate such risks, many financial systems employ hash functions, such as the widely used SHA-256, to map data to a large space of possible outputs. These hash functions, with their vast output range (e.g., $2^{256}), make collisions extremely unlikely, even for billions of transactions. By understanding the principles behind collision probabilities, financial institutions can design systems that maintain transaction integrity even under high workloads.\nIn this project, we will explore this critical aspect of financial systems by simulating transaction identifiers and analyzing collision probabilities. This exercise demonstrates how theoretical probability concepts translate directly to practical challenges in computational finance.8\n8 For more information and details you might want to check this source: https://www.hostmerchantservices.com/articles/what-are-transaction-ids/?utm_source=chatgpt.comIn this project, you will:\n\nSimulate a system assigning transaction identifiers.\nInvestigate the probability of identifier collisions under different scenarios.\nDiscuss implications for system design and security.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#simulate-transaction-identifiers",
    "href": "01-lecture1.html#simulate-transaction-identifiers",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.2 Simulate transaction identifiers",
    "text": "2.2 Simulate transaction identifiers\n\nWrite a function to simulate transaction identifiers, where each identifier is randomly chosen from a pool of size \\(M\\) (e.g. \\(M = 10^6\\) or \\(M = 10^9\\))\nUsing your function, simulate \\(n\\) transactions and check for duplicates using R’s duplicated() function.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#compute-collision-probabilities",
    "href": "01-lecture1.html#compute-collision-probabilities",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.3 Compute collision probabilities",
    "text": "2.3 Compute collision probabilities\n\nSimulate nn transactions multiple times and estimate the collision probability as the fraction of simulations with at least one collision.\nUse this function to compute the collision probability for \\(n = 10^3, n = 10^6, n = 10^9\\) and \\(M = 10^6, M = 10^9\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#visualize-the-results",
    "href": "01-lecture1.html#visualize-the-results",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.4 Visualize the results",
    "text": "2.4 Visualize the results\nPlot the collision probability as a function of nn for different values of \\(M\\). We did not cover Rs plotting functions in the lecture so far. We only showed some examples which we did not explain. But we explained how you could research R’s help system and how you could ask an LLM for help. With any of these resources you should be able to come up with some reasonable visualization.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#investigate-the-system-design",
    "href": "01-lecture1.html#investigate-the-system-design",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.5 Investigate the system design",
    "text": "2.5 Investigate the system design\n\nReflect on how the size of the pool \\(M\\) influences the collision risk.\nWhat happens when the transaction volume \\(n\\) increases significantly?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#financial-implications",
    "href": "01-lecture1.html#financial-implications",
    "title": "1  First probability ideas and first steps in R",
    "section": "2.6 Financial implications",
    "text": "2.6 Financial implications\n\nImagine a financial institution processing transactions with \\(M=10^6\\) identifiers.\nIf the daily transaction volume grows to \\(n=10^5\\), what is the risk of a collision?\nDiscuss potential consequences of a collision (e.g., failed transactions, fraud risks) and propose ways to mitigate these risks (e.g., increasing \\(M\\), using hash functions).\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#project-1-detecting-collisions-in-financial-transactions",
    "href": "01-lecture1.html#project-1-detecting-collisions-in-financial-transactions",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.5 Project 1: Detecting collisions in financial transactions",
    "text": "1.5 Project 1: Detecting collisions in financial transactions\n\n1.5.1 Introduction: Real-World Importance of Unique Identifiers in Finance\nIn modern financial systems, every transaction is assigned a unique identifier, allowing stakeholders to track, verify, and manage transaction details efficiently. These identifiers are critical for maintaining transparency, accountability, and security across high-volume transaction systems. However, as the volume of transactions grows, the risk of identifier collisions—where two different transactions are assigned the same identifier—increases.\nThis issue is analogous to the birthday problem: as the number of transactions increases, the likelihood of collisions rises due to the combinatorial nature of the problem. A collision could result in processing errors or even security vulnerabilities, such as fraudulent transactions or misattribution of funds.\nTo mitigate such risks, many financial systems employ hash functions, such as the widely used SHA-256, to map data to a large space of possible outputs. These hash functions, with their vast output range (e.g., $2^{256}), make collisions extremely unlikely, even for billions of transactions. By understanding the principles behind collision probabilities, financial institutions can design systems that maintain transaction integrity even under high workloads.\nIn this project, we will explore this critical aspect of financial systems by simulating transaction identifiers and analyzing collision probabilities. This exercise demonstrates how theoretical probability concepts translate directly to practical challenges in computational finance.8\n8 For more information and details you might want to check this source: https://www.hostmerchantservices.com/articles/what-are-transaction-ids/?utm_source=chatgpt.comIn this project, you will:\n\nSimulate a system assigning transaction identifiers.\nInvestigate the probability of identifier collisions under different scenarios.\nDiscuss implications for system design and security.\n\n\n\n1.5.2 Simulate transaction identifiers\n\nWrite a function to simulate transaction identifiers, where each identifier is randomly chosen from a pool of size \\(M\\) (e.g. \\(M = 10^6\\) or \\(M = 10^9\\))\nUsing your function, simulate \\(n\\) transactions and check for duplicates using R’s duplicated() function.\n\n\n\n1.5.3 Compute collision probabilities\n\nSimulate nn transactions multiple times and estimate the collision probability as the fraction of simulations with at least one collision.\nUse this function to compute the collision probability for \\(n = 10^3, n = 10^6, n = 10^9\\) and \\(M = 10^6, M = 10^9\\)\n\n\n\n1.5.4 Visualize the results\nPlot the collision probability as a function of nn for different values of \\(M\\). We did not cover Rs plotting functions in the lecture so far. We only showed some examples which we did not explain. But we explained how you could research R’s help system and how you could ask an LLM for help. With any of these resources you should be able to come up with some reasonable visualization.\n\n\n1.5.5 Investigate the system design\n\nReflect on how the size of the pool \\(M\\) influences the collision risk.\nWhat happens when the transaction volume \\(n\\) increases significantly?\n\n\n\n1.5.6 Financial implications\n\nImagine a financial institution processing transactions with \\(M=10^6\\) identifiers.\nIf the daily transaction volume grows to \\(n=10^5\\), what is the risk of a collision?\nDiscuss potential consequences of a collision (e.g., failed transactions, fraud risks) and propose ways to mitigate these risks (e.g., increasing \\(M\\), using hash functions).\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#exercises-and-project",
    "href": "01-lecture1.html#exercises-and-project",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.5 Exercises and Project:",
    "text": "1.5 Exercises and Project:\n\n1.5.1 Exercises\n\n1.5.1.1 Exercise 1: Simulate Rolling Two Dice\n\nWrite a function to simulate rolling a pair of dice. Each die produces a random number between 1 and 6. The function should return the sum of the two dice.\nSimulate 10,000 rolls and store the results. Use table() to compute the frequencies of each sum and visualize the results with a histogram.\nCreate a histogram. Use the help function of R to find out how to do this.\n\n\n\n1.5.1.2 Exercise 2: Analyze the Fairness of Dice\n\nCompute the theoretical probabilities of all possible outcomes (sums from 2 to 12) for fair dice. Recall that each outcome corresponds to different combinations of the two dice.\nCompare the theoretical probabilities to the simulated frequencies. Are the dice fair? Justify your conclusion.\n\n\n\n1.5.1.3 Exercise 3: Simulate Loaded Dice\n\nAssume the dice are “loaded” such that numbers 1, 2, 3, 4, and 5 have probabilities of ( ), while 6 has a probability of ( ).\nModify your simulation using the prob argument in sample() to reflect these probabilities.\nSimulate 10,000 rolls of the loaded dice and visualize the results. How does the distribution of sums change compared to fair dice?\n\n\n\n1.5.1.4 Exercise 4: The Probability of Specific Sums\n\nUsing your theoretical understanding of the sample space, calculate the probabilities of:\n\nRolling a sum of 7.\nRolling a sum of 2.\nRepeat for both fair and loaded dice.\n\nVerify your theoretical results with your simulations.\n\n\n\n1.5.1.5 Exercise 5: Evaluating a Betting Strategy\n\nImagine you are at a casino with fair dice. You have €150 to spend, and each bet on the sum of 7 costs €0.15. If you win, the casino pays €1. Otherwise, you lose your bet.\nSimulate 1,000 bets on the sum of 7:\n\nCalculate the total amount you win or lose.\nRepeat the simulation 10,000 times to estimate the expected profit or loss.\n\n\n\nIs betting on 7 a good strategy? Why, why not?\n\n\n\n1.5.1.6 Exercise 6: Explore Alternative Scenarios\n\nAssume the casino uses loaded dice (as defined earlier) instead of fair dice. How does this change your betting strategy and expected profit? Perform the simulation again under these conditions.\n\n-Reflect on the importance of fairness in games of chance and how loaded dice impact the house advantage.\n\n\n\n1.5.2 Project 1: Detecting collisions in financial transactions\n\n1.5.2.1 Introduction: Real-World Importance of Unique Identifiers in Finance\nIn modern financial systems, every transaction is assigned a unique identifier, allowing stakeholders to track, verify, and manage transaction details efficiently. These identifiers are critical for maintaining transparency, accountability, and security across high-volume transaction systems. However, as the volume of transactions grows, the risk of identifier collisions—where two different transactions are assigned the same identifier—increases.\nThis issue is analogous to the birthday problem: as the number of transactions increases, the likelihood of collisions rises due to the combinatorial nature of the problem. A collision could result in processing errors or even security vulnerabilities, such as fraudulent transactions or misattribution of funds.\nTo mitigate such risks, many financial systems employ hash functions, such as the widely used SHA-256, to map data to a large space of possible outputs. These hash functions, with their vast output range (e.g., $2^{256}), make collisions extremely unlikely, even for billions of transactions. By understanding the principles behind collision probabilities, financial institutions can design systems that maintain transaction integrity even under high workloads.\nIn this project, we will explore this critical aspect of financial systems by simulating transaction identifiers and analyzing collision probabilities. This exercise demonstrates how theoretical probability concepts translate directly to practical challenges in computational finance.9\n9 For more information and details you might want to check this source: https://www.hostmerchantservices.com/articles/what-are-transaction-ids/?utm_source=chatgpt.comIn this project, you will:\n\nSimulate a system assigning transaction identifiers.\nInvestigate the probability of identifier collisions under different scenarios.\nDiscuss implications for system design and security.\n\n\n\n1.5.2.2 Simulate transaction identifiers\n\nWrite a function to simulate transaction identifiers, where each identifier is randomly chosen from a pool of size \\(M\\) (e.g. \\(M = 10^6\\) or \\(M = 10^9\\))\nUsing your function, simulate \\(n\\) transactions and check for duplicates using R’s duplicated() function.\n\n\n\n1.5.2.3 Compute collision probabilities\n\nSimulate nn transactions multiple times and estimate the collision probability as the fraction of simulations with at least one collision.\nUse this function to compute the collision probability for \\(n = 10^3, n = 10^6, n = 10^9\\) and \\(M = 10^6, M = 10^9\\)\n\n\n\n1.5.2.4 Visualize the results\nPlot the collision probability as a function of nn for different values of \\(M\\). We did not cover Rs plotting functions in the lecture so far. We only showed some examples which we did not explain. But we explained how you could research R’s help system and how you could ask an LLM for help. With any of these resources you should be able to come up with some reasonable visualization.\n\n\n1.5.2.5 Investigate the system design\n\nReflect on how the size of the pool \\(M\\) influences the collision risk.\nWhat happens when the transaction volume \\(n\\) increases significantly?\n\n\n\n1.5.2.6 Financial implications\n\nImagine a financial institution processing transactions with \\(M=10^6\\) identifiers.\nIf the daily transaction volume grows to \\(n=10^5\\), what is the risk of a collision?\nDiscuss potential consequences of a collision (e.g., failed transactions, fraud risks) and propose ways to mitigate these risks (e.g., increasing \\(M\\), using hash functions).\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "index.html#using-an-llm-to-enhance-your-learning-experience",
    "href": "index.html#using-an-llm-to-enhance-your-learning-experience",
    "title": "An Introduction to Probability",
    "section": "Using an LLM to Enhance Your Learning Experience",
    "text": "Using an LLM to Enhance Your Learning Experience\nIn these lectures I try to help you building experience how to integrate large language models or LLMs into your learning and study process as well as to support you in coding and programming. There are several such models currently on the market. For this course I will use openAI’s chatGPT as a tool. ChatGPT is available in a free version and in an enhanced pay-version.\nThroughout this course, you’ll learn how to use ChatGPT to support your studies. As a tool, it offers capabilities that can make learning probability more interactive and engaging. Here’s how you can leverage it:\n\nClarify Concepts:\nAsk ChatGPT to explain concepts in simple terms or elaborate on topics you find difficult:\n\n“Explain the concept of a probability distribution with an example.”\n\nGenerate Examples:\nUse it to create additional examples for practice:\n\n“Can you give me an example of a random variable and how it applies in finance?”\n\nDebug R Code:\nIf you encounter issues with your R code, paste the code and ask:\n\n“Why does this R code not run, and how can I fix it?”\n\nPractice Exercises:\nRequest custom exercises:\n\n“Create three exercises to practice calculating probabilities for dice rolls.”\n\nSimulate Interactive Discussions:\nSimulate discussions with ChatGPT to test your understanding:\n\n“I think the variance of a constant is zero. Am I correct? Explain why or why not.”\n\nLearn Best Practices in R:\nAsk for tips to improve your coding practices:\n\n“What are the best practices for writing clean and efficient R code?”\n\n\nImportant Note:\nWhile ChatGPT is a powerful learning tool, remember to verify the outputs, especially for complex calculations, and always critically evaluate its suggestions. One such tool you might consider for cross checking is Wolfram | Alpha, which you can find here: https://www.wolframalpha.com/ in a free version.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-lecture1.html#project-designing-transaction-identifyers-for-digital-payment-systems",
    "href": "01-lecture1.html#project-designing-transaction-identifyers-for-digital-payment-systems",
    "title": "1  First probability ideas and first steps in R",
    "section": "1.5 Project: Designing transaction identifyers for digital payment systems",
    "text": "1.5 Project: Designing transaction identifyers for digital payment systems\n\n1.5.1 Introduction: Real-World Importance of Unique Identifiers in Finance\nIn modern financial systems, every transaction is assigned a unique identifier, allowing stakeholders to track, verify, and manage transaction details efficiently. These identifiers are critical for maintaining transparency, accountability, and security across high-volume transaction systems. However, as the volume of transactions grows, the risk of identifier collisions—where two different transactions are assigned the same identifier—increases.\nThis issue is analogous to the birthday problem: as the number of transactions increases, the likelihood of collisions rises due to the combinatorial nature of the problem. A collision could result in processing errors or even security vulnerabilities, such as fraudulent transactions or misattribution of funds.\nTo mitigate such risks, many financial systems employ hash functions, such as the widely used SHA-256, to map data to a large space of possible outputs. These hash functions, with their vast output range (e.g., \\(2^{256}\\)), make collisions extremely unlikely, even for billions of transactions. By understanding the principles behind collision probabilities, financial institutions can design systems that maintain transaction integrity even under high workloads.\nIn this project, we will explore this critical aspect of financial systems by simulating transaction identifiers and analyzing collision probabilities. This exercise demonstrates how theoretical probability concepts translate directly to practical challenges in computational finance.9\n9 For more information and details you might want to check this source: https://www.hostmerchantservices.com/articles/what-are-transaction-ids/?utm_source=chatgpt.comIn this project, you will:\n\nSimulate a system assigning transaction identifiers.\nInvestigate the probability of identifier collisions under different scenarios.\nDiscuss implications for system design and security.\n\n\n\n1.5.2 Simulate transaction identifiers\n\nWrite a function to simulate transaction identifiers, where each identifier is randomly chosen from a pool of size \\(M\\) (e.g. \\(M = 10^6\\) or \\(M = 10^9\\))\nUsing your function, simulate \\(n\\) transactions and check for duplicates using R’s duplicated() function.\n\n\n\n1.5.3 Compute collision probabilities\n\nSimulate nn transactions multiple times and estimate the collision probability as the fraction of simulations with at least one collision.\nUse this function to compute the collision probability for \\(n = 10^3, n = 10^6, n = 10^9\\) and \\(M = 10^6, M = 10^9\\)\n\n\n\n1.5.4 Visualize the results\nPlot the collision probability as a function of nn for different values of \\(M\\). We did not cover Rs plotting functions in the lecture so far. We only showed some examples which we did not explain. But we explained how you could research R’s help system and how you could ask an LLM for help. With any of these resources you should be able to come up with some reasonable visualization.\n\n\n1.5.5 Investigate the system design\n\nReflect on how the size of the pool \\(M\\) influences the collision risk.\nWhat happens when the transaction volume \\(n\\) increases significantly?\n\n\n\n1.5.6 Financial implications\n\nImagine a financial institution processing transactions with \\(M=10^6\\) identifiers.\nIf the daily transaction volume grows to \\(n=10^5\\), what is the risk of a collision?\nDiscuss potential consequences of a collision (e.g., failed transactions, fraud risks) and propose ways to mitigate these risks (e.g., increasing \\(M\\), using hash functions).\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html",
    "href": "02-lecture2.html",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "",
    "text": "2.1 Terminology\nProbability is a mathematical theory that provides a rigorous framework for understanding uncertainty. It is particularly useful when analyzing real-world phenomena, such as random sampling in surveys or fluctuations in stock prices.\nLet us now formalize the foundational concepts of this theory:\nTo clarify this concept, we define uncertain outcomes in advance. We pin down the possible outcomes by agreeing on the outset what we want to consider as the possible outcomes. Take the simple example of considering whether the price of a stock is going to rise or fall at the next day. In a practical situations the outcome of a move in the stock price can be that it rises or falls but it could in principle also stay the same. Still when we think about the experiment of observing the stock price tomorrow in many applications in Finance we usually agree that rise and fall are the only possible outcomes of this experiment. When we want to consider the random experiment that includes the case that the price stays the same we need to agree upfront the the possible outcomes are rise, fall and unchanged.\nThis collection of all possible outcomes in probability theory is called the\nIn the example before when we look at rises and falls in the stock price we would have \\({\\cal S} = \\{rise, fall \\}\\)\nExample: For {S} = {rise,fall}$, the event \\(\\{\\text{rise}\\}\\) is a simple event, while the event \\(\\{\\text{rise, fall}\\}\\) includes all outcomes and corresponds to the entire sample space.\nIn the first lecture we learned about two approaches to measure probability. But the theory of probability actually does not depend on how we measure it precisely. In the theory of probability this measure is an abstract concept.\nThus when we talk about probability in a precise and meaningful sense we can only do so in relation to a given sample space or to a certain conceptual experiment. In theory, probability is an abstract measure of uncertainty that does not depend on how it is observed or estimated. While practical applications may involve approximating probabilities using data, the mathematical theory assumes probabilities are predefined and satisfy specific rules.\nI have itroduced here probabilities with relation to a discrete sample space. The sample spaces discussed in Lecture 1 include finite sets, which can be small or large, such as the extensive output space of the SHA-256 hash function.\nThere are also more complicated discrete sample spaces: Think of the random experiment of tossing a coin as often as necessary to see Heads for the first time. We can begin writing down the basic outcomes as: \\(E_1=H, E_2=TH, E_3 = TTH, E_4 = TTTH, ...\\). An event where Heads never appear, denoted \\(E0\\), may also be considered. In this case, when the basic events can be arranged into a simple sequence. A sample space is called discrete if it contains only finitely many points, or infinitely many points which can be arranged into a simple sequence.\nNot all sample spaces are discrete. Except for the technical tools required there is no essential difference between the two cases. In our discussion of probability in this lecture we consider mostly discrete sample spaces, however we will also discuss some basic non-discrete sample spaces later in the lectures. Now let us state more formally what we mean by probability.\nBefore we go on let me further clarify what we mean when we say that in the theory, probability is an abstract measure of uncertainty which we take as given.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#terminology",
    "href": "02-lecture2.html#terminology",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "",
    "text": "Random Experiment\n\nA process with a set of possible outcomes, where the specific outcome cannot be predicted with certainty beforehand.\n\n\n\n\n\nSample space\n\nThe collection of all possible outcomes of an experiment is called the sample space and is denoted as the set \\({\\cal S}\\)\n\n\n\n\nBasic outcome, event, simple event\n\nA basic outcome is a single possible result of a random experiment. An event is a subset of the sample space, representing one or more outcomes. A simple event is an event containing exactly one basic outcome.\n\n\n\n\n\nProbability\n\nFor a given (discrete) sample space ${S}, Probability is a function that assigns a value to each event, representing its likelihood. The function has to fulfill three properties: 1. \\(P({\\cal S}) = 1\\). 2. For any event \\(A \\in {\\cal S}\\), \\(0 \\leq P(A) \\leq 1\\). The probability of an event can never be negative or larger than 1. 3. For mutually exclusive events \\(A\\) and \\(B\\), \\(P(A \\cup B) \\leq P(A) + P(B)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#probability-theory-and-applications",
    "href": "02-lecture2.html#probability-theory-and-applications",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.2 Probability in theory and applications of probability",
    "text": "2.2 Probability in theory and applications of probability\nProbabilities are expressed as numbers between 0 and 1. As mentioned by Feller (1968) in his famous probability textbook, these numbers are of the same nature as distances in geometry. In the theory we assume they are given to us.\nFrom the viewpoint of probability theory, we need not assume anything about how they are measured. In this sense probabilities in the theory of probability are an abstract measure of uncertainty.\nIn practical applications, determining probabilities or applying theory often requires sophisticated statistical methods. So, while the mathematical as well as the intuitive meaning of probability are clear only as we proceed with the theory we will get a better ability to see how we can apply this concept.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#basic-rules-of-probability",
    "href": "02-lecture2.html#basic-rules-of-probability",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.3 Basic rules of probability",
    "text": "2.3 Basic rules of probability\nWe already saw that the classical notion of probability directly lead to a few simple rules. In fact in the theory of probability we require these rules to hold more generally no matter how probability is measured in actual applications.\nWhen we talk of probabilities we mostly consider probabilities with relation to a discrete sample space. The sample spaces discussed in Lecture 1 include finite sets, which can be small or large, such as the extensive output space of the SHA-256 hash function..\nThere are also more complicated discrete sample spaces: Think of the random experiment of tossing a coin as often as necessary to see Heads for the first time. We can begin writing down the basic outcomes as: \\(E_1=H, E_2=TH, E_3 = TTH, E_4 = TTTH, ...\\). An event where Heads never appear, denoted \\(E0\\), may also be considered. In this case, when the basic events can be arranged into a simple sequence. A sample space is called discrete if it contains only finitely many points, or infinitely many points which can be arranged into a simple sequence.\nNot all sample spaces are discrete. Except for the technical tools required there is no essential difference between the two cases. In our discussion of probability in this lecture we consider mostly discrete sample spaces, however we will also discuss some basic non-discrete sample spaces later in the lectures. Now let us state more formally what we mean by probability.\n\nProbability\n\nGiven a (discrete) sample space \\({\\cal S}\\) the probability assigned to events in this sample space must always fulfill three rules:\n\n\n\n\\(P({\\cal S}) = 1\\), where \\({\\cal S}\\) is the sample space.\nFor any event \\(A \\in {\\cal S}\\), \\(0 \\leq P(A) \\leq 1\\). The probability of an event can never be negative or larger than 1.\nThe probability of the union of two events \\(A\\) and \\(B\\) is always smaller or equal to the sum of the probability of these events looked at in isolation: \\(P(A \\cup B) \\leq P(A) + P(B)\\).\n\nNote that these three properties resulted naturally from the classic idea to make equally probable cases and count. Property 3 looks slightly different but we will shortly see that it amounts to the same thing.\nIn the theory of probability we use the language of sets and set theory to describe relations among event. We have used one of these relations in rule three, where we expressed the relation between two sets as their union \\(\\cup\\). It is useful in studying probability to know a few of these set theoretic definitions.\nLet’s go through them and illustrate the concepts in the context of the examples we have already developed in lecture 1.\n\nUnion\n\nThe union of two events \\(A\\) and \\(B\\) is the set that contains all events that are either in \\(A\\) or in \\(B\\) or in both sets. Set union is written as \\(A \\cup B\\).\n\n\nLet us use the example of the experiment of rolling a die. The sample space \\({\\cal S}\\) is the set of all possible outcomes of rolling the dice \\({\\cal S}=\\{1,2,3,4,5,6\\}\\). Assume one event is that the outcome is 1, 2 or 3. In set notation, we would write \\(A = \\{1,2,3\\}\\). Let us also assume that the second event is that the outcome is some even number, i.e. 2, 4, 6. Again using set notation we would write \\(B = \\{2,4,6\\}\\). The event \\(A \\cup B\\) is then the set of all outcomes such that the outcome is in \\(A\\) or in \\(B\\) or in both or these sets. In such simple examples it is sometimes helpful to depict the situation graphically.\n\n\n\n\n\nThe meaning of set union\n\n\n\n\nThe sample space \\({\\cal S}\\) is the gray set containing all possible outcomes of our random experiment. Graphically the union of \\(A\\) and \\(B\\), \\(A \\cup B\\) is a subset of the sample space, the entire colored area.\nBy the way and as an aside: R provides functions for computing set operations. Let us use the occasion to show you briefly how to use these functions in the context of this example: We define the sets \\(A\\) and \\(B\\) first using the assignment operator:\n\nA &lt;- c(1,2,3)\nB &lt;- c(2,4,6)\n\nWe compute the union by using the function union()\n\nunion(A,B)\n\n[1] 1 2 3 4 6\n\n\nwhich gives us the result we have already derived graphically.\n\nIntersection\n\nThe intersection of two events is the set that contains all events that are both in \\(A\\) and in \\(B\\). Set intersection is written as \\(A \\cap B\\).\n\n\nWe can again illustrate this concept graphically\n\n\n\n\n\nThe meaning of set intersection\n\n\n\n\nThe intersection of \\(A\\) and \\(B\\), \\(A \\cap B\\) is the orange area containing the dice face with two points. Indeed two is both in \\(A\\) and in \\(B\\), which is exactly the meaning of set intersection.\nThe R function for computing intersections is called intersect(). We call this function for our example\n\nintersect(A,B)\n\n[1] 2\n\n\nwhich gives us the result we have already derived graphically.\n\nComplement\n\nThe complement of an event \\(A\\) with respect to an event \\(B\\) is the set of all elements that are in \\(B\\) but not in \\(A\\). Set difference is written as \\(A \\setminus B\\).\n\n\nLets say we want to get the complement, or the set difference of \\(A \\cup B\\) with respect to the sample space \\({\\cal S}\\). Let us show \\({\\cal S} \\setminus (A \\cup B)\\).\n\n\n\n\n\nThe meaning of complement\n\n\n\n\nThis complement is the dice shown in the light redish area, i.e. all the elements of \\({\\cal S}\\) which are not in \\(A \\cup B\\).\nThe R function for computing set complements is called setdiff(). You can try it with our example:\n\nS &lt;- c(1,2,3,4,5,6)\n\nsetdiff(S, union(A,B))\n\n[1] 5\n\n\nwhich is 5, as expected.\n\nMutually Exclusive\n\nTwo events \\(A\\) and \\(B\\) are said to be mutually exclusive if they have no basic outcomes in common. The notation is \\(A \\cap B = \\emptyset\\)\n\n\nAn example in our context is the set of even outcomes \\(B=\\{2,4,6\\}\\) and the set of odd outcomes, let us call it \\(C=\\{1,3,5\\}\\). If we intersect these sets\n\nB &lt;- c(2,4,6)\nC &lt;- c(1,3,5)\n\nintersect(B,C)\n\nnumeric(0)\n\n\nwe get the empty set, which is expressed by R by giving the data type, in this case numeric, because we are intersecting sets of numeric values, followed by (0). This means, there is no numeric value in the intersection of \\(B\\) and \\(C\\).\nLet us discuss probability rule 3 a bit further: Go to the picture we drew to illustrate the meaning of set union in figure @ref(fig:set-union). To compute the probability \\(P(A \\cup B)\\) we have to add the probabilities of all sample points that are contained either in \\(A\\) or in \\(B\\) but each point is to be counted only once. Therefore, probability rule 3, has a (weak) inequality: \\(P(A \\cup B) \\leq P(A) + P(B)\\).\nNow let \\(E\\) be a point contained both in \\(A\\) and in \\(B\\), in our example this would be \\(E = \\{2\\}\\), then \\(P(E)\\) occurs twice on the right hand side of our inequality but only once on the left. Therefore the right hand side exceeds the left by the amount \\(P(A \\cap B\\)). Thus we have \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).\nThis reasoning holds for arbitrary pairs of events and not only in our example. In the case \\(A\\) and \\(B\\) are arbitrary events and \\(A\\) and \\(B\\) are mutually exclusive, i.e. \\(A \\cap B = \\emptyset\\) then our inequality in rule 3 becomes an equality and we have \\(P(A \\cup B) = P(A) + P(B)\\).\nNow you recognize property 3 in lecture 1, where we said that: ” If \\(A\\) and \\(B\\) never occur in the same case, then \\(P(A \\,\\text{or}\\, B) = P(A) + P(B)\\).” The meaning of \\(A\\) and \\(B\\) not occurring in the same case is in set theoretic terms exactly the requirement that they are mutually exclusive events.\nTherefore in many treatments of probability you will find rule 3 formulated as:\n3-a: The probability of the union of two mutually exclusive events \\(A\\) and \\(B\\) is \\(P(A \\cup B) =  P(A) + P(B)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#frequency",
    "href": "02-lecture2.html#frequency",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.5 Probability and Frequency",
    "text": "2.5 Probability and Frequency\nNow it is time to explain a connection we made so far informally when we looked at visualizations and simulations, where we used connection between relative frequencies of probabilities in repetions of a random experiment.\nThe frequency interpretation of probability is a practical approach to understanding uncertainty. It defines the probability of an event \\(A\\) as:\n\\[\\begin{equation*}\nP(A) = \\frac{\\text{Number of times $A$ occurs in repeated identical trials}}{\\text{Total number of trials}}\n\\end{equation*}\\]\nThis interpretation is intuitive and often used in fields like engineering, finance, and natural sciences. However, it raises important questions about the connection between observed frequencies and theoretical probabilities.\nThe origins of this discussion reach back to the seventeenth century. The philosophers Gottfried Wilhelm Leibnitz (1646 - 1716) and Jacob Bernoulli (1655 - 1705) had great hopes for the new field of probability to find applications in fields like medicine, law, commerce and finance. This interest in exploring new fields of potential applications drove them to study frequency evidence of events. They felt that relying on intuitively equally probable cases might not be enough for these ambitious application attempts.\nJacob Bernoulli gave an answer which is among the great ideas in probability theory (see Diaconis and Skyrms (2019)), the weak law of large numbers. It establishes one of the most important connections between frequency and probability.\n\n2.5.1 The Weak Law of Large Numbers (WLLN)\nThe Weak Law of Large Numbers provides a rigorous mathematical foundation for the frequency interpretation of probability. It states:\nAs the number of independent and identically distributed (i.i.d.) trials increases, the relative frequency of an event converges to its true probability with high probability.\nNow let us pause here and restate what the weak law says: The law says that\n\nOver many trials, the observed frequencies of outcomes will get closer to their theoretical probabilities.\nThis convergence occurs with high likelihood as the number of trials increases.\n\nObserve also what the weak law does not say:\n\nFrequencies are not probabilities. Instead frequencies approximate probabilities as trials increase.\nThe weak law does not guarantee exact convergence in finite samples - it describes long run outcomes of repeating identical experiments many times.\n\nLet#s go back to the exmple of the fair coin we played with in lecture 1: There we constructed a fair coin toss where \\(P(\\text{Heads})=0.5\\). When we have just a few tosses, say 10 times, you might observe 6 Heads (60%), which is close to but not exactly 50%. As you increase the tosses, say to 100, the frequency might be 52 Heads (52%), closer to 50%. if you go to even more tosses, say 10,000 tosses, the frequency approaches 50%.\nThis demonstrates that, as the number of trials increases, the relative frequency converges to the theoretical probability.\nIt is crucial to understand that Bernoulli’s Law of Large Numbers does not assert that frequencies are probabilities. Rather, it describes how, under certain conditions, frequencies fall within specific bounds relative to probabilities. Specifically, given a theoretical probability, a desired margin of error, and a confidence level, Bernoulli’s result provides an upper bound on the number of trials required for the relative frequency of an event to approximate its true probability.\nThis distinction highlights an important limitation: the Weak Law of Large Numbers addresses the problem of inference from probabilities to frequencies, not the reverse. That is, it shows how probabilities can predict the behavior of frequencies in repeated trials, but it does not justify using observed frequencies as definitive probabilities. Instead, the theorem ensures that as the number of trials grows, the relative frequency of an event will converge to its probability, within specified bounds.\nAlthough we will frequently use frequency-based measures of probability in practical applications, it is essential to recognize the conceptual gap between the idealized mathematical theory and the empirical realities it seeks to describe. Probability theory relies on the concept of limiting relative frequencies, which exist only in an idealized framework of infinite trials. Real-world applications, however, involve finite data and inherently require interpretation, judgment, and domain expertise.\nThis distinction between theory and practice is more than a technical nuance—it addresses a fundamental challenge for any theory with practical ambitions: How does the idealized framework relate to the messy complexities of reality? In probability, as in other fields, there is no direct or naive application of theory to practice. Bridging this gap demands a sound understanding of the theory, along with careful consideration of real-world conditions and context.1\n1 For a formal statement of the weak law of large numbers, we need more concepts which we have not yet introduced, in particular the notion of a random variable. Let me give the formal statement here for those of you who are interested and know the concept of a random variable already: Let \\(X_1, X_2, \\dots, X_n\\) be \\(n\\) independently, identically distributed random variables with expected value \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\bar{X}_n\\) denote the sample mean. Then the weak law of large numbers states: \\(P(|\\bar{X}_n - \\mu | \\geq \\epsilon) \\to 0 \\quad \\text{as } n \\to \\infty\\). This means that the probability of the sample mean deviating significantly from the expected value diminishes as the number of trials increases.\n\n2.5.2 Exploring the Weak Law of Large Numbers with R\nIn the following code chunk we first define a coin as we did in lecture 1 with the understanding that 1 represents Heads and 0 represents Tails. Then we use the replicate function and the sample function to toss the coin 1000 times.\nNow in the next step we use the logical condition results == 1 to create a logical vector indicating whether each toss resulted in Heads. cumsum()then calculates the cumulative count of Heads after each toss. Dividing this cumulative count by (1:n) (the toss number) gives the cumulative relative frequency of Heads at each step.\nThen we plot these frequencies againts the number of tosses. The \\(x\\)-axis represents the number of tosses, while the \\(y\\)-axis represents the relative frequency of heads. We draw a red horizontal line where the relative frequency is identical to the theoretical probability of \\(0.5\\).\n\n# Define the coin\ncoin &lt;- c(1, 0)\n\n# Toss the coin n times\nn &lt;- 1000\nresults &lt;- replicate(n, sample(coin, size = 1))\n\n# Calculate cumulative frequency of Heads\n\nheads_freq &lt;- cumsum(results == 1) / (1:n)\n\n# Plot the convergence\nplot(1:n, heads_freq, type = \"l\", ylim = c(0.4, 0.6),\n     xlab = \"Number of Tosses\", ylab = \"Frequency of Heads\",\n     main = \"Convergence of Relative Frequency to True Probability\")\nabline(h = 0.5, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nThis plot illustrates how the relative frequency of Heads approaches \\(P(\\text{Heads})=0.5\\) as the number of tosses increases.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#independence",
    "href": "02-lecture2.html#independence",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.6 Independence",
    "text": "2.6 Independence\nThe idealized thought experiment behind the Weak Law of Large Numbers assumes the idea of independence. Let us define this concept more precisely and explain its implications. While we have already used independence implicitly in earlier examples, it is important to formalize and understand it carefully.\n\n2.6.1 Intuition Behind Independence\nTwo events are said to be independent if the occurrence of one event does not influence the probability of the other event occurring. In other words, knowing that one event has occurred provides no information about the likelihood of the other event.\nFor example, consider rolling a fair six-sided die twice. The outcome of the first roll does not affect the outcome of the second roll because the rolls are independent. The probability of any number appearing on the second roll remains \\(1/6\\), regardless of what happened on the first roll.\n\n\n2.6.2 A Worked Example\nLet us calculate the probability of rolling a 5 on the first roll and a 6 on the second roll.\nThis is the probability of the event “5 on the first roll and 6 on the second roll,” which we write as \\(P(5 \\cap 6)\\).\nSince the rolls are independent: \\(P(5 \\cap 6) = P(5) \\times P(6)\\).\n\nThe probability of rolling a 5 on a fair six-sided die is \\(P(5) = 1/6\\).\nThe probability of rolling a 6 on a fair six-sided die is \\(P(6) = 1/6\\).\n\nThus: \\(P(5 \\cap 6) = \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{1}{36}\\).\nThis calculation uses the multiplication rule for independent events, which states that if two events \\(A\\) and \\(B\\) are independent, then the probability of both occurring is the product of their individual probabilities.\n\n\n2.6.3 Definition of Independence\nWe now formalize this concept:\nIndependence:\nTwo events \\(A\\) and \\(B\\) are independent if and only if: \\(P(A \\cap B) = P(A) \\times P(B)\\).\nThis definition formalizes the idea that the occurrence of one event does not affect the likelihood of the other. Independence is an assumption we often make in theoretical models like the Weak Law of Large Numbers and many other probabilistic frameworks.\n\n\n2.6.4 Important Note: Independence vs. Multiplication Rule\nWhile independence allows us to use the multiplication rule \\(P(A \\cap B) = P(A) \\times P(B)\\), it is critical to remember that the reverse is not true:\n- Just because you can multiply probabilities does not necessarily mean the events are independent.\n- For independence to hold, the probabilities of the events must truly be unaffected by one another.\n\n\n2.6.5 Why Is Independence Important Here?\nIndependence is a key assumption in the Weak Law of Large Numbers, where we consider a series of independent and identically distributed (i.i.d.) trials. Without independence, the law’s guarantees about the behavior of frequencies would not hold. As we move forward, we will see independence as a foundational concept in many probabilistic models and calculations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#moreR",
    "href": "02-lecture2.html#moreR",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.7 Some more concepts from R: Reading Data, R Objects, Subsetting and Modifying Values",
    "text": "2.7 Some more concepts from R: Reading Data, R Objects, Subsetting and Modifying Values\nBut now enough of dices and coins. Let us try out and apply some of the new ideas we just have learned and use the opportunity to learn some more important R concepts in the context of an example with stock price data.\n\n2.7.1 Reading data in R\nIn financial analysis, the first step is often acquiring and loading data. This could be historical stock prices, economic indicators, or portfolio metrics. In this section, we’ll learn how to load a prepared dataset of Apple stock prices into R for analysis.\nWhy learn to load data? Loading data is the foundation for everything else we’ll do in this course. Whether you’re calculating probabilities or visualizing stock trends, understanding how to access and manipulate data is essential.\n\n2.7.1.1 Reading csv files\nFor this example, I’ve prepared a dataset recording daily stock price information for Apple in a CSV file (aapl_prices.csv). This file contains the opening, high, low, and closing prices, trading volume, and adjusted closing prices for each day starting in January 1990.\nTo load this file into R, we use the read.csv() function:\n\naapl_prices &lt;- read.csv(\"data/aapl_prices.csv\")\n\nHere, \"data/aapl_prices.csv\" specifies the path to the file relative to the working directory.2\n2 If you work with European-style decimals , instead of ., you might need read.csv2(). For now, we’ll stick to the default U.S. format.3 A great way to explore these options is to consult the R Data Import/Export Manual or experiment with sample datasets from various sources like Kaggle or the tidyverse’s guide.R is highly versatile when it comes to handling data formats. Beyond CSV files, R can read and write Excel files, JSON, XML, databases, and more. While many formats can be accessed using built-in functions with a syntax similar to read.csv(), others may require loading additional packages. For instance, the readxl package is excellent for Excel files, and the jsonlite package is ideal for JSON data.3\nTo check your current working directory, use the getwd() function:\n\ngetwd()\n\n[1] \"/home/martinsummer/Code/R/Probability_Introduction\"\n\n\nIf the file is not in the expected location, ensure the path is correct or adjust it as needed.\n\n\n2.7.1.2 Inspecting the data\nOnce the data is loaded, it’s important to inspect it. This helps you understand its structure and content. For example, we can use the head() function to view the first 10 rows:\n\nhead(aapl_prices, n = 10)\n\n   symbol       date     open     high      low    close    volume  adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2    AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3    AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4    AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6    AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n7    AAPL 1990-01-10 0.335938 0.335938 0.319196 0.321429 199718400 0.2533336\n8    AAPL 1990-01-11 0.323661 0.323661 0.308036 0.308036 211052800 0.2427781\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.2427781\n10   AAPL 1990-01-15 0.308036 0.319196 0.305804 0.305804 161739200 0.2410190\n\n\nThis reveals a table with columns such as the opening price, highest and lowest prices, and adjusted closing price for each day. While we won’t dive into the exact meaning of these financial terms just yet, this dataset will serve as the foundation for our analysis.\n\n\n2.7.1.3 Common pitfalls and tips\n\nIf you see an error like cannot open the connection, it likely means R can’t find the file. Use getwd() to confirm your working directory and ensure the file is in the specified location.\nExplore the dataset further! Try summary(aapl_prices) to get an overview of each variable or tail(aapl_prices, n = 10) to see the most recent rows.\n\nNow that we’ve successfully loaded and inspected the dataset, we’re ready to dive into analyzing it.\n\n\n\n2.7.2 R Objects and Stock Price Movements\nIn R, most data structures are built from atomic vectors, which are the simplest type of R objects. Atomic vectors store one-dimensional data, where all elements must be of the same type. Our stock price dataset contains examples of more complex structures made up of atomic vectors. Let’s explore atomic vectors using simplified stock price data to better understand how they work.\n\n2.7.2.1 Atomic Vectors in Stock Price Analysis\nTo introduce atomic vectors, consider a basic example of daily stock price movements. Suppose the price of a stock can change in one of three ways:\n\nDecrease by 1 (-1)\nStay the same (0)\nIncrease by 1 (+1)\n\nWe can represent these possible changes in price using an atomic vector:\n\nprice_changes &lt;- c(-1, 0, 1)\n\nAtomic vectors are one-dimensional and store data of a single type. In this case, price_changes is a numeric vector representing the possible daily price changes.\nYou can verify whether an object is an atomic vector using the is.vector() function:\n\nis.vector(price_changes)\n\n[1] TRUE\n\n\nThis will return TRUE since price_changes is indeed an atomic vector.\n\n\n2.7.2.2 Properties of Atomic Vectors\nEach atomic vector has several properties. For example:\n\nLength: The number of elements in the vector can be checked using length():\n\n\nlength(price_changes)\n\n[1] 3\n\n\nThis will return 3, as there are three possible price changes.\n\nData Type: Every atomic vector must have a single data type. You can determine the type using the typeof() function:\n\n\ntypeof(price_changes)\n\n[1] \"double\"\n\n\nIn this case, the type is double because the vector contains numeric data.\n\n\n2.7.2.3 Using Vectors in Simulations\nVectors are powerful tools for simulations. For example, we can simulate a week of stock price movements using the sample() function, which randomly selects elements from the vector:\n\nweek_movements &lt;- sample(price_changes, size = 7, replace = TRUE)\nweek_movements\n\n[1] -1  0 -1  1  0 -1  1\n\n\nHere, size = 7 simulates 7 days of movements, and replace = TRUE allows values to be selected more than once.\n\n\n2.7.2.4 Connection to Probability\nThis simulation ties into the concept of probability. Each price change (-1, 0, 1) can be treated as a basic outcome in a sample space defined by \\({\\cal S} = \\{-1,0,1\\}\\). By specifying probabilities, we can simulate scenarios where some outcomes are more likely than others. For example:\n\nweek_movements_weighted &lt;- sample(price_changes, \n                                  size = 7, \n                                  replace = TRUE, \n                                  prob = c(0.3, 0.4, 0.3))\nweek_movements_weighted\n\n[1] -1  1 -1 -1 -1 -1  0\n\n\nHere, the probabilities 0.3, 0.4, and 0.3 represent the likelihood of a decrease, no change, or an increase in price, respectively.\n\n\n2.7.2.5 More About Atomic Vectors\nR supports six basic types of atomic vectors:\n\nDouble: Numeric data with decimal precision (e.g., stock prices).\nInteger: Whole numbers (e.g., the number of shares traded).\nCharacter: Text strings (e.g., stock symbols like \"AAPL\").\nLogical: Boolean values (TRUE or FALSE, often used in comparisons).\nComplex: Numbers with imaginary components (not used in this course).\nRaw: Binary data (not used in this course).\n\nWe will primarily work with doubles, integers, characters, and logicals.We will primarily work with doubles, integers, characters, and logicals.4\n4 Why should we care for distinguishing integers from doubles? This has to do with how a computer performs computations. Sometimes, differences in precision can lead to surprising effects. In your computer, 64 bits of memory are allocated for each double in an R program. While this allows for precise representation of numbers, not all numbers can be exactly represented with 64 bits. For example, \\(\\pi\\) has an infinite sequence of digits and must therefore be rounded by the computer. Usually, rounding errors introduced into computations go unnoticed, but sometimes they become apparent. For instance, the expression \\(\\sqrt{2^2} - 2\\) results in a small floating-point error because the square root of 2 cannot be expressed precisely. These are called floating-point errors. While integers avoid floating-point errors, they are often impractical for many applications. Luckily, floating-point arithmetic is sufficiently precise for most use cases.For example, the stock price dataset contains variables of different types: - Prices (e.g., opening and closing prices) are stored as doubles. - Ticker symbols are stored as characters. - Logical vectors can be created by applying conditions to the data. For instance, finding days when the closing price was higher than the opening price:\n\naapl_prices$up_day &lt;- aapl_prices$close &gt; aapl_prices$open\nhead(aapl_prices$up_day, n = 10)\n\n [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n\n2.7.2.6 Attributes of Atomic Vectors\nAtomic vectors can have attributes, such as:\n\nNames: Metadata to describe each element.\nDimensions: Turning a vector into a matrix or array.\n\nFor example, we can assign names to the elements of price_changes:\n\nnames(price_changes) &lt;- c(\"Decrease\", \"No Change\", \"Increase\")\nprice_changes\n\n Decrease No Change  Increase \n       -1         0         1 \n\n\nThe names don’t affect the values but provide context for analysis. Here you see that the names are entered into R as a character vector stores, strings of text, which have to be put between quotation marks \"\". Strings are the individual elements of a character vector. We could use character vectors for instance in building our virtual coin earlier in the lecture in a more human readable form by using coin &lt;- c(\"H\", \"T\") isntead of coin &lt;- c(1,0).\nNote that a string can be more than just letters. If you type, for instance the number 1 with quotation marks, like \"1\" R would interpret the value as a string not as a number. Sometimes one can get confused in R because both objects and characters appear as text in R code. Object names are without quotation marks strings always are between quotation marks.\nWe can also reshape the vector into a matrix, which introduces dimensions:\n\ndim(price_changes) &lt;- c(3, 1)\nprice_changes\n\n     [,1]\n[1,]   -1\n[2,]    0\n[3,]    1\n\n\nThis transforms the vector into a matrix with 3 rows and 1 column.^[With the attribute system R allows you to represent more data types. R uses, for example a special class to represent dates and times. The data variable in our stock data is - for example - represents as a type of this kind.\nTo illustrate this we take the R-function Sys.time(). This function returns the current time on your computer. It looks like a character string when you display it but it is actually a double with class POSIXct, POSIXt (it has two classes):\n\nnow &lt;- Sys.time()\nnow\n\n[1] \"2024-12-30 16:28:40 CET\"\n\ntypeof(now)\n\n[1] \"double\"\n\nclass(now)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nPOSIXct is a widely used framework for representing dates and times. But we will skip the details here.]\n\n\n\n2.7.3 Extending R Objects: Factors, Data Frames, and Lists\nIn the previous section, we explored atomic vectors, the foundation of many R objects. However, real-world data often requires more sophisticated structures. Let’s extend our understanding by examining factors, data frames, and lists, using examples from stock price analysis.\n\n2.7.3.1 Factors in Stock Price Analysis\nFactors are used to store categorical data, which can have a fixed set of possible values (called levels). In stock market data, factors are useful for categorizing data, such as trading days or stock types.\nFor example, suppose we want to represent the days of the working week in our dataset:\n\ndays &lt;- factor(c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\ndays\n\n[1] Monday    Tuesday   Wednesday Thursday  Friday   \nLevels: Friday Monday Thursday Tuesday Wednesday\n\n\nThis factor has unique levels (\"Monday\", \"Tuesday\", etc.) that categorize the data. Factors can also be ordered, which is useful when the order matters:\n\nordered_days &lt;- factor(days, levels = c(\"Monday\", \"Tuesday\", \n                                        \"Wednesday\", \"Thursday\", \n                                        \"Friday\"), ordered = TRUE)\n\nordered_days\n\n[1] Monday    Tuesday   Wednesday Thursday  Friday   \nLevels: Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; Friday\n\n\nFactors are internally stored as integers but behave like characters when displayed. You can inspect their attributes to see how they’re stored:\n\nattributes(ordered_days)\n\n$levels\n[1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n\n$class\n[1] \"ordered\" \"factor\" \n\n\n\n\n2.7.3.2 Data Frames: Organizing Stock Price Data\nA data frame is a two-dimensional structure that organizes data into rows and columns, where each column can store a different type of data. The stock price dataset we loaded earlier is a perfect example of a data frame.\nLet’s revisit the aapl_prices dataset:\n\nhead(aapl_prices)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6   AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n  up_day\n1   TRUE\n2  FALSE\n3  FALSE\n4  FALSE\n5   TRUE\n6  FALSE\n\ntypeof(aapl_prices)\n\n[1] \"list\"\n\nclass(aapl_prices)\n\n[1] \"data.frame\"\n\n\nEach column in a data frame is an atomic vector, meaning it contains values of a single type:\n\nDoubles: Prices (e.g., opening, closing).\nCharacters: Ticker symbols (\"AAPL\").\nLogicals: New variables created based on conditions.\n\nFor example, we can create a logical column to indicate whether the stock closed higher than it opened:\n\naapl_prices$up_day &lt;- aapl_prices$close &gt; aapl_prices$open\nhead(aapl_prices[c(\"date\", \"close\", \"up_day\")], n = 5)\n\n        date    close up_day\n1 1990-01-02 0.332589   TRUE\n2 1990-01-03 0.334821  FALSE\n3 1990-01-04 0.335938  FALSE\n4 1990-01-05 0.337054  FALSE\n5 1990-01-08 0.339286   TRUE\n\n\nData frames are extremely flexible. You can subset them, filter rows, or add new columns:\n\n# Subset rows where the stock closed higher than it opened\n\nhigher_close &lt;- aapl_prices[aapl_prices$up_day == TRUE, ]\nhead(higher_close)\n\n   symbol       date     open     high      low    close    volume  adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.2427781\n11   AAPL 1990-01-16 0.299107 0.312500 0.292411 0.311384 214244800 0.2454167\n14   AAPL 1990-01-19 0.301339 0.308036 0.299107 0.305804 265137600 0.2410190\n17   AAPL 1990-01-24 0.290179 0.305804 0.287946 0.303571 169792000 0.2392588\n   up_day\n1    TRUE\n5    TRUE\n9    TRUE\n11   TRUE\n14   TRUE\n17   TRUE\n\n\n\n\n2.7.3.3 Lists: Combining Multiple Data Types\nWhile data frames organize tabular data, lists allow us to group together objects of different types and structures. This makes lists a powerful tool for storing heterogeneous data.\nFor example, let’s create a list to summarize key information about Apple’s stock prices:\n\nstock_summary &lt;- list(\n  ticker = \"AAPL\",\n  price_summary = summary(aapl_prices$close),\n  highest_price = max(aapl_prices$high, na.rm = TRUE),\n  date_range = range(aapl_prices$date, na.rm = TRUE)\n)\nstock_summary\n\n$ticker\n[1] \"AAPL\"\n\n$price_summary\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  0.1155   0.3834   3.3300  30.5265  28.8950 259.0200 \n\n$highest_price\n[1] 260.1\n\n$date_range\n[1] \"1990-01-02\" \"2024-12-27\"\n\n\nYou can access elements in a list using double brackets ([[):\n\nstock_summary[[\"highest_price\"]]\n\n[1] 260.1\n\n\nLists can also contain nested lists or data frames, enabling complex data structures. For example:\n\nnested_list &lt;- list(\n  summary = stock_summary,\n  recent_data = head(aapl_prices, n = 5)\n)\nnested_list\n\n$summary\n$summary$ticker\n[1] \"AAPL\"\n\n$summary$price_summary\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  0.1155   0.3834   3.3300  30.5265  28.8950 259.0200 \n\n$summary$highest_price\n[1] 260.1\n\n$summary$date_range\n[1] \"1990-01-02\" \"2024-12-27\"\n\n\n$recent_data\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n  up_day\n1   TRUE\n2  FALSE\n3  FALSE\n4  FALSE\n5   TRUE\n\n\n\n\n2.7.3.4 Factors, Data Frames, and Lists in Practice\nTo see how these structures work together, consider this small example. We’ll summarize Apple’s stock price movements over a week and organize the results in a list:\n\n# Simulate weekly price movements\nweek_movements &lt;- sample(c(-1, 0, 1), size = 5, replace = TRUE, \n                         prob = c(0.3, 0.4, 0.3))\ndays &lt;- factor(c(\"Monday\", \"Tuesday\", \"Wednesday\", \n                 \"Thursday\", \"Friday\"), ordered = TRUE)\n\n# Create a data frame\nweekly_data &lt;- data.frame(\n  day = days,\n  price_change = week_movements\n)\n\n# Summarize the data\nsummary_list &lt;- list(\n  week_data = weekly_data,\n  positive_days = sum(weekly_data$price_change &gt; 0),\n  total_change = sum(weekly_data$price_change)\n)\n\nsummary_list\n\n$week_data\n        day price_change\n1    Monday            0\n2   Tuesday           -1\n3 Wednesday            1\n4  Thursday            0\n5    Friday            0\n\n$positive_days\n[1] 1\n\n$total_change\n[1] 0\n\n\nThis example combines:\n\nA factor (days) to represent the days of the week.\nA data frame (weekly_data) to organize daily price changes.\nA list (summary_list) to store the data frame and summary statistics.\n\n\n\n2.7.3.5 Why These Structures Matter\nFactors, data frames, and lists are essential for organizing and analyzing real-world data. In financial analysis, they allow us to:\n\nCategorize data efficiently (factors).\nAnalyze structured datasets (data frames).\nIntegrate heterogeneous data (lists).\n\nArmed with these tools, you’re ready to perform deeper analyses and connect R concepts to practical financial problems. Next, we’ll use this knowledge to explore probabilistic questions with our stock price data.\n\n\n\n2.7.4 Example: Will the Stock Price of Apple Move Up or Down?\nLet us revisit the first lines of our dataset:\n\nhead(aapl_prices, n = 10)\n\n   symbol       date     open     high      low    close    volume  adjusted\n1    AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2    AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3    AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4    AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5    AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n6    AAPL 1990-01-09 0.339286 0.339286 0.330357 0.335938  86139200 0.2647689\n7    AAPL 1990-01-10 0.335938 0.335938 0.319196 0.321429 199718400 0.2533336\n8    AAPL 1990-01-11 0.323661 0.323661 0.308036 0.308036 211052800 0.2427781\n9    AAPL 1990-01-12 0.305804 0.310268 0.301339 0.308036 171897600 0.2427781\n10   AAPL 1990-01-15 0.308036 0.319196 0.305804 0.305804 161739200 0.2410190\n   up_day\n1    TRUE\n2   FALSE\n3   FALSE\n4   FALSE\n5    TRUE\n6   FALSE\n7   FALSE\n8   FALSE\n9    TRUE\n10  FALSE\n\n\n\n2.7.4.1 Understanding the Structure of the Data\nTo begin, let’s confirm the type and structure of aapl_prices:\n\ntypeof(aapl_prices)\n\n[1] \"list\"\n\nclass(aapl_prices)\n\n[1] \"data.frame\"\n\n\nAs expected, the object is a list with the class data.frame. This means the dataset organizes data in rows and columns. We can use the dim() function to see how many trading days are recorded:\n\ndim(aapl_prices)\n\n[1] 8815    9\n\n\nThe dataset contains price information for 8815 trading days.\n\n\n\n2.7.4.2 Subsetting Data: Accessing Specific Elements\nTo analyze the dataset, we need to extract specific values or subsets of data. R provides a powerful and flexible notation system for subsetting:\naapl_prices[row_indices, column_indices]\nHere are the six main subsetting methods in R:\n\nPositive Integers:\n\nExample: Select the closing price on the first trading day:\n\n\n\naapl_prices[1, \"close\"]\n\n[1] 0.332589\n\n\n\nSelect the first 5 closing prices:\n\n\naapl_prices[1:5, \"close\"]\n\n[1] 0.332589 0.334821 0.335938 0.337054 0.339286\n\n\n\nNegative Integers:\n\nExclude the first observation (show first 3 entries after this operation):\n\n\n\n     head(aapl_prices[-1, \"close\"], 3)\n\n[1] 0.334821 0.335938 0.337054\n\n\n\nZero:\n\nCreates an empty object:\n\n\n\n     aapl_prices[0, 0]\n\ndata frame with 0 columns and 0 rows\n\n\n\nBlank Spaces:\n\nSelect all values in a dimension:\n\n\n\n     sel &lt;- aapl_prices[, \"close\"]\n     length(sel)\n\n[1] 8815\n\n\n\nLogical Values:\n\nExample: Use a logical vector to select the first closing price:\n\n\n\n     aapl_prices[1, c(FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE)]\n\n      high    close\n1 0.334821 0.332589\n\n\n\nNames:\n\nSelect using column names:\n\n\n\naapl_prices[1, \"close\"]\n\n[1] 0.332589\n\n\n\n\n2.7.4.3 Calculating Daily Price Differences\nLet us compute the day-to-day differences in closing prices. Using indexing, we can calculate these differences manually:\n\naux_1 &lt;- aapl_prices[2:8044, \"close\"]\naux_2 &lt;- aapl_prices[1:8043, \"close\"]\ndiff_close &lt;- aux_1 - aux_2\nhead(diff_close, n = 10)\n\n [1]  0.002231985  0.001117021  0.001116008  0.002231985 -0.003347993\n [6] -0.014508992 -0.013393015  0.000000000 -0.002231985  0.005579978\n\n\nAlternatively, the built-in diff() function simplifies this task:\n\naapl_prices$diff &lt;- c(NA, diff(aapl_prices$close))\nhead(aapl_prices, n = 5)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n  up_day        diff\n1   TRUE          NA\n2  FALSE 0.002231985\n3  FALSE 0.001117021\n4  FALSE 0.001116008\n5   TRUE 0.002231985\n\n\nThe first observation is NA because there is no prior day to calculate a difference.\n\n\n2.7.4.4 Frequency-Based Probability of Upward Moves\nTo determine the probability of the stock moving up on any given day, we create a logical column that indicates whether the price difference is positive:\n\naapl_prices$diff_pos &lt;- aapl_prices$diff &gt; 0\nhead(aapl_prices, n = 5)\n\n  symbol       date     open     high      low    close    volume  adjusted\n1   AAPL 1990-01-02 0.314732 0.334821 0.312500 0.332589 183198400 0.2621293\n2   AAPL 1990-01-03 0.339286 0.339286 0.334821 0.334821 207995200 0.2638886\n3   AAPL 1990-01-04 0.341518 0.345982 0.332589 0.335938 221513600 0.2647689\n4   AAPL 1990-01-05 0.337054 0.341518 0.330357 0.337054 123312000 0.2656486\n5   AAPL 1990-01-08 0.334821 0.339286 0.330357 0.339286 101572800 0.2674077\n  up_day        diff diff_pos\n1   TRUE          NA       NA\n2  FALSE 0.002231985     TRUE\n3  FALSE 0.001117021     TRUE\n4  FALSE 0.001116008     TRUE\n5   TRUE 0.002231985     TRUE\n\n\nUsing the relative frequency approach, we calculate the probability of an upward move as:\n\nmean(aapl_prices$diff_pos, na.rm = TRUE)\n\n[1] 0.5073746\n\n\nor about 51 %.\nThis one line of code does several important things that are worth unpacking.\nFirst, R has an internal behavior known as type coercion, where logical values (TRUE and FALSE) are automatically converted into numerical values when used in numerical operations. Specifically:\n\nTRUE is coerced to 1\nFALSE is coerced to 0\n\nBy applying the mean() function to a logical vector, R computes the average of its numerical representation, effectively calculating the proportion of TRUE values. In this case, the mean gives the relative frequency of upward price movements in the dataset.\nAdditionally, this approach relies on the fact that the mean() function automatically divides the sum of the values by their count. Since TRUE values are coerced to 1, the sum of the vector corresponds to the total count of upward movements. 5\n5 In fact, using sum(aapl_prices$diff_pos, na.rm = TRUE) would provide the total number of days where the stock price increased, as sum() adds up the 1s in the binary vector.This step also highlights the flexibility and power of R’s vectorized operations, where simple functions like mean() and sum() can directly compute meaningful results for logical or binary data without additional loops or transformations.\n\n\n2.7.4.5 Applying Probability Concepts\nAssume that price movements are independent of one another. This means that the direction of the stock price movement today does not influence tomorrow’s movement. With this assumption, we can calculate compound probabilities over multiple days.\n\nProbability of Consecutive Increases: What is the probability that the stock price increases every day over a week (5 trading days)?\n\\(P(U \\cap U \\cap U \\cap U \\cap U) = P(U)^5 = 0.51^5 = 0.035\\)\nProbability of One Decrease and Four Increases: Consider the probability of a decrease on any one day, and increases on the other four:\n\\(P(D \\cap U \\cap U \\cap U \\cap U) = 0.49 \\cdot 0.51^4 = 0.033\\)\nSince there are 5 such (mutually exclusive) scenarios (one for each trading day), the total probability is:\n\\(5 \\cdot 0.033 = 0.132\\)\n\n\n\n2.7.4.6 Reflecting on Assumptions\nIs this analysis any good? How could we possibly judge this? Interestingly the relative frequencies of up and down moves look similar to a random experiment of a few thousand tosses of a fair coin. But can we learn anything from this? Are the up and down moves independent? Independence - of course - does not follow from the result we just got.\nThe idea that stock prices may fluctuate randomly was first discussed systematically by Louis Bachelier (1870 - 1946), a French mathematician who studied stock price movements mathematically. In 1965 the economist Samuelson (1965) published an article with the title “Proof that stock prices fluctuate randomly”. He argues in this paper that randomness comes about through the active participation of traders seeking to maximize their wealth. A huge army of investors would aggressively use the smallest informational advantage and in doing so, they incorporate the information into market prices, which quickly eliminates this profit opportunity. This lead to a cornerstone of modern Finance theory called the random walk hypothesis of stock price fluctuations.\nIf this theory was true, it would give an argument, why we might look at the up and down movements in the stock price of apple as if it was the outcome of tossing a fair coin. In this case the probability of an up or a down movement should be 1/2 and with the number of trials approaching infinity the frequency of ups and downs should approach this probability.\nThe literature on stock price fluctuations which came later, however, presented evidence that stock prices are predictable to a certain degree and do not fluctuate randomly. A good reference summarizing this evidence is Lo and MacKinlay (2019) In this case our approach would perhaps produce a misleading answer.\nWe cannot give a clear cut answer here. The point of this brief discussion is that you just cannot apply a theoretical machinery mechanically without giving it further thought and without maintaining a healthy amount of skepticism. It is fascinating that there are situations where abstract theories, like the theory of probability, show a robust relation to real world phenomena. But the nature, the precise meaning and the robustness of this relation has to be investigated for each single case.\nAs Louis Bachelier and Paul Samuelson noted, randomness in financial markets may emerge from the interplay of rational traders, yet this randomness is not absolute. A nuanced understanding of market dynamics is crucial for drawing meaningful conclusions.\n\n\n\n2.7.5 Example: Benford’s Law and Trading Volumes\nLet us conclude this lecture with an example that ties together our understanding of empirical probabilities and relative frequencies while also showcasing a surprising pattern in real-world data. This example will further strengthen your R skills and prepare you for the assigned project.\n\n2.7.5.1 The Curious Case of Leading Digits\nFirst, let us define leading digits: the leading digit of a number is simply its first non-zero digit. For instance: - The leading digit of 7829 is 7. - The leading digit of 0.00453 is 4. - The leading digit of 10892 is 1.\nYou might expect that in a random dataset, all leading digits from 1 to 9 occur with roughly equal frequencies (approximately 11% each). However, real-world datasets often show a very different pattern: smaller digits like 1 appear more frequently than larger digits like 9. This pattern follows a logarithmic distribution known as Benford’s Law, which states: \\(P(d) = \\log_{10}\\left(1 + \\frac{1}{d}\\right)\\)\nwhere \\(d \\in \\{1, 2, \\dots, 9\\}\\).\n\n\n2.7.5.2 Benford’s Law in Trading Volumes\nLet’s see if trading volumes follow Benford’s Law. First, we extract the trading volumes from the aapl_prices dataset:\n\nvolumes &lt;- aapl_prices$volume\n\nNext, we filter out invalid values (e.g., zeros or missing values) and extract the leading digits:\n\nvalid_volumes &lt;- volumes[volumes &gt; 0 & !is.na(volumes)]\n\nleading_digits &lt;- as.numeric(substr(as.character(valid_volumes), 1, 1))\n\nLet’s break down this code:\nHere’s an explanation of the two code lines tailored for students:\n1. Filter Valid Trading Volumes\n\nvalid_volumes &lt;- volumes[volumes &gt; 0 & !is.na(volumes)]\n\nThis line ensures that only valid trading volumes are included in the analysis. Let’s break it down:\n\nvolumes &gt; 0:\n\nThis condition filters out any non-positive values (e.g., 0 or negative numbers).\nThese values are not meaningful for analyzing leading digits because they do not have valid non-zero digits.\n\n!is.na(volumes):\n\nThe is.na() function checks if a value is missing (NA).\nThe ! operator negates this, keeping only non-missing values.\n\nCombining Conditions:\n\nThe & operator combines the two conditions. Only entries that satisfy both (i.e., are greater than 0 and not missing) are retained.\n\nSubsetting:\n\nvolumes[...] applies these conditions to the volumes vector. The result is a new vector, valid_volumes, containing only the valid trading volumes.\n\n\n2. Extract Leading Digits\n\nleading_digits &lt;- as.numeric(substr(as.character(valid_volumes), 1, 1))\n\nThis line extracts the leading digit of each valid trading volume. Here’s how it works:\n\nConvert to Character Strings:\n\n\nhead(as.character(valid_volumes),5)\n\n[1] \"183198400\" \"207995200\" \"221513600\" \"123312000\" \"101572800\"\n\n\n\nSince trading volumes are numeric, we need to treat them as text to extract specific characters.\nas.character() converts each number into a string representation.\n\n\nExtract the First Character:\n\n\nhead(substr(as.character(valid_volumes), 1, 1),5)\n\n[1] \"1\" \"2\" \"2\" \"1\" \"1\"\n\n\n\nThe substr() function extracts substrings from each string.\n1, 1 specifies that we extract the substring starting at position 1 and ending at position 1 (i.e., the first character of the string). This gives the leading digit as a character.\n\n\nConvert Back to Numeric:\nas.numeric(...): The extracted leading digit is initially a character. as.numeric() converts it back into a numeric value, allowing us to perform calculations like counting frequencies.\n\nNow we compute the empirical frequencies of the leading digits and compare them to the theoretical probabilities:\n\n# Tabulate empirical frequencies\nemp_freq &lt;- table(leading_digits) / length(leading_digits)\n\n# Create a data frame with empirical and Benford probabilities\nbenford &lt;- data.frame(\n  Digit = 1:9,\n  Empirical_Freq = as.numeric(emp_freq[1:9]), # Match digits 1 to 9\n  Benford_Prob = log10(1 + 1 / (1:9))\n)\n\nHere’s an explanation of the code chunk, step by step:\n1. Tabulate Empirical Frequencies\n\nemp_freq &lt;- table(leading_digits) / length(leading_digits)\n\n\ntable(leading_digits):\n\nThis function counts how many times each unique value appears in leading_digits.\nIn our case, leading_digits contains numbers between 1 and 9, so the output will have counts for each of these digits.\n\nlength(leading_digits):\n\nThis calculates the total number of entries in leading_digits, i.e., the total number of leading digits observed.\n\ntable(...) / length(...):\n\nDividing the counts from table(leading_digits) by the total count gives the relative frequencies of each digit.\nThese are the empirical probabilities of each digit appearing as the leading digit.\n\n\n\n\n2.7.5.3 2. Create a Data Frame for Comparison\n\nDigit = 1:9:\n\nThis creates a column in the data frame with the digits 1 through 9. These are the leading digits we’re analyzing.\n\nEmpirical_Freq = as.numeric(emp_freq[1:9]):\n\nSubsetting emp_freq[1:9]:\n\nThe table() function includes entries for all unique values in leading_digits. If there are missing digits (e.g., if leading_digits doesn’t include a certain number), emp_freq may not have all 9 entries.\nBy explicitly subsetting with [1:9], we ensure the frequencies for digits 1 through 9 are aligned with the Digit column, ignoring any gaps or extra entries (e.g., 0).\n\nas.numeric():\n\nThe table() output is a special type of object, not a standard numeric vector. Converting it ensures that the values can be used for computations and stored in the data frame.\n\n\nBenford_Prob = log10(1 + 1 / (1:9)):\n\nThis calculates the theoretical probabilities for digits 1 through 9 according to Benford’s Law: \\(P(d) = \\log_{10}\\left(1 + \\frac{1}{d}\\right)\\)\n1:9 provides the range of digits for which we compute probabilities.\n\nCombining into a Data Frame:\n\nThe data.frame() function organizes the information into a table with three columns:\n\nDigit: The digits 1 through 9.\nEmpirical_Freq: The observed relative frequencies of these digits from emp_freq.\nBenford_Prob: The theoretical probabilities predicted by Benford’s Law.\n\n\n\nWe can now display the results\n\n# Display the comparison\nknitr::kable(benford)\n\n\n\n\nDigit\nEmpirical_Freq\nBenford_Prob\n\n\n\n\n1\n0.3072036\n0.3010300\n\n\n2\n0.1674419\n0.1760913\n\n\n3\n0.1185479\n0.1249387\n\n\n4\n0.0976744\n0.0969100\n\n\n5\n0.0713556\n0.0791812\n\n\n6\n0.0670448\n0.0669468\n\n\n7\n0.0620533\n0.0579919\n\n\n8\n0.0584231\n0.0511525\n\n\n9\n0.0502552\n0.0457575\n\n\n\n\n\n\n\n2.7.5.4 Interpreting the Results\nThe table above compares the observed frequencies of leading digits in Apple’s trading volumes with the predictions of Benford’s Law. The match is stunningly close. The example illustrates how this surprising regularity applies even in financial datasets like trading volumes.\n\n\n2.7.5.5 Broader Applications\nBenford’s Law has been observed across diverse datasets, such as: - Market capitalization of companies, - GDP of countries or regions, - Population data of cities or counties.\nIts applicability hinges on datasets spanning multiple orders of magnitude and not being artificially bounded. Deviations from Benford’s Law have even been used to detect fraud in tax filings and financial records.\nFor your project, you will explore whether Benford’s Law holds for another financial dataset, allowing you to test this “law of leading digits” in a real-world context.\n\n\n\n2.7.6 Enhancing Your Learning with an LLM: Debugging and Exploring Data Analysis Pitfalls\nOne of the most powerful ways to use a Large Language Model (LLM), like ChatGPT, is as a debugging companion and a tool for uncovering potential pitfalls in your data analysis. This can be particularly valuable as you learn new concepts, such as the ones covered in this lecture on probability and data analysis with R.\nUse Case: Debugging R Code and Avoiding Common Mistakes\nWhen working with R, especially as a beginner, you might encounter errors, unexpected outputs, or difficulties understanding how certain functions behave. Here’s how you can use an LLM to enhance your learning:\n1. Explaining Error Messages\nWhen you run into an error message in R, an LLM can help you interpret it and suggest solutions. For example:\nError Message:\nError in table(leading_digits): all arguments must have the same length\nHow to Use an LLM: - Copy the error message and your code into the LLM, and ask:\n\n\n\n\n\n\nPrompt\n\n\n\nI encountered this error in R. Here is my code: [paste your code]. Can you explain what went wrong and suggest how to fix it?”\n\n\n2. Exploring Alternatives and Best Practices\nLLMs can also suggest alternative ways to achieve the same task or point out best practices you might not be aware of.\nExample: You might ask:\n\n\n\n\n\n\nPrompt\n\n\n\nI used substr(as.character(valid_volumes), 1, 1) to extract leading digits. Are there other, possibly better ways to do this in R?”\n\n\n3. Checking Your Understanding with “What-If” Questions\nLLMs are great for exploring “what-if” scenarios that test your understanding of concepts.\nExample Questions:\n\n\n\n\n\n\nPrompt\n\n\n\nWhat happens if I include zero or negative values in the dataset when applying Benford’s Law?“*\n\n\nor you might ask\n\n\n\n\n\n\nPrompt\n\n\n\nIf my dataset has fewer than 100 observations, how reliable are the empirical probabilities in approximating theoretical probabilities?”\n\n\nThis type of interaction helps you connect the theoretical content (e.g., the weak law of large numbers) with practical considerations.\n4. Summarizing and Synthesizing Knowledge\nYou can use an LLM to review and consolidate your learning by asking it to summarize key concepts or connect them in new ways.\n\n\n\n\n\n\nPrompt\n\n\n\nCan you summarize the main differences between empirical probabilities and theoretical probabilities, and explain how they relate to Benford’s Law?\n\n\nThis exercise reinforces your understanding while revealing gaps or misconceptions.\nDebugging and exploring potential pitfalls are essential skills for any data scientist or analyst. By leveraging an LLM, you not only solve immediate problems but also gain deeper insights into your learning process. This approach fosters independence and critical thinking, preparing you to tackle more complex challenges in the future.\nAs you work on the project for this lecture, try using an LLM to explain error messages, explore alternative solutions, and answer your “what-if” questions. You might be surprised at how much you can learn from this collaborative process!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#summary-lecture-2",
    "href": "02-lecture2.html#summary-lecture-2",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.8 Summary",
    "text": "2.8 Summary\nIn this lecture we have dived a bit deeper into probability. We made some of our introductory concepts more precise and more general and learned about the important concepts of independence and how probabilities and frequencies of events in series of identical independent experiments are and are not related. We also learned about tools to go back and forth between probability theory and data and encountered our first financial dataset, a data frame for the stock pirce of Apple.\nThese are the probability concepts we have covered in this lecture:\nIn this lecture we have given precise definitions of our basic concepts of sample space, basic outcome, events and probability. This time we gave an abstract definition of probability which shares the basic properties we required for classical probability. We explained what discrete probability means and explained how we can build events by set theoretic operations of union, intersection, complements. We have also explained the meaning of mutually exlusive. We have explained the notion of empirical probability or frequency probability. We have discussed the weak law of large numbers which establishes for a given probability the number of identical independent trials that are needed such that the relative frequency of an event falls within any given bound around this probability. While relative frequencies are not probabilities they approach given probabilities in the limit of a sequence of identical independent trials. We have finally learned about the definition of independence of events. If two events \\(A\\) and \\(B\\) are independent the probability that \\(A\\) and \\(B\\) occur is the product of their individual probabilities.\nThese are the R concepts covered in this lecture: We learned what R objects are and that R has six types of atomic vectors, namely double, integer, character, logical, complex and raw. We have learned how to subset R objects by positive integers, negative integers, zero, blank spaces, logicals and names.\nThese are the applications we have covered in this lecture. Tossing a fair coin by simulation. reading data from a stored csv files and also reading a csv file directly from the internet. Using R’s subsetting functions to analyze up and down movements of stock price data and use empirical probabilities together with an independence assumption to compute the probability of certain sequences of up and down moves. Benford’s law of leading digits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#project2",
    "href": "02-lecture2.html#project2",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.9 Project 2: Should we trust European supervisory bank data?",
    "text": "2.9 Project 2: Should we trust European supervisory bank data?\nThe European Banking Authority EBA conducts a biannual stress testing exercise for the biggest European banks every two years. In the stress test bank exposure data are collected and then particular stress scenarios are applied to the reported data. The goal is to find out whether the banks would have enough capital to withstand an adverse scenario. EBA makes a huge effort to publish the data that are the base of the stress test on its web site: https://www.eba.europa.eu/risk-analysis-and-data/eu-wide-stress-testing\nIn this project you are asked to use the knowledge you learned about empirical probabilities, R and Benford’s law to check whether the EBA stress testing data show the distribution of leading digits, you would expect from Benford’s law.\n\nGo to the EBA website https://www.eba.europa.eu/risk-analysis-and-data/eu-wide-stress-testing and download the file Credit Risk IRB (https://www.eba.europa.eu/assets/st21/full_database/TRA_CRE_IRB.csv). You can do so by downloading the file, storing it locally and then read it into R or you can read it directly from the web.\nWe want to study the distribution of leading digits in the Amounts reported in the EBA file of credit risk exposures. Now the csv file contains many different data that all somehow belong to the EBA stress test. We would not like to check all data but only exposure data. We need first to filter the data to make sure we have a meaningful collection of exposure data for all the banks. The description of the data and the data dimensions is in the files Metadata_TR.xlsx and Data_Dictionary.xlsx. You are welcome to study these data in detail. It will probably need more time than you have available. They are also quite complicated. Since the aim of this project is not directly to understand the eba-data but to work with your R-concepts and probability concepts, let me guide you here how to filter these data in 10 sequential steps. Note that this sequencing is for didactical reasons only and for the purpose not to loose oversight. With routine and experience all these steps can be done in one go as well:\n\nExtract all variables names, using the names()function.\nSelect all rows where the Scenario variable has value 1. Note that the symbol you need in the R syntax for equal is ==, the syntax is therefore Scenario == 1. You might check out the R-help entry Comparison for further details.\nFrom the resulting data-frame select all rows where the Country variable is not equal to 0. (hint: The not equal operator in the R syntax is !=). If you look into the Metadata-File you will see that 0 are all the aggregate exposures not broken down by country. Excluding these will give us country exposures.\nFrom the resulting data frame select all rows where the Portfolio variable has value 1 or 2.These codes describe the accounting rules under which the exposure values are reported, internal rating based (IRB) or standard approach (SA). As a hint you can use R’s subset operator %in% here so Portfolio %in% c(1,2) written with the approprate subsetting rule will select all rows where the Porfolio variable has value 1 or 2.\nFrom the resulting data frame choose all the rows where the Exposure variable is not 0.This gives again disaggregated numbers.\nFrom the resulting data frame choose all the rows where the Status variable has value 0.\nFrom the resulting data frame choose all the rows where the IFRS9_Stages variable has value 1,2, or 3.\nFrom the resulting data frame choose all the rows where the CR_guarantees variable is 0\nFrom the resulting data frame choose all the rows where the CR_exp_moratoria variable is 0.\nFrom the resulting data frame, drop all rows where the Amount variable is 0.\n\nCheck the type of the Amount variable.\nTransform the Amount variable to type numeric()\nCheck for NA in the Amount variable in the resulting data frame and if you find any, remove them.\nChange the Amount variable from the actual unit of Million Euros to the unit of 1 Euro 1 Euro and throw away data smaller than 1 after this transformation.\nSelect the leading digits from the Amount variable, using R’s string functions and add a variable with name LD to your data frame.\nCompare the empirical frequencies in the data with the theoretical frequencies from Benford’s law by creating an appropriate data frame. What can you say from this evidence?\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nLo, Andrew, and Craig MacKinlay. 2019. A Non-Random Walk down Wallstreet. Princeton University Press.\n\n\nSamuelson, Paul. 1965. “Proof That Properly Anticipated Prices Fluctuate Randomly.” Industrial Management Review 6.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#probability-and-the-language-of-sets",
    "href": "02-lecture2.html#probability-and-the-language-of-sets",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.3 Probability and the language of sets",
    "text": "2.3 Probability and the language of sets\nProbability theory relies on the language of sets to describe relationships between events. Understanding key set operations is essential for working with probabilities effectively.\nLet’s go through them and illustrate the concepts in the context of the examples we have already developed in lecture 1.\n\nUnion\n\nThe union of two events \\(A\\) and \\(B\\) represents all outcomes that belong to \\(A\\), \\(B\\) or in both. It is written \\(A \\cup B\\).\n\n\nLet us use the example of the experiment of rolling a die. The sample space \\({\\cal S}\\) is the set of all possible outcomes of rolling the dice \\({\\cal S}=\\{1,2,3,4,5,6\\}\\). Assume one event is that the outcome is 1, 2 or 3. In set notation, we would write \\(A = \\{1,2,3\\}\\). Let us also assume that the second event is that the outcome is some even number, i.e. 2, 4, 6. Again using set notation we would write \\(B = \\{2,4,6\\}\\). The event \\(A \\cup B\\) is then the set of all outcomes such that the outcome is in \\(A\\) or in \\(B\\) or in both or these sets.\nYou can implement set operations in R because R provides functions for computing set operations. Let us use the occasion to show you briefly how to use these functions in the context of this example: We define the sets \\(A\\) and \\(B\\) first using the assignment operator:\n\nA &lt;- c(1,2,3)\nB &lt;- c(2,4,6)\n\nWe compute the union by using the function union()\n\nunion(A,B)\n\n[1] 1 2 3 4 6\n\n\nwhich gives us the union of both sets.\nTo understand this operation better we can visualize the set union in this example by looking at Figure 2.1\n\n\n\n\n\n\n\n\nFigure 2.1: The meaning of set union\n\n\n\n\n\nThe sample space \\({\\cal S}\\) is the gray set containing all possible outcomes of our random experiment. Graphically the union of \\(A\\) and \\(B\\), \\(A \\cup B\\) is a subset of the sample space, the entire colored area.\n\nIntersection\n\nThe intersection of two events are all outcomes that are both in \\(A\\) and in \\(B\\). It is written as \\(A \\cap B\\).\n\n\nIn R we would implement this operation by using the function intersect() and apply it to our sets \\(A\\) and \\(B\\) we have defined before.\n\nintersect(A,B)\n\n[1] 2\n\n\nFigure 2.2 visualizes this operation graphically\n\n\n\n\n\n\n\n\nFigure 2.2: The meaning of set intersection\n\n\n\n\n\nThe intersection of \\(A\\) and \\(B\\), \\(A \\cap B\\) is the orange area containing the dice face with two points. Indeed two is both in \\(A\\) and in \\(B\\), which is exactly the meaning of set intersection.\n\nComplement\n\nThe complement of an event \\(A\\) within the sample space \\({\\cal S}\\) is the set of all outcomes that are in \\({\\cal S}\\) but not in \\(A\\). It is written as \\({\\cal S} \\setminus A\\).\n\n\nLets say we want to get the complement, or the set difference of \\(A \\cup B\\) with respect to the sample space \\({\\cal S}\\).\nThe R implementation of the set differecne operation is the function setdiff(). This is how we would tell R to compute the setdifference of the union of \\(a\\) and \\(B\\) and the sample space \\({\\cal S}\\) in our example of the die:\n\nS &lt;- c(1,2,3,4,5,6)\n\nsetdiff(S, union(A,B))\n\n[1] 5\n\n\nThis can again be visualized in ?fig-set-minus\n\n\n\n\n\nThe meaning of complement\n\n\n\n\nThis complement is the dice shown in the light redish area, i.e. all the elements of \\({\\cal S}\\) which are not in \\(A \\cup B\\).\n\nMutually Exclusive\n\nTwo events \\(A\\) and \\(B\\) are mutually exclusive if they can not occur simultaneously. This means \\(A \\cap B = \\emptyset\\), their intersection is empty.\n\n\nAn example in our context is the set of even outcomes \\(B=\\{2,4,6\\}\\) and the set of odd outcomes, let us call it \\(C=\\{1,3,5\\}\\). If we intersect these sets\n\nB &lt;- c(2,4,6)\nC &lt;- c(1,3,5)\n\nintersect(B,C)\n\nnumeric(0)\n\n\nwe get the empty set, which is expressed by R by giving the data type, in this case numeric, because we are intersecting sets of numeric values, followed by (0). This means, there is no numeric value in the intersection of \\(B\\) and \\(C\\).\nLet us discuss set operations a bit further by thinking about the probability of the union of two events \\(A\\) and \\(B\\) within the context of our visual examples. Remember that we had \\(A = \\{1,2,3 \\}\\) and $B = { 2,4,6}. Look at Figure 2.3\n\n\n\n\n\n\n\n\nFigure 2.3: The meaning of set union\n\n\n\n\n\nWe would like to know what is the probability of \\(P(A \\cup B)\\)? Now if we add up \\(P(A)\\) and \\(P(B)\\) we would assign a probability to the outcome \\(2\\) twice. Such double counting must be avoided and thus we have to subtract \\(P(A \\cap B)\\) the intersection, so that: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\). Now you see what the qualification mutually exclusive does in our probability rule 3. When \\(A\\) and \\(B\\) are mutually exclusive \\(A \\cap B = \\emptyset\\) and in this case \\(P(A \\union B) = P(A) + P(B)\\). We can add up the probabilities because when events are mutually exclusive we can not double count events by adding up individual probabilities.\n\n\n\n\n\n\nNow you try\n\n\n\nIf you drop the qualification that \\(A\\) and \\(B\\) must be mutually exclusive in point 3 in the definition of probability. How would rule 3 have to be changed?\n\n\nHere’s a tip you could share with your students to encourage effective use of a Large Language Model (LLM) for deepening their understanding of the concepts and definitions introduced in this section:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#using-an-llm-to-deepen-your-understanding-of-set-theory-in-probability",
    "href": "02-lecture2.html#using-an-llm-to-deepen-your-understanding-of-set-theory-in-probability",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.4 Using an LLM to Deepen Your Understanding of Set Theory in Probability",
    "text": "2.4 Using an LLM to Deepen Your Understanding of Set Theory in Probability\nAn LLM like ChatGPT can be an incredibly helpful tool to explore concepts and solidify your understanding. Here are some example of what you could do:\n\n2.4.1 Ask for Clarifications\nIf a definition or concept isn’t clear, ask the LLM to explain it in simpler terms or using different examples. For instance:\n\n\n\n\n\n\nPrompt\n\n\n\n“What is the difference between the union and intersection of sets in probability? Can you give examples?”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“Can you compare this to a real-life scenario, like rolling a die or flipping a coin?”\n\n\n\n\n2.4.2 Generate Additional Examples\nUse the LLM to create new examples similar to the ones in the lecture. This will give you more practice applying the concepts.\n\n\n\n\n\n\nPrompt\n\n\n\n“Give me an example of mutually exclusive events involving sports outcomes.”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Can you show a sample space and events for tossing two coins?”\n\n\n\n\n2.4.3 Simulate Visualizations and Code Interpretation\nWhile the LLM doesn’t directly produce visuals, you can ask it to describe how a diagram or R output would look. This helps connect theoretical concepts to their graphical representations. For example:\n\n\n\n\n\n\nPrompt\n\n\n\n“Describe what a Venn diagram looks like for \\(A \\cup B\\), \\(A \\cap B\\), and \\(A \\setminus B\\).”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“What does the R function union(A, B) compute? How is it related to \\(A \\cup B\\)?”\n\n\n\n\n2.4.4 Practice Applying Definitions\nUse the LLM to test your understanding by quizzing yourself.\n\n\n\n\n\n\nPrompt\n\n\n\n“Ask me questions about the definitions of sample spaces, union, intersection, and complement.”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Give me a scenario and ask which set operation applies.”\n\n\n\n\n2.4.5 Explore Real-World Applications\nUse the LLM to explore how these concepts apply in real-world contexts beyond the lecture.\n\n\n\n\n\n\nPrompt\n\n\n\n“How is the concept of set intersection used in data science or finance?”\n\n\n\n\n\n\n\n\nPrompt\n\n\n\n“Explain how mutually exclusive events are important in designing experiments.”\n\n\n\n\n2.4.6 Learn R Through Step-by-Step Guidance\nIf you’re new to R, ask the LLM to guide you through using functions like union(), intersect(), and setdiff() step by step.\n\n\n\n\n\n\nPrompt\n\n\n\n“Explain how to use setdiff() in R with an example involving dice rolls.”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“How does this output relate to the complement of a set?”\n\n\n\nDive Deeper into Probability Rule 3:\n\nUse the LLM to generate explanations and examples that clarify how probabilities relate to set operations.\n\n\n\n\n\n\nFollow up\n\n\n\n“Explain why \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).”\n\n\n\n\n\n\n\n\nFollow up\n\n\n\n“Can you provide a numerical example to illustrate this rule?”\n\n\n\n\n2.4.7 Simulate Discussions\nAsk the LLM to take the role of a peer or instructor to simulate a conversation about the material.\n\n\n\n\n\n\nPrompt\n\n\n\n“Pretend you are my study partner. Let’s discuss the complement of events and its significance in probability.”\n\n\nBy actively engaging with the LLM through these kinds of prompts, you can practice, explore, and deepen your understanding of the material beyond the lecture. Try it alone or with your group.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#summary",
    "href": "02-lecture2.html#summary",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.8 Summary",
    "text": "2.8 Summary\nIn this lecture, we explored foundational concepts in probability theory and developed practical skills for working with R. Here’s a summary of what we covered:\n\n2.8.0.1 Probability Concepts\n\nPrecise definitions of:\n\nSample space: The set of all possible outcomes of an experiment.\nBasic outcomes: Individual elements of the sample space.\nEvents: Subsets of the sample space.\nProbability: A numerical measure of the likelihood of an event, satisfying the key properties of non-negativity, normalization, and additivity.\n\nDiscrete probability:\n\nHow probabilities are assigned to events in a finite or countably infinite sample space.\nThe construction of events using set operations like union, intersection, and complement.\nThe concept of mutually exclusive events and its implications for probabilities.\n\nEmpirical probability (or frequency probability):\n\nThe relative frequency of an event in a series of repeated experiments.\nHow empirical probabilities approximate theoretical probabilities as the number of trials increases, as demonstrated by the weak law of large numbers.\n\nIndependence of events:\n\nDefinition: Two events (A) and (B) are independent if (P(A B) = P(A) P(B)).\nImplications for computing probabilities in repeated or combined experiments.\n\n\n\n\n2.8.0.2 R Concepts\n\nR Objects:\n\nOverview of atomic vectors and their six types: double, integer, character, logical, complex, and raw.\nAttributes of atomic vectors, such as names, dimensions, and classes.\nIntroduction to data frames, the primary data structure for organizing and analyzing tabular data in R.\n\nSubsetting in R:\n\nMethods for subsetting R objects using positive integers, negative integers, zero, blank spaces, logical vectors, and names.\nPractical examples to manipulate data frames and vectors efficiently.\n\nString manipulation in R:\n\nExtracting and processing information from character data.\n\n\n\n\n\n2.8.0.3 Applications\n\nSimulating coin tosses:\n\nUsing R to simulate repeated trials and compute relative frequencies.\n\nReading data:\n\nLoading datasets from stored CSV files and directly from online sources.\n\nStock price analysis:\n\nUsing subsetting techniques to analyze up and down movements in stock prices.\nComputing empirical probabilities of price movements and using independence assumptions to estimate probabilities of sequences of moves.\n\nBenford’s Law:\n\nExploring the surprising regularity of leading digits in real-world datasets.\nApplying Benford’s Law to trading volumes and comparing empirical distributions to theoretical predictions.\n\n\nThrough this lecture, we reinforced the connection between probability theory and data analysis, demonstrating how abstract concepts like empirical probabilities and independence can be applied in real-world scenarios using R. The integration of theoretical ideas with computational tools prepares you for more advanced analysis and real-world problem-solving.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "02-lecture2.html#project-2-financial-data-forensics-investigating-financial-reports-using-benfords-law",
    "href": "02-lecture2.html#project-2-financial-data-forensics-investigating-financial-reports-using-benfords-law",
    "title": "2  Probability: Basic Definitions and Rules",
    "section": "2.9 Project 2: Financial Data Forensics – Investigating Financial Reports Using Benford’s Law",
    "text": "2.9 Project 2: Financial Data Forensics – Investigating Financial Reports Using Benford’s Law\n\n2.9.1 Overview\nThis project challenges you to apply Benford’s Law to detect potential anomalies in financial data. By analyzing the leading digits of revenue and expenditure data for a set of companies, you will explore whether these datasets follow the natural logarithmic distribution predicted by Benford’s Law. Through this project, you will reinforce your understanding of empirical probabilities and their applications, while also practicing critical data analysis skills in R.\n\n\n2.9.2 Objectives\n\nAnalyze the distribution of leading digits in revenue and expenditure data.\nCompare empirical distributions with the theoretical predictions of Benford’s Law.\nIdentify and interpret deviations from Benford’s Law.\nReflect on the implications of your findings in the context of financial forensics.\n\n\n\n2.9.3 Steps\nStep 1: Understand the Research Question Your main tasks are: 1. To determine if the leading digits of revenues and expenditures conform to Benford’s Law. 2. To interpret deviations, particularly in expenditure data, which may suggest anomalies such as fraud or manipulation.\nStep 2: Obtain and Inspect the Dataset\n\nDownload the Dataset:\n\nMock Dataset: You will receive a CSV file named company_financials.csv, containing simulated data for revenues and expenditures of 200 companies. This dataset includes some subtle anomalies in the expenditures.\n\nInspect the Data:\n\nLoad the dataset in R and examine its structure using functions like head(), summary(), and str().\nEnsure you understand the data columns:\n\nCompanyID: A unique identifier for each company.\nRevenue: The revenue of the company (in dollars).\nExpenditure: The expenditure of the company (in dollars).\n\n\n\nStep 3: Prepare the Data\n\nFilter Valid Data:\n\nExclude invalid entries:\n\nNon-positive values (e.g., 0 or negative numbers).\nMissing values (NA).\n\n\nExtract Leading Digits:\n\nUse string manipulation to extract the first digit from each valid value:\n\n\nStep 4: Analyze the Data\n\nCompute Empirical Frequencies:\n\nTabulate the frequencies of the leading digits for revenues and expenditures:\n\nCompare with Benford’s Law:\n\nCreate data frames for comparison:\n\nVisualize the Results:\n\nPlot bar charts comparing empirical and theoretical distributions for revenues and expenditures.\n\n\nStep 5: Interpret the Results\n\nEvaluate Conformity:\n\nDoes the revenue data closely match Benford’s predictions?\nDo expenditures show significant deviations?\n\nHypothesize Causes:\n\nWhat might explain deviations in expenditures? Consider:\n\nRounded or artificial values.\nAnomalies such as fraud.\n\n\nProbability Context:\n\nRelate empirical frequencies to probabilities and discuss the implications of large sample sizes.\n\n\nStep 6: Submit Your Work\n\nA short report summarizing:\n\nYour approach to data preparation and analysis.\nA comparison of the revenue and expenditure distributions with Benford’s Law.\nYour interpretation of any anomalies.\n\nWell-commented R code for your analysis.\n\nDeliverables\n\nAnalysis Report:\n\nInclude plots and tables summarizing your findings.\n\nR Script:\n\nSubmit a clear and well-commented R script.\n\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nLo, Andrew, and Craig MacKinlay. 2019. A Non-Random Walk down Wallstreet. Princeton University Press.\n\n\nSamuelson, Paul. 1965. “Proof That Properly Anticipated Prices Fluctuate Randomly.” Industrial Management Review 6.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability: Basic Definitions and Rules</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html",
    "href": "03-lecture3.html",
    "title": "3  Conditional Probability",
    "section": "",
    "text": "3.1 Why neglecting conditional probability may be expensive: A case study.\nWith this motivation in mind let us turn to a historical example that demonstrates the importance of understanding conditional probability: the financial crisis of 2007-2008. This crisis revealed how wrong assumptions about independence and neglect of dependence in events and conditional probabilities can lead to systemic failures in structured finance.1\nTo understand the real world aspects of this example it is necessarry to understand some basic ideas of structured finance and the engineering of specific risk profiles from a portfolio of risky securities in the first place. I will explain the finance context with a simple and stylized example and then discuss how understanding conditional probability may help us to make better financial decisions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#understanding-structured-finance-bonds-pools-and-tranches",
    "href": "03-lecture3.html#understanding-structured-finance-bonds-pools-and-tranches",
    "title": "3  The Concept of Conditional Probability",
    "section": "",
    "text": "3.1.1 Bonds and Credit Risk\nBond, face value, par value : A bond is a financial instrument where the issuer agrees to pay the holder a specific amount, the face value or par value, at maturity. Bonds are widely used as fixed-income securities but carry the risk of default if the issuer faces financial difficulties.\nTo quantify this risk, bonds are rated by agencies such as Moody’s and Standard & Poor’s. Investment-grade bonds are considered low-risk, while speculative or “junk” bonds are riskier and more likely to default. Here is a summary of their rating schemes and what the ratings mean in words:\n\n\n\nRating Category\nMoody’s\nStandard & Poor’s\n\n\n\n\nHigh grade\nAaa\nAAA\n\n\n\nAa\nAA\n\n\nMedium grade\nA\nA\n\n\n\nBaa\nBBB\n\n\nSpeculative grade\nBa\nBB\n\n\n\nB\nB\n\n\nDefault danger\nCaa\nCCC\n\n\n\nCa\nCC\n\n\n\nC\nC\n\n\n\n\nD\n\n\n\n\n\n3.1.2 Pooling and Tranching: The Innovation\nStructured finance emerged in the early 2000s as a way to manage risk through pooling and tranching. By pooling risky assets and dividing cash flows into “tranches” with distinct risk profiles, financial engineers created new bonds, including investment-grade securities, from portfolios of junk bonds. A major product of this innovation was the mortgage-backed security (MBS).\n\n3.1.2.1 An Illustrative Example: Building Structured Finance in Steps\nLet us develop an intuitive understanding of structured finance and its relation to probability through a simplified example, which I learned from Karl Schmedder’s course mentioned at the beginning.2\n2 See https://www.coursera.org/learn/introductiontoprobability\n3.1.2.1.1 A Simple Event Tree for One Bond\nConsider a single bond you can own today that pays €1 at maturity at some point in the future. Time is often abbreviated as \\(t\\) and the points in time are symbolized by letting \\(t\\) take different values like \\(t=0\\) for today and \\(t=1\\) for a future point in time, say a year from today.\nThis bond has a 10% chance of default, meaning there is a 90% chance it will not default. With a default probability of 10%, this bond would likely receive a speculative grade rating, such as ‘B’ or ‘B-’ in the rating tables presented earlier. This poor rating reflects the significant risk of non-payment associated with such a bond, which could deter risk-averse investors and highlight its ‘junk’ bond status. The payoff is structured as follows:\n\nIf the bond does not default (“N”), the payoff is €1.\nIf the bond defaults (“D”), the payoff is €0.\n\nThis situation can be represented as a simple probability tree of Figure 3.1\n\n\n\n\n\n\n\n\nFigure 3.1: Event tree for one bond\n\n\n\n\n\nThe graph above visualizes the outcomes of a single bond. Each node represents a possible state of the bond at different times:\n\n\\(t=0\\) is the starting point.\n\\(t=1\\) No Default \\(N\\) occurs with a probability of \\(P(N) = 0.9\\).\n\\(t=1\\) Default \\(D\\) occurs with a probability of \\(P(D) = 0.1\\).\n\nYou could see this in analogy to the toss of a coin with the difference that both sides of the coin show with different probability.\n\n\n\n3.1.2.2 Combining Two Bonds with Independence Assumption\nWe now combine two such bonds into a portfolio. The assumption of independence implies that the defaults of these bonds occur independently of each other.\nWhile this assumption simplifies calculations, it was historically used by financial engineers to justify creating tranches from risky portfolios. The reasoning was that diversification reduces the likelihood of joint defaults, making some tranches appear safer.\nAt the time, financial engineers relied on historical data and market conditions to argue for this independence. Defaults were often uncorrelated under normal economic conditions, and diversification was seen as a proven strategy for mitigating risk. For example, if bond defaults were driven by isolated events (such as company-specific issues), the assumption of independence seemed reasonable. Moreover, the packaging of diverse assets from different industries into portfolios strengthened the appearance of safety, as individual economic shocks were less likely to affect the entire portfolio simultaneously.\nHowever, this reasoning neglected systemic risks. During economic downturns or financial crises, defaults often become highly correlated due to shared macroeconomic pressures, such as declining housing markets or credit tightening. For instance, in the lead-up to the 2008 financial crisis, rising mortgage defaults were driven by broader economic factors that impacted many bonds simultaneously. WIth this in mind it would be not pausible to assume that bonds can be stacked together in a portfolio without the default risks of one being not pushed up by the default risk of others.\nEven without the formal use of probability theory, financial engineers could have questioned whether diversification truly guaranteed independence in the context of systemic risks.\nThe idea that junk plus junk could be transformed into investment-grade bonds through pooling should have raised skepticism. Careful critical thinking—considering broader economic dependencies—would have revealed that this transformation was too good to be true. By ignoring these dependencies, financial engineers failed to see how small cracks in the system could cascade into systemic failures.3\n3  A famous voice at the time warning about the flawed reasoning was Raghuram Rajan, former chief economist of the International Monetary Fund. He warned that rather than reducing risk through diversification, CDOs and other derivatives spread risk and uncertainty about the value of the underlying assets more widely. But let us not jump ahead and try to see how the tree for two bonds looks like when we assume independence in Figure 3.2.\n\n\n\n\n\n\n\n\nFigure 3.2: Event tree for two bonds with independent default risk\n\n\n\n\n\nThe outcome of combining two bonds can be represented as a double event tree, showing all possible combinations of defaults and no defaults at \\(t=1\\): picture here\n\n\n3.1.2.3 Pooling and Tranching\nWhen you look at this portfolio under the independence assumption there seems to be room for reengineering the risk profile of this portfolio. In all outcomes but in the one where both bonds default you can guarantee a payoff of 1. There remains an adverse outcome, where both bonds default in which case you can pay out nothing. But under independence this risk is small. The probability of this event - remember our treatment of independent events in the first lecture - would be: \\(P(D) \\times P(D) = 0.1 \\times 0.1 = 0.01\\) Pretty low. For example assume that the probability of default refer to the probability of the bond defaulting over a year, the usual time frame taken in ratings, this would be a one in a hundered years event. In \\(99%\\) we would get a sure payoff of 1.\nSo this is the idea. We pool the payoffs of both securities and define two new securities by changing their payoff profile. The first one pays always 1 except when both bonds default in which case this bond pays 0. The other one will always pay 0 except in the case where both bonds do not default. This is under independence an event with probability \\(P(N) \\times P(N) = 0.81\\). Rember the complementarity rule? This says that the second bond will thus have a default probability of \\(19\\) % instead of \\(10%\\) it would be close to toxic junk now.\nHere is picture how you can visualize this piece fo financial magic.\n\n\n\n\n\n\n\n\nFigure 3.3: Event tree for two bonds with independent default risk with pooling and tranching\n\n\n\n\n\nWhat is done here is that the aggregate payoffs of both bonds are collected in a pool and new securities - called P - in this picture are issued against the pool. One, the upper one is now an investment grade asset paying 1 in every state except one and the other is a toxic junk bond paying always 0 except in one state. Note that the investment grade status could be engineered under the assumption that the risks are independent.\nNow, let us consider a hypothetical question: How would the event tree change if the independence assumption does not hold? Dependence would alter the probabilities in a way that reflects the increased likelihood of joint defaults during systemic events.\nSuppose we now assume that the probability of Bond 2 defaulting changes rather than staying unchanged under the condition that Bond 1 has defaulted:\n\nIf Bond 1 does not default, Bond 2 defaults with probability 0.1 (as before).\nIf Bond 1 does default, Bond 2 defaults with a higher probability of 0.6 due to systemic dependence.\n\nThis dependence reflects a scenario where defaults are more likely to occur together, such as during an economic downturn. The resulting event tree can be visualized as follows:\n\n\n\n\n\n\n\n\nFigure 3.4: Event tree for two bonds with independent default risk with pooling and tranching with dependent default risk\n\n\n\n\n\nOn the first sight this looks almost identical to the picture before. Only the numbers on the edges of the second tree have changed. These changed numbers reflect the idea that the event that one bond has defaultet changes the probability of the second bond defualting as well. How could such a dependence occur?\nHere’s a real-world example to illustrate how dependence can occur: In the context of bonds, dependence in default probabilities can arise from shared exposure to systemic risks or interconnected factors. For instance, consider two companies that issue bonds and operate in the same industry, such as the energy sector. If oil prices plummet due to an economic downturn or geopolitical instability, both companies might experience financial stress, making it more likely that one default is followed by another.\nAnother example is during a financial crisis, such as the 2008 global financial meltdown. A bank’s default on its obligations can lead to cascading defaults in other institutions due to counterparty risks or a general loss of confidence in the financial system. In such cases, the probability of a second default is no longer independent of the first because the events are tied to the same underlying macroeconomic factors.\nThese examples highlight that the assumption of independence between bond defaults might hold under normal market conditions but breaks down during systemic crises. Such dependencies must be carefully modeled to avoid underestimating risk, as was the case in structured finance products leading up to the 2008 crisis.\nA prudent risk manager must keep such a scenario in mind when he analyzes a portfolio. Think about it in the context of the toy example. In the first case the default risk of the first asset created by pooling and tranching was \\(P(D) \\times P(D) = 0.1 \\times 0.1 = 0.01\\). Under a scenario with dependent risks this changes to \\(P(D) \\times P(D | D) = 0.1 \\times 0.6\\), a risk larger by a factor of 6! While the first bond would be rated as investment grade, the second would be rated as speculative grade and the magic from pooling and tranching suddenly disappears. Junk plus junk remains junk after all.\n\n\n\n3.1.3 Analysis of Dependence\nIn this dependent tree:\n\nThe likelihood of both bonds defaulting increases significantly when one bond defaults.\nThe probability of joint defaults is no longer the product of independent probabilities but instead reflects their dependence.\n\nThis dependence shows why pooling junk bonds into tranches and assuming independence can be misleading. Junk plus junk remains junk if defaults are correlated. The assumption of independence is critical for the logic of pooling and tranching to create investment-grade securities.\n\n\n3.1.4 Conditions for Pooling and Tranching to Work\nFor pooling and tranching to reduce overall risk and create safe tranches:\n\nDiversification: Assets must come from independent sectors with minimal systemic risk.\nStable Macroeconomic Conditions: Systemic risks must be low to maintain independence assumptions.\nTransparent Modeling: Dependence structures must be explicitly modeled and accounted for in risk assessments.\n\nThe neglect of these conditions led to a flawed sense of security in structured finance, which contributed to the 2008 financial crisis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Concept of Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#motivating-conditional-probability",
    "href": "03-lecture3.html#motivating-conditional-probability",
    "title": "3  The Concept of Conditional Probability",
    "section": "3.2 Motivating Conditional Probability",
    "text": "3.2 Motivating Conditional Probability\nHaving seen the importance of independence and dependence in financial modeling, it is clear that these concepts directly relate to conditional probability. Conditional probability formalizes how the probability of one event changes when another event is known to occur, providing a framework for understanding dependencies quantitatively.\nWe now can give the\n\nFormal definition of conditional probability\n\nLet \\(A\\) and \\(B\\) be given events. We define the conditional probability of \\(A\\) given \\(B\\) as \\[\\begin{equation*}\nP(A\\,|\\,B) = \\frac{P(A \\cap B)}{P(B)}\\,\\,\\, \\text{provided}\\,\\,\\, P(B) \\neq 0\n\\end{equation*}\\]\n\n\nNote that for conditional probabilities we have for two events \\(A\\) and \\(B\\), that \\(P(A|B) \\neq P(B|A)\\). To see this assume that \\(P(A) \\neq P(B)\\) and \\(P(A) \\neq 0\\) and \\(P(B)\\neq 0\\).\nWe then get: \\[\\begin{equation*}\nP(A|B) = \\frac{P(A\\cap B)}{P(B)} = \\frac{P(B \\cap A)}{P(B)}\n\\end{equation*}\\] since \\(P(A \\cap B) = P(B \\cap A)\\). It follows that \\[\\begin{equation*}\n\\frac{P(B \\cap A)}{P(B)} \\neq \\frac{P(B \\cap A)}{P(A)} = P(B|A)\n\\end{equation*}\\] since we have assumed that \\(P(A) \\neq P(B)\\). Therefore \\(P(A|B) \\neq P(B|A)\\). You can check this with the stock example, we discussed before.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Concept of Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#the-multiplication-rule",
    "href": "03-lecture3.html#the-multiplication-rule",
    "title": "3  The Concept of Conditional Probability",
    "section": "3.3 The multiplication rule",
    "text": "3.3 The multiplication rule\nThe following formula is often called the multiplication rule and is just a rewritten form of the definition of conditional probability.\n\nMultiplication rule\n\nGiven events \\(A\\) and \\(B\\) it holds that \\[\\begin{equation*}\nP(A \\cap B) = P(A | B)\\times P(B)\n\\end{equation*}\\]\n\n\nThe multiplication rule can give us a deeper insight into the notion of independence, we discussed earlier. Remember that two events \\(A\\) and \\(B\\) are independent if \\(P(A \\cap B) = P(B \\cap A) = P(A) \\times P(B)\\).\nIf we combine this rule with the concept of conditional probability, we see that if two events \\(A\\) and \\(B\\) are independent, then \\[\\begin{equation*}\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A) \\times P(B)}{P(B)} = P(A)\n\\end{equation*}\\] and \\[\\begin{equation*}\nP(B|A) = \\frac{P(B \\cap A)}{P(A)} = \\frac{P(A) \\times P(B)}{P(A)} = P(B)\n\\end{equation*}\\]\nThis formula says that if two events are independent the probability of \\(A\\) is not influenced by the event \\(B\\) occurring and the probability of event \\(B\\) is not influenced by the event \\(A\\) occurring.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Concept of Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#law-of-total-probability",
    "href": "03-lecture3.html#law-of-total-probability",
    "title": "3  The Concept of Conditional Probability",
    "section": "3.4 Law of total probability",
    "text": "3.4 Law of total probability\nThe multiplication rule implies a further rule, which is often called the law of total probability in the literature. We state the law for a simple case for two disjoint events, which can be generalized in a straightforward way to \\(N\\) disjoint events.\n\nLaw of total probability\n\nSuppose the sample space \\({\\cal S }\\) is divided into two disjoint events \\(B_1\\) and \\(B_2\\), which cover the sample space then for any event \\(A\\) we have \\[\\begin{align}\nP(A)&= P(A\\cap B_1) + P(A\\cap B_2)\\\\\n&=P(A|B_1)P(B_1) + P(A|B_2)P(B_2)\n\\end{align}\\] Such covers of disjoint sets are called a partition of the sample space. It can abstractly be visualized as follows:\n\n\n\n\n\n\n\nA Visualization of total probability\n\n\n\n\nNote that in this picture \\(B_1 \\cap B_2 = \\emptyset\\) and \\(B_1 \\cup B_2 = {\\cal S}\\), hence \\(B_1\\) and \\(B_2\\) form a partition of the sample space. The probability of \\(A\\), the red set can then be determined by the law of total probability.\nAs an example, consider a stock analysis problem, where you learn that a company is planning to launch a new project that is likely to affect the companies stock price. There is a \\(60 \\%\\) probability that a project will be launched. If this happens there is \\(75 \\%\\) probability that the stock price will increase. In case the project is not launced this probability is only \\(30 \\%\\).\nYou might think of this situation in terms of a contingency table:\n\n\n\n\n\n\n\n\n\n\nLaunch Project\nDon’t Launch Project\n\n\n\n\n\nStock price rise\n0.45\n0.12\n0.57\n\n\nStock price not rising\n0.15\n0.28\n0.43\n\n\nTotal\n0.6\n0.4\n1\n\n\n\nSay we want to assess the probability that the stock price will rise, given these data. Then we can apply the law of total probability is: \\(0.45 + 0.12 = 0.57\\).\n\n\n\n\nTooze, Adam. 2018. Crashed. How a Decade of Financial Crisis Changed the World. Viking.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Concept of Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#why-neglecting-conditional-probability-may-be-expensive-a-case-study.",
    "href": "03-lecture3.html#why-neglecting-conditional-probability-may-be-expensive-a-case-study.",
    "title": "3  Conditional Probability",
    "section": "",
    "text": "1  See Tooze (2018): For an engaging and comprehensive exploration of the great financial crisis of 2007 - 2008 and its causes and aftermath. It is highly recommended for deeper study.\n\n3.1.1 Bonds and Credit Risk\nA bond is a financial instrument where the issuer agrees to pay the holder a specific amount, the face value or par value, at maturity. Bonds are widely used as fixed-income securities but carry the risk of default if the issuer faces financial difficulties.\nTo quantify this risk, bonds are rated by agencies such as Moody’s and Standard & Poor’s. Investment-grade bonds are considered low-risk, while speculative or “junk” bonds are riskier and more likely to default. Here is a summary of their rating schemes and what the ratings mean in words:\n\n\n\nRating Category\nMoody’s\nStandard & Poor’s\n\n\n\n\nHigh grade\nAaa\nAAA\n\n\n\nAa\nAA\n\n\nMedium grade\nA\nA\n\n\n\nBaa\nBBB\n\n\nSpeculative grade\nBa\nBB\n\n\n\nB\nB\n\n\nDefault danger\nCaa\nCCC\n\n\n\nCa\nCC\n\n\n\nC\nC\n\n\n\n\nD\n\n\n\n\n\n3.1.2 Pooling and Tranching: The Innovation\nStructured finance emerged in the early 2000s as a way to manage risk through pooling and tranching. By pooling risky assets and dividing cash flows into “tranches” with distinct risk profiles, financial engineers created new bonds, including investment-grade securities, from portfolios of bonds which individually would be rated as speculative grade or junk bonds. A major product of this innovation was the mortgage-backed security (MBS). Many other products were then invented using similar financial engineering ideas.\nLet us develop an intuitive understanding of structured finance and its relation to probability through a simplified example, which I learned from Karl Schmedder’s course.2\n2 See https://www.coursera.org/learn/introductiontoprobability\n\n3.1.3 A Simple Event Tree for One Bond\nConsider a single bond you can own today that pays €1 at maturity at some point in the future. Time is often abbreviated as \\(t\\) and the points in time are symbolized by letting \\(t\\) take different values like \\(t=0\\) for today and \\(t=1\\) for a future point in time, say a year from today.\nThis bond has a 10% chance of default, meaning there is a 90% chance it will not default. With a default probability of 10%, this bond would likely receive a speculative grade rating, such as ‘B’ or ‘B-’ in the rating tables presented earlier. This poor rating reflects the significant risk of non-payment associated with such a bond, which could deter risk-averse investors and highlight its ‘junk’ bond status. The payoff is structured as follows:\n\nIf the bond does not default (\\(N\\)), the payoff is €1.\nIf the bond defaults (\\(D\\)), the payoff is €0.\n\nThis situation can be graphically represented as a simple probability tree of Figure 3.1\n\n\n\n\n\n\n\n\nFigure 3.1: Event tree for one bond\n\n\n\n\n\nThe graph above visualizes the outcomes of a single bond. Each node represents a possible state of the bond at different times:\n\n\\(t=0\\) is the starting point.\n\\(t=1\\) No Default \\(N\\) occurs with a probability of \\(P(N) = 0.9\\).\n\\(t=1\\) Default \\(D\\) occurs with a probability of \\(P(D) = 0.1\\).\n\nYou could see this in analogy to the toss of a coin with the difference that both sides of the coin show with different probability. With this analogy - using the concepts of the last two lectures - you can understand the bond in probabilistic terms as a random experiment with a sample space consisting of two basic outcomes, \\(N\\) and \\(D\\) with given probabilities \\(P(N)\\) and \\(P(D)\\).\n\n\n3.1.4 Combining Two Bonds with Independence Assumption\nWe now combine two such bonds into a portfolio. The assumption of independence implies that the defaults of these bonds occur independently of each other. This means that the default of one bond has no influence on the probability of the other bond defaulting. Under the assumption of independence you would treat the default probability of one bond as unconnected with the default probability of the other.\nWhile this assumption simplifies calculations, it was historically used by financial engineers to justify creating tranches from risky portfolios. The reasoning was that diversification reduces the likelihood of joint defaults, making some tranches appear safer.\nAt the time, financial engineers relied on historical data and market conditions to argue for this independence. Defaults were often uncorrelated under normal economic conditions, and diversification was seen as a proven strategy for mitigating risk. For example, if bond defaults were driven by isolated events (such as company-specific issues), the assumption of independence seemed reasonable. Moreover, the packaging of diverse assets from different industries into portfolios strengthened the appearance of safety, as individual economic shocks were less likely to affect the entire portfolio simultaneously.\nHowever, this reasoning neglected systemic risks. During economic downturns or financial crises, defaults often become highly correlated due to shared macroeconomic pressures, such as declining housing markets or credit tightening. For instance, in the lead-up to the 2008 financial crisis, rising mortgage defaults were driven by broader economic factors that impacted many bonds simultaneously. With this in mind it would be not plausible to assume that bonds can be stacked together in a portfolio without the default risks of one being not pushed up by the default risk of others.\nEven without the formal use of probability theory, financial engineers could have questioned whether diversification truly guaranteed independence in the context of systemic risks.\nThe idea that junk plus junk could be transformed into investment-grade bonds through pooling should have raised skepticism. Careful critical thinking—considering broader economic dependencies—would have revealed that this transformation was too good to be true. By ignoring these dependencies, financial engineers failed to see how small cracks in the system could cascade into systemic failures.3\n3  A famous voice at the time warning about the flawed reasoning was Raghuram Rajan, former chief economist of the International Monetary Fund. He warned that rather than reducing risk through diversification, CDOs and other derivatives spread risk and uncertainty about the value of the underlying assets more widely. But let us not jump ahead and try to see how the tree for two bonds looks like when we assume independence in Figure 3.2.\n\n\n\n\n\n\n\n\nFigure 3.2: Event tree for two bonds with independent default risk\n\n\n\n\n\n\nThe outcome of combining two bonds can be represented as a double event tree, showing all possible combinations of defaults and no defaults at \\(t=1\\). Let me unpack this more complex tree for you.\nSince we have a portfolio of two bonds, instead of one as before, we have now two event trees combined. Since we have to take into account all of the possible combinations of basic outcomes this means that we have to attach a new bond event tree to each outcome of the initial tree.\nNote the time index. In the example the realizations of basic outcomes for the portfolio happen simultaneously. So the walk from the origin to the end of the tree along a path is taking place in one time step and only the outcomes at \\(t=1\\) are observed.\nAt the edges we write the probability of the outcomes. For example \\(P(N)\\) is the probability of the first bond (represented by the upper tree) is not defaulting whereas \\(P(D)\\) denotes the probability of the bond not defaulting. The assumption of independence is hidden in this tree by modelling the probabilities of \\(N\\) and \\(D\\) for the second bond in exactly the same way no matter whether the first bond defaults or not. It is modelled in anaology to the toss of two fair coins. The probability of the second coin showing Heads is \\(1/2\\) no matter whether the first coin shows Heads or Tails.\nAt the end of the tree we have written the outcome of each path in the notation \\[\\begin{equation*}\nB_{\\text{state of bond 1 at} \\, t = 1 \\, \\text{state of bond 2 at} \\, t=1} = \\binom{\\text{payoff of bond 1 at } \\, t= 1}{\\text{payoff of bond 2 at } \\, t = 1}\n\\end{equation*}\\] So, for example, \\(B_{NN} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) means that bond 1 does not default and bond two does not default (\\(B_{NN}\\)). Bond 1 has in this case a payoff of 1 and bond 2 also has a payoff of 1.\n\n\n\n3.1.5 Pooling and Tranching\nWhen you look at this portfolio under the independence assumption there seems to be room for re-engineering the risk profile of this portfolio. In all outcomes but in the one where both bonds default you can guarantee a payoff of 1. There remains an adverse outcome, where both bonds default in which case you can’t pay out anything. But under the assumption of independence this risk is small. The probability of this event - remember our treatment of independent events in the first lecture - would be: \\(P(D) \\times P(D) = 0.1 \\times 0.1 = 0.01\\). Pretty low, actually. For example assume that the probability of default refer to the probability of the bond defaulting over a year, the usual time frame taken in ratings, this would be a one in a hundred years event. In \\(99%\\) we would get a sure payoff of 1. So under this restructuring the first restructured bond would qualify as an investment grade bond.\nSo this is the idea. We pool the payoffs of both securities and define two new securities by changing their payoff profile. The first one pays always 1 except when both bonds default in which case this bond pays 0. The other one will always pay 0 except in the case where both bonds do not default. This is under independence an event with probability \\(P(N) \\times P(N) = 0.81\\). Rember the complementarity rule? This says that the second restructured bond will thus have a default probability of \\(19\\) % instead of \\(10%\\) it would be speculative grade or close to toxic junk now.\nHere is picture how you can visualize this piece fo financial magic. This picture can be read in exactly the same way as the previous picture. There is only one additional element. We have written the payoff of the original bonds by \\(B\\). Underneath these original bonds we draw a black horizontal like, think of it as the financial engineering lab that does the restructuring and below we get new bonds, with different payout promises, which we denote by \\(R\\) (for restructured).\n\n\n\n\n\n\n\n\nFigure 3.3: Event tree for two bonds with independent default risk with pooling and tranching\n\n\n\n\n\nWhat is done here is that the aggregate payoffs of both bonds are collected in a pool and new securities - called R - in this picture are issued against the pool. One, the upper one is now an investment grade asset paying 1 in every state except one and the other is a toxic junk bond paying always 0 except in one state. Note that the investment grade status could be engineered under the assumption that the risks are independent.\n\n\n3.1.6 Pooling and Tranching without Independent Risks\nNow, let us consider a hypothetical question: How would the event tree change if the independence assumption does not hold? Dependence would alter the probabilities in a way that reflects the increased likelihood of joint defaults during systemic events.\nSuppose we now assume that the probability of Bond 2 defaulting changes rather than staying unchanged under the condition that Bond 1 has defaulted:\n\nIf Bond 1 does not default, Bond 2 defaults with probability 0.1 (as before).\nIf Bond 1 does default, Bond 2 defaults with a higher probability of 0.6 due to systemic dependence.\n\nTo express this formally we need a piece of new notation. The convention in probability theory is that the notation is - for example - \\(P(N | D)\\). This would read as * bond 2 does not default given bond 1 has defaulted *. The event we are interested in is written first separated by a \\(|\\) from the conditioning event, which is in our case the outcome that bond 1 defaults.\nThis dependence reflects a scenario where defaults are more likely to occur together, such as during an economic downturn. The resulting event tree can be visualized as follows:\n\n\n\n\n\n\n\n\nFigure 3.4: Event tree for two bonds with independent default risk with pooling and tranching with dependent default risk\n\n\n\n\n\nOn the first sight this looks almost identical to the picture before. Only the numbers on the edges of the second tree have changed. These changed numbers reflect the idea that the event that one bond has defaulted changes the probability of the second bond defaulting as well. How could such a dependence occur?\nHere’s a real-world example to illustrate how dependence can occur: In the context of bonds, dependence in default probabilities can arise from shared exposure to systemic risks or interconnected factors. For instance, consider two companies that issue bonds and operate in the same industry, such as the energy sector. If oil prices plummet due to an economic downturn or geopolitical instability, both companies might experience financial stress, making it more likely that one default is followed by another.\nAnother example is during a financial crisis, such as the 2008 global financial meltdown. A bank’s default on its obligations can lead to cascading defaults in other institutions due to counterparty risks or a general loss of confidence in the financial system. In such cases, the probability of a second default is no longer independent of the first because the events are tied to the same underlying macroeconomic factors.\nThese examples highlight that the assumption of independence between bond defaults might hold under normal market conditions but breaks down during systemic crises. Such dependencies must be carefully modeled to avoid underestimating risk, as was the case in structured finance products leading up to the 2008 crisis.\nA prudent risk manager must keep such a scenario in mind when he analyzes a portfolio. Think about it in the context of the toy example. In the first case the default risk of the first asset created by pooling and tranching was \\(P(D) \\times P(D) = 0.1 \\times 0.1 = 0.01\\). Under a scenario with dependent risks this changes to \\(P(D) \\times P(D | D) = 0.1 \\times 0.6 = 0.06\\), a risk larger by a factor of 6! While in the first case the first restructured bond would be rated as investment grade, in the second case the same restructured bond would be rated as speculative grade and the magic from pooling and tranching suddenly disappears. Junk plus junk remains junk after all.\nFor pooling and tranching to reduce overall risk and create safe tranches:\n\nDiversification: Assets must come from independent sectors with minimal systemic risk.\nStable Macroeconomic Conditions: Systemic risks must be low to maintain independence assumptions.\nTransparent Modeling: Dependence structures must be explicitly modeled and accounted for in risk assessments.\n\nThe neglect of these conditions led to a flawed sense of security in structured finance, which contributed to the 2008 financial crisis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#conditional-probability",
    "href": "03-lecture3.html#conditional-probability",
    "title": "3  Conditional Probability",
    "section": "3.2 Conditional Probability",
    "text": "3.2 Conditional Probability\nConditional probability provides us with a concept to formalize how the probability of one event changes when another event is known to occur, providing a framework for understanding dependencies quantitatively.\nHere is the mathematical definition:\n\n\n\n\n\n\nDefinition: Conditional probability\n\n\n\nLet \\(B\\) be an event with positive probability. For an arbitrary event \\(A\\) we define the conditional probability of \\(A\\) given \\(B\\) as \\[\\begin{equation*}\nP(A\\,|\\,B) = \\frac{P(A \\cap B)}{P(B)}\\,\\,\\, \\text{provided}\\,\\,\\, P(B) \\neq 0\n\\end{equation*}\\]\n\n\nNote that conditional probabilities remain undefined when the conditioning event \\(B\\) has probability 0.4\n4 As pointed out in Feller (1968) p 115, this has no consequences in the case of discrete sample spaces but is important in the general theory.Let us clarify a few things about this concept. As in the example of the financial crisis, which we discussed before we really did not much more than introducing one piece of new notation to indicate that the probabilities now have changed.\n\n3.2.1 An illustration using old and new R concepts\nThis is an excellent moment in this lecture to make use of R to illustrate the concept and play with it. On the way we introduce some not yet covered R concepts.\nLet us stay with the example of defaultable bonds and use the freedom and the opprotunity of simulating probabilistic examples on the computer.\nLet us make use of the sample() function first to create a portfolio of bonds. The relevant data for this portfolio should be recorded in a dataframe.\nTo make the data reproducible we need to specify a random seed. Just like in Python, the set.seed() function in R ensures reproducibility of random numbers. When generating random data (like our simulated bond portfolio), R uses a pseudo-random number generator. By setting a “seed” value, you tell R to start its random number generator from a specific point. This guarantees that every time you run the code, you’ll get the same random results, which is crucial for debugging, sharing code, or teaching concepts.\nThe set.seed()functions requires and argument. We can for instance use a sequence of numbers, which can be arbitrary like 123or 42or 2025. What is important is that using the same seed will always give you the same numbers and thus make your example reproducible.\nHere is an example for illustration. Let’s go back to our old coin tossing example and let us toss our coin 10 times one time using the set.seed()function and one time not.\n\n# tossing a fair coin 10 times, two different runs without set.seed()\n\nexample_without_1 &lt;- sample(c(\"H\",\"T\"), size =10, replace = T)\nexample_without_2 &lt;- sample(c(\"H\",\"T\"), size =10, replace = T)\n\ncat(\"Without set.seed():\\n \")\n\nWithout set.seed():\n \n\nprint(example_without_1)\n\n [1] \"H\" \"T\" \"T\" \"H\" \"H\" \"T\" \"H\" \"H\" \"H\" \"T\"\n\nprint(example_without_2)\n\n [1] \"T\" \"T\" \"H\" \"T\" \"H\" \"H\" \"T\" \"H\" \"T\" \"T\"\n\n# tossing a fair coin 5 times, two different rund with set.seed()\n\nset.seed(123)\nexample_with_1 &lt;- sample(c(\"H\",\"T\"), size =10, replace = T)\nset.seed(123)\nexample_with_2 &lt;- sample(c(\"H\",\"T\"), size =10, replace = T)\n\ncat(\"With set.seed():\\n\")\n\nWith set.seed():\n\nprint(example_with_1)\n\n [1] \"H\" \"H\" \"H\" \"T\" \"H\" \"T\" \"T\" \"T\" \"H\" \"H\"\n\nprint(example_with_2)\n\n [1] \"H\" \"H\" \"H\" \"T\" \"H\" \"T\" \"T\" \"T\" \"H\" \"H\"\n\n\nNow let us go to our bind portfolio simulation. Let us simulate our data first:\n\n# set a random seed for reproduceability\nset.seed(123)\n\n# Number of bonds\n\nN &lt;- 5000 \n\n# Simulate portfolio data\n\nportfolio &lt;- data.frame(\n  BondID = 1:N,\n  CreditRating = sample(c(\"AAA\", \"BBB\", \"Junk\"), N, replace = TRUE, prob = c(0.5, 0.3, 0.2)),\n  Sector = sample(c(\"Finance\", \"Energy\", \"Real Estate\"), N, replace = TRUE),\n  Defaulted = NA  # Initialize with NA for later assignment\n)\n\nLet me unpack this a bit:\nFirst we create a dataframe using the data.frame()function of R.\nIt is used to create a structured dataset in R, similar to a table in Excel or a DataFrame in Python’s pandas library. Here, we are simulating a dataset called portfolio, which represents a collection of financial bonds. Each row corresponds to a bond, and each column represents an attribute (or variable) of that bond.\nIn the first column of our dataframe we store unique bond IDs by just assigning them a sequence of integers starting at 1 and counting to \\(N\\), the total number of bonds in the portfolio. Remember the colon operator : we have already used before to create such sequences.\nIn the next column of our dataframe we assign a credit rating randomly using a probability of 0.5 that the rating will be “AAA”, 0.3 that it will be “BBB” and 0.2 that it will be “Junk”. You know already how to do this using the sample function. Note that we have set replace = TRUE: This allows sampling with replacement, so the same credit rating can appear multiple times\nFinally we imagine that the bonds are issued by different sectors, Finance, Energy and Real Estate in the economy which we also assign randomly with equal probability for each sector using the sample function.\nThe final column creates a vector of unknown values, because we would like to assign these values in a separate step to be consistent with actual rating probabilities.\nThe way we do this is to assign the value NA to the variable Defaultedfor the moment. What does this variable mean in R?\nThe NA character is a special symbol in R. It stands for “not available” and can be used as a placeholder for missing information. R will treat NA exactly as you should want missing information treated. For example, what result would you expect if you add 1 to a piece of missing information?\n\n1 + NA\n\n[1] NA\n\n\nR will return a second piece of missing information. It would not be correct to say that 1 + NA = 1 because there is a good chance that the missing quantity is not zero. You do not have enough information to determine the result.\nThere are two functions which are very useful to know about, when working with data that contain NA which will be the case in almost all practical circumstances.\nWhile NA is useful for indicating missing information it can be annoying in practical data work. Assume you had a vector of 100 numbers and only one value, say at the beginning is NA.\n\nc(NA, 1:100)\n\n  [1]  NA   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n [19]  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n [37]  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n [55]  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n [73]  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n [91]  90  91  92  93  94  95  96  97  98  99 100\n\nmean(c(NA,1:100))\n\n[1] NA\n\n\nThis is clearly annoying and we would like to enforce a different behavior. Most R functions, such as mean() and others come with the option na.rm that controls R’s behavior when data contain missing information. Here is how it works in the case of mean:\n\nmean(c(NA,1:100), na.rm = TRUE)\n\n[1] 50.5\n\n\nA second useful function when working with missing data is is.na(). This is the case because the identity operator == used in logical subsetting does not work with NA.\n\nc(1,2,3,NA) == NA\n\n[1] NA NA NA NA\n\nis.na(c(1,2,3,NA))\n\n[1] FALSE FALSE FALSE  TRUE\n\n\nSo while the first approach will just yield NA, the second will identify which value in our vector is missing.\nNow putting it all together this will give us a dataframe where the data.frame() function combines these columns into a structured dataset, where: - BondID is an integer sequence labeling each bond. - CreditRating is a randomly assigned credit rating, weighted by the specified probabilities. - Sector is a randomly assigned economic sector. - Defaulted a vector with missing values yet to be determined.\nYou can inspect the first rows of this dataframe by\n\nhead(portfolio)\n\n  BondID CreditRating      Sector Defaulted\n1      1          AAA Real Estate        NA\n2      2          BBB      Energy        NA\n3      3          AAA      Energy        NA\n4      4         Junk      Energy        NA\n5      5         Junk     Finance        NA\n6      6          AAA Real Estate        NA\n\n\nNow we assign different default probabilities for the different credit ratings such that the simulated data show similar values than with actual ratings.\n\ndefault_probabilities &lt;- data.frame(\n  CreditRating = c(\"AAA\", \"BBB\", \"Junk\"),\n  DefaultProb = c(0.0001, 0.002, 0.05)  # Default probabilities\n)\n\n\ndefault_probabilities &lt;- data.frame(\n  CreditRating = c(\"AAA\", \"BBB\", \"Junk\"),\n  DefaultProb = c(0.0001, 0.002, 0.05)  # Default probabilities\n)\n\nWe do this in a separate rating dataframe, organizing compactly our rating information. To see what is happening here observe that a data.frame is created to store default probabilities for each credit rating. CreditRating is a character column containing the categories: \"AAA\", \"BBB\", and \"Junk\". DefaultProb is a numeric column with the corresponding default probabilities: AAA: \\(0.01\\%\\), BBB: \\(0.2\\%\\), Junk: \\(5\\%\\) This dataframe organizes the default probabilities in a clear and structured way, making the probabilities easy to reference if needed later.\nNow we can use the power of R’s subsetting rules to fill the last column in our portfolio dataframe such that we get realistic values. Let me show you the code and then unpack the elements step by step:\n\nportfolio$Defaulted[portfolio$CreditRating == \"AAA\"] &lt;- \n  sample(c(TRUE, FALSE), sum(portfolio$CreditRating == \"AAA\"), replace = TRUE, \n         prob = c(0.0001, 0.9999))\n\nportfolio$Defaulted[portfolio$CreditRating == \"BBB\"] &lt;- \n  sample(c(TRUE, FALSE), sum(portfolio$CreditRating == \"BBB\"), replace = TRUE, \n         prob = c(0.002, 0.998))\n\nportfolio$Defaulted[portfolio$CreditRating == \"Junk\"] &lt;- \n  sample(c(TRUE, FALSE), sum(portfolio$CreditRating == \"Junk\"), replace = TRUE, \n         prob = c(0.05, 0.95))\n\nHere’s a step-by-step explanation of the code, unpacking the logic and reinforcing R subsetting rules: Let’s look at the AAA bonds first.\nHere we see the code line portfolio$CreditRating == \"AAA\". This line checks each row in the CreditRating column to see if the value is \"AAA\". It returns a logical vector (e.g., TRUE for \"AAA\" rows, FALSE otherwise). Here you see two important R concepts at work. To check whether two expressions are equal the appropriate symbol is ==. It returns a logical TRUE when the equality holds and FALSEotherwise.\nWe use this logical vector created by the assignment to assign a probability to the Defaultedcolumn whenever the rating is AAA. So the operation selects the Defaulted column for rows where CreditRating is \"AAA\" and default status to these rows based on a random sample.\nNow for the random sample we use the sample() function we encountered before. It generates random values TRUEfor default and FALSEfor non-default, the two possible outcomes. It samples with replacement as many times as we have an AAArating. This count is achieved by sum(portfolio$CreditRating == \"AAA\") using R’s coercion rules. It returns the count because each TRUE is forced to 1 and contributes one count to the total whereas FALSEis forced to 0 and contributes nothing to the count. Finally prob = c(0.0001, 0.9999) specifies the probabilities for for this class. Note that here we could also have used the complement rule by writing prob = c(0.0001, 1 - 0.0001)\nNow we do the same thing for BBB bonds and the Junkbonds, which have each different probabilities from the AAAbinds and from each other.\nHere you have in one example the General Rules for R Subsetting reviewed\n\nLogical Conditions:\n\nUse conditions like == to create logical vectors.\nExample: portfolio$CreditRating == \"AAA\" checks for equality and returns a logical vector.\n\nRow Selection:\n\nLogical vectors are used to select rows in a data frame.\nExample: portfolio$Defaulted[...] updates only the rows where the condition is TRUE.\n\nColumn Access:\n\nUse $ to access specific columns in a data frame.\nExample: portfolio$Defaulted.\n\nCombining Logical Subsetting and Assignment:\n\nSubset rows using logical conditions, then assign values to those rows.\nExample: portfolio$Defaulted[portfolio$CreditRating == \"AAA\"] &lt;- sample(...)\n\n\n\n\n\n\n\n\nNow you try\n\n\n\nHere are some more assignment exercises for you to try:\n\nExplore Subsetting:\n\nPrint all AAA bonds using:\nportfolio[portfolio$CreditRating == \"AAA\", ]\nVerify the number of rows using:\nsum(portfolio$CreditRating == \"AAA\")\n\nCheck Assigned Values:\nConfirm that Defaulted is only updated for the relevant rows:\ntable(portfolio$Defaulted[portfolio$CreditRating == \"AAA\"])\n\n\n\nLet’s check how our portfoliodataframe looks now after the assignment.\n\nhead(portfolio)\n\n  BondID CreditRating      Sector Defaulted\n1      1          AAA Real Estate     FALSE\n2      2          BBB      Energy     FALSE\n3      3          AAA      Energy     FALSE\n4      4         Junk      Energy     FALSE\n5      5         Junk     Finance     FALSE\n6      6          AAA Real Estate     FALSE\n\n\nNow let us go on to the illustration of the conditional probability concept. If we choose a bond at random from this portfolio we should get a probability of\n\\(P(D) = \\frac{\\text{number of defaultet bonds}}{\\text{total number of bonds}}\\) and \\(P(N) = \\frac{\\text{number of non-defaultet bonds}}{\\text{total number of bonds}}\\)\nLet’s check these numbers in our sample, using the R subsetting rules and a new operator, the $ sign, which plays an important role in R data manipulation. Let me do the calculation and then explain:\n\nPD &lt;- mean(portfolio$Defaulted == TRUE)\nPN &lt;- 1 - PD\n\nPD\n\n[1] 0.0108\n\nPN\n\n[1] 0.9892\n\n\nLet me explain this bit of code. The name of our dataframe is portfolio. We may refer to this as an R-object in the R language. An operator that is frequently used in R to select a column from a dataframe is the colon operator $. If we tell R portfolio$Defaulted, R will select the column Defaultedfrom the dataframe portfolio. The same operator is used to select elements in an R list. This is no coincidence, since a dataframe is formally a list in the R language. You can see this by\n\ntypeof(portfolio)\n\n[1] \"list\"\n\nclass(portfolio)\n\n[1] \"data.frame\"\n\n\nIf you look at the type of a data frame, you will see that it is a list. In fact, each data frame is a list with class data.frame. You can see what types of objects are grouped together by a list (or data frame) with the str() function:\n\nstr(portfolio)\n\n'data.frame':   5000 obs. of  4 variables:\n $ BondID      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ CreditRating: chr  \"AAA\" \"BBB\" \"AAA\" \"Junk\" ...\n $ Sector      : chr  \"Real Estate\" \"Energy\" \"Energy\" \"Energy\" ...\n $ Defaulted   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n\n\nNow the numbers of the unconditional probabilities of a bind defaulting or not defaulting (note that we applied the complement rule here to compute this probability) is as it should be given the parametrisation of our example.\nWe now want to restrict our attention to the subset of our portfolio consisting of junk bonds. What is the probability that a bond chosen from this sub-population is in default?\n\n# Calculate conditional probability: P(Default | CreditRating = \"Junk\")\n\njunk_bonds &lt;- portfolio[portfolio$CreditRating == \"Junk\", ]\n\nP_Default_given_Junk &lt;- mean(junk_bonds$Defaulted)\n\ncat(\"P(Default | CreditRating = 'Junk'):\", P_Default_given_Junk, \"\\n\")\n\nP(Default | CreditRating = 'Junk'): 0.05353535 \n\n\nNow you see perhaps with your own hands how conditional probability works in terms of R’s subsetting rules. Among the subset of bonds which are classified as junk, how many are also defaulted and what is their relative frequency among the junk bonds? This would be in set notation: \\[\\begin{equation*}\n\\frac{P(\\text{set of all defaulted bonds} \\cap \\text{set of all junk bonds})}{P(\\text{set of all junk bonds})}\n\\end{equation*}\\] To provide an appropriate notation for this we have used: \\(P(\\text{defaulted} \\, | \\,\\text{junk})\\) This is read as the probability of event \\(A\\) (the bond is defaulted) assuming the event \\(B\\) (the bond is a junk bond).\nThus taking a conditional probability of various events with respect to a particular event \\(B\\) amounts to choosing \\(B\\) as a new sample sapce with the probabilities proportional to the original ones. The proportionality factor \\(P(B)\\) is necessary in order to make the probabilities in the new sample space sum up to 1. All general theorems on probabilities are valid also for conditional probabilities with respect to any particular event \\(B\\).\nNote that for conditional probabilities we have for two events \\(A\\) and \\(B\\), that \\(P(A|B) \\neq P(B|A)\\). To see this assume that \\(P(A) \\neq P(B)\\) and \\(P(A) \\neq 0\\) and \\(P(B)\\neq 0\\).\nWe then get: \\[\\begin{equation*}\nP(A|B) = \\frac{P(A\\cap B)}{P(B)} = \\frac{P(B \\cap A)}{P(B)}\n\\end{equation*}\\] since \\(P(A \\cap B) = P(B \\cap A)\\). It follows that \\[\\begin{equation*}\n\\frac{P(B \\cap A)}{P(B)} \\neq \\frac{P(B \\cap A)}{P(A)} = P(B|A)\n\\end{equation*}\\] since we have assumed that \\(P(A) \\neq P(B)\\). Therefore \\(P(A|B) \\neq P(B|A)\\).\nLet’s illustrate this remark with the example of our bond portfolio.\n\n# Calculate conditional probability: P(CreditRating = \"Junk\" | Default)\n\ndefaulted_bonds &lt;- portfolio[portfolio$Defaulted == TRUE, ]\n\nP_Junk_given_Default &lt;- mean(defaulted_bonds$CreditRating == \"Junk\")\n\ncat(\"P(Junk | Default):\", P_Junk_given_Default, \"\\n\")\n\nP(Junk | Default): 0.9814815 \n\n\nNow you can see clearly that \\(P(\\text{Default} \\, |\\, \\text{Junk} ) =\\) 0.0535354 which is clearly different from \\(P(\\text{Junk} \\, |\\, \\text{Default} ) =\\) 0.9814815.\nThe formula for conditional probability which we wrote down in the definition of Conditional Probability is often used in the form of the multiplication rule:\n\n\n\n\n\n\nDefinition: Multiplication rule\n\n\n\nGiven events \\(A\\) and \\(B\\), it holds that: \\(P(A \\cap B) = P(A \\mid B)\\times P(B)\\)\n\n\nwhich is just an equivalent way to write the formula for conditional probability. The multiplication rule can be thought of the AND rule of probability theory.\nWith the multiplication rule we can gain a deeper insight into the meaning of independence. Remember that two events \\(A\\) and \\(B\\) are independent if \\[\\begin{equation*}\nP(A \\cap B) = P(B \\cap A) = P(A) \\times P(B).\n\\end{equation*}\\]\nIf we combine this rule with the concept of conditional probability, we see that if two events \\(A\\) and \\(B\\) are independent, then \\[\\begin{equation*}\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A) \\times P(B)}{P(B)} = P(A)\n\\end{equation*}\\] and \\[\\begin{equation*}\nP(B|A) = \\frac{P(B \\cap A)}{P(A)} = \\frac{P(A) \\times P(B)}{P(A)} = P(B)\n\\end{equation*}\\]\nThis formula says that if two events are independent the probability of \\(A\\) is not influenced by the event \\(B\\) occurring and the probability of event \\(B\\) is not influenced by the event \\(A\\) occurring.\nHere’s a proposal for a draft subsection devoted to introducing advanced R concepts (Environments, Scoping Rules, Closures) while reinforcing conditional probability ideas. The examples are tied to a financial and conditional probability context, ensuring continuity with your lecture themes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#conditional-probability-1",
    "href": "03-lecture3.html#conditional-probability-1",
    "title": "3  Conditional Probability",
    "section": "3.3 Conditional probability",
    "text": "3.3 Conditional probability\nLet \\(B\\) be an event with positive probability. For an arbitrary event \\(A\\) we define the conditional probability of \\(A\\) given \\(B\\) as \\[\\begin{equation*}\nP(A\\,|\\,B) = \\frac{P(A \\cap B)}{P(B)}\\,\\,\\, \\text{provided}\\,\\,\\, P(B) \\neq 0\n\\end{equation*}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#advanced-r-concepts-environments-scoping-rules-and-closures",
    "href": "03-lecture3.html#advanced-r-concepts-environments-scoping-rules-and-closures",
    "title": "3  Conditional Probability",
    "section": "3.3 Advanced R Concepts: Environments, Scoping Rules, and Closures",
    "text": "3.3 Advanced R Concepts: Environments, Scoping Rules, and Closures\nIn this section, we will explore some advanced R programming concepts that are essential for understanding how R evaluates and stores variables, as well as how you can create reusable and dynamic functions. We will demonstrate these concepts through examples related to conditional probability and financial modeling.\n\n3.3.1 Introduction to Environments\nAn environment in R is where objects (variables, functions, etc.) are stored and looked up. R uses environments to determine where a variable exists and what its value is. The most common environment is the global environment, where user-created variables and functions are stored.\nExample: Setting Global and Local Variables\nSuppose we are modeling interest rates in a financial portfolio. Globally, we set the baseline interest rate. Locally, we may override this rate for specific calculations.\n\n# Global interest rate\ninterest_rate &lt;- 0.05  # 5%\n\n# Function to calculate interest payments\ncalculate_interest &lt;- function(principal, rate = interest_rate) {\n  interest &lt;- principal * rate  # Uses the rate passed to the function\n  return(interest)\n}\n\n# Global calculation\nglobal_interest &lt;- calculate_interest(1000)  # Uses global interest_rate\ncat(\"Global Interest:\", global_interest, \"\\n\")\n\nGlobal Interest: 50 \n\n# Local override\nlocal_interest &lt;- calculate_interest(1000, rate = 0.07)  # Overrides global interest_rate\ncat(\"Local Interest:\", local_interest, \"\\n\")\n\nLocal Interest: 70 \n\n\nGlobal variables are available everywhere, but local variables (like rate) take precedence within a function. Understanding this behavior is crucial for writing clear and predictable code.\n\n\n3.3.2 Scoping Rules\nR follows specific scoping rules to determine where and how to find variables. These rules become important when working with nested functions.\nExample: Variable Lookup in Nested Functions\nLet’s calculate conditional probabilities using nested functions. We simulate a financial scenario where we compute probabilities of default for different credit ratings.\n\n# Define global default rates for credit ratings\n\ndefault_rates &lt;- c(\n  AAA = 0.01,  # Global default rate for AAA bonds\n  BBB = 0.02,  # Global default rate for BBB bonds\n  Junk = 0.05  # Global default rate for Junk bonds\n)\n\n# Function to calculate conditional default probability\nconditional_default &lt;- function(rating) {\n  \n  # Lookup table for default rates\n  local_default_rates &lt;- c(\n    AAA = default_rates[\"AAA\"],  # Local default for AAA\n    BBB = default_rates[\"BBB\"],  # Local default for BBB\n    Junk = default_rates[\"Junk\"] # Local default for Junk\n  )\n  \n  # Return the default rate using vectorized subsetting\n  \n  return(local_default_rates[rating])\n}\n\n# Test the function\ncat(\"Default rate for Junk bonds:\", conditional_default(\"Junk\"), \"\\n\")\n\nDefault rate for Junk bonds: NA \n\ncat(\"Default rate for BBB bonds:\", conditional_default(\"BBB\"), \"\\n\")\n\nDefault rate for BBB bonds: NA \n\ncat(\"Default rate for AAA bonds:\", conditional_default(\"AAA\"), \"\\n\")\n\nDefault rate for AAA bonds: NA \n\n\nHer you see how R is using Lexical scoping. This ensures that R looks for variables in the closest environment first, then moves outward (from local to global). Nested functions can use both local and global variables.\nThis example uses a concept you might find useful in many other contexts: The lookup table. The concept of a lookup table is a simple yet powerful way to map input values to corresponding outputs. In R, we can create a lookup table using a named vector, where each element has a name (the input) and a value (the corresponding output). This allows us to retrieve the correct value by directly referencing the name.\nIn the example, we used a named vector local_default_rates to store the default probabilities for different credit ratings: “AAA”, “BBB”, and “Junk”. Each credit rating serves as a key, and the corresponding default probability serves as the value. When we pass the rating (e.g., “Junk”) to the function, R uses it to subset the vector and directly return the associated probability. This approach is efficient and avoids the need for verbose or complex conditional statements.\nBy using a lookup table, we also demonstrate an important principle of programming: separation of data and logic. The mapping of ratings to probabilities is encapsulated in a single data structure (local_default_rates), making the function simpler and easier to modify. For instance, if the default probabilities change, you only need to update the values in the vector—no changes to the function logic are required. This approach is especially useful in financial modeling, where mappings like these are common and can evolve over time.\n\n\n3.3.3 Closures\nA closure is a function that remembers the environment in which it was created. Closures are powerful for creating dynamic, reusable functions, such as calculators for different conditional probabilities.\nExample: Probability Calculator Factory\nThis code demonstrates a powerful concept in R: closures. A closure is a function that “remembers” the environment in which it was created, allowing you to dynamically generate new functions with specific behaviors. Let’s create a function factory that generates specific probability calculators based on a given event.\n\n# Function factory for conditional probability calculators\n\nprobability_calculator_factory &lt;- function(event_probability) {\n  function(conditional_probability) {\n    joint_probability &lt;- event_probability * conditional_probability\n    return(joint_probability)\n  }\n}\n\n# Create calculators for different events\n\njunk_calculator &lt;- probability_calculator_factory(0.05)  # Junk bonds\nbbb_calculator &lt;- probability_calculator_factory(0.02)   # BBB bonds\n\n# Calculate joint probabilities\n\njunk_joint &lt;- junk_calculator(0.1)  # P(Default | Junk) * P(Junk)\nbbb_joint &lt;- bbb_calculator(0.2)    # P(Default | BBB) * P(BBB)\n\ncat(\"Joint probability for Junk bonds:\", junk_joint, \"\\n\")\n\nJoint probability for Junk bonds: 0.005 \n\ncat(\"Joint probability for BBB bonds:\", bbb_joint, \"\\n\")\n\nJoint probability for BBB bonds: 0.004 \n\n\nLet’s unpack the code step by step:\nThe probability_calculator_factory is a function factory. It takes one argument, event_probability, and returns a new function that calculates the joint probability for a given conditional probability:\n\nInput:\n\nevent_probability: The probability of the event (e.g., the probability of a bond being “Junk”).\nThe returned function takes conditional_probability as its argument (e.g., the probability of default given the bond is “Junk”).\n\nOutput:\n\nThe joint probability, \\(P(A \\cap B) = P(A | B) \\times P(B)\\).\n\n\nThis structure encapsulates the logic for joint probability into a reusable framework.\nThe junk_calculator and bbb_calculator are functions created by the factory. Each calculator “remembers” the event_probability it was initialized with: - junk_calculator: \\(P(Junk) = 0.05\\). - bbb_calculator: \\(P(BBB) = 0.02\\).\nThese calculators are then used to compute joint probabilities by providing the corresponding conditional probabilities: - junk_joint &lt;- junk_calculator(0.1): - \\(P(\\text{Default} \\cap \\text{Junk}) = P(\\text{Default | Junk}) \\times P(\\text{Junk})\\). - \\(0.1 \\times 0.05 = 0.005\\) (0.5%). - bbb_joint &lt;- bbb_calculator(0.2): - \\(P(\\text{Default} \\cap \\text{BBB}) = P(\\text{Default | BBB}) \\times P(\\text{BBB})\\). - \\(0.2 \\times 0.02 = 0.004\\) (0.4%).\nThe cat() function displays the results:\nA closure allows you to “lock in” parameters (like event_probability) when the function is created, while still allowing flexibility for additional inputs.\n\n\n\n\n\n\nNow you try\n\n\n\n\nModify the calculate_interest function to add a penalty rate for overdue payments using local variables.\nExtend the conditional_default function to include an additional credit rating (e.g., “CC”).\nUse the probability_calculator_factory to compute joint probabilities for a new event, such as “Real Estate Sector Default.”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#updating-beliefs-bayes-rule",
    "href": "03-lecture3.html#updating-beliefs-bayes-rule",
    "title": "3  Conditional Probability",
    "section": "3.4 Updating beliefs: Bayes’ rule",
    "text": "3.4 Updating beliefs: Bayes’ rule\n\n\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nTooze, Adam. 2018. Crashed. How a Decade of Financial Crisis Changed the World. Viking.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#bayes-rule-one-of-the-great-ideas-in-probability",
    "href": "03-lecture3.html#bayes-rule-one-of-the-great-ideas-in-probability",
    "title": "3  Conditional Probability",
    "section": "3.4 Bayes’ Rule: One of the Great Ideas in Probability",
    "text": "3.4 Bayes’ Rule: One of the Great Ideas in Probability\nBayes’ Rule stands among the ten great ideas in probability. Its power lies in solving a fundamental problem: how to infer underlying chances from observed frequencies. This insight filled a critical gap left by Bernoulli’s weak law of large numbers, which explained how observed frequencies converge to probabilities but left unanswered the question of how to reason from those frequencies back to the chances that generated them.\nAt its core, Bayes’ Rule provides a framework for updating our beliefs in light of new information. But what does it mean to assign a probability to a belief? This idea rests on another deep insight in probability theory: judgments about uncertainty can be measured, and when those judgments are coherent, they follow the rules of probability. 5\n5 This idea goes back to the work of Frank Ramsey (1903–1930). He was a British mathematician, philosopher, and economist whose profound contributions spanned multiple fields despite his tragically short life. In probability, he established the foundation for subjective probability theory, showing that coherent judgments about uncertainty adhere to the axioms of probability. Ramsey also made groundbreaking advances in decision theory, logic, and economics, including the famous Ramsey pricing in public economics and his foundational work in mathematical logic. For more, on Ramsey’s ideas of connecting judegment to probability see chapter 2 in Diaconis and Skyrms (2019)The key breakthrough of Bayes’ Rule is that it ties these coherent judgments—our initial beliefs, or priors—to evidence, using conditional probability. This process transforms subjective judgments into a systematic method for reasoning under uncertainty, with profound applications across science, finance, and everyday decision-making.\n\n3.4.1 A simple example: Revising beliefs about market valuation\nWe discussed the big financial crisis of 2007-2008 earlier in this lecture. Imagine now you are an investor in the year 2011, just a bit later. The market is gripped by fear of another financial crisis, this time triggered by tensions in the Eurozone.\nLet’s imagine how you could have asessed the market situation by looking at financial data. This is also an excellent opportunity to introdcue you to one of many great opportunities to load real world data directly into R using add on packages. The package we are going to use here is called tidyquant. To be able to use it you must first install it using the install.packages()function of base R or the package installer pane in RStudio. Let us do that - assuming that this package has been installed. You might remember how to load a package from before.\n\nlibrary(tidyquant)\n\nNow tidyquant has many functions allowing you to retrieve and transform real world financial data. We do not go into any detail here. This is something you can do yourself using the excellent documentation of this package, the many examples on the web or by interrogating your LLM.6. One of the core workhorse functions in tidyquant is the function tq_get() which allows you to retrieve data.\n6 Check out https://cran.r-project.org/web/packages/tidyquant/index.htmlOne thing an anlyst might be interested in is how the stock market as measured by a broad index does at the moment compared to its historical values. Let’s say you are looking back 11 years from 2011 into the past and see how the S&P500 does now compared to this history. This is how you would use tidyquant to do this:\n\n# Fetch S&P 500 data from 2000 to 2011\n\nsp500_data &lt;- tq_get(\"SPY\", from = \"2000-01-01\", to = \"2011-12-31\")\n\nIn order to do load data you need the string of the name of the series you are interested in, which you can learn from the documentation. In our case this is “SPY” for the SP500. You can also specify a time range or just the beginning of the series. In the latter case it will give you all the data from the beginning up to the current or most recent trading day. We have written the data into an R object called sp500_data and now you might inspect it.\n\nhead(sp500_data)\n\n# A tibble: 6 × 8\n  symbol date        open  high   low close   volume adjusted\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 SPY    2000-01-03  148.  148.  144.  145.  8164300     92.7\n2 SPY    2000-01-04  144.  144.  140.  140.  8089800     89.1\n3 SPY    2000-01-05  140.  142.  137.  140  12177900     89.2\n4 SPY    2000-01-06  140.  142.  138.  138.  6227200     87.8\n5 SPY    2000-01-07  140.  146.  140.  146.  8066500     92.9\n6 SPY    2000-01-10  146.  147.  145.  146.  5741700     93.2\n\n\nThe otput says that the data object is a tibble(). Don’t worry about this detail at the moment and think of a tibble()as something equivalent to a dataframe.\nNow let us visualize the data using base R’s plot() function. Use the help facilities or your LLM to find out abut the syntax details.\n\n# Base R plot\nplot(\n  sp500_data$date, sp500_data$adjusted,\n  type = \"l\",                # Line plot\n  col = \"blue\",              # Line color\n  lwd = 2,                   # Line width\n  xlab = \"Date\",             # X-axis label\n  ylab = \"Adjusted Price\",   # Y-axis label\n  main = \"S&P 500 Price Trends (2000–2011)\" # Title\n)\n\n# Add grid lines for better readability\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nWe see that the index is about the same level as it was 10 years ago. The index alone does not tell you very much. How about relating this to some relevant market fundamentals?\nOne powerful feature of tidyquant() is that it can fetch data from various data sources. Here I get, for the sake of this example, data on corporate profits from the FRED database of the Fed St. Louis7\n7 See https://fred.stlouisfed.org/\n# Get corporate profits data from FRED\ncorporate_profits &lt;- tq_get(\"CP\", from = \"2000-01-01\", to = \"2011-12-31\", get = \"economic.data\")\n\n# Visualize corporate profits\nplot(\n  corporate_profits$date, corporate_profits$price,\n  type = \"l\",\n  col = \"blue\",\n  lwd = 2,\n  xlab = \"Year\",\n  ylab = \"Corporate Profits (Index)\",\n  main = \"Corporate Profits (2000–2011)\"\n)\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nProfits have roughly tripled over the same period. The market does seem to be undervalued. Let’s put this into perspective by looking at Price-Earning data or P/E ratios.\nNow we run into a problem which you will often encounter when working with data. It is rare that a tool covers all cases. P/E rations seem to be difficult to retrieve with tidyquant. These data do however exist on the web, for example at the website Macrotrends.8\n8 The P/E ratio for the SP500 can be found for instance here: https://www.macrotrends.net/2577/sp-500-pe-ratio-price-to-earnings-chartTo retrieve data from this site, I had to download the data locally first before I could get them into R. Let’s look at them a bit closer now using again R’s visualization tools.\n\n# Set the path to the downloaded CSV file\nfile_path &lt;- \"data/sp-500-pe-ratio-price-to-earnings-chart.csv\"\n\n# Read the CSV file into R\npe_data &lt;- read.csv(file_path, stringsAsFactors = FALSE, skip = 16,\n                    col.names = c(\"Date\", \"PE_ratio\"))\n\n# Convert the date column to Date class\npe_data$Date &lt;- as.Date(pe_data$Date, format = \"%Y-%m-%d\")\n\n# Filter data for the desired date range\npe_data_filtered &lt;- subset(pe_data, Date &gt;= as.Date(\"2000-01-01\") & \n                             Date &lt;= as.Date(\"2011-12-31\"))\n\n# Calculate the average P/E ratio over the specified period\naverage_pe &lt;- mean(pe_data_filtered$PE_ratio, na.rm = TRUE)\n\n# Plot the P/E ratio for the y-axis\nplot(\n  pe_data_filtered$Date, pe_data_filtered$PE_ratio,\n  type = \"l\",                   # Line plot\n  col = \"blue\",                 # Line color\n  lwd = 2,                      # Line width\n  xlab = \"Year\",                # X-axis label\n  ylab = \"P/E Ratio\", # Y-axis label\n  main = \"S&P 500 P/E Ratio (2000–2011)\", # Plot title\n)\n\n# Add a horizontal line for the average P/E ratio\nabline(h = average_pe, col = \"red\", lwd = 2, lty = 2)\n\n# Add a legend to the plot\nlegend(\n  \"topright\",                   # Legend position\n  legend = c(\"P/E Ratio\", \"Average P/E Ratio\"), # Labels\n  col = c(\"blue\", \"red\"),       # Line colors\n  lty = c(1, 2),                # Line types\n  lwd = c(2, 2)                 # Line widths\n)\n\n\n\n\n\n\n\n\nAt first glance, the market seems undervalued, with a trailing price-to-earnings (P/E) ratio of 14, well below the historical average of 29.\nAs a cautious investor, you form the hypothesis that these low valuations are deceptive. You believe that corporate profits, which are at record highs of 1672 compared to the historical average of 1119, will revert to the mean. When this happens, earnings will drop, pushing the P/E ratio from a seemingly cheap to expensive.\nWith this belief in mind, you decide to hold off on investing, waiting for valuations to normalize. However, as the years pass—2012, 2013, 2014, and beyond— corporate profits remain elevated, and the market continues to rally. Each year, your cautious stance leaves you with underwhelming returns.\n\n# Get corporate profits data from FRED\ncorporate_profits &lt;- tq_get(\"CP\", from = \"2000-01-01\", to = \"2019-12-31\", get = \"economic.data\")\n\n# Visualize corporate profits with a vertical line at 2011\nplot(\n  corporate_profits$date, corporate_profits$price,\n  type = \"l\",\n  col = \"blue\",\n  lwd = 2,\n  xlab = \"Year\",\n  ylab = \"Corporate Profits (Index)\",\n  main = \"Corporate Profits (2000–2019)\"\n)\n\n# Add a vertical line at 2011\nabline(v = as.Date(\"2011-01-01\"), col = \"red\", lwd = 2, lty = 2)\n\n# Add grid lines for readability\ngrid(nx = NULL, ny = NULL, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nBy 2019, you’re forced to confront the possibility that your belief about corporate profit mean-reversion might be wrong.\n\n3.4.1.1 The Need for Updating Probability Asessments\nThis scenario highlights the importance of updating beliefs in the face of new evidence. Initially, your hypothesis about profit margins reverting to the mean was reasonable, based on historical data. But as year after year passed without mean-reversion, the accumulating evidence should have prompted you to revise your prior beliefs.\nBayes’ Rule offers a principled way to do this. It allows you to combine your initial belief (the prior probability) with new evidence (e.g., sustained elevated profit margins) to calculate an updated belief (the posterior probability). This process ensures that your decisions adapt as reality unfolds, helping you avoid the dangers of clinging to outdated assumptions.\n\n\n\n3.4.2 Bayes’ Rule: Intuitive Understanding with a Speck of Sand\nBayes’ rule, one of the cornerstone ideas of probability, provides a systematic method for updating probabilities based on new evidence. It is formalized as: \\[\\begin{equation*}\nP(B|A) = \\frac{P(A|B) P(B)}{P(A)}\n\\end{equation*}\\]\nHere\nHere:\n\n\\(A\\): Represents the new evidence or data that has been observed.\n\\(B\\): Represents the hypothesis or prior belief about an event.\n\nThis formula arises naturally from the multiplication rule and the symmetry of \\((A \\cap B)\\). Here’s a quick derivation:\n\nBy the multiplication rule: \\(P(B|A)P(A) = P(A \\cap B)\\)\nSimilarly: \\(P(A|B)P(B) = P(A \\cap B)\\)\nEquating the two expressions for \\(P(A \\cap B)\\) and dividing by \\(P(A)\\): \\(P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\)\n\nBayes’ rule is often challenging to grasp intuitively. To build understanding, let’s explore a simple and vivid example involving a speck of sand.\n\n3.4.2.1 The Speck of Sand: An Intuitive Illustration\nImagine a square of area 1, representing our entire sample space. Within this square, there is a circle \\(B\\), with area equal to \\(P(B)\\). You have a tiny speck of sand on your finger, which accidentally falls somewhere within the square. The location of the speck is entirely random.\n\n\n\n\n\nWhere is the speck of sand?\n\n\n\n\nThe probability that the speck lands in \\(B\\) is simply the area of \\(B\\), \\(P(B)\\), since the speck could have landed anywhere within the square with equal likelihood.\n\n\n3.4.2.2 Updating Beliefs with New Information\nNow, suppose you are told that the speck landed within another circle \\(A\\) that also lies inside the square:\n\n\n\n\n\nIn fact we learn that the speck is inside circle \\(A\\)\n\n\n\n\nHow does this new information affect the probability that the speck is in \\(B\\) ? Mathematically, we now want to compute \\(P(B|A)\\), the probability that the speck is in \\(B\\), given that it is inside \\(A\\).\n\n\n3.4.2.3 Overlap Between \\(A\\) and \\(B\\)\nIntuitively, the updated probability \\(P(B|A)\\) depends on the overlap of \\(B\\) and \\(A\\). Specifically, it is the fraction of \\(A\\) that lies within \\(B\\), expressed as: \\[\\begin{equation*}\nP(B|A) = \\frac{\\text{Area of } A \\cap B}{\\text{Area of } A} = \\frac{P(A \\cap B)}{P(A)}\n\\end{equation*}\\]\n\nIf \\(A\\) and \\(B\\) overlap only slightly, \\(P(B|A)\\) will be small:\n\n\n\n\n\n\nSmall overlap between \\(A\\) and \\(B\\)\n\n\n\n\n\nIf the overlap is large, \\(P(B|A)\\) will be large:\n\n\n\n\n\n\nLarge overlap between \\(A\\) and \\(B\\)\n\n\n\n\n\n\n3.4.2.4 Bayesian Interpretation\nIn Bayesian reasoning:\n\n\\(P(B)\\) is the prior probability, representing our belief in the hypothesis \\(B\\) before observing the data.\n\\(P(A|B)\\) is the likelihood, describing how consistent the observed data \\(A\\) is with the hypothesis \\(B\\).\n\\(P(A)\\) normalizes the result, ensuring all probabilities sum to 1.\n\nThe speck-of-sand example illustrates how Bayesian updating works:\n\nStart with a prior \\(P(B)\\).\nReceive new evidence \\(P(A|B)\\).\nUpdate the probability of the hypothesis given the evidence \\((P(B|A)\\).\n\nBayes’ theorem quantifies this intuitive process of revising beliefs based on data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#summary",
    "href": "03-lecture3.html#summary",
    "title": "3  Conditional Probability",
    "section": "3.6 Summary",
    "text": "3.6 Summary\n\n3.6.1 Probability Concepts\nIn this lecture, we covered the following key probability concepts: - Conditional Probability: Understanding the probability of an event occurring given that another event has occurred, and its relevance in real-world scenarios, especially in finance. - Dependence and Independence: How events can be dependent or independent and the role of conditional probabilities in analyzing their relationships. - Bayes’ Rule: Using conditional probabilities to update beliefs based on new evidence.\nThese concepts were illustrated with practical examples, including a discussion of the financial crisis of 2007-2008 to highlight the risks of neglecting dependencies in probability modeling.\n\n\n3.6.2 R Concepts\nDuring the lecture, we used R to: - Simulate Conditional Probabilities: Generate random data and compute conditional probabilities to illustrate theoretical concepts. - Visualize Dependencies: Create intuitive visualizations of event overlaps and relationships between probabilities. - Practical Applications: Implement real-world examples to explore conditional probabilities and Bayes’ Rule, showcasing how to use R for data analysis in finance.\nThis lecture bridged theoretical probability concepts with practical computational tools in R, enabling a deeper understanding of the material and its applications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#project-for-lecture-3",
    "href": "03-lecture3.html#project-for-lecture-3",
    "title": "3  Conditional Probability",
    "section": "3.6 Project for lecture 3:",
    "text": "3.6 Project for lecture 3:\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nTooze, Adam. 2018. Crashed. How a Decade of Financial Crisis Changed the World. Viking.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#project-simulating-and-exploring-the-investment-game-in-r",
    "href": "03-lecture3.html#project-simulating-and-exploring-the-investment-game-in-r",
    "title": "3  Conditional Probability",
    "section": "3.6 Project: Simulating and Exploring the Investment Game in R",
    "text": "3.6 Project: Simulating and Exploring the Investment Game in R\nIn this project, you will explore an investment game designed to illustrate decision-making under uncertainty. The goal is to analyze how Bayesian updating can help refine estimates and guide better decisions. Through this project, you will learn how to simulate the game, apply probability concepts, and visualize outcomes.\n\n3.6.1 The Investment Game\nThe game involves the following setup and rules:\n\nCoins:\n\nThere are two types of coins:\n\nGreen coins: Land heads with a probability of \\(0.7\\) and tails with a probability of \\(0.3\\).\nRed coins: Land heads with a probability of \\(0.3\\) and tails with a probability of \\(0.7\\).\n\n\nRounds:\n\nThe game is played over several rounds. Each round involves flipping a single coin 100 times.\nAt the start of each round, the referee randomly selects one coin (green or red) from a bucket. The composition of the bucket is unknown to the players.\nThe referee hides the coin’s type, so players must infer it based on observed outcomes.\n\nBidding:\n\nBefore each flip, players bid for ownership of the flip. The winning bidder earns \\(2\\) if the coin lands heads and \\(0\\) if it lands tails.\nPlayers aim to estimate the expected value of a flip as accurately as possible to avoid overpaying in the auction.\n\nBelief Updating:\n\nAfter each flip, players can update their belief about the coin type using Bayes’ rule, based on the outcomes they observe.\n\nObjective:\n\nThe game continues until the rounds are completed. The winner is the player who grows their initial wealth the most.\n\n\n\n\n\n3.6.2 Objectives\nThe project focuses on the following learning objectives: 1. Understand the game rules and how to use Bayesian reasoning to update beliefs. 2. Implement a simulation of the game in R to model the dynamics of bidding, flipping, and belief updating. 3. Explore how different prior assumptions affect performance in the game. 4. Visualize the learning process to gain insights into the role of Bayesian updating in decision-making.\n\n\n3.6.3 Tasks and Questions\n\nGame Simulation:\n\nSimulate the coin flips and outcomes for multiple rounds of the game.\nImplement Bayesian updating to refine your belief about the coin type after each flip.\nCalculate the expected value of a flip based on your updated beliefs.\n\nExploring Priors:\n\nAssume the bucket contains \\(90\\%\\) green coins and \\(10\\%\\) red coins. Start with the following priors for the coin being green:\n\n\\(P(G) = 0.5\\) (no strong belief),\n\\(P(G) = 0.9\\) (likely green),\n\\(P(G) = 0.99\\) (almost surely green).\n\nFor each prior, investigate:\n\nHow quickly your beliefs converge to the correct probability.\nThe effect on your expected value estimates for each flip.\n\n\nImpact of Incorrect Priors:\n\nAssume the referee draws a red coin but your prior strongly favors a green coin (e.g., \\(P(G) = 0.9\\) or higher). Analyze:\n\nHow quickly Bayesian updating corrects the misestimation.\nThe number of flips required to approach the correct estimate of \\(P(G) = 0.1\\).\n\n\nVisual Analysis:\n\nUse R to visualize:\n\nThe evolution of posterior probabilities for the coin type over multiple flips.\nTrends in expected values for each flip across rounds.\n\n\nAdvanced Question:\n\nHow would the game change if the referee occasionally replaced the coin mid-round without notifying the players? Suggest how Bayesian reasoning might adapt to this added complexity.\n\n\n\n\n\n3.6.4 Deliverables\nBy the end of this project, students should: 1. Submit an R script or notebook simulating the game. 2. Provide visualizations that track posterior probabilities and expected values. 3. Write a short report summarizing findings, including the impact of different priors and key takeaways about Bayesian reasoning in decision-making.\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nTooze, Adam. 2018. Crashed. How a Decade of Financial Crisis Changed the World. Viking.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#project-evaluating-credit-risk-using-conditional-probabilities",
    "href": "03-lecture3.html#project-evaluating-credit-risk-using-conditional-probabilities",
    "title": "3  Conditional Probability",
    "section": "3.7 Project: Evaluating Credit Risk Using Conditional Probabilities",
    "text": "3.7 Project: Evaluating Credit Risk Using Conditional Probabilities\n\n3.7.1 Problem Description\nA bank is evaluating a loan application using historical data to estimate the likelihood of default. The borrower has a low credit score, and the bank has the following data:\n\nDefault Rates:\n\nProbability of default for all customers: \\(P(D) = 0.04\\).\nProbability of non-default for all customers: \\(P(ND) = 0.96\\).\n\nEvidence:\n\nProbability of a low credit score given default: \\(P(L|D) = 0.7\\).\nProbability of a low credit score given non-default: \\(P(L|ND) = 0.1\\).\n\n\nThe goal is to determine the posterior probability of default given the borrower’s low credit score, \\(P(D|L)\\), using Bayes’ Rule. Additionally, you will verify this theoretical result by simulating customer data and analyzing outcomes.\n\n\n\n3.7.2 Questions\n\nCompute \\(P(D|L)\\) Theoretically:\n\nUse Bayes’ Rule to calculate the posterior probability of default given a low credit score.\n\nSimulate the Scenario in R:\n\nSimulate a dataset of 10,000 customers where each customer is randomly assigned a default status based on \\(P(D)\\).\nBased on the assigned default status, simulate whether each customer has a low credit score using \\(P(L|D)\\) and \\(P(L|ND)\\).\n\nCompute \\(P(D|L)\\) from Simulated Data:\n\nUse the simulated data to compute \\(P(D|L)\\) and compare it to the theoretical result.\n\nVisualize Results:\n\nCreate a simple bar plot comparing the simulated and theoretical probabilities. If you are unfamiliar with visualization tools, check out the barplot() function in base R.\n\n\n\n\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nTooze, Adam. 2018. Crashed. How a Decade of Financial Crisis Changed the World. Viking.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "03-lecture3.html#using-an-llm-to-explore-conditional-probability",
    "href": "03-lecture3.html#using-an-llm-to-explore-conditional-probability",
    "title": "3  Conditional Probability",
    "section": "3.5 Using an LLM to Explore Conditional Probability",
    "text": "3.5 Using an LLM to Explore Conditional Probability\nIn each lecture I try to involve a use case showing you how you could leverage the power of LLMs to deepen your learning experience. Conditional probability is an excellent topic to explore with the LLM. Understanding conditional probability can be challenging because our intuitions about likelihoods often conflict with the precise rules of probability theory. Unlike geometric intuition which is hardwired in the human mind through the needs of our visual system, we humans lack an innate intuitive understanding of probability. This is something that has to be acquired through training and practice. An LLM like ChatGPT can serve as a dialogue partner to help you explore this gap and refine your understanding.\nHere is a suggested use case: You can use an LLM to simulate and analyze scenarios where our intuition might mislead us. Here’s how to structure a dialogue with the model:\n\nPose a Real-World Scenario:\n\nExample: “Suppose you test positive for a rare disease with a prevalence of 1%. The test is 95% accurate, meaning it correctly identifies positives 95% of the time and negatives 95% of the time. What is the probability that you actually have the disease?”\n\nEngage in a Dialogue:\n\nAsk the model to explain how to approach the problem using conditional probability.\nExplore intermediate steps such as calculating the prior probability, likelihood, and posterior probability using Bayes’ Rule.\n\nChallenge Your Intuition:\n\nShare your intuitive guess about the probability with the model.\nLet the model explain where intuition might diverge from formal reasoning and why.\n\nExperiment with Variations:\n\nModify the problem (e.g., change the disease prevalence or test accuracy) and ask the model to recompute probabilities. Compare how small changes in assumptions affect outcomes.\n\n\n\n3.5.1 Example Dialogue\nHere’s an example interaction you could try:\nYou: “I think the probability of actually having the disease should be very high since the test is 95% accurate. Is this correct?”\nModel: “Let’s compute it formally using Bayes’ Rule: \\[\\begin{equation*}\nP(D|T) = \\frac{P(T|D) \\cdot P(D)}{P(T|D) \\cdot P(D) + P(T|ND) \\cdot P(ND)}\n\\end{equation*}\\] Where: - \\(P(D) = 0.01\\) (prevalence), - \\(P(ND) = 1 - P(D) = 0.99\\), - \\(P(T|D) = 0.95\\) (true positive rate), - \\(P(T|ND) = 0.05\\) (false positive rate).\nSubstituting the values: \\(P(D|T) = \\frac{0.95 \\cdot 0.01}{0.95 \\cdot 0.01 + 0.05 \\cdot 0.99} \\approx 0.16\\)\nThis means the probability you actually have the disease is about 16%, much lower than intuition might suggest because the disease is so rare.”\nWhy can exercises like this be valuable?\n\nRefines Understanding: Engaging in such dialogues clarifies the application of Bayes’ Rule and highlights the importance of considering base rates.\nReveals Pitfalls of Intuition: You’ll learn to appreciate why our gut feelings can sometimes mislead us in probabilistic reasoning.\nEncourages Exploration: By modifying scenarios and asking “what if” questions, you deepen your grasp of conditional probabilities in diverse contexts.\n\nLeverage this approach to build a stronger connection between the theory and its real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html",
    "href": "04-lecture4.html",
    "title": "4  Random Variables",
    "section": "",
    "text": "4.1 Random Variables and Distributions\nA random variable is a numerical outcome of a random phenomenon. Formally, a random variable is a function that assigns a real number to each outcome in the sample space of a random experiment. More explicitly:\nThis formal definition allows us to generalize and analyze a wide variety of real-world scenarios. For instance:\nBy explicitly recognizing these as random variables, we can now apply a systematic framework to quantify their behavior and analyze them.\nNote that it is a widely held convention in probability theory to use capital letters such as \\(X\\) and \\(Y\\) etc. as symbols of a random variable.\nRandom variables come in two varieties, depending on the properties of the sample space \\({\\cal S}\\). If the sample space is a finite or countably finite set, the sample space is discrete and we talk of a discrete random variables.\nSometimes it is natural to consider continuous sample spaces. For example when we consider the return of an asset over a year or the price of a stock at a specific time. In this case we call a random variable continuous. With continuous sample spaces we will need tools from calculus. We will discuss continuous random variables in the next lecture. Here we stick with the concept of a discrete random variable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#discrete-distributions",
    "href": "04-lecture4.html#discrete-distributions",
    "title": "4  Random Variables",
    "section": "",
    "text": "Random variable\n\nA random variable is a function defined on the sample space.\n\n\n\n\n\n\n\n\nProbability distribution\n\nThe function \\(P(X = x_j) = p(x_j)\\) for \\(j = 1, \\cdots ,n\\) is called the (probability) distribution of the random variable \\(X\\).\n\n\n\n\n\nCumulative distribution\n\nThe cumulative distribution function - abbreviated CDF - shows the probability that a random variable \\(X\\) take a value less than or equal to a given value \\(x\\). It is usually denoted as \\(F(x) = P(X \\leq x)\\) where \\(-\\infty &lt; x &lt; \\infty\\).\n\n\n\n\n\n\n\n\n\n\n\nJoint probability distribution\n\nThe function \\[\\begin{eqnarray*}\nP(X = x_i, Y = y_j) = p(x_i, y_j), \\quad i = 1, \\cdots, n \\,\\, j = 1, \\cdots, m\n\\end{eqnarray*}\\] is called the joint probability distribution of \\(X\\) and \\(Y\\).\n\n\n\n\n\n\nConditional probability\n\nThe conditional probability of the event \\(Y = y_j\\) given that \\(X = x_i\\) is \\[\\begin{equation*}\nP(Y = y_j | X = x_i) = \\frac{p(x_i, y_j)}{f(x_i)}\n\\end{equation*}\\] where we need to require that \\(f(x_i) &gt; 0\\).\n\n\n\n\nMutually independent\n\nTwo random variables are called mutually independent if for any combination of values \\(x_1, ..., x_n\\), \\(y_1, ..., y_m\\) \\[\\begin{equation*}\nP(X = x_i, Y = y_j) = P(X = x_i) P(Y = y_j)\n\\end{equation*}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#expected-value",
    "href": "04-lecture4.html#expected-value",
    "title": "4  Random Variables",
    "section": "4.2 Expected value",
    "text": "4.2 Expected value\nSometimes the entire probability distribution is difficult to grasp at once. It is common to describe probability distributions therefore by a summary measure. Among the typical summary measures the expectation or the mean is by far the most important one.\n\nExpected Value\n\nThe expected value \\(\\mathrm{E}(X)\\) of a discrete random variable \\(X\\) is the probability weighted sum of all its possible values: \\[\\begin{equation*}\n\\mathrm{E}(X) = x_1p(x_1) + x_2 p(x_2) + \\cdots + x_n p(x_n)\n\\end{equation*}\\] Note that in the definition we have covered the case where the number of possible outcomes is finite. In the other case where the number of possible outcomes is countably infinite we need additional convergence conditions for the infinite series defined by the sum of the \\(x_i p(x_i)\\). We ignore this case here.\n\n\nThe term expected value can be a bit misleading, for it might suggest that the expected value is something we “expect” to occur. It is - to the contrary - pretty often unlikely and sometimes even impossible.\nAs an example, remember our fair die. Its expected value is, if you apply the definition above:\n\n1*1/6+2*1/6+3*1/6+4*1/6+5*1/6+6*1/6\n\n[1] 3.5\n\n\na value that will never actually come up.\nWe will discuss the specific properties of expected value within an extended example a bit later in this lecture.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#variation",
    "href": "04-lecture4.html#variation",
    "title": "4  Random Variables",
    "section": "4.3 Variation",
    "text": "4.3 Variation\nWhile the expected value of a random variable provides a very useful summary measure for a distribution, we typically also want information about the amount of variation of a a random variable. One such measure, which gives us a degree of possible deviation from the mean is the variance.\n\nVariance\n\nThe variance of a random variable \\(X\\) is the expected value of the squared deviation from the expected value of \\(X\\) or \\[\\begin{equation*}\n\\mathrm{Var}(X) = \\mathrm{E}\\left(X -\\mathrm{E}(X) \\right)^2\n\\end{equation*}\\]\n\n\nNote that we can simplify the expression for the variance as follows: \\[\\begin{align*}\n\\mathrm{E}((X-\\mathrm{E}(X)^2)) & = \\mathrm{E}(X^2 - 2 X \\mathrm{E}(X) + \\mathrm{E}(X)^2) \\\\\n              & = \\mathrm{E}(X^2) - 2 \\mathrm{E}(X)\\mathrm{E}(X) + \\mathrm{E}(X)^2 \\\\\n              & = \\mathrm{E}(X^2) - \\mathrm{E}(X)^2\n\\end{align*}\\]\nIn the case of a discrete random variable, the case we discuss in this lecture, we have \\(\\mathrm{Var}(X) = \\sum_{i=1}^{n} \\left(x_i - \\mathrm{E}(X) \\right)^2 p(x_i)\\).\nNote that the units of the variance are the squared units of the random variable. This is often not very useful for interpretations. This leads us to the concept of a standard deviation:\n\nStandard Deviation\n\nThe standard deviation of a random variable \\(X\\) is the square root of the variance of \\(X\\), denoted \\(\\mathrm{SD}(X) = \\sqrt{\\mathrm{Var}(X)}\\).\n\n\nLet’s discuss a few properties of the variance also in the context of the example. In this way we keep faithful to the approach of this lecture to make the abstract concepts tangible.\nAssume we have two random variables \\(X\\) and \\(Y\\) then we define their covariance as follows.\n\nCovariance\n\nThe covariance for two random variables \\(X\\) and \\(Y\\), is defined as \\[\\begin{equation*}\n\\mathrm{Cov}(X,Y) = \\mathrm{E}((X - \\mathrm{E}(X))(Y-\\mathrm{E}(Y)))\n\\end{equation*}\\]\n\n\nNote that \\(\\mathrm{Cov}(X,Y) = \\mathrm{E}(XY) - \\mathrm{E}(X)\\mathrm{E}(Y)\\). If \\(X\\) and \\(Y\\) are independent, then \\(\\mathrm{E}(XY) = \\mathrm{E}(X)\\mathrm{E}(Y)\\), hence if \\(X\\) and \\(Y\\) are independent, \\(\\mathrm{Cov}(X,Y) = 0\\). Note, however, that the converse is not true. A covariance of 0 does not imply independence.\nLet me illustrate this important point with the example of a variation of the game of craps in project 1. Let \\(X\\) the random variable showing the sum points on the two dice and - for the sake of this example \\(Y\\) the random variable showing the difference between the points on the two dice. The points on the individual dice ( the \\(\\{1,2,3,4,5,6\\}\\)) can be themselves be understood as random variables \\(U\\) and \\(V\\). Then \\(\\mathrm{E}(XY) = \\mathrm{E}\\left( (U+V)(U-V) \\right)\\). This is \\(\\mathrm{E}(U^2) - \\mathrm{E}(V^2) = 0\\). Note also that \\(\\mathrm{E}(Y) = 0\\), hence \\(\\mathrm{Cov}(X,Y) = 0\\). But when you go through the point combinations in this situation in project 1, you will realize that \\(X\\) and \\(Y\\) are either both odd or both even. Thus they are dependent. For those of you who can not see this, they can convince themselves by a little piece of R code:\n\nU &lt;- 1:6\nV &lt;- 1:6\n\ndf &lt;- expand.grid(U = U, V = V)\ndf$\"U+V\" &lt;- df$U + df$V\ndf$\"U-V\" &lt;- df$U - df$V\n\nknitr::kable(df)\n\n\n\n\nU\nV\nU+V\nU-V\n\n\n\n\n1\n1\n2\n0\n\n\n2\n1\n3\n1\n\n\n3\n1\n4\n2\n\n\n4\n1\n5\n3\n\n\n5\n1\n6\n4\n\n\n6\n1\n7\n5\n\n\n1\n2\n3\n-1\n\n\n2\n2\n4\n0\n\n\n3\n2\n5\n1\n\n\n4\n2\n6\n2\n\n\n5\n2\n7\n3\n\n\n6\n2\n8\n4\n\n\n1\n3\n4\n-2\n\n\n2\n3\n5\n-1\n\n\n3\n3\n6\n0\n\n\n4\n3\n7\n1\n\n\n5\n3\n8\n2\n\n\n6\n3\n9\n3\n\n\n1\n4\n5\n-3\n\n\n2\n4\n6\n-2\n\n\n3\n4\n7\n-1\n\n\n4\n4\n8\n0\n\n\n5\n4\n9\n1\n\n\n6\n4\n10\n2\n\n\n1\n5\n6\n-4\n\n\n2\n5\n7\n-3\n\n\n3\n5\n8\n-2\n\n\n4\n5\n9\n-1\n\n\n5\n5\n10\n0\n\n\n6\n5\n11\n1\n\n\n1\n6\n7\n-5\n\n\n2\n6\n8\n-4\n\n\n3\n6\n9\n-3\n\n\n4\n6\n10\n-2\n\n\n5\n6\n11\n-1\n\n\n6\n6\n12\n0\n\n\n\n\n\nThe choice of an origin and unit of measurement of a random variable is to a large degree arbitrary. It is thus often most convenient to take the mean as the origin and the standard deviation as unit. Denote the mean of a random variable \\(X\\) as \\(\\mu\\) and the variance as \\(\\sigma^2\\). In this case \\(X-\\mu\\) has mean zero and variance \\(\\sigma^2\\), and hence the variable \\(X^* = \\frac{(X-\\mu)}{\\sigma}\\) has mean 0 and variance 1. If we take the covariance of two such normalized random variables we get the so called correlation coefficient \\(\\rho(X,Y)\\).\n\nCorrelation Coefficient\n\nThe correlation coefficient of two random variables \\(X\\) and \\(Y\\) is defined as \\[\\begin{equation*}\n\\rho(X,Y) = \\mathrm{Cov}(X^*,Y^*) = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_x \\sigma_y}\n\\end{equation*}\\]\n\n\nThe correlation coefficient is thus just a particular way - a dimensionless way - of writing the covariance.\nNow it is time we get back to R and play around with these concepts a bit. As a finance class you should be motivated to know these concepts well because they are a sort of cap-stone for investment science and portfolio management.\nIndeed random variables are the most important models in Finance. In the next example we will study the concepts of this section by putting them in a Finance context. In doing so we will also embark into a new territory of R. We will now also learn the basics of writing R programs using portfolio and investment theory as an example.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#random-variables-the-cornerstone-for-modeling-investments-under-uncertainty",
    "href": "04-lecture4.html#random-variables-the-cornerstone-for-modeling-investments-under-uncertainty",
    "title": "4  Random Variables",
    "section": "4.4 Random variables: The cornerstone for modeling investments under uncertainty",
    "text": "4.4 Random variables: The cornerstone for modeling investments under uncertainty\nAn important case where probability is applied in Finance is the problem of analyzing risky investment decisions: When you make an investment with a known initial outlay of capital the amount to be returned from the investment in the future is unknown. In some cases, say when you invest in very liquid assets classes which are constantly traded on markets, like stocks and bonds, you might have an idea on the probability of certain return prospects for investment informed by past data. This case of random returns is one of the most intensively used applications of probability to Finance. Since random variables are the models of choice in this case, we study such an investment problem now.\nIt is at the same time an ideal examples to push the frontier of your knowledge of R, because we can discuss in this context how to write programs and run more complex simulations than we did so far.\nLet us focus first on a class of investment problems where we only have a single investment period: Money is invested at an initial time and a payoff is attained at the end of this period. Random variables are a concept that allows us to construct models for thinking about and analyzing the uncertainty inherent in such practical situation. While the assumption of a single period is an idealization that does not apply to public stocks such models are applied to this common class of investments as a simplification. For other situations the one period model is sometimes a pretty good approximation. Think of a zero-coupon bond that will be held to maturity. Or think about an investment in a physical project that only will yield a payoff when completed.\nThese kinds of simplifications are - by the way - a cornerstone in modelling as well as in programming. We build solutions to complex problems by starting with a simple problem and then build complexity step by step.\n\n4.4.1 Asset return\nBefore we dive into the modelling of investment problems with uncertainty using probability and random variables as a tool, we need to introduce some financial concepts first: An investment instrument that can be bought and sold is called an asset in Finance. If \\(X_0\\) is the amount invested today and \\(X_1\\) is the amount received “tomorrow”, say for example in a year from now, the total return is defined as the ratio of the amount received to the amount invested or formally \\(R = \\frac{X_1}{X_0}\\). Often we simply use the term return. The rate of return is defined as the difference between the amount received and the amount invested in relation to the amount invested or \\(r = \\frac{X_1-X_0}{X_0}\\). Often people also speak of return when they refer to the rate of return. Both notions are related by the equation \\(R = 1+r\\) and the equation for the rate of return can be rewritten as \\(X_1 = (1+r)X_0\\). This shows that the rate of return acts much like an interest rate.\nWhen an asset is initially acquired, its rate of return is uncertain. Modelling the rate of return as a random variable is one possibility to describe and analyze this situation of uncertain returns.\n\n\n4.4.2 Random asset returns\nNow you can already see, how random variables can act as a model for investment under uncertainty. In an uncertain situation, the amount received at the end of the period \\(X_1\\) is a random variable. The underlying sample space has to be specified appropriately and there are various ways to do so in an applied situation. We will soon go in detail through an example with a discrete sample space. For returns, it is - of course - more natural to have a continuous sample space, as we will see in the next lecture.\nIf \\(X_1\\) is a random variable, so is of course \\(R\\) and \\(r\\) as a consequence and we are in an investment situation with rabdom returns.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#modelling-random-asset-returns-first-steps-in-r-programing",
    "href": "04-lecture4.html#modelling-random-asset-returns-first-steps-in-r-programing",
    "title": "4  Random Variables",
    "section": "4.5 Modelling random asset returns: First steps in R programing",
    "text": "4.5 Modelling random asset returns: First steps in R programing\nWe are now going to build a model of random returns that we can use to simulate the payoff consequences of portfolio investment decisions as well as for giving a concrete context to the abstract notions we introduced before. This will show you their power as well as their practical relevance in Finance.\nHow do actual returns look like? Let’s go back to the example of the S&P 500 - a broad stock market index - we discussed before.\n\nlibrary(tidyquant)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n── Attaching core tidyquant packages ──────────────────────── tidyquant 1.0.9 ──\n✔ PerformanceAnalytics 2.0.4      ✔ TTR                  0.24.4\n✔ quantmod             0.4.26     ✔ xts                  0.14.1\n── Conflicts ────────────────────────────────────────── tidyquant_conflicts() ──\n✖ zoo::as.Date()                 masks base::as.Date()\n✖ zoo::as.Date.numeric()         masks base::as.Date.numeric()\n✖ PerformanceAnalytics::legend() masks graphics::legend()\n✖ quantmod::summary()            masks base::summary()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nsp500 &lt;- tq_get(\"^GSPC\", get = \"stock.prices\", from = \"2011-01-03\", to = \"2021-12-22\")\nhead(sp500, n = 5)\n\n# A tibble: 5 × 8\n  symbol date        open  high   low close     volume adjusted\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 ^GSPC  2011-01-03 1258. 1276. 1258. 1272. 4286670000    1272.\n2 ^GSPC  2011-01-04 1273. 1274. 1263. 1270. 4796420000    1270.\n3 ^GSPC  2011-01-05 1269. 1278. 1265. 1277. 4764920000    1277.\n4 ^GSPC  2011-01-06 1276. 1278. 1270. 1274. 4844100000    1274.\n5 ^GSPC  2011-01-07 1274. 1277. 1262. 1272. 4963110000    1272.\n\n\nNow let’s use R to add a column with daily returns based on the variable adjusted the adjusted index value, which takes into account dividend payments and other things. Our knowldege of how to manipulate R objects by subsetting rules and the application of functions is already there from our lecture on conditional probability:\nWe add a column of first differences \\(X_{t+1} - X_t\\) for \\(t \\in \\{2, \\cdots ,2762\\}\\). We have to start at the second row because for the first observation we have no previous observation to deduct. Thus when we add a new column to the data frame we need a column of equal length, so we put an NA in the beginning of the returns column. We then have to divide by \\(X_t\\) for \\(t \\in \\{ 1,\\cdots, 2761\\}\\) because for the last observation we have no next observation to deduct the last one from and divide by it.\nRemember that we can refer to elements in an R object by indexing with numbers. If we apply the length()function to an object we get the number of components and thus the number of the last element in an object. This we drop. The first difference we already know how to deal with. So let’s add returns\n\nsp500$returns &lt;- c(NA, (diff(sp500$adjusted, lag = 1)/\n                          sp500$adjusted[-length(sp500$adjusted)]))\n\nNow it is always a good idea when you write complex commands like this to check whether the code is doing what you want. The second and the first value of the adjustedvariable is: 1270.1999512 and 1271.8699951. Thus the return is -0.0013131. The first value in our returns variable is:\n\nsp500$returns[2]\n\n[1] -0.001313062\n\n\nThis looks good. Now let us use our little graphical knowledge to look at the distribution of returns in our data using the hist()function from base R and using the option freq = TRUE to get empirical frequencies. We drop the NA and type hist(sp500$returns[-1]) which will give us:\n\nhist(sp500$returns[-1])\n\n\n\n\n\n\n\n\nThis picture suggests that (daily) returns are pretty much distributed symmetrically around 0, and in very rare cases rise and drop a bit more than 5 % per day.\nNote that this is a histogram, which means that we binned the return data into buckets of 1 % and plotted it. It would be natural to assume a continuous sample space for modeling returns. We will discuss such models in the next lecture.\nHere we take a choice where we model randomness using discrete random variables. We do so by combining two random devices, constructed as a wheel of fortune and a coin flip. Both devices are useful for the study of investments.\nOur wheel of fortune looks like this:\n\n\n\n\n\nA wheel of fortune. If the wheel stops at the segment under the marker your payoff is determined according to the rules of the game\n\n\n\n\nIf you invest an amount of money in the wheel, your payoff will be determined according to the rules of the game depending which segment ends up under the marker (the triangle above the wheel). The wheel is constructed such that it stops at segment \\(H\\) with probability \\(\\frac{1}{3}\\), at \\(L\\) with probability \\(\\frac{1}{6}\\) and at \\(N\\) with probability \\(\\frac{1}{2}\\).\nThere is another random device, before the wheel is spun, which is a coin, which is flipped and ends up on \\(B\\) with probability \\(\\frac{3}{5}\\) or \\(R\\) with probability \\(\\frac{2}{5}\\).\nThis creates in total 6 outcomes \\({\\cal S} = \\{BH, BL, BN, RH, RL, RN\\}\\) with probabilities \\(p(x) = \\{ 0.2, 0.1, 0.3, 0.13, 0.07, 0.2\\}\\).\nIn this game you have some initial wealth, which you may invest into 3 alternative assets. They have a payoff-structure described in the following tables:\nIf the coin lands on \\(B\\):\n\n\n\nWheel\nAsset 1\nAsset 2\nAsset 3\n\n\n\n\nH\n1\n10\n6\n\n\nL\n1\n2\n4\n\n\nN\n1\n6\n5\n\n\n\nIf the coin lands on \\(R\\):\n\n\n\nWheel\nAsset 1\nAsset 2\nAsset 3\n\n\n\n\nH\n1\n5\n3\n\n\nL\n0\n1\n2\n\n\nN\n1\n3\n2.5\n\n\n\nYou can allocate your wealth between the three assets and have to pay the following prices per unit \\(q = \\{0.93, 5.86, 4.625 \\}\\). You can not invest more than your initial wealth of 1000. You can neither sell nor short sell any of the assets.\nYou may think of these assets as a bond an two stocks. Asset 1, the bond, pays a fixed amount in each state, except when things go very bad and it defaults and pays nothing. Asset 2 and Asset 3 could be thought of as two stocks where Asset 2 has a higher variation in its payments across states. These are the rules of the game.\nNow here is the programming challenge we undertake in several steps: We build a function that randomly chooses an outcome according to the probabilities we have specified and then determine the payoff of each asset according to the rules of the game. When we are finished with this step it will look something like this: When you give your portfolio to the outcome() function you will get the state, the payoff of the different assets and the return. outcome(portfolio)\nThe first step in this program, you already know from previous lectures. Let us call this function get_state(). We have to randomly draw a state by flipping the coin and spinning the wheel, just as we rolled the dice in lecture 1:\n\nget_state &lt;- function(){\n\n  coin  &lt;- c(\"B\", \"R\") \nwheel &lt;- c(\"H\", \"L\", \"N\")\n\nstate &lt;- c(sample(coin, size = 1, replace = T, prob = c(3/5, 2/5)), \n           sample(wheel, size = 1, replace = T, prob = c(1/3, 1/6, 1/2))) |&gt;\n  paste(collapse = \"\")\n\nreturn(state)\n}\n\nIn this program, we have created inside the function a coin and a wheel, determined a state vector by sampling form the coin and the wheel according to our probabilities and then concatenate the two draws - the two strings - by using the paste() function. Here we have used R’s pipe operator |&gt;, which channels the output of c() into the paste()function as an argument. Use the help menu to check how it works in detail.\nWe can now use get_stateto flip the coin and spin the wheel to generate a state of the world:\n\nget_state()\n\n[1] \"BH\"\n\n\nThe monetary value of the outcome depends on the portfolio of assets you have chosen. We write a program to compute the payoff and save it as a function, so for instance, if we get payoff(\"RH\", portfolio) will output the payoffs and returns or the portfolio you have chosen.\nAfter that the full portfolio simulator will look something like this\noutcome &lt;- function(portfolio){\n  state &lt;- get_state()\n\na vector showing the state and the portfolio return computed by a function  \npayoff(state, portfolio)\n}\nNote that in lecture 1 we showed and encouraged you to write all of your R code in an R script, a text file where you can compose your code. This will become very important as you work through this part of lecture 4. Remember that you can open an R script in RStudio by going to the menu bar and clicking on File &gt; New File &gt; R Script.\n\n4.5.1 A strategy for writing a program\nA coding task like writing a function to determine the random outcome of an investment decision is a complex task. Such a task can be tackled best, by using a fairly simple strategy:\n\nBreak complex tasks into simple sub-tasks\nUse concrete examples for testing\nDescribe your solution first in words or a picture before you convert it into code.\n\nA program is a step by step instruction to your computer to perform certain tasks. All instructions together will accomplish something sophisticated, but taken apart, each single step will likely to be straightforward.\nWriting a program is easier when you break it into sub-tasks and work on them separately first. R programs contain typically two types of subtasks, sequential steps and parallel cases.\nThe sequential steps we have already identified. We give the portfolio to the function, it draws a state, displays the result and then computes the portfolio payoff and the return.\nParallel cases occur when there are groups of similar processes within a task. Some tasks also require different algorithms for different kinds of input. If you can find these groups, you can work out the algorithms one at a time.\nFor example, payoff() will need to compute the portfolio payoff and the return, depending on the state. In all cases in our example, though, the function will use the same algorithm.\n\n\n4.5.2 if Statements and if-else Statements\nWhen you have to link parallel cases together, the program faces a fork in its process whenever it has to choose between cases. If statements help the program to do so.\nAn if statement in R tells R to do certain things for a particular case. The sytnax of telling R to do something if a case occurs is:\nif (this) { \n  that\n}\nSo an example in our case would be:\nif (state == \"BH\"){\n  print(\"BH\")\n}\nThe condition in an if statement must evaluate to a single TRUEof FALSE. If statements don’t have to be limited to a single line of code. Between the braces there can be arbitrary lines.\nif-statements tell R what to do if a condition is true.\nBut you can also tell R what to do if a condition is false. A simple code example would be:\nif (this){\nPlan A\n} else {\nPlan B\n}\nIf the case has more than two mutually exclusive cases, you can string multiple if and else statements together by adding an if statement immediately after the else. R will work then through if conditions until one evaluates to TRUE and then ignore the rest.\nNow this suggests to handle the cases of different states for the portfolio like this:\nif (state == \"BN\"){\n  return &lt;- sum(c(1/q1 -1 ,10/q2 -1 ,6/q3 -1 )*portfolio) \n} else if (state == \"BH\"){\n  return &lt;- sum(c(1/q1 -1 ,6/q2 -1 ,5/q3 -1 )*portfolio)\n} else if (state == \"BL\"){\n  return &lt;- sum(c(1/q1 -1 ,2/q2 -1 ,4/q3 -1 )*portfolio)\n} else if (state == \"RN\"){\n  return &lt;- sum(c(1/q1 -1 ,3/q2 -1 ,2.5/q3 -1 )*portfolio)\n} else if (state == \"RH\"){\n  return &lt;- sum(c(0/q1 -1 ,1/q2 -1 , 2/q3 -1 )*portfolio)\n} else if (state == \"RL\"){\n  return &lt;- sum(c(0/q1 -1 ,1/q2 -1 , 2/q3 -1 )*portfolio)\n}\nWhile this method works, it is for various reasons not very elegant.\n\nThe first unsatisfactory thing here is that the payoff numbers are hard coded into the if statement. If we would like to consider other examples, or different payoff structures, we would have to rewrite the entire if statement. This is tedious and prone to mistakes.\nThe second not so elegant thing about this code chunk is that we need to compose a multiplication and an addition for computing the payoff. It would be more efficient, if we could do this in a single operation.\nFinally the cascade of if else statements is quite unwieldy and difficult to read. Maybe we can do better than that?\n\nOne way to address concern 1 would be to define portfolio payoffs and portfolios outside of the function as abstract objects, so our function will work with any payoff structure. The second concern can be addressed if we reveal the linear algebra nature of the problem and unleash R’s power to deal with linear algebra problems. The address of the third concern, will lead us towards the concept of lookup-tables.\nThe asset payoffs could be mathematically represented as a matrix, where the rows are the states and the columns are the assets. We might also add an extra column with prices to our matrix, so we have all the data in one object\nHow do we specify the portfolio? We have an initial wealth and the wealth is invested into the three assets. We do not say anything here how the allocation is decided but once a decision has been taken, the portfolio will be a vector of weights \\((w^1, w^2, w^3)\\) with \\(0 \\leq w^i \\leq 1\\) for \\(i = 1,2,3\\).\n\n\n4.5.3 Portfolios and linear algebra\nLet us address the first issue and find an abstract representation of our portfolio. A very efficient and convenient way for this is to think of the payoffs in terms of linear algebra as a matrix. For example, the matrix of asset payoffs when the coin lands on \\(B\\) could be written as a \\(3 \\times 3\\) matrix where the rows show the payoff of each asset, in each of the states \\(N,H,L\\) chosen by the wheel of fortune. \\[\\begin{equation*}\nA_B =\n\\begin{pmatrix}\n1 & 10 & 6 \\\\\n1 &  2 & 4 \\\\\n1 &  6 & 5\n\\end{pmatrix}\n\\end{equation*}\\] Likewise, you can write \\[\\begin{equation*}\nA_R =\n\\begin{pmatrix}\n1 &  5 & 3 \\\\\n0 &  1 & 2 \\\\\n1 &  3 & 2.5\n\\end{pmatrix}\n\\end{equation*}\\] Since we have two random devices combined, the coin and the wheel we ultimately get six possible outcomes. Our state space is \\({\\cal S} = \\{BH, BN, BL, RH, RN, RL\\}\\). Thus if we combine all possible payoffs in a matrix we get a \\(6 \\times 3\\) matrix, where we have a row for each outcome in the state space and a column for each asset.\nIf you still remember what we discussed in lecture 2, you know that we can store objects in R as a matrix. We do this by telling R that we now specify a matrix and tell the column and row numbers to R and show the \\((A_B, A_R)\\) matrix to see what we get\n\nA &lt;- matrix(c(1,10,6,1,2,4,1,6,5,1,5,3,0,1,2,1,3,2.5), nrow = 6, ncol = 3, byrow = TRUE)\nA\n\n     [,1] [,2] [,3]\n[1,]    1   10  6.0\n[2,]    1    2  4.0\n[3,]    1    6  5.0\n[4,]    1    5  3.0\n[5,]    0    1  2.0\n[6,]    1    3  2.5\n\n\nNote two things here. We gave all the matrix entries as one vector of values, where we read the entries from left to right going form the first to the last column, starting with the payoffs in B and then going on to R. Then we specify the dimensions of the matrix. Finally we set the byrow argument to TRUE to tell R that it should fill up the values by row instead of its default behavior which would be a fill up by column.\nNow the R system shows us abstract row and column indices. It would be easier for us to read the matrices correctly, if we had names as references.\nHere the attribute system of R helps us, for an object of class matrixcan have row and column names as attributes which we can set using appropriate functions. This names our rows and columns but otherwise leaves the mathematical properties of \\(A\\) untouched. We can do matrix computations with it like we had no names. Let’s do this and see what we get by inspecting \\(A\\) again\n\ncolnames(A) &lt;- c(\"asset_1\", \"asset_2\", \"asset_3\")\nrownames(A) &lt;- c(\"BH\", \"BL\", \"BN\", \"RH\", \"RL\", \"RN\")\nA\n\n   asset_1 asset_2 asset_3\nBH       1      10     6.0\nBL       1       2     4.0\nBN       1       6     5.0\nRH       1       5     3.0\nRL       0       1     2.0\nRN       1       3     2.5\n\n\nTo compute returns, we also need price information in addition to the information on our asset payoffs. Our prices are \\(q = \\{0.93, 5.86, 4.625 \\}\\).\nNow with this approach we can now address point 1. and 2: With this construction we could write our payoff function maybe like this. It turns our asset-payoffs into returns:\n\nasset_returns &lt;- function(assets, prices){\n\n  # construct auxiliary matrix by stacking the price vector to get a matrix with similar\n  # dimensions as assets\n    \n    aux &lt;- matrix(rep(prices, dim(assets)[1]), \n                  nrow = dim(assets)[1], ncol = dim(assets)[2], byrow = T)\n    \n  # Compute return rates by component wise division of assets with \n  # aux and deduct 1 from each component\n    \n  r &lt;-   (assets/aux) - 1\n  \n  r\n}\n\nLet us go through this function step by step: The arguments are the payoff matrix and the asset prices. Then we create an object inside the function, we call aux (an abbreviation for auxiliary). To compute returns we have to divide every payoff by the price we paid. We do this by constructing an auxiliary matrix with the same dimensions as the payoff matrix ( 6 times 3). We then make a matrix, telling R to fill the matrix row-wise. The number of repetitions of the price vector we need is equal to the number of rows of our asset matrix. This number can be retrieved by the command dim(assets)[1] because dim applied to the asset matrix will return two numbers, the number of rows and the number of columns.It allows us to apply a component wise division and use R’s recycling rules to turn the matrix of asset payoffs and the vector of prices into a matrix of returns.\nYour see here that we can assign the particular values we need for this computation outside of this function. This allows us to compute returns for any value of the arguments. No specific values are hardcoded into the function.\nNotice also the lines, which begin with a # symbol. These are comments. Writing comments into your code should become second nature for you, when you write code. It describes in words what you write in R. It helps you to communicate with others, who might read your code. This includes your future self. As you gain more routine in writing little programs and procedures you will experience that upon returning to your code from a few weeks ago you sometimes do not understand anymore what you were doing. Comments help you to avoid such confusion. In R the escape character for comments is #. Every line beginning with this symbol will be understood to contain comments by R and is not going to be processed by the function.\nLet us test this function on our data:\n\nassets &lt;- A\nprices &lt;- c(0.93, 5.86, 4.625)\n\nasset_returns(assets, prices)\n\n       asset_1     asset_2     asset_3\nBH  0.07526882  0.70648464  0.29729730\nBL  0.07526882 -0.65870307 -0.13513514\nBN  0.07526882  0.02389078  0.08108108\nRH  0.07526882 -0.14675768 -0.35135135\nRL -1.00000000 -0.82935154 -0.56756757\nRN  0.07526882 -0.48805461 -0.45945946\n\n\n\n\n4.5.4 Lookup Tables\nOften the simplest way to do something in R involves subsetting. Our function gives us a column vector with rownames for the returns in each basic outcome of our random experiment.\n\nreturns &lt;- asset_returns(assets, prices)\nreturns\n\n       asset_1     asset_2     asset_3\nBH  0.07526882  0.70648464  0.29729730\nBL  0.07526882 -0.65870307 -0.13513514\nBN  0.07526882  0.02389078  0.08108108\nRH  0.07526882 -0.14675768 -0.35135135\nRL -1.00000000 -0.82935154 -0.56756757\nRN  0.07526882 -0.48805461 -0.45945946\n\n\nIf we know, for instance, that the state is \\(BH\\) we can extract the state by\n\nreturns[\"BH\", ]\n\n   asset_1    asset_2    asset_3 \n0.07526882 0.70648464 0.29729730 \n\n\nThis opens an option to avoid the complicated if tree. Assume a state is chosen by our get_state() function.\n\nstate &lt;- get_state()\n\nThen we can compute the portfolio return for this state by subsetting:\n\nreturns[state, ]\n\n   asset_1    asset_2    asset_3 \n0.07526882 0.02389078 0.08108108 \n\n\nThe vector of row names\n\nrownames(A)\n\n[1] \"BH\" \"BL\" \"BN\" \"RH\" \"RL\" \"RN\"\n\n\nfunctions as a so called lookup table. It is a complete list of the possible states. R can refer to this state by subsetting and choose the right row. In the end we want to keep just the return number. We can strip of the name by the unname() function. This helps us to avoid a complicated if-else-tree in this case.\n\n\n4.5.5 Writing the outcome program\nNow we can put together the different parts to implement our envisaged skeleton\noutcome &lt;- function(portfolio){\n  state &lt;- get_state()\n\na vector showing the state and the asset returns computed by a function  \npayoff(state, portfolio)\n}\nWe have broken this down to sub-tasks which we now weave together:\n\nret &lt;- asset_returns(assets, prices)\n\noutcome &lt;- function(returns = ret){\n  \n  # Draw a state by flipping the coin and spinning the wheel\n  \n  state &lt;- get_state()\n  \n  # Select the returns for this state and strip off names\n  \n  return &lt;- unname(ret[state, ])\n  \n  # pack result into a data frame\n  \n  data.frame(State = state, Asset_1 = return[1], \n             Asset_2 = return[2], \n             Asset_3 = return[3], stringsAsFactors = FALSE)\n \n\n}\n\nLet us test this function using the values we have already specified:\n\na &lt;- A\npr &lt;- c(0.93, 5.86, 4.625)\n\nret &lt;- asset_returns(assets = a, prices = pr)\n\noutcome(returns = ret)\n\n  State    Asset_1    Asset_2    Asset_3\n1    RH 0.07526882 -0.1467577 -0.3513514\n\n\nThis does not look so bad, but what can we do with this? Well now we are in the position to study a particular portfolio through simulation. We can play with the coin and the wheel random mechanism to see what it does to our asset returns and to our portfolio returns. We can study the expected return and the variation of returns. For this we need another flow control construct\n\n\n4.5.6 for Loops\nA for loop is a command that repeats a chunk of code many times, once for each element in a set of inputs. A for loop tells R “do this for every value of that”. In the syntax of R this looks like this:\nfor (value in that) {\n   this\n}\nThe that object should be a set of objects (often a vector of numbers or character strings). The for loop will run the code that appears between the braces once for each member of that.\nFor example, the for loop below runs print(\"one run\") once for each element in the vector of character strings:\n\nfor (value in c(\"My\", \"first\", \"for\", \"loop\")){\n  print(\"one run\")\n}\n\n[1] \"one run\"\n[1] \"one run\"\n[1] \"one run\"\n[1] \"one run\"\n\n\nThe valuesymbol in a for loop acts like an argument in a function. The for loop will create an object named valueand assign it a new value on each run of the loop. The code in your loop can access this value by calling the valueobject.\nWhat values will the loop assign to value? It will use the elements in the set that you run the loop on. for starts with the first element and then assigns a different element to valueon each run of the for loop, until all of the elements have been assigned to value. For example, the for loop below will run print(value) four times and will print out one element of c(\"My\", \"second\", \"for\", \"loop\")each time:\n\nfor (value in c(\"My\", \"second\", \"for\", \"loop\")){\n  print(value)\n}\n\n[1] \"My\"\n[1] \"second\"\n[1] \"for\"\n[1] \"loop\"\n\n\nOf course there is nothing special in using the name value. Any other name will do as well.\nIf you look at the valueafter the loop runs, you will see that it still contains the value of the last element in the set. Usually a for loop won’t return anything, unless you save its own output as it runs. The examples before appeared to return something because we called the print() function at every call. if we typed for example:\n\nfor(i in c(\"My\", \"third\", \"for\", \"loop\")){\n  value\n}\n\nwe get no output. To get an output you must fill either a vector or a list, which will then contain the output of every iteration in the for loop.\nLet’s see how this works in the next example:\n\nchars &lt;- vector(mode = \"character\", length = 4)\n\nwords &lt;- c(\"My\", \"forth\", \"for\", \"loop\")\n\nfor( i in 1:4){\n  \nchars[i] &lt;- words[i]\n}\n\nchars\n\n[1] \"My\"    \"forth\" \"for\"   \"loop\" \n\n\nWhat is going on here? We have first preallocated a vector of type character with length 4. The vector does not yet contain anything. Then we declared the four words we are going to use in our loop and saved them in an object called “word”. Then we start the loop and save at each of the four slots in the character vector one word from “words”, one after the other. At the end of the loop chars is filled with the words from word and can be inspected.\n\n\n4.5.7 Portfolio expected Return and Variance\nLet us now analyze the expected return and the variance of our portfolio using a simulation approach by writing a for loop and by using our outcome function. Our horizon for computing returns is one day. So let us look at the expected daily return over a period similar to our SP500 data. This comprised 2762 trading days. So let’s write a for loop with length 2762 and show the states and the asset returns on each day.\n\n# Preallocate a list in which we then can store the values of the for loop\n\nsimulated_returns &lt;- vector(mode = \"list\", length = 2762)\n\n# Transform payoffs into returns\n\nreturns &lt;- asset_returns(assets = A, prices = prices)\n\n# Compute 250 random outcomes and random returns\n\nfor(i in 1:2762){\n  \nsimulated_returns[[i]]  &lt;- outcome(returns)\n  \n}\n\nNow let’s inspect the output of this run by looking at the first element:\n\nsimulated_returns[[1]]\n\n  State    Asset_1    Asset_2    Asset_3\n1    RH 0.07526882 -0.1467577 -0.3513514\n\n\nThis looks good. We now have a list with 2762 data-frames all with the same variables. What we would like to have instead is one single data frame. Here is a way how to merge them:\n\nsimulation_outcome &lt;- do.call(rbind, simulated_returns)\nhead(simulation_outcome, n = 5)\n\n  State    Asset_1     Asset_2     Asset_3\n1    RH 0.07526882 -0.14675768 -0.35135135\n2    BN 0.07526882  0.02389078  0.08108108\n3    BN 0.07526882  0.02389078  0.08108108\n4    BH 0.07526882  0.70648464  0.29729730\n5    BN 0.07526882  0.02389078  0.08108108\n\n\nLet me explain. Dataframes with the same variables can be merged by the function rbind(). This can be applied to an arbitrary (finite) number of dataframes. do.call() is a function which is applied to a list. Its first argument is a function (what should be done), the second argument is a list (to what should the function be applied). The call of this functions binds all the data frames stored in our list together in one single frame.\nNow let us examine our simulated output a bit further. First let us check whether the relative frequencies of states are near the values of the probabilities with which they should occur given our coin and wheel random device:\n\nrel_freq_states &lt;- prop.table(table(simulation_outcome$State))\nrel_freq_states\n\n\n        BH         BL         BN         RH         RL         RN \n0.21071687 0.10173787 0.29797248 0.14120203 0.06372194 0.18464881 \n\n\nRemember from before that our theoretical probability distribution was given by \\[\\begin{eqnarray*}\n{\\cal S} &=& \\{BH, BL, BN, RH, RL, RN\\} \\\\\np(x) &=& \\{0.2, 0.1, 0.3, 0.13, 0.07, 0.2\\}\n\\end{eqnarray*}\\]\nThis looks pretty good and illustrates what we should expect from the weak law of large numbers. We have given the probabilities by \\(p(x)\\) and a repetition of 2762 independent draws from this distribution results in relative frequencies of the individual states which are already fairly close to these theoretical probabilities.\nIn a “notebook-view”, the expected value can be seen as the long term average of a random variable in a series of a repeated random experiment. Let us check this for our data but using the portfolio weights. We add the weights for each asset and then compute \\(E(w^1 r^1), E(w^2 r^2), E(w^3 r^3)\\) from our data and compare to the theoretical expected value according to our definition.\n\nsimulation_outcome$Weight_1 &lt;- 1/4\nsimulation_outcome$Weight_2 &lt;- 3/8\nsimulation_outcome$Weight_3 &lt;- 3/8\n\nhead(simulation_outcome, n = 5)\n\n  State    Asset_1     Asset_2     Asset_3 Weight_1 Weight_2 Weight_3\n1    RH 0.07526882 -0.14675768 -0.35135135     0.25    0.375    0.375\n2    BN 0.07526882  0.02389078  0.08108108     0.25    0.375    0.375\n3    BN 0.07526882  0.02389078  0.08108108     0.25    0.375    0.375\n4    BH 0.07526882  0.70648464  0.29729730     0.25    0.375    0.375\n5    BN 0.07526882  0.02389078  0.08108108     0.25    0.375    0.375\n\n\n\nE_1 &lt;- mean(simulation_outcome$Asset_1*simulation_outcome$Weight_1)\nE_2 &lt;- mean(simulation_outcome$Asset_2*simulation_outcome$Weight_2)\nE_3 &lt;- mean(simulation_outcome$Asset_3*simulation_outcome$Weight_3)\n\nround(c(E_1, E_2, E_3), 2)\n\n[1]  0.00 -0.03 -0.04\n\n\nLet us compare this result with the theoretical expected value.\n\nprobabilities &lt;- c(0.2, 0.1, 0.3, 0.13, 0.07, 0.2)\n\ne_1 &lt;- sum(returns[, \"asset_1\"]*0.25*probabilities)\ne_2 &lt;- sum(returns[, \"asset_2\"]*0.375*probabilities)\ne_3 &lt;- sum(returns[, \"asset_3\"]*0.375*probabilities)\n\nround(c(e_1, e_2, e_3), 2)\n\n[1]  0.00 -0.03 -0.04\n\n\nThis looks good. Now Let us take this opportunity to establish a few properties of expected value. These properties are very important and you will need it over and over in applied probability. I hope that with the hands on approach of our simulation setup you will see more easily why they must hold rather than in a purely formal derivation.\n\n\n4.5.8 Properties of expected returns and portfolio expected return\nExpectation is linear: If \\(X\\) and \\(Y\\) are random variables then \\[\\begin{equation*}\nE(a\\,X + b\\,Y) = a\\,E(X) + b\\, E(Y)\n\\end{equation*}\\] for any real values of \\(a\\) and \\(b\\).\nNow look at this property from the viewpoint of our data frame. If we computed the empirical expected value by:\n\nE_1 &lt;- unique(simulation_outcome$Weight_1)*mean(simulation_outcome$Asset_1)\nE_2 &lt;- unique(simulation_outcome$Weight_2)*mean(simulation_outcome$Asset_2)\nE_3 &lt;- unique(simulation_outcome$Weight_3)*mean(simulation_outcome$Asset_3)\n\nround(c(E_1, E_2, E_3), 2)\n\n[1]  0.00 -0.03 -0.04\n\n\nWe get exactly the same value. Note that I have used the function unique()here. This is because if simulation_outcome$Weight_1 is taken out of the mean function it is a \\(2762 \\times 1\\) vector containing the same weight value for every daily observation. Unique reduces this column to a single scalar.\n****: This is an immediate consequence of the linearity of expectations, where \\(a\\) and \\(b\\) take the roles of weights.\nThis property should jump right into the eye if you think of the problem in terms of our data frame. Clearly, if you take a weighted sum over rows and then average across this sum it is the same as if you take a weighted average across each column and then sum up. Let’s do it with our data frame:\n\n# Version 1: Take expected return of each asset and then compute weighted average: \n# w^1 E(r^1) + w^2 E(r^2) + w^3 E(r^3)\nround(E_1 + E_2 + E_3, 2)\n\n[1] -0.06\n\n# Version 2: Compute E(w^1 r^1 + w^2 r^2 + w^3 r^3)\n\nmean(simulation_outcome$Weight_1*simulation_outcome$Asset_1 + simulation_outcome$Weight_2*simulation_outcome$Asset_2 + simulation_outcome$Weight_3*simulation_outcome$Asset_3) |&gt;\nround(2)\n\n[1] -0.06\n\n\nFinally, note that the expected value of a constant is the value of the constant:\nProperty 3: For a constant \\(a\\), \\(E(a) = a\\).\nWhat about the variance? Let us also use our data frame to establish some important properties.\n\n\n4.5.9 Properties of variance and portfolio variance\nProperty 1 If \\(X\\) is a random variable, \\[\\begin{equation*}\nVar(X) = E(X^2) - (E(X))^2\n\\end{equation*}\\]\nLet us check this with the random returns of asset 1:\n\n# variance of returns rounded to two digits:\n\nvar(simulation_outcome$Asset_1) |&gt; round(2)\n\n[1] 0.07\n\n# variance computed by applying Property 1:\n\n(mean(simulation_outcome$Asset_1^2) - mean(simulation_outcome$Asset_1)^2) |&gt; round(2)\n\n[1] 0.07\n\n\nProperty 2 If \\(X\\) is a random variable and \\(a\\) is a real number we have \\[\\begin{equation*}\nVar(a\\,X) = a^2 Var(X)\n\\end{equation*}\\]\nThis says intuitively that if you multiply a random variable by, say 5, then its average squared distance from the mean should increase by a factor of 25. Let us check this again with our data frame for asset 1:\n\n# Var(w^1*r^1)\nvar(simulation_outcome$Asset_1*simulation_outcome$Weight_1) |&gt; round(4)\n\n[1] 0.0043\n\n# (w^1)^2 Var(r^1)\n\n((unique(simulation_outcome$Weight_1))^2 * var(simulation_outcome$Asset_1)) |&gt; round(4)\n\n[1] 0.0043\n\n\nProperty 3 If \\(X\\) is a random variable and \\(a\\) is a constant then \\[\\begin{equation*}\nVar(X + a) = Var(X)\n\\end{equation*}\\] This should also be intuitive, because a constant has no variation so it should not contribute to the variance of a random variable. Let us check this again with our data frame:\n\n# Var(r^1 + 1000)\nvar(simulation_outcome$Asset_1 + 1000)\n\n[1] 0.06900574\n\n# Var(r^1)\nvar(simulation_outcome$Asset_1)\n\n[1] 0.06900574\n\n\nFinally we have a very important property of variance which is particularly relevant in the context of portfolios.\nProperty 4 If \\(X\\) and \\(Y\\) are random variables and \\(a\\) and \\(b\\) are real numbers \\[\\begin{equation*}\nVar(a\\,X + b\\, Y) = a^2\\, Var(X) + b^2 \\, Var(Y) + 2 ab \\, Cov(X,Y)\n\\end{equation*}\\]\nIf \\(X\\) and \\(Y\\) are independent, then \\(Cov(Y,Y) = 0\\) and in this particular case the sum of the variance is equal to the variance of the sum. Now property 4 can be generalized to \\(n\\) random variables. Let \\(r^i\\) denote random returns and \\(w^i\\) portfolio weights. In this case the formula in property 4 becomes: \\[\\begin{equation*}\nVar(\\sum_i w^i\\,r^i) = \\sum_{i=1}^n (w^i)^2 Var(r^i) +  \\sum_{i \\neq j } w^i w^j Cov(r^i, r^j)\n\\end{equation*}\\] Le us check this formula again with our data frame:\n\n# Var(\\sum_i w^i\\,r^i)\naux_1 &lt;- simulation_outcome$Weight_1*simulation_outcome$Asset_1 + simulation_outcome$Weight_2*simulation_outcome$Asset_2 + simulation_outcome$Weight_3*simulation_outcome$Asset_3\n\nvar(aux_1) |&gt; round(4)\n\n[1] 0.101\n\n# \\sum_{i=1}^n (w^i)^2 Var(r^i) +  \\sum_{i \\neq j } w^i w^j Cov(r^i, r^j)\n\nw1 &lt;- unique(simulation_outcome$Weight_1)\nw2 &lt;- unique(simulation_outcome$Weight_2)\nw3 &lt;- unique(simulation_outcome$Weight_3)\n\n\n\nVarportf &lt;- w1^2*var(simulation_outcome$Asset_1) + w2^2*var(simulation_outcome$Asset_2) + w3^2*var(simulation_outcome$Asset_3) + w1*w2*cov(simulation_outcome$Asset_1,simulation_outcome$Asset_2) + w1*w3*cov(simulation_outcome$Asset_1, simulation_outcome$Asset_3) + w2*w1*cov(simulation_outcome$Asset_1, simulation_outcome$Asset_2) + w2*w3*cov(simulation_outcome$Asset_2, simulation_outcome$Asset_3) + w3*w1*cov(simulation_outcome$Asset_1, simulation_outcome$Asset_3) +\nw3*w2*cov(simulation_outcome$Asset_2, simulation_outcome$Asset_3)\n\nVarportf |&gt; round(4)\n\n[1] 0.101\n\n\nIt works great but this notation is pretty awkward. Here is how we can do better. With more than two random variables it is convenient to use linear algebra or matrix notation and to make use of R’s capabilities to do do.\nLet me give you a brief a reminder how this works. We let \\(x = (x_1, x_2, \\cdots, x_n)\\) denote a vector of \\(n\\) random variables. When \\(x\\) is a column vector, then it’s transpose is a row vector. The usual notation is \\(x^T\\) for transposition. The covariance associated with a vector \\(x\\) is most conveniently written as a \\(m \\times n\\) matrix \\(C\\) where \\(C_{ij} = cov(x_{i}, x_{j})\\). If \\(x\\) is rewarded as a column vector and \\(x^T\\) as the corresponding row vector then \\(C\\) can be expressed as \\[\\begin{equation*}\nC = E\\left( (x - E(x))(x-E(x))^T \\right)\n\\end{equation*}\\] If \\(w\\) is a vector of portfolio weights we can write the variance of the portfolio as \\[\\begin{equation*}\nw^T C w\n\\end{equation*}\\] If you multiply a matrix by a column vector, you have to take the inner-product of the vector with each row of the matrix, like this:\n\n\n\n\n\nWhen mutilpying a column vector with a matrix take the inner product of the vector with every row of the matrix. The result will be a row vector\n\n\n\n\nLikewise if you multiply a row vector with a matrix, you have to (symmetrically) take the inner product of the row vector with each column of the matrix.\nLets apply this for the example we wrote down very tediously before. To understand this example, you need to know that in R a vector is just a special matrix, with one column (or one row) and the operator for an inner product is %*%. This is important, because if you use the * operator R will take a component-wise instead of an inner product. Finally, the R-function for transpositions is t(). Armed with this knowledge lets redo the previous example:\n\n# w^T%*%(Var(r^1)+Var(r^2)+Var(r^3))%*%w + w^T C w\n\nw1 &lt;- unique(simulation_outcome$Weight_1)\nw2 &lt;- unique(simulation_outcome$Weight_2)\nw3 &lt;- unique(simulation_outcome$Weight_3)\n\nw &lt;- matrix(c(w1,w2,w3), nrow = 3, ncol = 1)\n \nR &lt;- cbind(simulation_outcome$Asset_1, \n           simulation_outcome$Asset_2, \n           simulation_outcome$Asset_3)\n\nVp &lt;- t(w)%*% cov(R)%*%w\n\nVp |&gt; round(4)\n\n      [,1]\n[1,] 0.101\n\n\nMuch more elegant and less error prone, isn’t it?\nadd expectation and variance of functions of a RV \\(X\\).\nOur portfolio seems to be a bad deal. Over the course of the holding period it had an average daily return of \\(-0.08\\) with a standard deviation of 0.33. It has an negative return on average and it is pretty risky. We should be able to do better than that.\nHow we can combine models of random returns with considerations of optimal portfolio choice is subject of a whole body of Finance theory, which you will learn and hear about in other classes. The key words for this body of applied theory is mean variance analysis, CAPM and the Markowitz Model",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#the-binomial-distribution-and-models-of-asset-price-dynamics",
    "href": "04-lecture4.html#the-binomial-distribution-and-models-of-asset-price-dynamics",
    "title": "4  Random Variables",
    "section": "4.6 The Binomial Distribution and models of asset price dynamics",
    "text": "4.6 The Binomial Distribution and models of asset price dynamics\n\n4.6.1 The binomial distribution in applied computational Finance\nIn the last section of this lecture we discuss one particular discrete distribution: The binomial distribution. This is a discrete distribution which is particularly important in applied work and it is the foundation of one of the most important models of asset price dynamics in applied work in Finance. With respect to R, the discussion of the binomial distribution also provides an important lesson to us. The base version of R has implemented all random variables and their distributions in special functions. We learn how to use these functions. Their syntax is always the same, and thus, once you know one of them you also know the syntax for all the others.\n\n\n4.6.2 Bernoulli random variables as a building block of the binomial distribution\nThe sample space of a Bernoulli experiment has only two possible outcomes, often denoted as “success” and “failure”: \\({\\cal S} = \\{\\text{success}, \\text{failure} \\}\\). We have encountered this random variable already in form of coin flips in lectures 2, 3 and 4. The flipping of a coin would be a typical Bernoulli experiment, where the appearance of heads could be the success and tails the failure (or the other way round, this is a convention).\n\nBernoulli random variable\n\nThe bernoulli random variable takes a value of 1 or 0. The probability of success is given \\(p\\) and of failure \\(1-p\\). The notation for a bernoulli random variable is \\(X \\sim Ber(p)\\). This means: \\(X\\) is a random variable that has bernoulli distribution with success probability \\(p\\) (and thererfore failure probability \\(1-p\\)).\n\n\nThe expected value of a bernoulli random variable \\(X\\) is \\(E(X) = p\\) and its variance is \\(Var(X) = p (1-p)\\). You may try to derive this statement from the definitions of expected value and variance we introduced in this lecture.\n\nBinomial random variable\n\nThe binomial random variable generalizes the idea of a Bernoulli random variable. A binomial random variable \\(X\\) is the sum of \\(n\\) independent and identically distributed bernoulli random variables. The random variable is defined by two parameters, the total number of trials \\(n\\) and the probability of “success” \\(p\\). It is useful for caclulating the probability of getting \\(k\\) successes in \\(n\\) trials. The notation is \\(X \\sim Bin(n,p)\\).\n\n\nThe expected value of a binomial random variable \\(X\\) is \\(E(X) = n\\,p\\) and its variance is \\(Var(X) = n\\,p\\,(1-p)\\)\nTo develop a bit of intuition about bernoulli and binomial random variables, let us go back to our virtual die and consider rolling the die five times. The random variable \\(X\\) we are interested in is the number of sixes occuring in these five rolls.\nWe can figure out the probability that a six is shwon in all five rolls. We can use independence and the multiplication rule:\n\\[\\begin{equation*}\nP(X = 5) = \\left( \\frac{1}{6} \\right)^5\n\\end{equation*}\\]\n\n(1/6)^5\n\n[1] 0.0001286008\n\n\nLikewise we can figure out the probability of no sixes\n\\[\\begin{equation*}\nP(X = 0) = \\left( \\frac{5}{6} \\right)^5\n\\end{equation*}\\]\n\n(5/6)^5\n\n[1] 0.4018776\n\n\nThese cases were easy. But what about the probability of exactly one six? There are five outcomes with \\(X = 1\\), since the six can occur at the first, the second and so until at the fifth roll. Each of these cases has probability\n\\[\\begin{equation*}\nP(X = 1) = 5 \\left( \\frac{5}{6} \\right)^4 \\left( \\frac{1}{6} \\right)\n\\end{equation*}\\]\n\n5*(5/6)^4*(1/6)\n\n[1] 0.4018776\n\n\nWhat about two sixes? You can figure out that there are 10 different ways to get two sixes in five rolls. Generalizing this reasoning leads to a general formula for the probability distribution function of a binomial random variable. If \\(X\\) is a random variable representing the number of successes, then the probability of getting exactly \\(k\\) successes is:\n\\[\\begin{equation*}\nP(X = k) = \\binom{n}{k}\\, p^k \\, (1-p)^{(n-k)}\n\\end{equation*}\\]\n\\(\\binom{n}{k}\\) is the binomial coefficient and means \\(\\frac{n !}{k! (n-k)!}\\). You see that the formulas become quickly quite complicated. But here we can reap the advantages of the computer. R has a syntax for all known random variables with a parametric distribution. The syntax is always the same. Let us introduce this syntax for the binomial distribution:\n\ndbinom(x,n,p) will return the probability that \\(X\\) is equal to \\(x\\) where \\(X \\sim \\textrm{Binom}(n,p)\\). This is the distribution function for the binomial random variable.\npbinom(x,n,p) will return the probability that \\(X\\) is less or equal to \\(x\\), where \\(X \\sim \\textrm{Binom}(n,p)\\). This is the cumulative distribution function.\nrbinom(N,n,p) will return \\(N\\) randomly generated values from a binomial random variable.\nqbinom(r,n,p) will return the smallest value \\(x\\), such that \\(P(X \\leq x) \\geq r\\) where \\(X \\sim \\textrm{Binom}(n,p)\\). This is called a quantile function and we will learn about its meaning in the next lecture.\n\nLet us show an application case for these function from a context we already know. Let us take our dice rolling example. Let us take the example discussed before, the probability of seeing one six in five rolls of a dice. We solved such a problem by simulation in the previous lectures. We write a function replicate it many times and then plot the distribution we get in this way:\n\nroll_die_five &lt;- function(){\n  \n  # create a six sided die\n  die &lt;- 1:6\n  # roll the die five times\n  res &lt;- sample(die,5, replace = TRUE)\n  #count the sixes\n  count &lt;- sum((res == 6))\n}\n\nsim_rolls &lt;- replicate(10^5, roll_die_five())\n\nWe can now plot a histogram of the outcomes:\n\nlibrary(ggplot2)\nqplot(sim_rolls, binwidth = 1)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nWe could do the same experiment now with only one line of code by using the rbinom() function.\n\nsim_roll_with_rbinom &lt;- rbinom(10^5,5,1/6)\n\nNow let us compare the histogram:\n\nqplot(sim_roll_with_rbinom, binwidth = 1)\n\n\n\n\n\n\n\n\nThe probability to get 3 sixes according to the simulation (relative frequency) is:\n\nsum(sim_rolls == 3)/length(sim_rolls)\n\n[1] 0.03267\n\n\nThis could be also be compute directly by dbinom()like this:\n\ndbinom(3, 5, 1/6)\n\n[1] 0.03215021\n\n\nI think you start getting the idea. We now know more functionality which can substitute for basic approaches we built ourselves from scratch already before. However, once you go to developing code for industrial strength simulations and more realistic real world applications, speed and performance will become essential. Using built in functions whenever possible will support this goal. We will talk a bit more systematically about efficient and fast computation in the next lecture.\n\n\n4.6.3 Modeling asset price dynamics with binomial trees\nThe investment problem we analyzed in this lecture has only one period. Most real world investment problems fluctuate and can be traded frequently. They pay dividends, face fluctuating interest rates and are subject to various other uncertainties over time.\nThe binomial distribution while conceptually very simple, allows to model asset price fluctuations conveniently and realistically. In Finance there are two basic workhorse models to represent asset price dynamics: Binomial lattices and Ito processes. Both are rooted in probability but binomial lattices are conceptually simpler. Yet they are an excellent basis for computational work associated with investment problems.\nTo define a binomial lattice model, a basic period of length has to be established, such as a day or a week. According to the model, if the price is known at the beginning of a period, the price at the beginning of the next period is one of only two possible values. Usually these two possible values are defined as multiples of the price in the previous period, a multiple \\(u\\) for up and a multiple \\(d\\) for down. Both \\(u\\) and \\(d\\) are positive with \\(u &gt; 1\\) and usually \\(d &lt; 1\\).\nHence, if the price at the beginning of the period is \\(S\\), it will be either \\(uS\\) or \\(dS\\) at the next period. The probabilities of these outcomes are \\(p\\) and \\(1-p\\) respectively, for some \\(p\\) with \\(0 &lt; p &lt; 1\\). This model continues for many time steps. If the time step are short, there are many possibilities for the price at the end of the process, even though the possibilities at each time step are confined to only two.\n\n\n\n\n\nA binomial lattice with three time steps\n\n\n\n\nTo specify such a binomial lattice model, we must select values for \\(u\\) and \\(d\\) and the probability \\(p\\). These should be chosen such as to reflect the true stochastic nature of the stock price as faithfully as possible.\nSince the model is multiplicative and \\(u &gt; 0\\) and \\(d &gt; 0\\), the price will never become negative. Accordingly, we can define \\(\\nu\\) as the expected annual growth rate by \\[\\begin{equation*}\n\\nu = E\\left( \\ln(S_T/S_0) \\right)\n\\end{equation*}\\] where \\(S_0\\) is the initial stock price and \\(S_T\\) is the stock price after a year.\nLikewise we define \\(\\sigma\\) as the annual standard deviation of the growth rate \\[\\begin{equation*}\n\\sigma^2 = Var\\left( \\ln(S_T/S_0) \\right)\n\\end{equation*}\\]\nIf we choose a period of \\(\\Delta t\\), which is small compared to 1, the parameters of the binomial lattice can be selected as \\[\\begin{eqnarray*}\np&=&\\frac{1}{2} + \\frac{1}{2}\\left(\\frac{\\nu}{\\sigma} \\right) \\sqrt{\\Delta t} \\\\\nu&=&\\exp(\\sigma \\sqrt{\\Delta t}) \\\\\nd&=&\\exp(-\\sigma \\sqrt{\\Delta t})\n\\end{eqnarray*}\\] We are not able to go into the details of why this works, we can claim here only that with this choice, the binomial model will closely match the value of \\(\\nu\\) and \\(\\sigma\\). The closeness of the match improves if \\(\\Delta t\\) is made smaller and smaller becoming exact as \\(\\Delta t\\) goes to zero.\nNow let us check this using our R-tools learned so far. Take again the S&P 500 as our test data set. Remember our SP500 data we used before:\n\nhead(sp500, n=5)\n\n# A tibble: 5 × 9\n  symbol date        open  high   low close     volume adjusted  returns\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 ^GSPC  2011-01-03 1258. 1276. 1258. 1272. 4286670000    1272. NA      \n2 ^GSPC  2011-01-04 1273. 1274. 1263. 1270. 4796420000    1270. -0.00131\n3 ^GSPC  2011-01-05 1269. 1278. 1265. 1277. 4764920000    1277.  0.00501\n4 ^GSPC  2011-01-06 1276. 1278. 1270. 1274. 4844100000    1274. -0.00212\n5 ^GSPC  2011-01-07 1274. 1277. 1262. 1272. 4963110000    1272. -0.00184\n\n\nWe first need the annual returns. In order to do so we automatically extract the first price in each year.\nThis can be done in many ways. Here is a solution, that works: We first extract the year from the data column and add a new variable to the sp500 data frame. For this we use a function from the lubridate package. This function is called year() and if applied to a date object extracts the year only. Then we split the data frame according to the year using R’s split() function. This will create a list of data frames of daily records, where each list element contains the records of one year. We then write a little function, which takes a data frame as argument and selects the first row. Finally we apply this function to each element of the list using R’s function lapply(). Finally we bind all observations together with the do.call() function. This is admittedly a piece of advanced R acrobatics, so if this sounds weird or difficult to comprehend, don’t worry.\n\n# add a column with the year using year function from lubridate package\nsp500$year &lt;- lubridate::year(sp500$date)\n# split dataframe according to year\nsplit_by_year &lt;- split(sp500, sp500$year)\n# write a function selecting the first row of a given data frame\nsel &lt;- function(x){x[1,]}\n# apply this function to each element of the list.\naux &lt;- lapply(split_by_year, sel)\n# recombine the list entries into a data frame but drop the returns column\nsp500_annual &lt;- do.call(rbind, aux) |&gt; subset(select = -returns)\n\nhead(sp500_annual, n = 3)\n\n# A tibble: 3 × 9\n  symbol date        open  high   low close     volume adjusted  year\n  &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 ^GSPC  2011-01-03 1258. 1276. 1258. 1272. 4286670000    1272.  2011\n2 ^GSPC  2012-01-03 1259. 1285. 1259. 1277. 3943710000    1277.  2012\n3 ^GSPC  2013-01-02 1426. 1462. 1426. 1462. 4202600000    1462.  2013\n\n\nNow we can compute annual log returns:\n\nsp500_annual$returns &lt;- c(NA, diff(log(sp500_annual$adjusted), lag = 1))\n\nNow we are ready to compute \\(\\nu\\) and \\(\\sigma^2\\).\n\nnu &lt;- mean(sp500_annual$returns, na.rm = T)\nsigma &lt;- sqrt(var(sp500_annual$returns, na.rm = T))\nnu\n\n[1] 0.106802\n\nsigma\n\n[1] 0.1070446\n\n\nNow we wish to make a binomial price model of the sp500 based on these data for a daily period. Assume the year has 250 trading days. Then we get:\n\nu &lt;- exp(sigma*sqrt(1/250))\nd &lt;- exp(- sigma*sqrt(1/250))\np &lt;- 1/2*(1 +(nu/sigma)*sqrt(1/250))\n\nu\n\n[1] 1.006793\n\nd\n\n[1] 0.9932528\n\np\n\n[1] 0.5315511\n\n\nWe can now generate a binomial tree by a function:\n\nbinomial_tree &lt;- function(S0, up, down, steps){\n  \n  # initialize tree as a matrix\n  \n  tree &lt;- matrix(0, nrow = steps+1, ncol = steps + 1)\n  \n  # fill up tree in a double for-loop\n  \n  for(i in 1:(steps + 1)){\n    for(j in (1:i)){\n      \n      tree[i,j] &lt;- S0*up^(j -1)*down^((i-1)-(j-1))\n      \n    }\n    \n  }\n\n  return(tree)  \n  \n}\n\nLets generate a tree with three steps:\n\nbinomial_tree(S0=100, up = u, down = d, steps = 3)\n\n          [,1]      [,2]     [,3]     [,4]\n[1,] 100.00000   0.00000   0.0000   0.0000\n[2,]  99.32528 100.67931   0.0000   0.0000\n[3,]  98.65511 100.00000 101.3632   0.0000\n[4,]  97.98946  99.32528 100.6793 102.0518\n\n\nHow is this related to the binomial distribution? The random variable, which we are studying is a random variable \\(X\\) with \\(P(X = u) = p\\) and \\(P(X = d) = 1-p\\). The stock price \\(S\\) follows the process \\[\\begin{equation*}\nS_{n+1} = S_n X_{n+1} \\quad \\text{for}\\,\\, n \\geq 0\n\\end{equation*}\\] Thus given the value of \\(S_n\\) \\[\\begin{equation*}\nS_{n+1} =\n\\begin{cases}\nu\\,S_n \\, \\mbox{with probability} \\,\\,p \\\\\nd\\, S_n \\, \\mbox{with probability}\\,\\, (1-p)\n\\end{cases}\n\\mbox{for}\\,\\, n \\geq 0\n\\end{equation*}\\] independent of the past. Thus in each time period the stock price goes either up or down and the randomness comes from identical independently distributed bernoulli random variables.\nIf we expand the recursion for the stock price we get \\[\\begin{equation*}\nS_n = S_0 \\times X_1 \\times \\cdots \\times X_n, \\,\\,\\, n\\geq 0\n\\end{equation*}\\] From this expansion we can conclude that for a given \\(n\\) \\[\\begin{equation*}\nS_n = u^i d^{(n-i)} S_0 \\,\\,\\, \\mbox{for some} \\,\\,\\, i \\in \\{1, \\cdots, n\\}\n\\end{equation*}\\] meaning that the stock price went up \\(i\\) times and down \\((n-i)\\) times with the corresponding probabilities determined by \\[\\begin{equation*}\nP(S_n = u^i d^{(n-i)} S_0) = \\binom{n}{i}p^i\\,(1-p)^{(n-i)},\\,\\,\\, 0\\leq i \\leq n\n\\end{equation*}\\]\nLet us use these insights and what we learned about handling of binomial random variables in R by simulating an asset price process for the sp500 for a year - i.e. for 250 daily time steps and plot the probability mass function of the resulting price distribution.\n\n# Simulate 250 time steps of the asset price process starting with S_0 = 100. \ntree &lt;- binomial_tree(S0=100, up = u, down = d, steps = 250) \n# Select\n# the last row of the resulting matrix representing the final outcomes after 250 time steps.  \noutcomes &lt;- (tree[dim(tree)[1], ])\nsuccess &lt;- 0:250\n\n# plot the probability distribution of outcomes\n\nplot(outcomes, dbinom(success, size = 250, prob = p), type = \"h\")\nabline(v=sum(outcomes*dbinom(success, size = 250, prob = p)),col=\"blue\",lwd=3)\nabline(v = 110.6802, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nThe blue vertical line shows the expected value of the asset value after a year, given the simulated asset price process. The red line shows the expected value, of an asset multiplied by the expected annual return, we have extracted from the data and which we have used to calibrate our binomial pricing model.\nYou will encounter this model of asset price dynamics often in your career in Finance, in particular, when it comes to pricing derivatives.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#project-credit-risk-model-and-independence",
    "href": "04-lecture4.html#project-credit-risk-model-and-independence",
    "title": "4  Random Variables",
    "section": "4.7 Project: Credit risk model and independence",
    "text": "4.7 Project: Credit risk model and independence\nIn this project you will study the probability of credit risk, the risk that one or more creditors in a portfolio of loans will default. We have already touched a bit of this problem in our discussion of the role of the assumptions about dependence and independence of risks in structured finance. Clearly in banking, credit risk and its analysis is the key issue, since banking is basically about borrowing and lending of money. In this project you will discover that the probability concepts we have covered in this lecture and in the previous lectures allow you to already go very far towards a deeper understanding of credit risk.\n\nLet us now make use of your knowledge of R and probability to make a first step towards modelling credit default and portfolio losses. We will take a common and time proven strategy in problem solving, when we face a very complex and difficult situation like this. It is usually a good idea to think about a similar but simpler situation first, and then build from there. Let me help you with the simplification. Let us make the following assumptions. We fix a time horizon \\(T\\), in credit risk this would be something like one year - for example - and in addition we assume:\n\n\nthat our portfolio consists of \\(N\\) identical borrowers. In credit risk lingo these borrowers are also often referred to as obligors.\nAll obligors have an exposure of size \\(L\\) (some Euro amount for instance) and when one obligor defaults, your bank for which you risk manage the loan portfolio, will be able to recover a fraction \\(c\\), called the recovery rate.\nAll defaults over \\(T\\) happen independently with the same probability of default \\(p\\). Already now we know that this is a simplification which puts us far away from reality. Still it is useful for us to build a step by step understanding of the problem. The key point is not to forget about this simplification and how it can be dropped later when we can build already on some of our experience with the simpler problem. This is by the way quite a common and good strategy both to solve analytical as well as programming problems. Here we develop a bit of practice with this habit.\n\nUse R and your knowledge about the binomial distribution to formulate a model for a random variable \\(X\\) describing the number of defaults that occur until \\(T\\) and produce a plot of the probability distribution of losses from default assuming a portfolio size \\(N\\) of 100 and individual default probabilities \\(p = 5 \\%\\).\n\nNow let us think a bit what is needed, if we want to get away from our oversimplified framework we have so far and take a step towards more realism. For such reflections it is often a good idea to drop superfluous structure for the question at hand. This is also a time proven strategy in problem solving. Ask yourself, what is the minimal setup I need to make my point. This is a question of the right amount of simplification, where the goal is to be as simple as possible but not to oversimplify. Let me help you a bit with this tactics: Assume now you have only two instead of \\(N\\) obligors in your portfolio. Let us call them borrower \\(A\\) and borrower \\(B\\). You would like to think about the risk they could default on their obligations over a time horizon \\(T\\), say 1 year from now but you can not assume independence. Your staff has produced intelligence from a detailed data analysis and has provided you with the the numbers \\(p_A\\), the probability that the first borrower will default within the time horizon \\(T\\) as well as \\(p_B\\), the probability that the second borrower will default over the horizon \\(T\\). Would it help you towards more realism, if you had this information? Would it help you to find out\n\n\nThe probability that \\(A\\) and \\(B\\) will default before \\(T\\), denoted \\(P_{AB}\\) (joint probability of default). Note that this would entail the knowledge of the probability that \\(A\\) defaults before \\(T\\) given that \\(B\\) has defaulted before \\(T\\), or the probability that \\(B\\) has defaults given that \\(A\\) has defaulted. Let us denote these conditional probabilities by \\(p_{A|B}\\) and \\(p_{B|A}\\).\nThe correlation coefficient \\(\\rho_{AB}\\) between the event that \\(A\\) defaults and \\(B\\) defaults.\n\n\nGoing back to your credit risk model, what does it tell you about extreme cases? What is the probability that all \\(N\\) obligors default? What is the probability that none defaults? Note that these extreme situations are still compatible with the assumption that each obligor defaults with probability \\(5 \\%\\). Can you recover any information about the likelihood of joint defaults in these two extreme situations?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#random-variables-and-distributions",
    "href": "04-lecture4.html#random-variables-and-distributions",
    "title": "4  Random Variables",
    "section": "",
    "text": "Definition: Random Variable\n\n\n\nA random variable \\(X\\) is a function \\(X:\\mathcal{S} \\to \\mathbb{R}\\) from the sample space (the set of all possible outcomes of the random experiment) to the real numbers.\n\n\n\n\nIn the coin-flipping example from Lecture 2, the number of heads in 10 flips was a random variable.\nIn the investment game from Lecture 3, the outcome of each coin flip (profit or loss) was implicitly treated as a random variable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#probability-distributions",
    "href": "04-lecture4.html#probability-distributions",
    "title": "4  Random Variables",
    "section": "4.2 Probability Distributions",
    "text": "4.2 Probability Distributions\nA random variable is characterized by its probability distribution, which describes how probabilities are assigned to its possible values.\n\n\n\n\n\n\nDefinition: Probability Mass Function of a Discrete Random Variable\n\n\n\nThe probability distribution of a discrete random variable is described by its probability mass function (PMF): \\[\\begin{equation*}\nP(X = x) = p(x)\n\\end{equation*}\\] where \\(p(x)\\) satisfies:\n\n\\(0 \\leq p(x) \\leq 1\\) for all \\(x\\).\n\\(\\sum_{x} p(x) = 1\\).\n\n\n\nWe can visualize a PMF. Consider a random variable \\(X\\) that represents the outcome of rolling a fair six-sided die. The PMF is: \\(P(X = x) = \\frac{1}{6}, \\quad x = 1, 2, 3, 4, 5, 6\\).\nThis can be visualized as a bar plot:\n\nx &lt;- 1:6\nprob &lt;- rep(1/6, 6)\nbarplot(prob, names.arg = x, col = \"blue\", \n        main = \"Probability Mass Function\", \n        xlab = \"Outcome\", \n        ylab = \"Probability\")\n\n\n\n\n\n\n\n\nThis plot shows that each outcome has an equal probability of \\(\\frac{1}{6}\\) symbolized as a bar. The geometry of this bar has no meaning. The bars could be thinner or thicker while displaying the same information.\nLet me discuss some common confusions that arise often with understanding the cocnept of a random variable.\n\nRandom Variable vs. Outcome: A random variable is not the same as an individual outcome. It is a function that assigns values to outcomes. The confusion is partially created by the name. Maybe a better term would be a random mapping. Anyway, keep in mind that a random variable is a function.\nProbability Distribution vs. Histogram: A probability distribution represents theoretical probabilities. Don’t mix this concept up with the concept of a histogram, known from statistics and data analysis, which shows frequencies of empirical data.\nDiscrete vs. Continuous: Discrete variables take specific values (e.g., dice outcomes), while continuous variables can take any value in a range. Dealing with continuous variables needs specifc tools which we discuss in lecture 5.\n\n\n\n\n\n\n\nNow You Try: Other Examples\n\n\n\nThink of scenarios where you could define a discrete random variable.\nFor instance:\n\nThe number of defective products in a batch of 20.\nThe number of rainy days in a week.\n\nSimulate and visualize the PMF of these examples in R. Use the barplot() function to create intuitive visualizations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#probability-distributions-and-cumulative-probability-distributions",
    "href": "04-lecture4.html#probability-distributions-and-cumulative-probability-distributions",
    "title": "4  Random Variables",
    "section": "4.2 Probability Distributions and Cumulative Probability Distributions",
    "text": "4.2 Probability Distributions and Cumulative Probability Distributions\nA random variable is characterized by its probability distribution, which describes how probabilities are assigned to its possible values.\n\n\n\n\n\n\nDefinition: Probability Mass Function of a Discrete Random Variable\n\n\n\nThe probability distribution of a discrete random variable is described by its probability mass function (PMF): \\[\\begin{equation*}\nP(X = x) = p(x)\n\\end{equation*}\\] where \\(p(x)\\) satisfies:\n\n\\(0 \\leq p(x) \\leq 1\\) for all \\(x\\).\n\\(\\sum_{x} p(x) = 1\\).\n\n\n\nWe can visualize a PMF. Consider a random variable \\(X\\) that represents the outcome of rolling a fair six-sided die. The PMF is: \\(P(X = x) = \\frac{1}{6}, \\quad x = 1, 2, 3, 4, 5, 6\\).\nThis can be visualized as a bar plot:\n\nx &lt;- 1:6\nprob &lt;- rep(1/6, 6)\nbarplot(prob, names.arg = x, col = \"blue\", \n        main = \"Probability Mass Function\", \n        xlab = \"Outcome\", \n        ylab = \"Probability\")\n\n\n\n\n\n\n\n\nThis plot shows that each outcome has an equal probability of \\(\\frac{1}{6}\\) symbolized as a bar. The width of these bars has no meaning. The bars could be thinner or thicker while displaying the same information.\nLet me discuss some common confusions that arise often with understanding the concept of a random variable.\n\nRandom Variable vs. Outcome: A random variable is not the same as an individual outcome. It is a function that assigns values to outcomes. The confusion is partially created by the name. Maybe a better term would be a random mapping. Anyway, keep in mind that a random variable is a function.\nProbability Distribution vs. Histogram: A probability distribution represents theoretical probabilities. Don’t mix this concept up with the concept of a histogram, known from statistics and data analysis, which shows frequencies of empirical data.\nDiscrete vs. Continuous: Discrete variables take specific values (e.g., dice outcomes), while continuous variables can take any value in a range. Dealing with continuous variables needs specifc tools which we discuss in lecture 5.\n\n\n\n\n\n\n\nNow You Try: Other Examples\n\n\n\nThink of scenarios where you could define a discrete random variable.\nFor instance:\n\nThe number of defective products in a batch of 20.\nThe number of rainy days in a week.\n\nSimulate and visualize the PMF of these examples in R. Use the barplot() function to create intuitive visualizations.\n\n\nA related concept to the probability mass function (PMF), is the cumulative distribution function (CDF). It can also be used to describe a discrete random variable. The CDF provides the probability that the random variable \\(X\\) takes a value less than or equal to a specific value \\(x\\):\n\n\n\n\n\n\nDefinition: Cumulative Distribution Function (CDF)\n\n\n\nThe cumulative distribution function (CDF) of a discrete random variable \\(X\\) is defined as: \\[F(x) = P(X \\leq x) = \\sum_{t \\leq x} P(X = t),\\]\nwhere \\(F(x)\\) is non-decreasing and satisfies:\n\n\\(F(x) \\in [0, 1]\\) for all \\(x\\).\n\\(\\lim_{x \\to -\\infty} F(x) = 0\\) and \\(\\lim_{x \\to \\infty} F(x) = 1\\).\n\n\n\nConsider again the random variable \\(X\\) representing the outcome of rolling a fair six-sided die. The PMF is: \\[P(X = x) = \\frac{1}{6}, \\quad x = 1, 2, 3, 4, 5, 6.\\]\nThe CDF, \\(F(x)\\), can be computed as:\n\n\\(F(1) = P(X \\leq 1) = P(X = 1) = \\frac{1}{6}\\),\n\\(F(2) = P(X \\leq 2) = P(X = 1) + P(X = 2) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6}\\),\n\\(F(3) = P(X \\leq 3) = \\frac{3}{6}\\),\n…\n\\(F(6) = P(X \\leq 6) = \\frac{6}{6} = 1\\).\n\nThe CDF can be visualized as a step function, showing the cumulative probabilities:\n\nx &lt;- 1:6\ncdf &lt;- cumsum(rep(1/6, 6))\nplot(x, cdf, type = \"s\", col = \"blue\", lwd = 2, \n     main = \"Cumulative Distribution Function\", \n     xlab = \"Outcome\", \n     ylab = \"F(x)\")\n\n\n\n\n\n\n\n\nHere are some key points to keep in mind when working with cumulative distribution functions (CDFs):\n\nThe CDF gives cumulative probabilities (\\(P(X \\leq x)\\)), not individual probabilities like the PMF.\nThe CDF is always non-decreasing and reaches 1 for the largest possible value of \\(X\\).\nWhile the CDF for discrete variables has a step-like shape, it can also be smooth for continuous variables.\n\nIf you’re ever unsure, visualize the CDF to better understand how probabilities accumulate!\nThe concepts of a probability mass function (PMF) and cumulative distribution function (CDF) can be extended to more than one random variable. When working with multiple random variables, we are often interested in their joint distribution, which describes how they behave together, and conditional distributions, which describe how one variable behaves given specific information about another. I discuss here the generalisation to two to avoid the tedious and sometimes confusing index notation that would be needed when we go beyond two dimensions. Two dimensions can be furthermore visualized helping our intuition. All of the follwing generalizes to more than two random variables.\n\n\n\n\n\n\nJoint Probability Mass Function\n\n\n\nThe joint PMF of two discrete random variables \\(X\\) and \\(Y\\) is defined as: \\[P(X = x, Y = y) = p(x, y),\\] where \\(p(x, y)\\) satisfies:\n\n\\(0 \\leq p(x, y) \\leq 1\\) for all \\(x, y\\),\n\\(\\sum_{x} \\sum_{y} p(x, y) = 1\\).\n\n\n\nLet me give an example: Imagine rolling two fair six-sided dice. Let \\(X\\) and \\(Y\\) represent the outcomes of the first and second dice, respectively. The joint PMF is: \\[P(X = x, Y = y) = \\frac{1}{36}, \\quad x, y = 1, 2, 3, 4, 5, 6.\\]\nThis joint PMF captures the probability of every possible pair of outcomes, such as \\((X = 2, Y = 5)\\).\nA visualization tool for two dimensional PMFs is the heatmap. The \\(x\\) and \\(y\\) axis of the heat map symbolized the values of \\(X\\) and \\(Y\\) whereas the third dimension visualizes the probability of each pair of values \\((X,Y)\\) by a colur code. So, for examplem when the dice are fair we should have a probability of \\(1/36\\) for each basic outcome, so you should see only one uniform color. To illustrate this visualization concept, imagine an example where the dice are biased:\n\n# Define the outcomes and probabilities for two biased dice\noutcomes &lt;- 1:6\nprob_die1 &lt;- c(0.05, 0.1, 0.2, 0.25, 0.2, 0.2)  # Probabilities for die 1\nprob_die2 &lt;- c(0.1, 0.15, 0.2, 0.25, 0.2, 0.1)  # Probabilities for die 2\n\n# Compute the joint PMF as the outer product of the two probability vectors\njoint_pmf &lt;- outer(prob_die1, prob_die2)\n\n# Create a heatmap using the image() function\nimage(\n  1:6, 1:6, joint_pmf,\n  col = colorRampPalette(c(\"white\", \"blue\"))(100),\n  xlab = \"Outcome of Die 2\",\n  ylab = \"Outcome of Die 1\",\n  main = \"Heatmap of Joint PMF (Two Biased Dice)\",\n  axes = FALSE\n)\n\n# Add axis labels\naxis(1, at = 1:6, labels = outcomes)\naxis(2, at = 1:6, labels = outcomes)\n\n# Add a color legend\nlegend(\n  \"topright\", \n  legend = round(seq(min(joint_pmf), max(joint_pmf), length.out = 5), 3),\n  fill = colorRampPalette(c(\"white\", \"blue\"))(5),\n  title = \"Probability\"\n)\n\n\n\n\n\n\n\n\nHere you see immediately that something must be going on with the dice. Would the dice be fair there should be a uniform color all over the heatmap with a color at the value of \\(1/36\\) or \\(0.03\\). Try it!\nIn a similar way as in the case of the PMF we can define:\n\n\n\n\n\n\nJoint Cumulative Distribution Function\n\n\n\nThe joint CDF of two random variables \\(X\\) and \\(Y\\) is defined as: \\[F(x, y) = P(X \\leq x, Y \\leq y).\\]\nThis function provides the cumulative probability for pairs of random variables being less than or equal to specified values.\n\n\nFinally when we have more than one random variable we encounter conditional distributions.\n\n\n\n\n\n\nConditional PMF\n\n\n\nWhen we have additional information about one random variable, we can refine our understanding of the other through a conditional distribution.\nThe conditional PMF of \\(X\\) given \\(Y = y\\) is:\n\\[P(X = x \\mid Y = y) = \\frac{P(X = x, Y = y)}{P(Y = y)},\\] provided \\(P(Y = y) &gt; 0\\).\n\n\nCan joint PMFs include dependent variables? Yes, joint PMFs can include dependent variables. In fact, joint PMFs are a powerful tool for describing the relationship between two (or more) discrete random variables, whether they are independent or dependent.\n\n\n\n\n\n\nIndependent Random Variables\n\n\n\nFor independent random variables \\(X\\) and \\(Y\\), the joint PMF satisfies: \\(P(X = x, Y = y) = P(X = x) \\cdot P(Y = y)\\)\n\n\nThe probabilities of \\(X\\) and \\(Y\\) are completely separate and don’t influence each other. Example: Rolling two fair dice where the outcome of one die does not influence the other.\nFor dependent random variables, the joint PMF does not satisfy the independence condition. Instead, the joint PMF captures the interaction between \\(X\\) and \\(Y\\). For example: \\(P(X = x, Y = y) \\neq P(X = x) \\cdot P(Y = y)\\).\nThe probabilities of \\(X\\) and \\(Y\\) are intertwined, reflecting their dependency. A classical example is the drawing two cards from a deck without replacement. Let \\(X\\) be the value of the first card, and \\(Y\\) the value of the second card. Once the first card is drawn, the probabilities for \\(Y\\) change because there is one less card in the deck. An example nearer to finance is the case of a portfolio of financial assets each with risky payoff. You can then model each asset as a random variable. In a portfolio the different assets will in general not be independent. Common factors, such as general market sentiment, macroeconomic conditions, dependecne in production through supply chains etc., may drive the uncertainty of several assets simultaneously.\nThe dependency between (X) and (Y) can also be described using conditional probabilities:\n\n\n\n\n\n\nDependent Random Variables\n\n\n\nFor dependent random variables \\(X\\) and \\(Y\\) the joint PMF satisfies \\[\\begin{equation*}\nP(X = x, Y = y) = P(X = x \\mid Y = y) \\cdot P(Y = y).\n\\end{equation*}\\]\n\n\nThis representation emphasizes how the probability of \\(X\\) depends on \\(Y\\). For instance, in the dice example, if one die is biased to always match the other, \\(P(X = x \\mid Y = y) = 1\\) when \\(x = y\\).\n\n# Define a joint PMF for dependent dice: one die matches the other\noutcomes &lt;- 1:6\njoint_pmf &lt;- diag(1/6, 6, 6)  # Probability of matching outcomes is 1/6 for each pair\n\n# Create heatmap for the dependent joint PMF\nimage(\n  1:6, 1:6, joint_pmf,\n  col = colorRampPalette(c(\"white\", \"blue\"))(100),\n  xlab = \"Outcome of Die 2\",\n  ylab = \"Outcome of Die 1\",\n  main = \"Heatmap of Joint PMF (Dependent Dice)\",\n  axes = FALSE\n)\naxis(1, at = 1:6, labels = outcomes)\naxis(2, at = 1:6, labels = outcomes)\n\n# Add a color legend\nlegend(\n  \"topright\",\n  legend = round(seq(min(joint_pmf), max(joint_pmf), length.out = 5), 3),\n  fill = colorRampPalette(c(\"white\", \"blue\"))(5),\n  title = \"Probability\"\n)\n\n\n\n\n\n\n\n\nKeep in mind:\n\nJoint Distributions describe how two random variables behave together.\nConditional Distributions refine our understanding of one variable based on information about another.\nJoint PMFs naturally generalize to describe both independent and dependent random variables. For dependent variables, the joint PMF captures the interaction and dependencies between the variables, often requiring conditional probabilities to explain the relationships.\nThese concepts are foundational for understanding dependencies in financial modeling, such as asset correlations or portfolio risk.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#expected-value-variance-covariance",
    "href": "04-lecture4.html#expected-value-variance-covariance",
    "title": "4  Random Variables",
    "section": "4.3 Expected Value, Variance, Covariance",
    "text": "4.3 Expected Value, Variance, Covariance\nWhen working with random variables, we often need to summarize their behavior in a way that allows us to make decisions, compare outcomes, or model uncertainty. This is where the concepts of expected value, variance, and covariance come into play:\n\nExpected value provides a measure of the central tendency of a random variable, giving us a “weighted average” that reflects the most likely or typical outcome over the long run. It allows us to predict the average result in situations with uncertainty.\nVariance and its square root, the standard error, quantify the spread or variability of a random variable’s outcomes. These measures are crucial for understanding risk in contexts like finance, where variability often corresponds to uncertainty or volatility.\nCovariance extends this idea to pairs of random variables, capturing how changes in one variable are related to changes in another. This is particularly important in portfolio theory, where understanding how asset returns move together enables better diversification and risk management.\n\nTogether, these tools provide a mathematical framework for understanding, summarizing, and modeling uncertainty, making them indispensable in fields ranging from finance to engineering to data science.\nSo let us dive into the formal definitions.\n\n4.3.1 Expected Value\nThe expected value of a random variable is a summary measure that captures the central tendency of its distribution. It represents the long-run average value of the variable if the experiment is repeated many times.\n\n\n\n\n\n\nDefinition: Expected Value\n\n\n\nThe expected value \\(\\mathrm{E}(X)\\) of a discrete random variable \\(X\\) is the probability-weighted sum of all its possible values: \\[\n\\mathrm{E}(X) = \\sum_{i=1}^{n} x_i p(x_i),\n\\] where \\(x_i\\) are the possible values of \\(X\\) and \\(p(x_i)\\) are their associated probabilities.\n\n\nNote that for countably infinite outcomes, convergence conditions are required, but we focus on finite cases here.\nAs an example, consider a fair six-sided die. The possible outcomes are \\(\\{1, 2, 3, 4, 5, 6\\}\\), each with probability \\(1/6\\). The expected value is:\n\n# Expected value of a fair die\n\noutcomes &lt;- 1:6\n\nprobabilities &lt;- rep(1/6, 6)\n\nexpected_value &lt;- sum(outcomes * probabilities)\n\nexpected_value # output\n\n[1] 3.5\n\n\nAlthough the expected value is 3.5, this value will never actually occur in a single roll—it is a theoretical average that represents the long-term central tendency of the outcomes if the die is rolled repeatedly. In other words, the expected value is not necessarily a possible outcome but rather a weighted average that balances all possible outcomes according to their probabilities. This highlights a common misunderstanding: the expected value is not a prediction of what will happen in a single trial, but a summary of what we can expect over many repetitions of the experiment.\n\n\n4.3.2 Variance and Standard Error\nWhile the expected value provides the central tendency, the variance measures the spread of a random variable around its expected value.\n\n\n\n\n\n\nDefinition: Variance\n\n\n\nThe variance of \\(X\\) is the expected value of the squared deviations from its expected value: \\[\n\\mathrm{Var}(X) = \\mathrm{E}[(X - \\mathrm{E}(X))^2].\n\\]\n\n\nFor discrete random variables, this can be computed as: \\[\n\\mathrm{Var}(X) = \\sum_{i=1}^{n} \\left(x_i - \\mathrm{E}(X)\\right)^2 p(x_i).\n\\]\nUsing algebra, variance can also be written as: \\[\n\\mathrm{Var}(X) = \\mathrm{E}[X^2] - (\\mathrm{E}[X])^2.\n\\]\nThe standard error is the square root of the variance: \\[\n\\mathrm{SE}(X) = \\sqrt{\\mathrm{Var}(X)}.\n\\] What is the advantage to think in terms of standard errors rather than in terms of variance itself, when we think about the variability of a random variable? To see why, suppose \\(X\\) represents the daily high temperature in a specific city during the month of July, measured in degrees Celsius (\\(^\\circ C\\)). For simplicity, let’s assume \\(X\\) is modeled as a random variable with the following characteristics based on historical data:\n\nMean (Expected Value):\nThe average high temperature in July is \\(\\mathbb{E}(X) = 25 \\,^\\circ C\\).\nVariance:\nThe variance of daily high temperatures is: \\[\\mathrm{Var}(X) = 9 \\, (\\text{^\\circ C}^2)\\].\nWhile mathematically meaningful, the variance is expressed in squared degrees Celsius \\(^\\circ C^2\\), which is unintuitive. People don’t typically think about “squared temperature.”\nStandard Error:\nTaking the square root of the variance gives: \\[\\mathrm{SE}(X) = \\sqrt{\\mathrm{Var}(X)} = \\sqrt{9} = 3 \\,^\\circ C.\\] A variance of \\(9 \\,^\\circ C^2\\) is difficult to relate to because we don’t experience temperature in squared units. A standard error of \\(3 \\,^\\circ C\\), however, directly informs us that the daily high temperature typically deviates from the average (\\(25 \\,^\\circ C\\)) by about \\(3 \\,^\\circ C\\).\n\n\n4.3.2.1 Example\nUsing the fair die example, calculate the variance and standard error:\n\n# Variance and standard deviation of a fair die\nE_X_squared &lt;- sum(outcomes^2 * probabilities)\nvariance &lt;- E_X_squared - (expected_value^2)\nstd_dev &lt;- sqrt(variance)\n\nvariance  # Output: 2.916667\n\n[1] 2.916667\n\nstd_dev   # Output: 1.707825\n\n[1] 1.707825\n\n\n\n\n\n4.3.3 Covariance\nThe covariance measures the linear relationship between two random variables.\n\n\n\n\n\n\nDefinition: Covariance\n\n\n\nFor two random variables \\(X\\) and \\(Y\\) their covariance is defined as It is defined as: \\[\n\\mathrm{Cov}(X, Y) = \\mathrm{E}[(X - \\mathrm{E}(X))(Y - \\mathrm{E}(Y))].\n\\]\n\n\nAlternatively: \\[\n\\mathrm{Cov}(X, Y) = \\mathrm{E}[XY] - \\mathrm{E}[X]\\mathrm{E}[Y].\n\\] If \\(X\\) and \\(Y\\) are independent, then \\(\\mathrm{Cov}(X, Y) = 0\\). However, the converse is not true: a covariance of 0 does not imply independence.\nExample: Consider two dice, \\(X\\) and \\(Y\\), rolled simultaneously. Let \\(X\\) represent the outcome of the first die and \\(Y\\) the second. If the dice are independent, the covariance is 0:\n\n# Covariance of two independent dice\njoint_pmf &lt;- outer(probabilities, probabilities)\nE_XY &lt;- sum(outcomes %*% t(outcomes) * joint_pmf)\ncovariance &lt;- E_XY - (expected_value^2)\ncovariance  # Output: 0\n\n[1] 0\n\n\nNow, consider a scenario where \\(X\\) and \\(Y\\) are dependent but still have a covariance of 0. Let \\(X\\) represent a random variable that takes the values \\(\\{-1, 0, 1\\}\\), and let \\(Y\\) be defined as \\(Y = X^2\\). The joint distribution of \\((X, Y)\\) is as follows:\n\n\n\n\\(X\\)\n\\(Y\\)\nProbability\n\n\n\n\n-1\n1\n\\(1/3\\)\n\n\n0\n0\n\\(1/3\\)\n\n\n1\n1\n\\(1/3\\)\n\n\n\nThe random variables \\(X\\) and \\(Y\\) are clearly dependent because \\(Y\\) is fully determined by \\(X\\). However, their covariance is 0. Check it!1\n1 \\(\\mathbb{E}(X) = (-1) \\cdot \\frac{1}{3} + 0 \\cdot \\frac{1}{3} + 1 \\cdot \\frac{1}{3} = 0\\) \\(\\mathbb{E}(Y) = 1 \\cdot \\frac{1}{3} + 0 \\cdot \\frac{1}{3} + 1 \\cdot \\frac{1}{3} = \\frac{2}{3}\\) \\(\\mathbb{E}(XY) = (-1)(1) \\cdot \\frac{1}{3} + (0)(0) \\cdot \\frac{1}{3} + (1)(1) \\cdot \\frac{1}{3} = 0\\) \\(\\mathrm{Cov}(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y) = 0 - (0 \\cdot \\frac{2}{3}) = 0\\) Thus, \\(\\mathrm{Cov}(X, Y) = 0\\), even though \\(X\\) and \\(Y\\) are not independentThis example highlights that while independence guarantees a covariance of 0, the reverse is not true. The correlation coefficient normalizes the covariance to a dimensionless measure: \\[\n\\rho(X, Y) = \\frac{\\mathrm{Cov}(X, Y)}{\\mathrm{SD}(X) \\cdot \\mathrm{SD}(Y)}.\n\\] The correlation coefficient ranges from \\(-1\\) (perfect negative correlation) to \\(1\\) (perfect positive correlation).\nWhat does it mean if covariance is called a linear measure of dependence? Let me explain: The covariance measures the degree to which two random variables change together, focusing specifically on their linear relationship. But what does it mean for the relationship to be linear, and what happens if the relationship is not linear?\nA linear relationship implies that one random variable can be expressed approximately as a linear function of the other:\n\\[Y = aX + b\\]\nwhere \\(a\\) and \\(b\\) are constants. If two variables are positively linearly related, an increase in one variable tends to be associated with a proportional increase in the other. Conversely, if they are negatively linearly related, an increase in one variable tends to correspond to a proportional decrease in the other.\nCovariance captures this pattern by examining how the deviations of \\(X\\) and \\(Y\\) from their respective means \\((X - \\mathbb{E}[X])\\) and \\((Y - \\mathbb{E}[Y])\\) vary together.\n\nA positive covariance indicates that larger values of \\(X\\) are associated with larger values of \\(Y\\).\nA negative covariance indicates that larger values of \\(X\\) are associated with smaller values of \\(Y\\).\nA covariance near zero suggests no linear relationship, though other forms of relationships might still exist.\n\nCovariance is limited to linear relationships. If the dependence between two random variables is non-linear—such as quadratic, exponential, or periodic—covariance might be zero, even though the variables are strongly related.\nFor example:\n\nIf \\(Y = X^2\\), the relationship is perfectly deterministic. However, if \\(X\\) is symmetrically distributed around zero the covariance could still be zero.\nFor a sinusoidal relationship like \\(Y = \\sin(X)\\), covariance would also fail to capture the dependence.\n\n\n\n4.3.4 Intuition for Covariance\nTo understand covariance, think of it as quantifying how closely the relationship between two variables resembles a straight-line pattern. The stronger the alignment with a straight line (positive or negative slope), the larger the magnitude of the covariance. However, when the relationship follows a curve or more complex pattern, covariance might fail to reflect the dependence, and other tools may be needed.2\n2 To detect and measure non-linear relationships, alternative statistical tools are required: 1. Correlation Ratio \\((\\eta^2)\\): Measures the proportion of variance in one variable that can be explained by a non-linear function of the other. 2. Mutual Information: Captures all forms of dependencies (linear and non-linear) by quantifying the shared information between two random variables. 3. Kendall’s Tau / Spearman’s Rank Correlation: Rank-based measures that can detect monotonic (but not necessarily linear) relationships. 4. Distance Correlation: A measure capable of capturing both linear and non-linear relationships by analyzing distances between data points.\n\n4.3.5 A Note on Terminology\nSome of you might have wondered why I use distinct terminology like mean and expected value or standard deviation and standard error. Why do I do this?\nLet me explain: In probability and statistics, certain terms are often used interchangeably, leading to potential confusion. To ensure clarity, we distinguish between empirical measures, which describe observed data, and theoretical measures, which describe properties of random variables and probability distributions.\nIn an empirical context, when working with data we speak of\n\nMean: The arithmetic average of observed data points. It provides a measure of central tendency for the sample or population.\nStandard Deviation: A measure of the spread or variability of the observed data around the mean.\n\nThese quantities are calculated directly from data and are specific to the observed sample.\nIn a theoretical context, when we work with random variables and distributions we speak of\n\nExpected Value: The theoretical long-run average of a random variable, derived from its probability distribution. It represents the “center” of the distribution in a probabilistic sense.\nStandard Deviation: The theoretical spread of a random variable’s values around its expected value, also derived from the probability distribution.\n\nThese measures rely on the underlying distribution of the random variable and are not directly observable.\nWhy does this distinction matter?\n\nClarity in Context: Empirical measures describe what we observe, while theoretical measures describe the underlying process or model.\nFoundation for Inference: Understanding the relationship between empirical and theoretical measures is key to making statistical inferences.\nPrecision in Communication: Using terms consistently helps avoid ambiguity in academic and professional discussions.\n\n\n\n4.3.6 Summary Table\n\n\n\n\n\n\n\n\n\nContext\nMeasure\nTerm\nDefinition/Role\n\n\n\n\nEmpirical Data\nCentral Tendency\nMean\nThe average of observed data points.\n\n\n\nSpread\nStandard Deviation\nThe dispersion of data points around the mean.\n\n\nProbability Distributions\nCentral Tendency\nExpected Value\nThe theoretical average of a random variable based on its probability distribution.\n\n\n\nSpread\nStandard Deviation\nThe spread of a random variable’s values around its expected value.\n\n\nSampling Distributions\nSpread of Estimate\nStandard Error\nThe standard deviation of a sample statistic, indicating estimation precision.\n\n\n\nBy maintaining this distinction, we connect the descriptive analysis of data with the theoretical underpinnings of probability and the tools of inferential statistics.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#random-variables-simulating-portfolio-returns-with-discrete-outcomes",
    "href": "04-lecture4.html#random-variables-simulating-portfolio-returns-with-discrete-outcomes",
    "title": "4  Random Variables",
    "section": "4.4 Random Variables: Simulating Portfolio Returns with Discrete Outcomes",
    "text": "4.4 Random Variables: Simulating Portfolio Returns with Discrete Outcomes\nIn this section, we build a simple portfolio simulation using discrete random variables to model asset returns. Based on this cas study we will push the frontier of your R knowledge a bit further giving you the tools to efficiently write more complex programs in R.\nThis example demonstrates modular R programming, flow control s tructures like if-else statements and loops, and the use of lists for organizing and processing data.\n\n4.4.1 Asset Returns: A Key Concept in Finance\nBefore we model investment problems using probability and random variables, let’s establish some essential financial concepts.\nIn Finance, an asset refers to any investment instrument that can be bought or sold, such as stocks, bonds, real estate, or even a savings account. Assets are the building blocks of investment decisions.\nWhen you invest in an asset, you expect to receive some form of payoff in the future. Let:\n\n\\(X_0\\): The amount of money invested today.\n\\(X_1\\): The amount received at a future time, say one year from now.\n\nThe total return (\\(R\\)) is the ratio of the amount received to the amount invested: \\[R = \\frac{X_1}{X_0}\\]\nOften, practitioners refer to the rate of return (\\(r\\)), which measures the proportionate change in value: \\[r = \\frac{X_1 - X_0}{X_0}\\]\nThe two measures are closely related: \\[R = 1 + r \\quad \\text{or equivalently,} \\quad X_1 = (1 + r) X_0\\]\nThe rate of return (\\(r\\)) behaves similarly to an interest rate, describing the growth (or decline) of your investment relative to the initial outlay.\nIn many cases, the rate of return is not known in advance. For example:\n\nA stock’s future price may depend on market conditions.\nA bond may carry the risk of default, affecting its final payoff.\nReal estate returns depend on fluctuating property values and rental income.\n\nThis uncertainty in returns is a fundamental aspect of investment. To analyze it rigorously, we model rates of return as random variables.\nRandom variables provide a mathematical framework for capturing uncertainty. For instance:\n\n\\(X_1\\), the future payoff of an asset, is modeled as a random variable because its value depends on uncertain factors.\nSince \\(R = \\frac{X_1}{X_0}\\) and \\(r = \\frac{X_1 - X_0}{X_0}\\), both the total return (\\(R\\)) and rate of return (\\(r\\)) are also random variables.\n\nModeling returns as random variables allows us to:\n\nDescribe the range of possible outcomes: For example, \\(X_1\\) could be high, low, or anywhere in between.\nQuantify likelihoods: We assign probabilities to outcomes, such as a 20% chance of a large gain or a 5% chance of a significant loss.\nAnalyze investment risk and reward: Using tools like expected value, variance, and covariance, we can measure and manage the trade-off between risk and return.\n\n\n\n4.4.2 Analyzing Returns with Discrete and Continuous Models\nIn this lecture, we focus on discrete models, where the possible outcomes of returns are finite and can be explicitly listed. For example, a stock might have three possible states—high, medium, or low returns—with assigned probabilities. Continuous models, where returns can take any value within a range, will be introduced in the next lecture.\n\n\n4.4.3 Portfolio Setup\nWe model the returns of three assets using discrete random variables. Each asset has three possible returns, and their joint outcomes are determined by a random state (e.g., economic scenarios: “High”, “Medium”, “Low”). These states have associated probabilities.\nWhere do these probabilities come from? In probability theory, probabilities are treated as fundamental quantities, much like length or area in geometry. They are simply “given” and form the foundation for analyzing uncertainty. For example, we might assume that the probability of a “High” economic scenario is 30%, while “Medium” and “Low” scenarios occur with probabilities of 50% and 20%, respectively.\nHowever, in practical applications, these probabilities are rarely known a priori. Assigning probabilities to real-world events is a complex task that often requires the use of data and statistical models. In Finance, this is the domain of econometrics and related disciplines.\nFor instance:\n\nHistorical data on market conditions might be analyzed to estimate the likelihood of different economic scenarios.\nExpert judgment or forecasts (e.g., from central banks or financial analysts) might supplement data to refine these probabilities.\nBayesian methods can combine prior beliefs with new data to update probabilities dynamically.\n\nIn our example, we assume the probabilities are already known to focus on understanding how probability theory can be applied to investment problems. This assumption simplifies the analysis and allows us to explore key concepts like expected value and variance without the added complexity of estimating probabilities.\nIt’s important to recognize that the choice of probabilities—whether derived from data, expert judgment, or theoretical models—directly influences the results of any analysis. In real-world applications, the validity of conclusions often hinges on how well these probabilities capture the underlying uncertainties.\nSo let us start our example by first defining asset returns and probabilities using R:\n\n# Define states and their probabilities\n\nstates &lt;- c(\"High\", \"Medium\", \"Low\")\nprobabilities &lt;- c(0.3, 0.5, 0.2)  # Must sum to 1\n\n# Define asset returns for each state\n\nasset_returns &lt;- list(\n  asset_1 = c(0.08, 0.04, -0.02), # Returns for Asset 1\n  asset_2 = c(0.12, 0.03, -0.05), # Returns for Asset 2\n  asset_3 = c(0.05, 0.02, 0.01)   # Returns for Asset 3\n)\n\n\n\n4.4.4 Writing Modular Functions\nTo model portfolio returns, we break the problem into smaller tasks and write modular functions. This approach makes the code more manageable, reusable, and easier to debug. Modularizing your code is a principle you should follow always when writing smaller or larger programs. One strategy how this can be done is breaking down a bigger task into smaller ones and implement the smaller tasks as functions.\nIn our case where we want to implement a portfolio simulation we need:\n\nSimulate a Random State\nGet the Asset Returen for each state\nCompute the portfolio returns\nSimulate portfolio performance\n\nLet me go through these steps function by function.\n\n4.4.4.1 Simulate a Random State\nWe begin by simulating a random economic state (e.g., “High,” “Medium,” or “Low”) based on the probabilities assigned to each state.\n\nsimulate_state &lt;- function(states, probabilities) {\n  sample(states, size = 1, replace = TRUE, prob = probabilities)\n}\n\nThe simulate_state function uses R’s sample() function to randomly choose one of the states based on their respective probabilities. This is a tool you already know. I do not have to say much more on the sample function.\nAs a remark, observe that it is a good idea to give to your functions names that you or somebody can easily understand. A good approach is to think in verbs, what is the function doing? When you need more than one word an often used convention in coding is to use and underscore to connect words.\nOur function is constructed such that it can take any finite state vector and any given state probabilities as input.\n\n\n4.4.4.2 Get Asset Returns for a State\nOnce a state is selected, we retrieve the corresponding returns for each asset. To accomplish this, we use the flexibility and power of R’s list structure.\nLet us recall the list structure which we have touched only very briefly in previous lectures.\nA list in R is a data structure that can hold multiple objects of different types and lengths. Lists are highly versatile, making them particularly useful for functional programming. For example, you can store vectors, matrices, data frames, and even other lists within a single list.\nHere’s an example of creating and accessing a list:\n\n# Create a list\n\nmy_list &lt;- list(\n  vector = c(1, 2, 3),\n  matrix = matrix(1:4, nrow = 2),\n  data_frame = data.frame(a = 1:2, b = 3:4)\n)\n\n# Access elements of the list\n\nmy_list$vector   # By name\n\n[1] 1 2 3\n\nmy_list[[2]]     # By position\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nmy_list[[\"matrix\"]] # Equivalent to my_list[[2]]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nLists allow you to organize related but heterogeneous data in a structured way. For example, in our case, the returns for each asset are stored as individual vectors within a list, where each vector corresponds to one asset.\nLists are particularly advantageous in programming because of their:\n\nFlexibility: They can hold objects of different types and sizes, making them a universal container.\nIndexing: Elements can be accessed using names or positions, enabling both clarity and adaptability.\nExtensibility: Lists can easily grow or change in structure without altering the overall program logic.\nIntegration with Functions: R functions like lapply, sapply, and map are designed to work seamlessly with lists, promoting concise and efficient code.\n\nThe get_returns function uses a list (asset_returns) to store the possible returns for each asset across states. Here’s the function:\n\nget_returns &lt;- function(state, states, asset_returns) {\n  index &lt;- which(states == state)\n  sapply(asset_returns, `[`, index)\n}\n\nLet’s break it down step by step:\n\nInput Arguments:\n\nstate: The randomly chosen state (e.g., “High”).\nstates: A vector of all possible states (e.g., c(\"High\", \"Medium\", \"Low\")).\nasset_returns: A list where each element is a vector of returns for an asset, corresponding to each state.\n\nFind the Index of the State:\n\nThe which function finds the position of the selected state within the states vector:\nindex &lt;- which(states == state)\nFor example, if state = \"Medium\" and states = c(\"High\", \"Medium\", \"Low\"), the result will be index = 2.\n\nExtract Returns Using sapply:\n\nThe sapply function applies the subsetting operator [ to each element of the asset_returns list:\nsapply(asset_returns,[, index)\n\nsapply(asset_returns, ...) iterates over each element of the list.\nThe operator [, with index as the argument, extracts the value at the index position from each vector in the list.\n\nFor example, if:\nasset_returns &lt;- list(      asset_1 = c(0.08, 0.04, -0.02),      asset_2 = c(0.12, 0.03, -0.05),      asset_3 = c(0.05, 0.02, 0.01)    )\nand index = 2, the result will be:\nc(asset_1 = 0.04, asset_2 = 0.03, asset_3 = 0.02)\nThe sapply function is part of a family of functions in R designed for iterating over elements of lists (or other objects like vectors). These functions enable you to apply a specified operation to each element of a list in a concise and readable way. This approach often replaces traditional for loops and promotes a functional programming style.\nsapply()is one member of a family of R’s functions written for operations on lists.\nHere’s an overview of the main functions in this family:\n\nlapply:\n\nApplies a function to each element of a list and returns a list of the results. It is useful when the output needs to remain as a list.\nExample:\n\nmy_list &lt;- list(a = 1:3, b = 4:6, c = 7:9)\nlapply(my_list, sum)\n\n$a\n[1] 6\n\n$b\n[1] 15\n\n$c\n[1] 24\n\n\n\nsapply: A simplified version of lapply that attempts to return results as a vector or matrix, if possible. If simplification isn’t feasible, it falls back to returning a list.\n\nExample:\n\nmy_list &lt;- list(a = 1:3, b = 4:6, c = 7:9)\nsapply(my_list, sum)\n\n a  b  c \n 6 15 24 \n\n\n\nvapply: Similar to sapply, but requires you to specify the type of the returned result (e.g., numeric, character). Useful for ensuring type consistency and avoiding unexpected results.\n\nExample:\n\nvapply(my_list, sum, numeric(1))\n\n a  b  c \n 6 15 24 \n\n\n\ntapply: Applies a function to subsets of a vector, grouped by a factor or factors. It is a very powerful workhorses for data analysis, commonly used for grouped computations.\n\nExample:\n\n   x &lt;- c(1, 2, 3, 4, 5, 6)\n   groups &lt;- factor(c(\"A\", \"A\", \"B\", \"B\", \"C\", \"C\"))\n   tapply(x, groups, sum)\n\n A  B  C \n 3  7 11 \n\n\n\nmapply:\n\nA multivariate version of sapply, allowing iteration over multiple inputs simultaneously.\nExample:\n\n   x &lt;- 1:3\n   y &lt;- 4:6\n   mapply(sum, x, y)\n\n[1] 5 7 9\n\n\nSo why Use sapply in our example?\nThe sapply function is well-suited for our example because: 1. The asset_returns object is a list, where each element corresponds to the returns of a single asset. 2. We want to extract a single value (the return for a given state) from each list element. 3. sapply simplifies the output into a named vector, making it easier to use for further computations.\nLet’s revisit the get_returns function:\n\nget_returns &lt;- function(state, states, asset_returns) {\n  index &lt;- which(states == state)  # Find the index of the selected state\n  sapply(asset_returns, `[`, index)  # Extract the return for the state from each asset\n}\n\nsapply(asset_returns, \\[`, index)applies the subsetting operator ([) to each element of theasset_returnslist, usingindex` as the argument. The result is a named vector of returns for the specified state, with names corresponding to the assets.\nBy understanding sapply and its related functions, you gain access to powerful tools for working with lists and other data structures in R. These functions are essential for writing efficient, modular, and readable R code.\n\n\n4.4.4.3 Compute Portfolio Return\nThe portfolio return is calculated as the weighted sum of the individual asset returns.\n\ncompute_portfolio_return &lt;- function(weights, returns) {\n  sum(weights * returns)\n}\n\nThis function multiplies each asset’s return by its respective weight and sums the result, yielding the total portfolio return.\n\n\n4.4.4.4 Putting Everything Together: Simulate Portfolio Performance\nWe now combine the functions we created earlier into a larger function, simulate_portfolio, that simulates portfolio performance over multiple time periods. Each simulation’s results are stored in a list, leveraging modularity for clarity and reusability. Modular code makes it easier to debug, document, and extend.\nR provides several ways to iterate over data, with for-loops being a fundamental and widely used method. In a for-loop, R sequentially applies the specified code to each element of a sequence or range.\nHere’s a basic example of a for-loop in R:\n\n# Example of a simple for-loop\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nFor-loops are intuitive and work well for many tasks. However, when iterating over larger datasets, it’s important to write efficient loops. One key best practice is preallocating space for the output of the loop. Without preallocation, R has to repeatedly resize the output object, which can slow down the computation significantly.\nIn our simulate_portfolio function, we preallocate space for the results using vector(\"list\", n_periods). This creates a list of length n_periods before entering the loop. Preallocation ensures that each iteration simply fills an existing slot in the list, avoiding costly memory reallocation.\nHere’s how it looks in our function:\n\nsimulate_portfolio &lt;- function(n_periods, weights, states, probabilities, asset_returns) {\n\n  # Preallocate space for the results\n  results &lt;- vector(\"list\", n_periods)\n\n  for (i in 1:n_periods) {\n    # Simulate a random state\n    state &lt;- simulate_state(states, probabilities)\n\n    # Retrieve the returns for the selected state\n    returns &lt;- get_returns(state, states, asset_returns)\n\n    # Compute the portfolio return\n    portfolio_return &lt;- compute_portfolio_return(weights, returns)\n\n    # Store the results for this iteration in the preallocated list\n    results[[i]] &lt;- list(\n      state = state,\n      returns = returns,\n      portfolio_return = portfolio_return\n    )\n  }\n\n  results\n}\n\nIn R, for-loops and lists are often used together but serve distinct purposes: - For-loops: Provide a mechanism for iterating over elements or ranges, applying code to each iteration step. They are versatile and easy to understand but can become slow without preallocation for large datasets. - Lists: Offer a flexible data structure for storing diverse types of output (e.g., vectors, data frames, or even other lists). They work well with for-loops when preallocated and are essential for modular code and functional programming.\nIn our example, the for-loop performs the iteration over n_periods, while the list (results) stores the output for each period. This combination strikes a balance between simplicity and efficiency.\nLet me unpack the simulate_portfoliofunction a bit further.\nThe simulate_portfolio function runs a simulation for n_periods. Each iteration:\n\nSimulates a random state using simulate_state.\nRetrieves the returns for all assets in that state using get_returns.\nComputes the portfolio return using compute_portfolio_return.\nStores the results (state, returns, portfolio return) in the preallocated results list.\n\nThe function returns a list where each element contains:\n\nThe random state: The economic scenario for the period.\nThe asset returns: A named vector of returns for each asset.\nThe portfolio return: The weighted average return for the portfolio.\n\nHere’s an example of what the output might look like for a single simulation:\n\n# Output for one period\nlist(\n  state = \"Medium\",\n  returns = c(asset_1 = 0.04, asset_2 = 0.03, asset_3 = 0.02),\n  portfolio_return = 0.032\n)\n\n$state\n[1] \"Medium\"\n\n$returns\nasset_1 asset_2 asset_3 \n   0.04    0.03    0.02 \n\n$portfolio_return\n[1] 0.032\n\n\nBy preallocating space and combining modular functions, this approach ensures efficient and readable code, while also allowing students to explore each step of the simulation in isolation or as part of the larger process.\nThis refined section introduces iteration concepts, explains the importance of preallocation, and highlights the synergy between for-loops and lists. Let me know if you’d like further enhancements!\nWe now define the portfolio weights, which must sum to 1. This says that we have invested 50 % of our wealth into asset 1, 30 % into asset 2 and 20 % into asset 3.\n\nweights &lt;- c(0.5, 0.3, 0.2)  # Weights for the three assets\n\nWe can now simulate the portfolio performance over any numebr of periods using the functions we created. Let#s say we do a simulation for 1000 periods.\n\nset.seed(42)  # For reproducibility\nn_periods &lt;- 1000\n\nsimulation_results &lt;- simulate_portfolio(n_periods, weights, states, probabilities, asset_returns)\n\nNow we have a huge pile of data stored in an object with the name simulated_results. What do we do with this?\nWe extract the portfolio returns from the simulation and compute summary statistics such as the mean and variance of the returns.\n\n# Extract portfolio returns\n\nportfolio_returns &lt;- sapply(simulation_results, `[[`, \"portfolio_return\")\n\n# Compute summary statistics\n\nmean_return &lt;- mean(portfolio_returns)\nvariance_return &lt;- var(portfolio_returns)\n\nlist(mean = mean_return, variance = variance_return)\n\n$mean\n[1] 0.036705\n\n$variance\n[1] 0.001380402\n\n\nHere we have summarized the distribution of our random variables using the concepts we learned before, thereby documenting their average return and thus the average return of the portfolio as well as how spread out these returns are.\nWe can also create a histogram to visualize the full distribution of the simulated portfolio returns.\n\nhist(portfolio_returns, breaks = 3, main = \"Portfolio Returns\", xlab = \"Return\")\n\n\n\n\n\n\n\n\nLet us explore the list structure we have chosen here to demonstarte how it helps us to access simulation details.\nEach element of simulation_results is itself a list, containing:\n\nThe random state.\nThe returns for each asset in that state.\nThe computed portfolio return.\n\nFor example, to inspect the first simulation we would type\n\nsimulation_results[[1]]\n\n$state\n[1] \"Low\"\n\n$returns\nasset_1 asset_2 asset_3 \n  -0.02   -0.05    0.01 \n\n$portfolio_return\n[1] -0.023\n\n\nLet us also compute state frequencies to check whether our random number generator has actually implemented what we have given him in terms of probabilities as arguments to the sample()function.\n\nstate_counts &lt;- table(sapply(simulation_results, `[[`, \"state\"))\nstate_frequencies &lt;- prop.table(state_counts)\nstate_frequencies\n\n\n  High    Low Medium \n 0.277  0.196  0.527 \n\n\nHere’s the refined passage with the formula and explanation added:\n\nFinally, we can compare the results of our simulation to key statistics of the portfolio, such as the expected portfolio return. These statistics can also be computed theoretically using the definitions of expected value and portfolio return.\nThe expected return of each asset is calculated as: \\[E(r^i) = \\sum_{j=1}^m p_j \\cdot r^i_j\\] where:\n\n\\(r^i_j\\) is the return of asset \\(i\\) in state \\(j\\),\n\\(p_j\\) is the probability of state \\(j\\),\n\\(m\\) is the total number of states.\n\nThe expected portfolio return is then the weighted sum of the expected returns of the individual assets: \\[E(R_{\\text{portfolio}}) = \\sum_{i=1}^n w^i \\cdot E(r^i)\\] where:\n\n\\(w^i\\) is the weight of asset \\(i\\) in the portfolio,\n\\(n\\) is the total number of assets.\n\nThe following R code computes the expected portfolio return using the formulas above:\n\n# Compute expected returns for each asset\nexpected_returns &lt;- sapply(asset_returns, function(asset) sum(asset * probabilities))\n\n# Compute the expected portfolio return\nexpected_portfolio_return &lt;- sum(weights * expected_returns)\n\n# Display the result\nexpected_portfolio_return\n\n[1] 0.0377\n\n\nLet me unpack this a bit:\n\nExpected Returns of Individual Assets:\n\n\nThe sapply function iterates over the asset_returns list, which contains the returns for each asset across the states.\nFor each asset, it computes the weighted sum of its returns using the probabilities of the states:\n\nsum(asset * probabilities)\nThis corresponds directly to the formula \\(E(r^i) = \\sum_{j=1}^m p_j \\cdot r^i_j\\).\n\nExpected Portfolio Return:\n\n\nThe weights vector contains the portfolio allocation for each asset.\nThe sum(weights * expected_returns) computation takes the weighted sum of the expected returns of the individual assets:\n\nsum(weights * expected_returns)\nThis corresponds to the formula \\(E(R_{\\text{portfolio}}) = \\sum_{i=1}^n w^i \\cdot E(r^i)\\).\nBy computing these values theoretically, we can validate the results of our simulation and gain confidence in both our simulation setup and the mathematical underpinnings of portfolio theory.\n\n\n\n\n\n\nNow You Try\n\n\n\nNow that we’ve calculated the expected portfolio return both theoretically and through simulation, take the next step and compute the theoretical variance of the portfolio.\nRecall the formula for the variance of a portfolio: \\[\n\\text{Var}(R_{\\text{portfolio}}) = \\sum_{i=1}^n (w^i)^2 \\cdot \\text{Var}(r^i) + \\sum_{i \\neq j} w^i \\cdot w^j \\cdot \\text{Cov}(r^i, r^j)\n\\] where: - \\(\\text{Var}(r^i) = \\sum_{j=1}^m p_j \\cdot (r^i_j - E(r^i))^2\\) is the variance of asset \\(i\\), - \\(\\text{Cov}(r^i, r^j) = \\sum_{j=1}^m p_j \\cdot (r^i_j - E(r^i)) \\cdot (r^j_j - E(r^j))\\) is the covariance between assets \\(i\\) and \\(j\\).\n\n4.4.5 Steps to Get Started\n\nCompute the Variance of Each Asset: Use the formula for variance to calculate \\(\\text{Var}(r^i)\\) for each asset based on the asset_returns and probabilities.\nCompute the Covariance Between Pairs of Assets: Calculate \\(\\text{Cov}(r^i, r^j)\\) for all pairs of assets.\nCombine Variances and Covariances: Use the portfolio weights and the formula above to compute \\(\\text{Var}(R_{\\text{portfolio}})\\).\n\n\n\n4.4.6 Tips\n\nUse sapply to efficiently compute variances and covariances across assets.\nStore the covariance values in a matrix for easier calculations.\n\nOnce you’ve completed the theoretical calculations, compare the theoretical portfolio variance to the variance obtained from your simulation. Does it match? What could explain any differences?\n\n\n\nHere’s a refined and streamlined version of the section, with improved structure, clarity, and flow, while keeping the main content intact. I also adjusted some arguments to improve accuracy and teaching focus:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#asset-returns-a-key-concept-in-finance",
    "href": "04-lecture4.html#asset-returns-a-key-concept-in-finance",
    "title": "4  Random Variables",
    "section": "4.5 Asset Returns: A Key Concept in Finance",
    "text": "4.5 Asset Returns: A Key Concept in Finance\nBefore we model investment problems using probability and random variables, let’s establish some essential financial concepts.\nIn Finance, an asset refers to any investment instrument that can be bought or sold, such as stocks, bonds, real estate, or even a savings account. Assets are the building blocks of investment decisions.\nWhen you invest in an asset, you expect to receive some form of payoff in the future. Let:\n\n\\(X_0\\): The amount of money invested today.\n\\(X_1\\): The amount received at a future time, say one year from now.\n\nThe total return (\\(R\\)) is the ratio of the amount received to the amount invested: \\[R = \\frac{X_1}{X_0}\\]\nOften, practitioners refer to the rate of return (\\(r\\)), which measures the proportionate change in value: \\[r = \\frac{X_1 - X_0}{X_0}\\]\nThe two measures are closely related: \\[R = 1 + r \\quad \\text{or equivalently,} \\quad X_1 = (1 + r) X_0\\]\nThe rate of return (\\(r\\)) behaves similarly to an interest rate, describing the growth (or decline) of your investment relative to the initial outlay.\nIn many cases, the rate of return is not known in advance. For example:\n\nA stock’s future price may depend on market conditions.\nA bond may carry the risk of default, affecting its final payoff.\nReal estate returns depend on fluctuating property values and rental income.\n\nThis uncertainty in returns is a fundamental aspect of investment. To analyze it rigorously, we model rates of return as random variables.\nRandom variables provide a mathematical framework for capturing uncertainty. For instance:\n\n\\(X_1\\), the future payoff of an asset, is modeled as a random variable because its value depends on uncertain factors.\nSince \\(R = \\frac{X_1}{X_0}\\) and \\(r = \\frac{X_1 - X_0}{X_0}\\), both the total return (\\(R\\)) and rate of return (\\(r\\)) are also random variables.\n\nModeling returns as random variables allows us to:\n\nDescribe the range of possible outcomes: For example, \\(X_1\\) could be high, low, or anywhere in between.\nQuantify likelihoods: We assign probabilities to outcomes, such as a 20% chance of a large gain or a 5% chance of a significant loss.\nAnalyze investment risk and reward: Using tools like expected value, variance, and covariance, we can measure and manage the trade-off between risk and return.\n\n\n4.5.1 Analyzing Returns with Discrete and Continuous Models\nIn this lecture, we focus on discrete models, where the possible outcomes of returns are finite and can be explicitly listed. For example, a stock might have three possible states—high, medium, or low returns—with assigned probabilities. Continuous models, where returns can take any value within a range, will be introduced in the next lecture.\n\n\n4.5.2 Portfolio Setup\nWe model the returns of three assets using discrete random variables. Each asset has three possible returns, and their joint outcomes are determined by a random state (e.g., economic scenarios: “High”, “Medium”, “Low”). These states have associated probabilities.\nWhere do these probabilities come from? In probability theory, probabilities are treated as fundamental quantities, much like length or area in geometry. They are simply “given” and form the foundation for analyzing uncertainty. For example, we might assume that the probability of a “High” economic scenario is 30%, while “Medium” and “Low” scenarios occur with probabilities of 50% and 20%, respectively.\nHowever, in practical applications, these probabilities are rarely known a priori. Assigning probabilities to real-world events is a complex task that often requires the use of data and statistical models. In Finance, this is the domain of econometrics and related disciplines.\nFor instance:\n\nHistorical data on market conditions might be analyzed to estimate the likelihood of different economic scenarios.\nExpert judgment or forecasts (e.g., from central banks or financial analysts) might supplement data to refine these probabilities.\nBayesian methods can combine prior beliefs with new data to update probabilities dynamically.\n\nIn our example, we assume the probabilities are already known to focus on understanding how probability theory can be applied to investment problems. This assumption simplifies the analysis and allows us to explore key concepts like expected value and variance without the added complexity of estimating probabilities.\nIt’s important to recognize that the choice of probabilities—whether derived from data, expert judgment, or theoretical models—directly influences the results of any analysis. In real-world applications, the validity of conclusions often hinges on how well these probabilities capture the underlying uncertainties.\nSo let us start our example by first defining asset returns and probabilities using R:\n\n# Define states and their probabilities\n\nstates &lt;- c(\"High\", \"Medium\", \"Low\")\nprobabilities &lt;- c(0.3, 0.5, 0.2)  # Must sum to 1\n\n# Define asset returns for each state\n\nasset_returns &lt;- list(\n  asset_1 = c(0.08, 0.04, -0.02), # Returns for Asset 1\n  asset_2 = c(0.12, 0.03, -0.05), # Returns for Asset 2\n  asset_3 = c(0.05, 0.02, 0.01)   # Returns for Asset 3\n)\n\n\n\n4.5.3 Writing Modular Functions\nTo model portfolio returns, we break the problem into smaller tasks and write modular functions. This approach makes the code more manageable, reusable, and easier to debug. Modularizing your code is a principle you should follow always when writing smaller or larger programs. One strategy how this can be done is breaking down a bigger task into smaller ones and implement the smaller tasks as functions.\nIn our case where we want to implement a portfolio simulation we need:\n\nSimulate a Random State\nGet the Asset Returen for each state\nCompute the portfolio returns\nSimulate portfolio performance\n\nLet me go through these steps function by function.\n\n4.5.3.1 Simulate a Random State\nWe begin by simulating a random economic state (e.g., “High,” “Medium,” or “Low”) based on the probabilities assigned to each state.\n\nsimulate_state &lt;- function(states, probabilities) {\n  sample(states, size = 1, replace = TRUE, prob = probabilities)\n}\n\nThe simulate_state function uses R’s sample() function to randomly choose one of the states based on their respective probabilities. This is a tool you already know. I do not have to say much more on the sample function.\nAs a remark, observe that it is a good idea to give to your functions names that you or somebody can easily understand. A good approach is to think in verbs, what is the function doing? When you need more than one word an often used convention in coding is to use and underscore to connect words.\nOur function is constructed such that it can take any finite state vector and any given state probabilities as input.\n\n\n4.5.3.2 Get Asset Returns for a State\nOnce a state is selected, we retrieve the corresponding returns for each asset. To accomplish this, we use the flexibility and power of R’s list structure.\nLet us recall the list structure which we have touched only very briefly in previous lectures.\nA list in R is a data structure that can hold multiple objects of different types and lengths. Lists are highly versatile, making them particularly useful for functional programming. For example, you can store vectors, matrices, data frames, and even other lists within a single list.\nHere’s an example of creating and accessing a list:\n\n# Create a list\n\nmy_list &lt;- list(\n  vector = c(1, 2, 3),\n  matrix = matrix(1:4, nrow = 2),\n  data_frame = data.frame(a = 1:2, b = 3:4)\n)\n\n# Access elements of the list\n\nmy_list$vector   # By name\n\n[1] 1 2 3\n\nmy_list[[2]]     # By position\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nmy_list[[\"matrix\"]] # Equivalent to my_list[[2]]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nLists allow you to organize related but heterogeneous data in a structured way. For example, in our case, the returns for each asset are stored as individual vectors within a list, where each vector corresponds to one asset.\nLists are particularly advantageous in programming because of their:\n\nFlexibility: They can hold objects of different types and sizes, making them a universal container.\nIndexing: Elements can be accessed using names or positions, enabling both clarity and adaptability.\nExtensibility: Lists can easily grow or change in structure without altering the overall program logic.\nIntegration with Functions: R functions like lapply, sapply, and map are designed to work seamlessly with lists, promoting concise and efficient code.\n\nThe get_returns function uses a list (asset_returns) to store the possible returns for each asset across states. Here’s the function:\n\nget_returns &lt;- function(state, states, asset_returns) {\n  index &lt;- which(states == state)\n  sapply(asset_returns, `[`, index)\n}\n\nLet’s break it down step by step:\n\nInput Arguments:\n\nstate: The randomly chosen state (e.g., “High”).\nstates: A vector of all possible states (e.g., c(\"High\", \"Medium\", \"Low\")).\nasset_returns: A list where each element is a vector of returns for an asset, corresponding to each state.\n\nFind the Index of the State:\n\nThe which function finds the position of the selected state within the states vector:\nindex &lt;- which(states == state)\nFor example, if state = \"Medium\" and states = c(\"High\", \"Medium\", \"Low\"), the result will be index = 2.\n\nExtract Returns Using sapply:\n\nThe sapply function applies the subsetting operator [ to each element of the asset_returns list:\nsapply(asset_returns,[, index)\n\nsapply(asset_returns, ...) iterates over each element of the list.\nThe operator [, with index as the argument, extracts the value at the index position from each vector in the list.\n\nFor example, if:\nasset_returns &lt;- list(      asset_1 = c(0.08, 0.04, -0.02),      asset_2 = c(0.12, 0.03, -0.05),      asset_3 = c(0.05, 0.02, 0.01)    )\nand index = 2, the result will be:\nc(asset_1 = 0.04, asset_2 = 0.03, asset_3 = 0.02)\nThe sapply function is part of a family of functions in R designed for iterating over elements of lists (or other objects like vectors). These functions enable you to apply a specified operation to each element of a list in a concise and readable way. This approach often replaces traditional for loops and promotes a functional programming style.\nsapply()is one member of a family of R’s functions written for operations on lists.\nHere’s an overview of the main functions in this family:\n\nlapply:\n\nApplies a function to each element of a list and returns a list of the results. It is useful when the output needs to remain as a list.\nExample:\n\nmy_list &lt;- list(a = 1:3, b = 4:6, c = 7:9)\nlapply(my_list, sum)\n\n$a\n[1] 6\n\n$b\n[1] 15\n\n$c\n[1] 24\n\n\n\nsapply: A simplified version of lapply that attempts to return results as a vector or matrix, if possible. If simplification isn’t feasible, it falls back to returning a list.\n\nExample:\n\nmy_list &lt;- list(a = 1:3, b = 4:6, c = 7:9)\nsapply(my_list, sum)\n\n a  b  c \n 6 15 24 \n\n\n\nvapply: Similar to sapply, but requires you to specify the type of the returned result (e.g., numeric, character). Useful for ensuring type consistency and avoiding unexpected results.\n\nExample:\n\nvapply(my_list, sum, numeric(1))\n\n a  b  c \n 6 15 24 \n\n\n\ntapply: Applies a function to subsets of a vector, grouped by a factor or factors. It is a very powerful workhorses for data analysis, commonly used for grouped computations.\n\nExample:\n\n   x &lt;- c(1, 2, 3, 4, 5, 6)\n   groups &lt;- factor(c(\"A\", \"A\", \"B\", \"B\", \"C\", \"C\"))\n   tapply(x, groups, sum)\n\n A  B  C \n 3  7 11 \n\n\n\nmapply:\n\nA multivariate version of sapply, allowing iteration over multiple inputs simultaneously.\nExample:\n\n   x &lt;- 1:3\n   y &lt;- 4:6\n   mapply(sum, x, y)\n\n[1] 5 7 9\n\n\nSo why Use sapply in our example?\nThe sapply function is well-suited for our example because: 1. The asset_returns object is a list, where each element corresponds to the returns of a single asset. 2. We want to extract a single value (the return for a given state) from each list element. 3. sapply simplifies the output into a named vector, making it easier to use for further computations.\nLet’s revisit the get_returns function:\n\nget_returns &lt;- function(state, states, asset_returns) {\n  index &lt;- which(states == state)  # Find the index of the selected state\n  sapply(asset_returns, `[`, index)  # Extract the return for the state from each asset\n}\n\nsapply(asset_returns, \\[`, index)applies the subsetting operator ([) to each element of theasset_returnslist, usingindex` as the argument. The result is a named vector of returns for the specified state, with names corresponding to the assets.\nBy understanding sapply and its related functions, you gain access to powerful tools for working with lists and other data structures in R. These functions are essential for writing efficient, modular, and readable R code.\n\n\n4.5.3.3 Compute Portfolio Return\nThe portfolio return is calculated as the weighted sum of the individual asset returns.\n\ncompute_portfolio_return &lt;- function(weights, returns) {\n  sum(weights * returns)\n}\n\nThis function multiplies each asset’s return by its respective weight and sums the result, yielding the total portfolio return.\n\n\n4.5.3.4 Putting Everything Together: Simulate Portfolio Performance\nWe now combine the functions we created earlier into a larger function, simulate_portfolio, that simulates portfolio performance over multiple time periods. Each simulation’s results are stored in a list, leveraging modularity for clarity and reusability. Modular code makes it easier to debug, document, and extend.\nR provides several ways to iterate over data, with for-loops being a fundamental and widely used method. In a for-loop, R sequentially applies the specified code to each element of a sequence or range.\nHere’s a basic example of a for-loop in R:\n\n# Example of a simple for-loop\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nFor-loops are intuitive and work well for many tasks. However, when iterating over larger datasets, it’s important to write efficient loops. One key best practice is preallocating space for the output of the loop. Without preallocation, R has to repeatedly resize the output object, which can slow down the computation significantly.\nIn our simulate_portfolio function, we preallocate space for the results using vector(\"list\", n_periods). This creates a list of length n_periods before entering the loop. Preallocation ensures that each iteration simply fills an existing slot in the list, avoiding costly memory reallocation.\nHere’s how it looks in our function:\n\nsimulate_portfolio &lt;- function(n_periods, weights, states, probabilities, asset_returns) {\n\n  # Preallocate space for the results\n  results &lt;- vector(\"list\", n_periods)\n\n  for (i in 1:n_periods) {\n    # Simulate a random state\n    state &lt;- simulate_state(states, probabilities)\n\n    # Retrieve the returns for the selected state\n    returns &lt;- get_returns(state, states, asset_returns)\n\n    # Compute the portfolio return\n    portfolio_return &lt;- compute_portfolio_return(weights, returns)\n\n    # Store the results for this iteration in the preallocated list\n    results[[i]] &lt;- list(\n      state = state,\n      returns = returns,\n      portfolio_return = portfolio_return\n    )\n  }\n\n  results\n}\n\nIn R, for-loops and lists are often used together but serve distinct purposes: - For-loops: Provide a mechanism for iterating over elements or ranges, applying code to each iteration step. They are versatile and easy to understand but can become slow without preallocation for large datasets. - Lists: Offer a flexible data structure for storing diverse types of output (e.g., vectors, data frames, or even other lists). They work well with for-loops when preallocated and are essential for modular code and functional programming.\nIn our example, the for-loop performs the iteration over n_periods, while the list (results) stores the output for each period. This combination strikes a balance between simplicity and efficiency.\nLet me unpack the simulate_portfoliofunction a bit further.\nThe simulate_portfolio function runs a simulation for n_periods. Each iteration:\n\nSimulates a random state using simulate_state.\nRetrieves the returns for all assets in that state using get_returns.\nComputes the portfolio return using compute_portfolio_return.\nStores the results (state, returns, portfolio return) in the preallocated results list.\n\nThe function returns a list where each element contains:\n\nThe random state: The economic scenario for the period.\nThe asset returns: A named vector of returns for each asset.\nThe portfolio return: The weighted average return for the portfolio.\n\nHere’s an example of what the output might look like for a single simulation:\n\n# Output for one period\nlist(\n  state = \"Medium\",\n  returns = c(asset_1 = 0.04, asset_2 = 0.03, asset_3 = 0.02),\n  portfolio_return = 0.032\n)\n\n$state\n[1] \"Medium\"\n\n$returns\nasset_1 asset_2 asset_3 \n   0.04    0.03    0.02 \n\n$portfolio_return\n[1] 0.032\n\n\nBy preallocating space and combining modular functions, this approach ensures efficient and readable code, while also allowing students to explore each step of the simulation in isolation or as part of the larger process.\nThis refined section introduces iteration concepts, explains the importance of preallocation, and highlights the synergy between for-loops and lists. Let me know if you’d like further enhancements!\nWe now define the portfolio weights, which must sum to 1. This says that we have invested 50 % of our wealth into asset 1, 30 % into asset 2 and 20 % into asset 3.\n\nweights &lt;- c(0.5, 0.3, 0.2)  # Weights for the three assets\n\nWe can now simulate the portfolio performance over any numebr of periods using the functions we created. Let#s say we do a simulation for 1000 periods.\n\nset.seed(42)  # For reproducibility\nn_periods &lt;- 1000\n\nsimulation_results &lt;- simulate_portfolio(n_periods, weights, states, probabilities, asset_returns)\n\nNow we have a huge pile of data stored in an object with the name simulated_results. What do we do with this?\nWe extract the portfolio returns from the simulation and compute summary statistics such as the mean and variance of the returns.\n\n# Extract portfolio returns\n\nportfolio_returns &lt;- sapply(simulation_results, `[[`, \"portfolio_return\")\n\n# Compute summary statistics\n\nmean_return &lt;- mean(portfolio_returns)\nvariance_return &lt;- var(portfolio_returns)\n\nlist(mean = mean_return, variance = variance_return)\n\n$mean\n[1] 0.036705\n\n$variance\n[1] 0.001380402\n\n\nHere we have summarized the distribution of our random variables using the concepts we learned before, thereby documenting their average return and thus the average return of the portfolio as well as how spread out these returns are.\nWe can also create a histogram to visualize the full distribution of the simulated portfolio returns.\n\nhist(portfolio_returns, breaks = 3, main = \"Portfolio Returns\", xlab = \"Return\")\n\n\n\n\n\n\n\n\nLet us explore the list structure we have chosen here to demonstarte how it helps us to access simulation details.\nEach element of simulation_results is itself a list, containing:\n\nThe random state.\nThe returns for each asset in that state.\nThe computed portfolio return.\n\nFor example, to inspect the first simulation we would type\n\nsimulation_results[[1]]\n\n$state\n[1] \"Low\"\n\n$returns\nasset_1 asset_2 asset_3 \n  -0.02   -0.05    0.01 \n\n$portfolio_return\n[1] -0.023\n\n\nLet us also compute state frequencies to check whether our random number generator has actually implemented what we have given him in terms of probabilities as arguments to the sample()function.\n\nstate_counts &lt;- table(sapply(simulation_results, `[[`, \"state\"))\nstate_frequencies &lt;- prop.table(state_counts)\nstate_frequencies\n\n\n  High    Low Medium \n 0.277  0.196  0.527 \n\n\nHere’s the refined passage with the formula and explanation added:\n\nFinally, we can compare the results of our simulation to key statistics of the portfolio, such as the expected portfolio return. These statistics can also be computed theoretically using the definitions of expected value and portfolio return.\nThe expected return of each asset is calculated as: \\[E(r^i) = \\sum_{j=1}^m p_j \\cdot r^i_j\\] where:\n\n\\(r^i_j\\) is the return of asset \\(i\\) in state \\(j\\),\n\\(p_j\\) is the probability of state \\(j\\),\n\\(m\\) is the total number of states.\n\nThe expected portfolio return is then the weighted sum of the expected returns of the individual assets: \\[E(R_{\\text{portfolio}}) = \\sum_{i=1}^n w^i \\cdot E(r^i)\\] where:\n\n\\(w^i\\) is the weight of asset \\(i\\) in the portfolio,\n\\(n\\) is the total number of assets.\n\nThe following R code computes the expected portfolio return using the formulas above:\n\n# Compute expected returns for each asset\nexpected_returns &lt;- sapply(asset_returns, function(asset) sum(asset * probabilities))\n\n# Compute the expected portfolio return\nexpected_portfolio_return &lt;- sum(weights * expected_returns)\n\n# Display the result\nexpected_portfolio_return\n\n[1] 0.0377\n\n\nLet me unpack this a bit:\n\nExpected Returns of Individual Assets:\n\n\nThe sapply function iterates over the asset_returns list, which contains the returns for each asset across the states.\nFor each asset, it computes the weighted sum of its returns using the probabilities of the states:\n\nsum(asset * probabilities)\nThis corresponds directly to the formula \\(E(r^i) = \\sum_{j=1}^m p_j \\cdot r^i_j\\).\n\nExpected Portfolio Return:\n\n\nThe weights vector contains the portfolio allocation for each asset.\nThe sum(weights * expected_returns) computation takes the weighted sum of the expected returns of the individual assets:\n\nsum(weights * expected_returns)\nThis corresponds to the formula \\(E(R_{\\text{portfolio}}) = \\sum_{i=1}^n w^i \\cdot E(r^i)\\).\nBy computing these values theoretically, we can validate the results of our simulation and gain confidence in both our simulation setup and the mathematical underpinnings of portfolio theory.\n\n\n\n\n\n\nNow You Try\n\n\n\nNow that we’ve calculated the expected portfolio return both theoretically and through simulation, take the next step and compute the theoretical variance of the portfolio.\nRecall the formula for the variance of a portfolio: \\[\n\\text{Var}(R_{\\text{portfolio}}) = \\sum_{i=1}^n (w^i)^2 \\cdot \\text{Var}(r^i) + \\sum_{i \\neq j} w^i \\cdot w^j \\cdot \\text{Cov}(r^i, r^j)\n\\] where: - \\(\\text{Var}(r^i) = \\sum_{j=1}^m p_j \\cdot (r^i_j - E(r^i))^2\\) is the variance of asset \\(i\\), - \\(\\text{Cov}(r^i, r^j) = \\sum_{j=1}^m p_j \\cdot (r^i_j - E(r^i)) \\cdot (r^j_j - E(r^j))\\) is the covariance between assets \\(i\\) and \\(j\\).\n\n4.5.4 Steps to Get Started\n\nCompute the Variance of Each Asset: Use the formula for variance to calculate \\(\\text{Var}(r^i)\\) for each asset based on the asset_returns and probabilities.\nCompute the Covariance Between Pairs of Assets: Calculate \\(\\text{Cov}(r^i, r^j)\\) for all pairs of assets.\nCombine Variances and Covariances: Use the portfolio weights and the formula above to compute \\(\\text{Var}(R_{\\text{portfolio}})\\).\n\n\n\n4.5.5 Tips\n\nUse sapply to efficiently compute variances and covariances across assets.\nStore the covariance values in a matrix for easier calculations.\n\nOnce you’ve completed the theoretical calculations, compare the theoretical portfolio variance to the variance obtained from your simulation. Does it match? What could explain any differences?\n\n\n\nHere’s a refined and streamlined version of the section, with improved structure, clarity, and flow, while keeping the main content intact. I also adjusted some arguments to improve accuracy and teaching focus:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "04-lecture4.html#the-binomial-distribution-and-its-role-in-finance",
    "href": "04-lecture4.html#the-binomial-distribution-and-its-role-in-finance",
    "title": "4  Random Variables",
    "section": "4.5 The Binomial Distribution and its role in Finance",
    "text": "4.5 The Binomial Distribution and its role in Finance\nIn this final section, we explore the binomial distribution, a discrete distribution with significant applications in Finance. It forms the foundation for one of the most important models of asset p rice dynamics: binomial lattices. This discussion not only deepens our understanding of probability but also introduces tools in R to work with parametric distributions.\n\n4.5.1 Bernoulli and Binomial Random Variables\n\n4.5.1.1 Bernoulli Random Variable: The Building Block\nA Bernoulli experiment has two possible outcomes, commonly labeled as “success” and “failure.” The corresponding Bernoulli random variable takes a value of 1 for success and 0 for failure, with a success probability \\(p\\) and failure probability \\(1-p\\).\nIf \\(X \\sim \\text{Bernoulli}(p)\\), then: - \\(E(X) = p\\)(expected value), - \\(\\text{Var}(X) = p(1-p)\\) (variance).\nTry deriving these properties from the definitions of expectation and variance!\n\n\n4.5.1.2 Binomial Random Variable: Extending the Bernoulli\nThe binomial random variable is the sum of \\(n\\) independent Bernoulli random variables, modeling the total number of successes in \\(n\\) trials. If \\(X \\sim \\text{Binomial}(n, p)\\), then: - \\(E(X) = n \\cdot p\\), - \\(\\text{Var}(X) = n \\cdot p \\cdot (1-p)\\).\nThe probability of exactly \\(k\\) successes is given by: \\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k},\\] where \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\) is the binomial coefficient.\n\n\n4.5.1.3 Intuition Through an Example\nLet’s consider rolling a fair die five times, with success defined as rolling a six. The number of sixes follows a binomial distribution with \\(n = 5\\) and \\(p = 1/6\\). Here’s how we compute probabilities:\n\nAll Sixes (\\(X = 5\\)):\n\\(P(X = 5) = (1/6)^5\\).\n\n\n  (1/6)^5\n\n[1] 0.0001286008\n\n\n\nNo Sixes (\\(X = 0\\)):\n\\(P(X = 0) = (5/6)^5\\).\n\n\n  (5/6)^5\n\n[1] 0.4018776\n\n\n\nExactly One Six (\\(X = 1\\)):\n\\(P(X = 1) = 5 \\cdot (5/6)^4 \\cdot (1/6)\\).\n\n\n  5 * (5/6)^4 * (1/6)\n\n[1] 0.4018776\n\n\n\n\n\n4.5.2 Working with the Binomial Distribution in R\nR provides built-in functions for parametric distributions. For the binomial distribution: 1. dbinom(x, n, p): Probability \\(P(X = x)\\). 2. pbinom(x, n, p): Cumulative probability \\(P(X \\leq x)\\). 3. rbinom(N, n, p): Generate \\(N\\) random samples from a binomial distribution. 4. qbinom(r, n, p): Quantile function.\n\n4.5.2.1 Simulating and Comparing Approaches\nTo illustrate, let’s simulate rolling a die five times and compare it with the binomial model:\n\n# Custom simulation\n\nroll_die_five &lt;- function() {\n  res &lt;- sample(1:6, 5, replace = TRUE)\n  sum(res == 6)\n}\n\n# Simulate 100,000 trials\n\nsim_rolls &lt;- replicate(10^5, roll_die_five())\n\n# Binomial distribution simulation\n\nsim_roll_with_rbinom &lt;- rbinom(10^5, 5, 1/6)\n\nPlot the distributions and compare:\n\n# Plot histogram for custom simulation\n\nhist(sim_rolls, \n     breaks = seq(-0.5, max(sim_rolls) + 0.5, by = 1), \n     main = \"Custom Simulation\", \n     xlab = \"Number of Sixes\", \n     col = \"lightblue\", \n     border = \"black\")\n\n\n\n\n\n\n\n# Plot histogram for rbinom simulation\n\nhist(sim_roll_with_rbinom, \n     breaks = seq(-0.5, max(sim_roll_with_rbinom) + 0.5, by = 1), \n     main = \"R's rbinom Function\", \n     xlab = \"Number of Sixes\", \n     col = \"lightgreen\", \n     border = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.3 Modeling Asset Price Dynamics with Binomial Trees\n\n4.5.3.1 Real-World Asset Price Dynamics\nIn practice, asset prices fluctuate due to market forces, dividends, interest rates, and other uncertainties. Binomial trees are a powerful and simple way to model these dynamics over time. Unlike single-period models, binomial trees capture the evolution of prices across multiple periods.\n\n\n4.5.3.2 Constructing the Binomial Model\nFor a basic period (e.g., a day), assume:\n\nThe price can either increase by a factor \\(u\\) or decrease by a factor \\(d\\).\nThe probabilities of up and down movements are \\(p\\) and \\(1-p\\).\n\nIf the initial price is \\(S_0\\), then after \\(n\\) periods: \\[S_n = S_0 \\cdot u^i \\cdot d^{n-i},\\] where \\(i\\) is the number of up movements.\n\n\n4.5.3.3 Calibrating the Model\nGiven annualized return (\\(\\nu\\)) and volatility (\\(\\sigma\\)), we set: \\[u = e^{\\sigma \\sqrt{\\Delta t}}, \\quad d = e^{-\\sigma \\sqrt{\\Delta t}}, \\quad p = \\frac{1}{2} + \\frac{\\nu}{2\\sigma} \\sqrt{\\Delta t},\\] where \\(\\Delta t\\) is the fraction of a year (e.g., 1/250 for daily).\n\n\n4.5.3.4 Building the Binomial Tree in R\nHere’s a function to generate a binomial tree:\n\nbinomial_tree &lt;- function(S0, up, down, steps) {\n  tree &lt;- matrix(0, nrow = steps + 1, ncol = steps + 1)\n  for (i in 1:(steps + 1)) {\n    for (j in 1:i) {\n      tree[i, j] &lt;- S0 * up^(j - 1) * down^((i - 1) - (j - 1))\n    }\n  }\n  return(tree)\n}\n\nTest the function with three steps:\n\ntree &lt;- binomial_tree(S0 = 100, up = 1.1, down = 0.9, steps = 3)\ntree\n\n      [,1]  [,2]  [,3]  [,4]\n[1,] 100.0   0.0   0.0   0.0\n[2,]  90.0 110.0   0.0   0.0\n[3,]  81.0  99.0 121.0   0.0\n[4,]  72.9  89.1 108.9 133.1\n\n\n\n\n4.5.3.5 Simulating SP500 Prices\nUsing historical SP500 data, calculate: - \\(\\nu\\): Average annual log return. - \\(\\sigma\\): Annualized volatility.\nThen, simulate the binomial price process and visualize the probability distribution of final prices:\nwe need to work this out still using concrete returns.\n\n\n\n\n\n\n\n\n\n4.5.4 Subsection: Leveraging an LLM to Enhance Learning from Binomial Models\n\n4.5.4.1 Using an LLM for Advanced Financial Modeling\nAn LLM like ChatGPT can significantly enhance your learning experience from this lecture by helping you: 1. Understand Complex Concepts: Get detailed explanations of topics like binomial trees, asset price dynamics, and probability distributions. 2. Generate Customized Code: Create tailored R scripts to experiment with different stocks, time horizons, or assumptions. 3. Validate Your Work: Debug your code or verify theoretical calculations. 4. Extend the Model: Explore advanced variations, such as incorporating dividends or multi-asset portfolios.\n\n\n\n4.5.4.2 Using an LLM to enhance you understanding: Debugging and Extending the Binomial Tree Model\nImagine you’ve written an R function to generate a binomial tree but are unsure if the logic is correct. You can ask the LLM to review your code, explain its functionality, and suggest improvements.\nFor example: Prompt to the LLM:\n\n“I wrote this function to generate a binomial tree in R. Can you check if it’s correct? If not, how can I fix it?”\n\nbinomial_tree &lt;- function(S0, up, down, steps) {\n  tree &lt;- matrix(0, nrow = steps + 1, ncol = steps + 1)\n  for (i in 1:(steps + 1)) {\n    for (j in 1:i) {\n      tree[i, j] &lt;- S0 * up^(j - 1) * down^((i - 1) - (j - 1))\n    }\n  }\n  return(tree)\n}\nThe LLM can: - Analyze the logic and verify the implementation. - Suggest optimizations, like avoiding redundant calculations. - Explain how to test the function with sample inputs and interpret the results.\n\n\n\n4.5.4.3 Use Case: Generating Advanced Visualizations\nYou want to visualize not just the final outcomes of the binomial model but also the entire tree structure over time. While this isn’t covered in the lecture, you can ask: &gt; “How can I visualize the full binomial tree structure generated by my function?”\nThe LLM can suggest: - Using packages like DiagrammeR or igraph for structured visualizations. - Overlaying probabilities or expected values on the tree for deeper insights.\n\n\n\n4.5.4.4 Use Case: Customizing the Project\nYou’ve completed the project but want to explore additional scenarios. For example: - Use different assets (e.g., cryptocurrencies or ETFs). - Model dividends or transaction costs. - Extend the tree to multiple assets for portfolio modeling.\nPrompt to the LLM: &gt; “How can I extend my binomial tree model to include transaction costs of 0.1% for every price change?”\nThe LLM can guide you step-by-step: 1. Modify the price calculation at each node to subtract transaction costs. 2. Adjust the recursion logic to account for the modified prices. 3. Validate the results and interpret the impact of transaction costs.\n\n\n\n4.5.4.5 Why This Matters\nUsing an LLM allows you to: - Gain insights beyond the lecture material. - Accelerate your coding and problem-solving skills. - Explore advanced scenarios without requiring pre-existing expertise.\nBy leveraging these tools effectively, you can turn theoretical concepts into practical, actionable skills that prepare you for real-world challenges in Finance and beyond.\n\nWould you like this subsection expanded or tailored further to specific aspects of the lecture?\n\n\n\n4.5.5 Project Proposal: Simulating and Analyzing Real-World Asset Price Dynamics with Binomial Trees\n\n4.5.5.1 Objective\nCreate a project where students use real-world stock price data (retrieved via tidyquant) to: 1. Build a binomial tree model for a selected stock’s price dynamics. 2. Calibrate the model using historical data (e.g., average annual return and volatility). 3. Simulate the stock’s potential price distribution over a defined time horizon. 4. Analyze and visualize the results to compare simulated outcomes with theoretical values.\nThis project ties together the lecture’s concepts, emphasizes practical applications, and encourages exploration of real data.\n\n\n\n4.5.6 Step-by-Step Structure for the Project\n\n4.5.6.1 Step 1: Data Retrieval\n\nUse the tidyquant package to download historical stock price data for a user-selected stock (e.g., “AAPL” or “MSFT”).\nCompute daily log returns and annualized statistics:\n\nAverage annual log return (( )).\nAnnualized volatility (( )).\n\n\n\n\n4.5.6.2 Example Code:\n\nlibrary(tidyquant)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n── Attaching core tidyquant packages ──────────────────────── tidyquant 1.0.9 ──\n✔ PerformanceAnalytics 2.0.4      ✔ TTR                  0.24.4\n✔ quantmod             0.4.26     ✔ xts                  0.14.1\n── Conflicts ────────────────────────────────────────── tidyquant_conflicts() ──\n✖ zoo::as.Date()                 masks base::as.Date()\n✖ zoo::as.Date.numeric()         masks base::as.Date.numeric()\n✖ PerformanceAnalytics::legend() masks graphics::legend()\n✖ quantmod::summary()            masks base::summary()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::%||%()   masks base::%||%()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::first()  masks xts::first()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::last()   masks xts::last()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Retrieve historical stock prices\nstock_data &lt;- tq_get(\"AAPL\", from = \"2018-01-01\", to = \"2023-01-01\")\n\n# Compute daily log returns\nstock_data &lt;- stock_data %&gt;%\n  mutate(log_return = log(adjusted / lag(adjusted)))\n\n# Annualized statistics\nnu &lt;- mean(stock_data$log_return, na.rm = TRUE) * 252\nsigma &lt;- sd(stock_data$log_return, na.rm = TRUE) * sqrt(252)\n\nnu\n\n[1] 0.2312935\n\nsigma\n\n[1] 0.3350538\n\n\n\n\n4.5.6.3 Step 2: Calibrate the Binomial Tree Parameters\n\nDefine the time step as daily (\\(\\Delta t = 1/252\\)).\nCompute the up factor ($ u \\(), **down factor** (\\) d \\(), and **probability** (\\)p$) using the formulas:\n\n\\(u = e^{\\sigma \\sqrt{\\Delta t}}\\)\n\\(d = e^{-\\sigma \\sqrt{\\Delta t}}\\)\n\\(p = \\frac{1}{2} + \\frac{\\nu}{2\\sigma} \\sqrt{\\Delta t}\\).\n\n\n\n\n4.5.6.4 Example Code:\n\ndelta_t &lt;- 1 / 252\nu &lt;- exp(sigma * sqrt(delta_t))\nd &lt;- exp(-sigma * sqrt(delta_t))\np &lt;- 0.5 + (nu / (2 * sigma)) * sqrt(delta_t)\n\nu\n\n[1] 1.021331\n\nd\n\n[1] 0.9791148\n\np\n\n[1] 0.521743\n\n\n\n\n4.5.6.5 Step 3: Build the Binomial Tree\n\nWrite a function to generate a binomial tree for a given stock.\nUse the calibrated parameters to simulate the stock price evolution over a given horizon (e.g., 1 year with 252 steps).\n\n\n\n4.5.6.6 Example Code:\n\nbinomial_tree &lt;- function(S0, u, d, p, steps) {\n  tree &lt;- matrix(0, nrow = steps + 1, ncol = steps + 1)\n  \n  for (i in 1:(steps + 1)) {\n    for (j in 1:i) {\n      tree[i, j] &lt;- S0 * u^(j - 1) * d^((i - 1) - (j - 1))\n    }\n  }\n  \n  tree\n}\n\n# Generate a binomial tree for 1 year\nS0 &lt;- 100  # Starting stock price\ntree &lt;- binomial_tree(S0, u, d, p, steps = 252)\n\n\n\n\n4.5.6.7 Step 4: Simulate and Visualize Final Price Distribution\n\nExtract the last row of the binomial tree to analyze final stock price outcomes.\nUse the binomial distribution to compute the probabilities for each outcome.\nPlot the probability mass function (PMF) and compare it to the simulated distribution.\n\n\n\n4.5.6.8 Example Code:\n\n# Extract final prices and probabilities\nfinal_prices &lt;- tree[dim(tree)[1], ]\nprobabilities &lt;- dbinom(0:252, size = 252, prob = p)\n\n# Plot PMF\nplot(final_prices, probabilities, type = \"h\", \n     main = \"Probability Mass Function\", \n     xlab = \"Final Stock Price\", ylab = \"Probability\")\n\n\n\n\n\n\n\n\n\n\n\n4.5.6.9 Step 5: Comparison and Analysis\n\nCompare the expected value of the final stock price from the model to the expected value based on the annualized return.\nDiscuss discrepancies and assumptions (e.g., independence of returns, constant volatility).\n\n\n\n\n4.5.7 Deliverables\n\nCode Implementation: A complete script that retrieves data, calibrates the model, builds the binomial tree, and visualizes the results.\nReport:\n\nExplanation of the binomial model and its calibration.\nInterpretation of the results (e.g., insights from the PMF, expected value, etc.).\nReflection on the limitations of the model and suggestions for improvement.\n\n\n\n\n\n4.5.8 Why This Project?\n\nPractical Relevance: Students work with real data, gaining insights into real-world financial modeling.\nIntegration of Concepts: Combines probability, binomial distributions, asset price dynamics, and R programming.\nExploration Opportunities: Students can tweak parameters, test different stocks, or extend the model (e.g., incorporating dividends or multi-asset portfolios).\n\nWould you like me to expand any section or provide additional example code?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html",
    "href": "05-lecture5.html",
    "title": "5  Continuous random variables",
    "section": "",
    "text": "5.1 Continuous random variables and probability\nWe will discuss here the most important concepts needed for the practical work with continuous random variables. We just note that for a mathematically rigorous treatment of this case we would need advanced techniques such as measure theory. We will not do that here (see for example Billingsley (1995)) because we want to focus on the more applied or practical aspects of continuous random variables.\nNote that random variables that can take on a continuum of values rather than only discrete values like the fair die, we discussed before, play an important role in practical situations. Just think of the possible values of asset prices or returns. The toy examples we studied with a model of random returns generated by a flipping coin and a wheel of fortune, were clearly artificial. The price of a stock, for example, should in principle be able to take any value in \\([0,\\infty)\\), or shouldn’t it? This is a modelling choice that could of course also be challenged on the grounds of realism. After all stock prices are quoted in some currency, which has a smallest unit, like a cent or a penny. We already discussed the assumption of admitting the possibility that a stock price could rise without bound in the very first lecture, when we discussed the concept of a sample space. Other practically important cases where a continuum of outcomes would be a very natural model is the length of finishing a task, or the lengths and the weight of an item.\nYou might object that these examples are also not really continuous. Take time. We measure it in hours, minutes or seconds. But we can measure it on a much finer grid and ultimately this process finishes at some smallest unit only because of our limitations to measure time at even smaller scales. Time is a continuous variable. It does not jump. In the case of stock prices the situation is different. We have a smallest monetary unit, in the Eurozone this would be cents. Still it turns out that in the case of prices practical modelling can be simpler if we think of prices as continuous.\nEven if we circumvent the mathematical machinery of measure theory to discuss continuous random variable it is still important to spend some time thinking of what a continuous random variable means.\nIn order to do this let us think of a continuous random variable which can take any value in the interval \\([0,1]\\). We can generate such numbers very easily in R using the function runif(). Let’s create 10 such numbers:\nNow you could ask: What is the probability that this random variable takes a particular value of, say \\(0.4848450\\), one value in our list? We could try simulation. Let’s simulate one million such uniformly distributed random numbers in \\({\\cal S}=[0,1]\\) and compute the relative frequency with which \\(0.4848450\\) occurs.\nIt is zero! Here this is literally zero. Even in \\(10^6\\) draws the random number generator of R drew 10 different numbers at each of the one million draws. The probability of hitting a particular one is 0. Indeed there is an infinity of numbers between \\(0\\) and \\(1\\) and you can not count them. Give me any number in this interval and there will be infinitely many numbers larger as well as smaller than this number. We cannot give positive probability to any particular one. If we did this and sum up the infinitely many numbers we would end up with a number larger than 1. But this is not allowed because we require probabilities to sum up to 1. The continuum does not allow us to give positive probabilities to any one number. No given number in the continuum can have a positive probability. This is key: For every continuous random variable \\(X\\) we have \\(P(X = x) = 0\\) for all \\(x\\). Thus in the case of a continuous random variable we can no longer talk about individual probabilities.\nNow let us think about a different question. What is the probability that the uniformly distributed random variable \\(X \\sim U[0,1]\\) comes up with a value between \\(0\\) and \\(1/4\\)? Lets check with our simulated numbers:\nIt says 25 %, as we expect from a uniformly distributed probabilities. Let’s try R’s cumulative distribution function, or \\(P(X \\leq 1/4)\\) for the uniform distribution on \\({\\cal S}=[0,1]\\):\nSo as soon as we have range, we can talk about positive probabilities. But this is the big change from discrete to continuous random variables. Whereas in the discrete case we had points and could talk about probabilities of points, in the continuous case we represent probabilities as areas under a curve. Take the example we have just discussed. aui density-of-uniform-distributio, out.width='90%', fig.align='center', fig.cap='With continuous random variables probabilities are areas under the density function', echo = F} knitr::include_graphics('figures/uniform_dist.png') Mathematically, areas under a curve are integrals. The representation of probabilities of a continuous random variable corresponds to what you learned in high-school about integration under a curve.\nThe probability density function is defined for the continuum as follows:\nSince we can talk only about areas under the curve for continuous random variables the really important function is the cumulative distribution function or CDF. Let us give the definition here:\nNote that the CDF can be used to compute probabilities because \\(P(a &lt; X &lt; b) = P(a &lt; X \\leq b) = P(a \\leq X \\leq b) = F(b) - F(a) = \\int_a^b f(x) dx\\). The following figure illustrates this idea: ```aui int-example, out.width=‘90%’, fig.align=‘center’, fig.cap=‘Computing probability of continuous random variable using CDF’, echo = F} knitr::include_graphics(‘figures/integral_example.png’)\nWe see that the mean is at about 0 and the standard deviation is about \\(1 \\%\\). Let us use this model to compute the probability that the daily returns stay within the bounds of plus or minus \\(1 \\%\\) from the mean. Theoretically this can be computed by \\(F(0.1) - F(-0.1)\\) where \\(F\\) is the CDF of the normal distribution. Now if you look at the formula for the normal distribution, this would amount to handle quite a daunting integration exercise. But here we can use the built in R-functions, with the syntax we already know from the binomial distribution.\nThis actually a general property of a normal distribution. For a normally distributed random variable \\(68 \\%\\) of the probability mass lies in the interval \\([\\mu - \\sigma, \\mu + \\sigma]\\). You can check that there are \\(95 \\%\\) in the interval \\([\\mu - 2 \\sigma, \\mu + 2 \\sigma]\\) and \\(99.7 \\%\\) in the interval \\([\\mu - 3 \\sigma, \\mu + 3 \\sigma]\\).\nNow let us repeat the computation for the standardized normal distribution for the standardized sp500 returns, defined as: \\[\\begin{equation*}\n\\frac{X - \\mu}{\\sigma}\n\\end{equation*}\\]\nExactly the same result. This means that this is a property of the normal distribution, which is independent in which unit we measure the random variable. The standardization has no influence on the distribution. It is a simple change of units.\nFinally, let us check how well the normal random variable catches the empirical return distribution we see in the data.\nNot quite. This is by the way typical for return distributions of daily stock returns. Empirical return distributions of daily returns are often slightly smaller near the mean and larger at the extreme values, as a normal distribution with the same mean and standard deviation. This is usually referred to as fat tails, a property which becomes quite important when we consider extreme risks. Let us contrast the empirical and the theoretical distribution with the same mean and standard deviation in the case of the sp500 daily returns.\nThe property of the empirical distribution being smaller near the mean is quite clearly visible in our data. The higher mass at the tails is not so clearly visible from this picture, but let us zoom in to the negative tail and look a bit from closer:\nIn the zoom you can see very clearly that there is much more mass in the tail than we would expect under a normal distribution.\nWhat does this mean? The first lesson you should draw from this is that their is no automatism from using models. While they are convenient and allow us to compute many things, the appropriate application context needs reflection, knowledge of context and decisions from the modeler. It requires autonomous and critical thinking going beyond the technicalities of models. For instance, if you rely on the normal distribution model for problems where most of the data are within one or two standard deviations from the mean, the normal random varible is perhaps good enough of a model in many application contexts. If you are, on the other hand, concerned with extreme risks, you are bound to underestimate their probability, if you have a normal distribution in mind. We will come to this problem again during this lecture.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#continuous-random-variables-and-probability",
    "href": "05-lecture5.html#continuous-random-variables-and-probability",
    "title": "5  Continuous random variables",
    "section": "",
    "text": "Continuous random variable\n\nA continuous random variable \\(X\\) can take on a continuum of possible values.\n\n\n\n\n\n\nrunif(10, 0, 1)\n\nuniform_rv &lt;- runif(10^6, 0,1)\n\nsum(uniform_rv == 0.4848450)/length(uniform_rv)\n\n\nsum(0 &lt;= uniform_rv & uniform_rv &lt;= 1/4)/length(uniform_rv)\n\npunif(1/4)\n\n\n\nProbability density function\n\nFor a continuous random variable, the probability density function, usually denoted by \\(f(x)\\), has the following properties:\n\n\n\n\\(f(x) \\geq 0\\)\n\\(\\int_{-\\infty}^{+\\infty} f(x) dx = 1\\)\nIf \\(X\\) is a continuous random variable with a density function \\(f\\), then for any \\(a &lt; b\\) , the probability that \\(X\\) falls in the interval \\((a,b)\\) is the area under the density function between \\(a\\) and \\(b\\): \\(P(a &lt; X &lt; b) = \\int_a^b f(x) dx\\)\n\n\n\nCumulative Distribution Function\n\nThe cumulative distribution function (abbreviated CDF) shows the probability that the random variable \\(X\\) takes a value less than or equal to a deterministic value \\(x\\). It shows the area under the probability density function from \\(-\\infty\\) to \\(x\\). The notation is \\(F(x) = P( X \\leq x)\\) where \\(- \\infty &lt; x &lt; \\infty\\).\n\n\n\nThe figure gives a graphical example. Suppose we have a probability density function on the\nsample space ${\\cal S} = [0, \\infty)$ which is drawn as the curve shown in the picture. We would\nlike to compute the probability that the random variable takes a value larger or equal to \nsome value $a$, shown in the picture. Graphically this corresponds to the red area under the curve.\nFor the computation we work with the complementary event. The total area under the density\ncurve must be 1 according to the laws of probability. Thus if we compute $1$ minus the \nprobability that the value of $X$ is lower than $a$ we get the result. Now $P(X \\leq a)$ is\nthe integral from $0$ to $a$ under the density curve $f(x)$, which is by definition the value of\nthe CDF at $a$ or $F(a)$. \n\nWe will do a couple of such computations during the course of our lecture. We are now\nready to discuss the most important continuous distribution, which plays a key role in\nprobability and all of its applications: The **normal distribution**.\n\n## Normal Distribution\n\nNormal Distribution\n: The **normal distribution** is a continuous probability distribution that is \ncentered around the mean, bell-shaped, symmetric and completely determined by\ntwo parameters: The mean $\\mu$ and the variance $\\sigma^2$. The notation\nis $X \\sim N(\\mu, \\sigma^2)$. The probability density\nfunction of the normal distribution with mean $\\mu$ and variance $\\sigma^2$ \nis given by\n\\begin{equation*}\nf(x,\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(\\frac{-(x - \\mu)^2}{2 \\sigma^2}\\right)\n\\end{equation*}\nThe normal distribution is also often referred to as a **Gaussian** or **Gauss-Laplace** \ndistribution named after the German mathematician Karl Friedrich Gauss (1777-1855) and the\nfrench Astronomer, Physiscist and Mathematician Pierre Simon de Laplace (1749-1827). Another\nterm also often used for the probability density function and its particular shape is\nthe **bell curve**.\n\nA normally distributed random variable $X$ can be standardized to a normally distributed\nrandom variable with mean $0$ and variance $1$. This is called the\n\nStandard Normal Distribution\n: The **standard normal distribution** is a normal distribution with mean $\\mu = 0$ and\nvariance $\\sigma^2 = 1$. Any normally distributed random variable $X$ with mean\n$\\mu$ and variance $\\sigma^2$ can be rewritten as a standard normal random variable\n$Z$ in the following way:\n$Z = \\frac{X - \\mu}{\\sigma}$. By definition $Z \\sim N(0,1)$\n\nLet's look at an example and go back to our stock data of the S&P 500. Let us compute it's\ndaily returns ans *assume* that these returns are random returns from a normal distribution. We\nhave already computed the returns in a previous lecture and added them to our data frame. \n\nThe normal distribution has two parameters, the mean $\\mu$ and the variance $\\sigma$. We compute \nthese values for the daily returns in our data.\n```{r} return-distribution-sp500}\nmean_ret_sp500 &lt;- mean(sp500$returns, na.rm = T)\nsigma_ret_sp500 &lt;- sqrt(var(sp500$returns, na.rm = T))\n\nmean_ret_sp500\nsigma_ret_sp500\n\npnorm(mean_ret_sp500 + sigma_ret_sp500, mean = mean_ret_sp500, sigma_ret_sp500) -\npnorm(mean_ret_sp500 - sigma_ret_sp500, mean = mean_ret_sp500, sigma_ret_sp500)\n\n\npnorm(1, mean = 0, sd = 1) -\npnorm(-1, mean = 0, sd = 1)\n\n\nsum(sp500$returns &gt;= (mean_ret_sp500 - sigma_ret_sp500) & \n    sp500$returns &lt;= (mean_ret_sp500 + sigma_ret_sp500), na.rm = T)/\n    length(sp500$returns)\n\n\ny &lt;- seq(min(sp500$returns, na.rm = T), max(sp500$returns, na.rm = T), length.out = 50)\n\nhist(sp500$returns, freq = F, breaks = y, \n     main = \"Histogram of S&P500 daily returns with normal curve\")\n\nfun &lt;- dnorm(y, mean = mean_ret_sp500, sd = sigma_ret_sp500) \nlines(y,fun,col=2,lwd=2)\n\nsp500_returns_zoom &lt;- sp500$returns[sp500$returns &gt;= - 0.06 & sp500$returns &lt;= -0.02]\n\nz &lt;- seq(-0.06, -0.02, length.out = 20)\n\nhist(sp500_returns_zoom, freq = F, breaks = y, \n     main = \"Zoom of S&P500 daily returns with normal curve to lower tail\")\n\nfun &lt;- dnorm(z, mean = mean_ret_sp500, sd = sigma_ret_sp500) \nlines(z,fun,col=2,lwd=2)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#inverse-normal-and-quantiles",
    "href": "05-lecture5.html#inverse-normal-and-quantiles",
    "title": "5  Continuous random variables",
    "section": "5.2 Inverse Normal and Quantiles",
    "text": "5.2 Inverse Normal and Quantiles\nIn risk management we often ask the following question: For a given time horizon we would like to know the threshold loss value, such that the probability that the portfolio loss exceeds this value over the given time horizon is \\(p\\).\nNote that this is a different question than we asked before. Before we had a given threshold and we were interested in the probability to get a value below the threshold.\nNow we have an inverse problem. We have a given probability \\(p\\) and we want to know the threshold such that this threshold is exceeded with this given probability. A typical context for such problems in finance is risk management. Assume I manage a portfolio with uncertain (random) returns. I should be interested in asking myself: Is the probability that my portfolio is going to make a loss larger than my equity can cover, small enough to consider my exposure prudent? As a risk manager you would like to make this probability of exceeding such a wipe out threshold very small.\nLet us formulate these ideas in the form of a few definitions:\n\nInverse Normal Distribution\n\nWith the inverse normal distribution, it is possible to calculate the value \\(x\\) such that \\(P(X \\leq x)\\) is equal to probability \\(p\\) for a given normally distributed random variable.\n\n\nThe functions that provide the necessary computation in R has the syntax qnorm() for the normal distribution. It is similar for other models, like in the binomial model from the last lecture, where the syntax would be qbinom().\nqnorm(0.01, mean = mean_ret_sp500, sd = sigma_ret_sp500)\nThis function solves for the problem. What is the threshold \\(x\\) such that \\[\\begin{equation*}\nP(r \\leq x) = 0.01\n\\end{equation*}\\] Under the assumption that \\(R\\) is a normal random variable with mean mean_ret_sp500 and standard deviation sigma_ret_sp500, the function qnorm solves this problem for \\(x\\), giving you the threshold. It inverts the CDF and therefore it is often called an inverse problem.\n\nQuantile\n\nThe \\(pth\\) quantile (or percentile) of a probability distribution is the value \\(x\\) such that \\(P(X \\leq x) = p\\). If \\(X\\) is normally distributed, \\(X \\sim N(\\mu, \\sigma^2)\\) then the \\(x\\) value such that \\(P(X \\leq x) = p\\) is given by the inverse CDF of the normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\) for given \\(p\\).\n\n\nComing back to our risk management problem, we can now see that we are looking for a certain quantile of the portfolio loss distribution. This quantile is called the value at risk.\n\nValue-at-Risk\n\nGiven a confidence level \\(\\alpha \\in (0,1)\\), Value-at-Risk (VaR) is defined as \\(VaR_\\alpha = F_L(\\alpha)^{-1} = \\inf\\{x \\in \\mathbb{R} | F_L(x) \\geq \\alpha \\}\\) With a normal loss distribution \\(F_L\\) we can compute the value at risk in an easy way.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#example-value-at-risk-and-the-problem-of-fat-tails",
    "href": "05-lecture5.html#example-value-at-risk-and-the-problem-of-fat-tails",
    "title": "5  Continuous random variables",
    "section": "5.3 Example Value at Risk and the problem of fat tails",
    "text": "5.3 Example Value at Risk and the problem of fat tails\nAssume you are in charge of a stock portfolio worth 100 million Euro and fully invested in the S&P 500. You make portfolio adjustments on a weekly basis and you would like to know whether your value at risk is such that you would be able to cover a severe portfolio loss, given your equity of 10 Million.\nLet us use the same approach, which we used in the last lecture to extract the weekly prices from the sp500 daily data and compute the weeky log returns.\n# add a column with the year using year function from lubridate package\nsp500$week &lt;- lubridate::week(sp500$date)\n# split dataframe according to year\nsplit_by_week &lt;- split(sp500, sp500$week)\n# write a function selecting the first row of a given data frame\nselw &lt;- function(x){x[1,]}\n# apply this function to each element of the list.\nauxw &lt;- lapply(split_by_week, selw)\n# recombine the list entries into a data frame but drop the returns column\nsp500_weekly &lt;- do.call(rbind, auxw) |&gt; subset(select = -returns)\n\n# compute weekly log-returns and add to data frame\n\nsp500_weekly$log_returns &lt;- c(NA, diff(log(sp500_weekly$adjusted), lag = 1))\n\nSuppose you decided to consider as a model for this risk situation a normally distributed random variable, \\(X \\sim N(\\mu,\\sigma)\\) which is supposed to represent the random return of your sp500 portfolio over the coming week. You want to estimate your \\(95 %\\) value at risk as well as you \\(99 \\%\\) value at risk from this model.\nIn order to do so we must first estimate our mean (log) returns and their standard deviation from the data.\nmuw &lt;- mean(sp500_weekly$log_returns, na.rm = T)\nsdw &lt;- sqrt(var(sp500_weekly$log_returns, na.rm = T))\n\nmuw\nsdw\nUsing the functionality of R this becomes a one-liner, or actually two one liners because you are interested in the \\(95 \\%\\) var as well as in the \\(99 \\%\\) var.\nqnorm(0.05, mean = muw, sd = sdw)\nqnorm(0.01, mean = muw, sd = sdw)\nThe weekly \\(95 \\%\\) value at risk is thus \\(6\\,465\\,511\\) and the \\(99 \\%\\) Value at risk is \\(9\\, 222\\, 811\\) Euro. In both cases your equity will protect you from a wipe out. In the first case with a bit of a margin, if not very much, in the second case the margin is so thin that you might be worried still.\nIf you think about the problem a bit more and if you remember the pictures of empirical returns versus theoretical returns with parameters estimated from empirical, you see that these computations will most likely be over optimistic, since the actual return distribution will have more mass at the tails of the distribution. This naive approach to compute the value at risk will thus most likely underestimate the actual risk.\nSo why don’t we look at the actual data to get a valdiation. With the small and one dimensional data set this can be done quite easily.\nWe first have to sort the weekly log returns in ascending order:\nsorted_returns &lt;- sort(sp500_weekly$log_returns)\nThe empirical cumulative distribution function at those observations takes values\necdf &lt;- 1:length(sorted_returns)/length(sorted_returns)\nNow let us sketch the ECDF for the 95 % Value at risk:\nplot(sorted_returns, ecdf, type = \"s\")\nabline(h = 0.05, lty = 3)\nAnd here for the 99 % value at risk\nplot(sorted_returns, ecdf, type = \"s\")\nabline(h = 0.01, lty = 3)\nFor the 95 % quantile we look sort of ok. There is an intersection with the CDF at a log return of about \\(-0.06\\). For the 99 % quantile we can’t tell from the graph, we might as will be off by quite a far margin.\nTo check this, let us look at the first value of sorted_returns such that \\(P(\\text{sorted_returns}) &gt; 0.05\\) or \\(P(\\text{sorted_returns}) &gt; 0.01\\)\nsorted_returns[which(ecdf &gt;= 0.05)[1]]\nsorted_returns[which(ecdf &gt;= 0.01)[1]]\nSo actually for the 99 % our model has underestimated the Var substantially by about 5 Million. With a worst case loss of 14 Million our equity would have been wiped out.\nNow we have shown you one way how you can use R to reflect on how appropriate a model might be for your problem at hand, here of assessing a Value at Risk. You have seen that R provides powerful tool that allow you to play with data and make many complex thought experiments. But there is no foolproof recipe. No theory in the world can give you relief from using your brain, your creativity and the energy of your whole person to properly understand your situation and make a serious attempt to judge at each step whether you are on the right track when using a model. Always bear in mind the assumptions you are making and when and where things can or might go wrong. Only in the hands of a human being with such a critical mindset the mathematical theory and the computer can unfurl its power.\nSometimes a checklist of tools can help. For instance, a useful graphic tool to identify fat tails are so called Q-Q-plots. What is this?\n\nQ-Q-Plot\n\nIn statistics, a Q-Q plot (abbreviation for quantile-quantile plot) is a probability plot and is a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions are similar, then all points of the QQ-plot will lie roughly on the 45 degree line.\n\n\nThe QQ-plot does basically in a different way what we tried to figure out by using the empirical CDF. Let us do this for our return data.\nLets take the weekly log return data sp500_weekly$log_returns as our example data:\n# how many data points are in sp500_weekly$log_returns removing NA\nnobs &lt;- length(na.omit(sp500_weekly$log_returns))\n# draw nobs normally distributed random variables with muw and sdw\nx &lt;- rnorm(nobs, mean = muw, sd = sdw)\ny &lt;- na.omit(sp500_weekly$log_returns)\n# normal QQ-plot\nqqplot(x, y, xlab = \"Normal Distribution\", ylab = \"Weekly log returns\", main = \"Q-Q Plot\")\nYou can get some evidence here that the empirical distribution agrees quite well at the center but it seems to do less so at the tails, where the empirical distribution deviates, something we have already observed before.\nWe can also have alook at the QQ-plot of our daily returns data where we have more observations.\n# how many data points are in sp500_weekly$log_returns removing NA\nnobsd &lt;- length(na.omit(sp500$returns))\n# draw nobs normally distributed random variables with muw and sdw\nxd &lt;- rnorm(nobsd, mean = mean(sp500$returns, na.rm = T), sd = sqrt(var(sp500$returns, na.rm = T)))\nyd &lt;- na.omit(sp500$returns)\n# normal QQ-plot\nqqplot(xd, yd, xlab = \"Normal Distribution\", ylab = \"Daily returns\", main = \"Q-Q Plot\")\nHere you see the same thing. The normal distribution seems to be a bad model for looking at the tails and thus a poor guide for assessing extreme risks.\nA very famous financial disaster the failure of the American based Hedge Fund Long-Term Capital Management L.P. or short LTCM. This hedge fund collapsed in the early 1990ies by making the wrong bet on the outcome of a trade. The disaster was caused by ignoring large low-probability events, because these large losses were considered too unlikely, by relying on the models in use at the time at LTCM. The unlikely event that brought LTCM under water with billions of losses was a financial crisis in Russia at the time which brought large moves in interest rates. The case got notorious, because the size of the losses were so large that the Fed had to coordinate the investors in LTCM in a loss and restructuring negation to prevent a further meltdown of the wider Financial system. The press commented on the case with some barely hidden glee because one of the main partners in LTCM were the nobel price winning Financial theorist Robert Merton, a world famous figure in Finance and financial modelling. So without being malicious, we could at least conclude from this case that uncertainty and risk, and probability models to think about these phenomena can even fail and fool smart people. A good and very readable discussion of the LTCM case can be found in Dunbar (2000).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#more-than-one-normal-random-variable",
    "href": "05-lecture5.html#more-than-one-normal-random-variable",
    "title": "5  Continuous random variables",
    "section": "5.4 More than one normal random variable",
    "text": "5.4 More than one normal random variable\nWe have already discussed a situation where we have more than one random variable simultaneously, the concept of covariance and correlation as well as the properties of expecation and variance when we combine several random variables and consider their sums or functions of them. In the case of the normal distribution we have an additional stronger property, which you should know about.\n\nSummation property of the normal distribution\n\nIf \\(X\\) and \\(Y\\) are jointly normally distributed random variables, then all random variables of the form \\(a\\, X + b\\, Y\\), where \\(a\\) and \\(b\\) are real numbers, are also normally distributed. This holds also for sums of more than two normally distributed random variables.\n\n\nIf we have a vector \\(x\\) of \\(n\\) jointly normally distributed random variables, their joint probability density is given by \\[\\begin{equation*}\np(x) = \\frac{1}{(2 \\, \\pi)^{\\frac{n}{2}} |C|^{\\frac{1}{2}}}\\exp(-\\frac{1}{2}((x - E(x))^T C^{-1}(x - E(x)))\n\\end{equation*}\\] In this formula \\(C\\) denoted the variance-covariance function and \\(| |\\) denotes the determinant. Don’t worry if you do not exactly know what this is or if you find this formula daunting. We will not use it in this form but, if at all, rather through the implemented R functions. But the formula is so important and famous that you should have seen it at least once in your life.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables</span>"
    ]
  },
  {
    "objectID": "05-lecture5.html#project-portfolio-credit-risk-with-dependent-defaults",
    "href": "05-lecture5.html#project-portfolio-credit-risk-with-dependent-defaults",
    "title": "5  Continuous random variables",
    "section": "5.5 Project: Portfolio credit risk with dependent defaults",
    "text": "5.5 Project: Portfolio credit risk with dependent defaults\nThis last project will tie together most probability and R concepts we have learned in our course. The context that will help us to do this is again credit risk and we will build on the project from lecture 4 on modelling credit risk.\nA model which is able to represent default correlation in a simple way is due to Vasicek 1997 and works as follows:\n\nObligor value model\n\nThe simplified obligor value model makes the following basic assumptions:\n\n\n\nEach obligor in the loan portfolio is characterized by the monetary value of assets. The value of assets of obligor \\(n\\) at time \\(t\\) is \\(V_n(t)\\).\nIt is assumed that the asset value at the time horizon \\(T\\) has a normal distribution and the asset values are standardized, so t_hat \\(V_n(0)=0\\) and \\(V_n(T) \\sim N(0,1)\\).\nObligor \\(n\\) defaults, if its firm value at the time horizon \\(T\\), \\(V(T)\\) falls below a prespecified barrier \\(K_n\\), such that \\(V_n(T) \\leq K_n\\). The asset values of different obligors are correlated with each other. The variance covariance matrix of \\(V_1(T), V_2(T), \\cdots, V_n(T)\\) is denoted by \\(C\\).\nYou and your staff have jointly developed a model for the asset values of the different obligors. In this model the values of the assets of the obligors are driven by a common factor \\(Y\\) which is a standard normally distributed random variable and an idiosyncratic factor \\(\\epsilon_n\\) which is for all obligors \\(n\\) also a standard normal random variable. Thus the total value follows \\[\\begin{equation*}\nV_n(T) = \\sqrt{\\rho}\\, Y + \\sqrt{1-\\rho}\\epsilon_n \\quad \\text{for all}\\,\\,\\, n \\leq N\n\\end{equation*}\\] The parameter \\(\\rho\\) describes the correlation coefficient between the value of assets of any two obligors \\(n \\neq m\\) and its value is also provided to you be your staff. Now you make an additional important assumption which will allow you to leverage all your hard work from the previous project: Conditional on the realization of the systematic factor \\(Y\\), firm values and firm defaults are independent.\n\nYour staff has produced data analysis on the covariance matrix of obligors asset values and provides you with \\(C\\). Furthermore it provides you with the default probabilities of each obligor \\(n\\), given by \\(p_n\\). Now you are going to analyze, contrary to the simplified model from before, a more realistic situation where defaults are no longer independent by using the model assumptions outlined above.\n\nYour first task is that you take the \\(p_n\\) provided to you by your staff and calibrate the default barriers \\(K_n\\) from these data.\nConditional on \\(Y = y\\), what is the probability of having \\(n\\) defaults in terms of the formula for the probability distribution of a binomial random varible?\nUsing the assumptions on the model for the firm value, can you figure out which expression you need to plug into qnorm() to get the probability of \\(p_n(y) = P(V_n(T) &lt; K_n | Y = y)\\)?\nPlease combine the answers to 2 and 3 to simulate and plot the loss distribution assuming for your given \\(\\rho\\).\nPlot distribution under higher and lower correlation as well as under \\(\\rho = 0\\).\nWhat is the \\(99\\%\\) Value at risk, given \\(\\rho\\) provided by your staff?\n\n\n\n\n\nBillingsley, Patrick. 1995. Probability and Measure. Wiley.\n\n\nDunbar, Nicolas. 2000. Inventing Money: The Story of Long-Term Capital Management and the Legends Behind It. Wiley.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous random variables</span>"
    ]
  },
  {
    "objectID": "06-references.html",
    "href": "06-references.html",
    "title": "6  References",
    "section": "",
    "text": "Billingsley, Patrick. 1995. Probability and Measure. Wiley.\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about\nChance. Princeton University Press.\n\n\nDunbar, Nicolas. 2000. Inventing Money: The Story of Long-Term\nCapital Management and the Legends Behind It. Wiley.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its\nApplications. 3rd ed. Vol. 1. Wiley.\n\n\nGilboa, Ithak. 2009. Theory of Decision Under Uncertainty.\nCambridge University Press.\n\n\nGlasserman, Paul. 2003. Monte Carlo Methods in Financial\nEngineering. Springer.\n\n\nGrolemund, Garrett. 2014. Hands on Programming with\nR. O’Reilly. https://rstudio-education.github.io/hopr/.\n\n\nHalpern, Joseph. 2017. Reasoning about Uncertainty. 2nd ed. MIT\nPress.\n\n\nHealy, Kirean. 2019. Data Visualization: A Practical\nIntroduction. Princeton University Press.\n\n\nLiu, Jun S. 2001. Monte Carlo Strategies in Scientific\nComputing. Springer.\n\n\nLo, Andrew, and Craig MacKinlay. 2019. A Non-Random Walk down\nWallstreet. Princeton University Press.\n\n\nLuenberger, David. 2009. Investment Science. Oxford University\nPress.\n\n\nMcNeil, Alexander, Paul Embrechts, and Rudiger Frey. 2015.\nQuantitative Risk Management. Princeton University Press.\n\n\nSamuelson, Paul. 1965. “Proof That Properly Anticipated Prices\nFluctuate Randomly.” Industrial Management Review 6.\n\n\nTaleb, Nassim Nicolas. 2007. Fooled by Randomenss. Penguin.\n\n\nTooze, Adam. 2018. Crashed. How a Decade of Financial Crisis Changed\nthe World. Viking.\n\n\nWickham, Hadley. 2019. Advanced r. 2nd ed. Taylor; Francis.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data\nScience. O’Reilly. https://r4ds.had.co.nz/.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "07-appendix.html",
    "href": "07-appendix.html",
    "title": "7  Appendix",
    "section": "",
    "text": "7.1 Further Reading\nThis is the end of this lecture and these lecture notes which have made an attempt to teach you introductory probability by building probability concepts with R in a finance application context. The field of probability and its applications is huge an rich and only a tiny part of it could be covered in this course. The hope is, of course, that the lecture has been able to teach you some tools and concepts which will enable those of you who found this subject interesting to explore it on your own, using your own interests and the literature on probability as a guide. For those of you who want to do so, let me give you a few pointers to the literature, which might cover for some of you the next steps.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "07-appendix.html#further-reading",
    "href": "07-appendix.html#further-reading",
    "title": "7  Appendix",
    "section": "",
    "text": "7.1.1 Probability theory and mathematics\nI have confessed already in the beginning of this lecture that I think that among the many probability books on the market the book by Feller (1968) is still an outstanding master piece in exposition. It is full of passion and enthusiasm for the subject and interesting throughout. Its orientation is clearly mathematical but the confinement to discrete sample spaces allows him to cover much ground with minimal machinery. Reading or studying this books is perhaps even more fun today since you can complement by using the computer to study and think about the many interesting examples contained in it, something you could not do when the book first appeared.\nAmong the big ideas in probability was the attempt to build a general, axiomatic framework for the field that would establish probability theory firmly as a field of mathematics. The modern mathematical formulation of probability is perhaps nowhere better presented than in Billingsley (1995). It is a very good source for those of you who have or had some more advanced training in mathematics.\n\n\n7.1.2 Further studies of R\nIf you are a beginner of R and if you found the language and what it can do useful and interesting I recommend to study the book by Grolemund (2014). It is a great read and I have relied very much on it for this course. From this book I tried to emulate the idea that R is best taught withing a concrete context instead of teaching the language as such perhaps with a few toy examples and then apply the machinery afterwards. The book is also available online and I recommend the beginners among you very warmly to study it. You can get the book on the internet at https://rstudio-education.github.io/hopr/\nMore advanced students in the group who want to learn and understand R seriously as a programming language, should at some stage study Wickham (2019). This book is also freely available in an online edition at https://adv-r.hadley.nz/.\nFinally let me point out to you an excellent guide to the huge number of useful books many of them freely available at the site https://www.bigbookofr.com/ which is a regularly updated list of books about R and applications of R.\n\n\n7.1.3 Probability, Philosophy and Concepts\nFor those of you who have an interest in conceptual and foundational discussions and reflections, I would like to point you to four sources in particular. First of all, I think you should at some stage study the outstanding discussion by Diaconis and Skyrms (2019). I made several references to it in these lecture notes. It is a truly good read.\nFor those of you who have the training and the interest in more formal discussions, I would like to recommend the monographs of Gilboa (2009) and Halpern (2017)\nAn outstanding and very famous book, which is more of a meditation about randomness also with respect to financial markets is Taleb (2007). If you had to choose one and only one book in this section, it is perhaps this one.\n\n\n7.1.4 Monte Carlo Simulation and Computing\nWe discussed a lot about simulation in this lecture. If you want to learn about Monte Carlo simulation seriously, you should at some stage study Liu (2001). A standard reference for Monte Carlo Simulation in Finance and Financial Engineering is Glasserman (2003)\n\n\n7.1.5 Finance and Risk Management\nThere are many books on Finance, some very technical other verbose and business like. I believe that among the Finance books I know the book by Luenberger (2009) is truly outstanding. A comprehensive overview on the methods and mathematics of risk management in a financial context is McNeil, Embrechts, and Frey (2015)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBillingsley, Patrick. 1995. Probability and Measure. Wiley.\n\n\nDiaconis, Persi, and Brian Skyrms. 2019. 10 Great Ideas about Chance. Princeton University Press.\n\n\nFeller, William. 1968. An Introduction to Probability Theory and Its Applications. 3rd ed. Vol. 1. Wiley.\n\n\nGilboa, Ithak. 2009. Theory of Decision Under Uncertainty. Cambridge University Press.\n\n\nGlasserman, Paul. 2003. Monte Carlo Methods in Financial Engineering. Springer.\n\n\nGrolemund, Garrett. 2014. Hands on Programming with R. O’Reilly. https://rstudio-education.github.io/hopr/.\n\n\nHalpern, Joseph. 2017. Reasoning about Uncertainty. 2nd ed. MIT Press.\n\n\nLiu, Jun S. 2001. Monte Carlo Strategies in Scientific Computing. Springer.\n\n\nLuenberger, David. 2009. Investment Science. Oxford University Press.\n\n\nMcNeil, Alexander, Paul Embrechts, and Rudiger Frey. 2015. Quantitative Risk Management. Princeton University Press.\n\n\nTaleb, Nassim Nicolas. 2007. Fooled by Randomenss. Penguin.\n\n\nWickham, Hadley. 2019. Advanced r. 2nd ed. Taylor; Francis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "01-lecture1.html#footnotes",
    "href": "01-lecture1.html#footnotes",
    "title": "1  First probability ideas and first steps in R",
    "section": "",
    "text": "In this discussion I use chapter 1 in Diaconis and Skyrms (2019)↩︎\nAs of R version 4.2.0 (released in April 2022), the behavior regarding the recycling rule has changed. In older versions of R, a warning was issued when the length of one vector was not a multiple of the length of the other vector during arithmetic operations. The reasoning behind this change was to reduce unnecessary noise in the output. Many users found the warnings redundant in cases where the recycling rule was intentional and understood. However, this change means you need to be more cautious, as unexpected behavior might go unnoticed without the warning.↩︎\nA very good starting point is for instance Healy (2019), in case you are interested. Also a good source is the first chapter in Wickham and Grolemund (2017).↩︎\nWe follow here the discussion in Diaconis and Skyrms (2019)↩︎\nMathematically you can describe the sample space for the birthday problem as a set with \\(365^n\\) ordered \\(n\\)-tuples or sequences of birthdays: \\({\\cal S} = \\{x | x \\in 365^n \\}\\) This is the sample space of this experiment, written as in set theoretic notation. It means the sample space is the set of all ordered \\(n-tuples\\) from the Cartesian product \\(365^n\\). Remember that in mathematics, a Cartesian product is a mathematical operation that returns a set (or product set or simply product) from multiple sets (in this case the sample space, \\({\\cal S}\\). That is, for sets \\(A\\) and \\(B\\), the Cartesian product \\(A \\times B\\) is the set of all ordered pairs \\((a,b)\\) where \\(a \\in A\\) and \\(b \\in B\\).↩︎\nI used ggplot2 before to explain the concept of a package and how to load it to enhance R’s functionality. Base R containts a lot of powerful plotting functions which work independently of gglot2.↩︎\nbirthday_collisions Function: This function calculates the probability of at least one shared birthday for a group of nn people using the formula derived earlier. group_sizes and collision_probs: group_sizes defines a range of group sizes (1 to 50). sapply applies the birthday_collisions function to each group size, computing the collision probabilities. Visualization: A line plot (type = \"l\") is used to visualize how the collision probability increases with group size. A horizontal red dashed line (abline) highlights the 50% threshold for better visual context.↩︎\nA bit, short for binary digit, is defined as the most basic unit of data in telecommunications and computing. Each bit is represented by either a 1 or a 0↩︎\nFor more information and details you might want to check this source: https://www.hostmerchantservices.com/articles/what-are-transaction-ids/?utm_source=chatgpt.com↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First probability ideas and first steps in R</span>"
    ]
  }
]